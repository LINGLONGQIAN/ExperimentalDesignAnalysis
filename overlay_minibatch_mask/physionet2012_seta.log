2024-05-23 17:30:48 [INFO]: Have set the random seed as 2023 for numpy and pytorch.
2024-05-23 17:30:49 [INFO]: Using the given device: cuda:0
2024-05-23 17:30:49 [INFO]: Model files will be saved to overlay_minibatchmasking_saved_results/round_0/SAITS_physionet_2012_seta/20240523_T173049
2024-05-23 17:30:49 [INFO]: Tensorboard file will be saved to overlay_minibatchmasking_saved_results/round_0/SAITS_physionet_2012_seta/20240523_T173049/tensorboard
2024-05-23 17:30:49 [INFO]: SAITS initialized with the given hyperparameters, the number of trainable parameters: 5,322,806
2024-05-23 17:30:51 [INFO]: Epoch 001 - training loss: 1.0743, validation loss: 0.4896
2024-05-23 17:30:52 [INFO]: Epoch 002 - training loss: 0.7874, validation loss: 0.4185
2024-05-23 17:30:53 [INFO]: Epoch 003 - training loss: 0.6070, validation loss: 0.3880
2024-05-23 17:30:55 [INFO]: Epoch 004 - training loss: 0.5305, validation loss: 0.3764
2024-05-23 17:30:56 [INFO]: Epoch 005 - training loss: 0.4848, validation loss: 0.3583
2024-05-23 17:30:57 [INFO]: Epoch 006 - training loss: 0.4510, validation loss: 0.3495
2024-05-23 17:30:59 [INFO]: Epoch 007 - training loss: 0.4328, validation loss: 0.3359
2024-05-23 17:31:00 [INFO]: Epoch 008 - training loss: 0.4154, validation loss: 0.3248
2024-05-23 17:31:01 [INFO]: Epoch 009 - training loss: 0.4042, validation loss: 0.3166
2024-05-23 17:31:02 [INFO]: Epoch 010 - training loss: 0.3834, validation loss: 0.3056
2024-05-23 17:31:04 [INFO]: Epoch 011 - training loss: 0.3747, validation loss: 0.2988
2024-05-23 17:31:05 [INFO]: Epoch 012 - training loss: 0.3606, validation loss: 0.2938
2024-05-23 17:31:06 [INFO]: Epoch 013 - training loss: 0.3516, validation loss: 0.2860
2024-05-23 17:31:07 [INFO]: Epoch 014 - training loss: 0.3415, validation loss: 0.2810
2024-05-23 17:31:09 [INFO]: Epoch 015 - training loss: 0.3356, validation loss: 0.2752
2024-05-23 17:31:10 [INFO]: Epoch 016 - training loss: 0.3310, validation loss: 0.2772
2024-05-23 17:31:11 [INFO]: Epoch 017 - training loss: 0.3249, validation loss: 0.2687
2024-05-23 17:31:12 [INFO]: Epoch 018 - training loss: 0.3222, validation loss: 0.2651
2024-05-23 17:31:14 [INFO]: Epoch 019 - training loss: 0.3190, validation loss: 0.2616
2024-05-23 17:31:15 [INFO]: Epoch 020 - training loss: 0.3092, validation loss: 0.2593
2024-05-23 17:31:16 [INFO]: Epoch 021 - training loss: 0.3064, validation loss: 0.2634
2024-05-23 17:31:18 [INFO]: Epoch 022 - training loss: 0.3078, validation loss: 0.2573
2024-05-23 17:31:19 [INFO]: Epoch 023 - training loss: 0.3001, validation loss: 0.2629
2024-05-23 17:31:20 [INFO]: Epoch 024 - training loss: 0.3017, validation loss: 0.2517
2024-05-23 17:31:21 [INFO]: Epoch 025 - training loss: 0.2994, validation loss: 0.2541
2024-05-23 17:31:22 [INFO]: Epoch 026 - training loss: 0.2940, validation loss: 0.2527
2024-05-23 17:31:24 [INFO]: Epoch 027 - training loss: 0.2902, validation loss: 0.2502
2024-05-23 17:31:25 [INFO]: Epoch 028 - training loss: 0.2918, validation loss: 0.2480
2024-05-23 17:31:26 [INFO]: Epoch 029 - training loss: 0.2871, validation loss: 0.2442
2024-05-23 17:31:27 [INFO]: Epoch 030 - training loss: 0.2860, validation loss: 0.2423
2024-05-23 17:31:29 [INFO]: Epoch 031 - training loss: 0.2863, validation loss: 0.2419
2024-05-23 17:31:30 [INFO]: Epoch 032 - training loss: 0.2873, validation loss: 0.2423
2024-05-23 17:31:31 [INFO]: Epoch 033 - training loss: 0.2786, validation loss: 0.2414
2024-05-23 17:31:32 [INFO]: Epoch 034 - training loss: 0.2806, validation loss: 0.2404
2024-05-23 17:31:33 [INFO]: Epoch 035 - training loss: 0.2804, validation loss: 0.2395
2024-05-23 17:31:35 [INFO]: Epoch 036 - training loss: 0.2786, validation loss: 0.2339
2024-05-23 17:31:36 [INFO]: Epoch 037 - training loss: 0.2776, validation loss: 0.2376
2024-05-23 17:31:37 [INFO]: Epoch 038 - training loss: 0.2787, validation loss: 0.2419
2024-05-23 17:31:38 [INFO]: Epoch 039 - training loss: 0.2737, validation loss: 0.2316
2024-05-23 17:31:40 [INFO]: Epoch 040 - training loss: 0.2718, validation loss: 0.2428
2024-05-23 17:31:41 [INFO]: Epoch 041 - training loss: 0.2702, validation loss: 0.2408
2024-05-23 17:31:42 [INFO]: Epoch 042 - training loss: 0.2723, validation loss: 0.2296
2024-05-23 17:31:43 [INFO]: Epoch 043 - training loss: 0.2695, validation loss: 0.2251
2024-05-23 17:31:45 [INFO]: Epoch 044 - training loss: 0.2664, validation loss: 0.2279
2024-05-23 17:31:46 [INFO]: Epoch 045 - training loss: 0.2677, validation loss: 0.2316
2024-05-23 17:31:47 [INFO]: Epoch 046 - training loss: 0.2671, validation loss: 0.2304
2024-05-23 17:31:48 [INFO]: Epoch 047 - training loss: 0.2663, validation loss: 0.2254
2024-05-23 17:31:49 [INFO]: Epoch 048 - training loss: 0.2661, validation loss: 0.2298
2024-05-23 17:31:51 [INFO]: Epoch 049 - training loss: 0.2639, validation loss: 0.2321
2024-05-23 17:31:52 [INFO]: Epoch 050 - training loss: 0.2655, validation loss: 0.2255
2024-05-23 17:31:53 [INFO]: Epoch 051 - training loss: 0.2614, validation loss: 0.2309
2024-05-23 17:31:54 [INFO]: Epoch 052 - training loss: 0.2625, validation loss: 0.2196
2024-05-23 17:31:56 [INFO]: Epoch 053 - training loss: 0.2643, validation loss: 0.2277
2024-05-23 17:31:57 [INFO]: Epoch 054 - training loss: 0.2618, validation loss: 0.2222
2024-05-23 17:31:58 [INFO]: Epoch 055 - training loss: 0.2613, validation loss: 0.2171
2024-05-23 17:31:59 [INFO]: Epoch 056 - training loss: 0.2606, validation loss: 0.2229
2024-05-23 17:32:01 [INFO]: Epoch 057 - training loss: 0.2570, validation loss: 0.2244
2024-05-23 17:32:02 [INFO]: Epoch 058 - training loss: 0.2610, validation loss: 0.2228
2024-05-23 17:32:03 [INFO]: Epoch 059 - training loss: 0.2573, validation loss: 0.2162
2024-05-23 17:32:04 [INFO]: Epoch 060 - training loss: 0.2573, validation loss: 0.2169
2024-05-23 17:32:05 [INFO]: Epoch 061 - training loss: 0.2588, validation loss: 0.2223
2024-05-23 17:32:07 [INFO]: Epoch 062 - training loss: 0.2581, validation loss: 0.2154
2024-05-23 17:32:08 [INFO]: Epoch 063 - training loss: 0.2542, validation loss: 0.2210
2024-05-23 17:32:09 [INFO]: Epoch 064 - training loss: 0.2552, validation loss: 0.2163
2024-05-23 17:32:10 [INFO]: Epoch 065 - training loss: 0.2524, validation loss: 0.2266
2024-05-23 17:32:12 [INFO]: Epoch 066 - training loss: 0.2532, validation loss: 0.2155
2024-05-23 17:32:13 [INFO]: Epoch 067 - training loss: 0.2532, validation loss: 0.2194
2024-05-23 17:32:14 [INFO]: Epoch 068 - training loss: 0.2517, validation loss: 0.2193
2024-05-23 17:32:15 [INFO]: Epoch 069 - training loss: 0.2535, validation loss: 0.2142
2024-05-23 17:32:17 [INFO]: Epoch 070 - training loss: 0.2520, validation loss: 0.2370
2024-05-23 17:32:18 [INFO]: Epoch 071 - training loss: 0.2504, validation loss: 0.2215
2024-05-23 17:32:19 [INFO]: Epoch 072 - training loss: 0.2511, validation loss: 0.2273
2024-05-23 17:32:20 [INFO]: Epoch 073 - training loss: 0.2495, validation loss: 0.2224
2024-05-23 17:32:22 [INFO]: Epoch 074 - training loss: 0.2495, validation loss: 0.2166
2024-05-23 17:32:23 [INFO]: Epoch 075 - training loss: 0.2480, validation loss: 0.2198
2024-05-23 17:32:24 [INFO]: Epoch 076 - training loss: 0.2489, validation loss: 0.2171
2024-05-23 17:32:25 [INFO]: Epoch 077 - training loss: 0.2494, validation loss: 0.2234
2024-05-23 17:32:27 [INFO]: Epoch 078 - training loss: 0.2492, validation loss: 0.2168
2024-05-23 17:32:28 [INFO]: Epoch 079 - training loss: 0.2504, validation loss: 0.2193
2024-05-23 17:32:28 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-05-23 17:32:28 [INFO]: Finished training. The best model is from epoch#69.
2024-05-23 17:32:28 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_0/SAITS_physionet_2012_seta/20240523_T173049/SAITS.pypots
2024-05-23 17:32:28 [INFO]: SAITS on PhysioNet-2012-seta: MAE=0.2088, MSE=0.2337
2024-05-23 17:32:28 [INFO]: Successfully saved to overlay_minibatchmasking_saved_results/round_0/SAITS_physionet_2012_seta/imputation.pkl
2024-05-23 17:32:28 [INFO]: Using the given device: cuda:0
2024-05-23 17:32:28 [INFO]: Model files will be saved to overlay_minibatchmasking_saved_results/round_0/Transformer_physionet_2012_seta/20240523_T173228
2024-05-23 17:32:28 [INFO]: Tensorboard file will be saved to overlay_minibatchmasking_saved_results/round_0/Transformer_physionet_2012_seta/20240523_T173228/tensorboard
2024-05-23 17:32:28 [INFO]: Transformer initialized with the given hyperparameters, the number of trainable parameters: 2,152,613
2024-05-23 17:32:29 [INFO]: Epoch 001 - training loss: 1.1882, validation loss: 0.5945
2024-05-23 17:32:30 [INFO]: Epoch 002 - training loss: 0.7639, validation loss: 0.4934
2024-05-23 17:32:30 [INFO]: Epoch 003 - training loss: 0.6452, validation loss: 0.4611
2024-05-23 17:32:31 [INFO]: Epoch 004 - training loss: 0.5931, validation loss: 0.4324
2024-05-23 17:32:32 [INFO]: Epoch 005 - training loss: 0.5559, validation loss: 0.4086
2024-05-23 17:32:32 [INFO]: Epoch 006 - training loss: 0.5256, validation loss: 0.4052
2024-05-23 17:32:33 [INFO]: Epoch 007 - training loss: 0.5017, validation loss: 0.3884
2024-05-23 17:32:34 [INFO]: Epoch 008 - training loss: 0.4840, validation loss: 0.3846
2024-05-23 17:32:34 [INFO]: Epoch 009 - training loss: 0.4658, validation loss: 0.3867
2024-05-23 17:32:35 [INFO]: Epoch 010 - training loss: 0.4506, validation loss: 0.3681
2024-05-23 17:32:36 [INFO]: Epoch 011 - training loss: 0.4386, validation loss: 0.3613
2024-05-23 17:32:36 [INFO]: Epoch 012 - training loss: 0.4279, validation loss: 0.3564
2024-05-23 17:32:37 [INFO]: Epoch 013 - training loss: 0.4206, validation loss: 0.3557
2024-05-23 17:32:38 [INFO]: Epoch 014 - training loss: 0.4087, validation loss: 0.3509
2024-05-23 17:32:38 [INFO]: Epoch 015 - training loss: 0.4029, validation loss: 0.3412
2024-05-23 17:32:39 [INFO]: Epoch 016 - training loss: 0.3940, validation loss: 0.3364
2024-05-23 17:32:40 [INFO]: Epoch 017 - training loss: 0.3893, validation loss: 0.3336
2024-05-23 17:32:40 [INFO]: Epoch 018 - training loss: 0.3780, validation loss: 0.3283
2024-05-23 17:32:41 [INFO]: Epoch 019 - training loss: 0.3750, validation loss: 0.3261
2024-05-23 17:32:42 [INFO]: Epoch 020 - training loss: 0.3724, validation loss: 0.3274
2024-05-23 17:32:43 [INFO]: Epoch 021 - training loss: 0.3678, validation loss: 0.3181
2024-05-23 17:32:43 [INFO]: Epoch 022 - training loss: 0.3624, validation loss: 0.3199
2024-05-23 17:32:44 [INFO]: Epoch 023 - training loss: 0.3570, validation loss: 0.3117
2024-05-23 17:32:45 [INFO]: Epoch 024 - training loss: 0.3521, validation loss: 0.3102
2024-05-23 17:32:45 [INFO]: Epoch 025 - training loss: 0.3537, validation loss: 0.3064
2024-05-23 17:32:46 [INFO]: Epoch 026 - training loss: 0.3484, validation loss: 0.3070
2024-05-23 17:32:47 [INFO]: Epoch 027 - training loss: 0.3439, validation loss: 0.3007
2024-05-23 17:32:47 [INFO]: Epoch 028 - training loss: 0.3430, validation loss: 0.3002
2024-05-23 17:32:48 [INFO]: Epoch 029 - training loss: 0.3376, validation loss: 0.2976
2024-05-23 17:32:49 [INFO]: Epoch 030 - training loss: 0.3364, validation loss: 0.2917
2024-05-23 17:32:49 [INFO]: Epoch 031 - training loss: 0.3348, validation loss: 0.2995
2024-05-23 17:32:50 [INFO]: Epoch 032 - training loss: 0.3350, validation loss: 0.2910
2024-05-23 17:32:51 [INFO]: Epoch 033 - training loss: 0.3285, validation loss: 0.2945
2024-05-23 17:32:51 [INFO]: Epoch 034 - training loss: 0.3285, validation loss: 0.2908
2024-05-23 17:32:52 [INFO]: Epoch 035 - training loss: 0.3283, validation loss: 0.2872
2024-05-23 17:32:53 [INFO]: Epoch 036 - training loss: 0.3230, validation loss: 0.2887
2024-05-23 17:32:54 [INFO]: Epoch 037 - training loss: 0.3228, validation loss: 0.2839
2024-05-23 17:32:54 [INFO]: Epoch 038 - training loss: 0.3216, validation loss: 0.2798
2024-05-23 17:32:55 [INFO]: Epoch 039 - training loss: 0.3168, validation loss: 0.2778
2024-05-23 17:32:56 [INFO]: Epoch 040 - training loss: 0.3139, validation loss: 0.2789
2024-05-23 17:32:56 [INFO]: Epoch 041 - training loss: 0.3157, validation loss: 0.2771
2024-05-23 17:32:57 [INFO]: Epoch 042 - training loss: 0.3124, validation loss: 0.2788
2024-05-23 17:32:58 [INFO]: Epoch 043 - training loss: 0.3133, validation loss: 0.2776
2024-05-23 17:32:58 [INFO]: Epoch 044 - training loss: 0.3106, validation loss: 0.2796
2024-05-23 17:32:59 [INFO]: Epoch 045 - training loss: 0.3079, validation loss: 0.2758
2024-05-23 17:33:00 [INFO]: Epoch 046 - training loss: 0.3069, validation loss: 0.2726
2024-05-23 17:33:00 [INFO]: Epoch 047 - training loss: 0.3045, validation loss: 0.2744
2024-05-23 17:33:01 [INFO]: Epoch 048 - training loss: 0.3114, validation loss: 0.2751
2024-05-23 17:33:02 [INFO]: Epoch 049 - training loss: 0.3049, validation loss: 0.2734
2024-05-23 17:33:02 [INFO]: Epoch 050 - training loss: 0.3021, validation loss: 0.2728
2024-05-23 17:33:03 [INFO]: Epoch 051 - training loss: 0.3024, validation loss: 0.2715
2024-05-23 17:33:04 [INFO]: Epoch 052 - training loss: 0.3021, validation loss: 0.2707
2024-05-23 17:33:04 [INFO]: Epoch 053 - training loss: 0.2965, validation loss: 0.2671
2024-05-23 17:33:05 [INFO]: Epoch 054 - training loss: 0.2987, validation loss: 0.2692
2024-05-23 17:33:06 [INFO]: Epoch 055 - training loss: 0.2974, validation loss: 0.2705
2024-05-23 17:33:06 [INFO]: Epoch 056 - training loss: 0.2967, validation loss: 0.2643
2024-05-23 17:33:07 [INFO]: Epoch 057 - training loss: 0.3007, validation loss: 0.2632
2024-05-23 17:33:08 [INFO]: Epoch 058 - training loss: 0.2976, validation loss: 0.2611
2024-05-23 17:33:08 [INFO]: Epoch 059 - training loss: 0.2947, validation loss: 0.2632
2024-05-23 17:33:09 [INFO]: Epoch 060 - training loss: 0.2939, validation loss: 0.2660
2024-05-23 17:33:10 [INFO]: Epoch 061 - training loss: 0.2919, validation loss: 0.2673
2024-05-23 17:33:10 [INFO]: Epoch 062 - training loss: 0.2946, validation loss: 0.2581
2024-05-23 17:33:11 [INFO]: Epoch 063 - training loss: 0.2918, validation loss: 0.2562
2024-05-23 17:33:12 [INFO]: Epoch 064 - training loss: 0.2920, validation loss: 0.2607
2024-05-23 17:33:13 [INFO]: Epoch 065 - training loss: 0.2902, validation loss: 0.2607
2024-05-23 17:33:13 [INFO]: Epoch 066 - training loss: 0.2917, validation loss: 0.2595
2024-05-23 17:33:14 [INFO]: Epoch 067 - training loss: 0.2920, validation loss: 0.2589
2024-05-23 17:33:15 [INFO]: Epoch 068 - training loss: 0.2867, validation loss: 0.2595
2024-05-23 17:33:15 [INFO]: Epoch 069 - training loss: 0.2871, validation loss: 0.2576
2024-05-23 17:33:16 [INFO]: Epoch 070 - training loss: 0.2877, validation loss: 0.2557
2024-05-23 17:33:17 [INFO]: Epoch 071 - training loss: 0.2833, validation loss: 0.2557
2024-05-23 17:33:17 [INFO]: Epoch 072 - training loss: 0.2836, validation loss: 0.2536
2024-05-23 17:33:18 [INFO]: Epoch 073 - training loss: 0.2862, validation loss: 0.2536
2024-05-23 17:33:19 [INFO]: Epoch 074 - training loss: 0.2848, validation loss: 0.2483
2024-05-23 17:33:20 [INFO]: Epoch 075 - training loss: 0.2815, validation loss: 0.2512
2024-05-23 17:33:20 [INFO]: Epoch 076 - training loss: 0.2822, validation loss: 0.2493
2024-05-23 17:33:21 [INFO]: Epoch 077 - training loss: 0.2824, validation loss: 0.2533
2024-05-23 17:33:22 [INFO]: Epoch 078 - training loss: 0.2822, validation loss: 0.2530
2024-05-23 17:33:22 [INFO]: Epoch 079 - training loss: 0.2804, validation loss: 0.2534
2024-05-23 17:33:23 [INFO]: Epoch 080 - training loss: 0.2794, validation loss: 0.2481
2024-05-23 17:33:24 [INFO]: Epoch 081 - training loss: 0.2784, validation loss: 0.2478
2024-05-23 17:33:24 [INFO]: Epoch 082 - training loss: 0.2799, validation loss: 0.2483
2024-05-23 17:33:25 [INFO]: Epoch 083 - training loss: 0.2790, validation loss: 0.2413
2024-05-23 17:33:26 [INFO]: Epoch 084 - training loss: 0.2737, validation loss: 0.2481
2024-05-23 17:33:26 [INFO]: Epoch 085 - training loss: 0.2734, validation loss: 0.2493
2024-05-23 17:33:27 [INFO]: Epoch 086 - training loss: 0.2745, validation loss: 0.2460
2024-05-23 17:33:28 [INFO]: Epoch 087 - training loss: 0.2735, validation loss: 0.2470
2024-05-23 17:33:28 [INFO]: Epoch 088 - training loss: 0.2767, validation loss: 0.2477
2024-05-23 17:33:29 [INFO]: Epoch 089 - training loss: 0.2772, validation loss: 0.2423
2024-05-23 17:33:30 [INFO]: Epoch 090 - training loss: 0.2764, validation loss: 0.2476
2024-05-23 17:33:31 [INFO]: Epoch 091 - training loss: 0.2739, validation loss: 0.2435
2024-05-23 17:33:31 [INFO]: Epoch 092 - training loss: 0.2736, validation loss: 0.2416
2024-05-23 17:33:32 [INFO]: Epoch 093 - training loss: 0.2735, validation loss: 0.2403
2024-05-23 17:33:33 [INFO]: Epoch 094 - training loss: 0.2745, validation loss: 0.2414
2024-05-23 17:33:33 [INFO]: Epoch 095 - training loss: 0.2718, validation loss: 0.2439
2024-05-23 17:33:34 [INFO]: Epoch 096 - training loss: 0.2702, validation loss: 0.2466
2024-05-23 17:33:35 [INFO]: Epoch 097 - training loss: 0.2708, validation loss: 0.2445
2024-05-23 17:33:35 [INFO]: Epoch 098 - training loss: 0.2684, validation loss: 0.2388
2024-05-23 17:33:36 [INFO]: Epoch 099 - training loss: 0.2707, validation loss: 0.2410
2024-05-23 17:33:37 [INFO]: Epoch 100 - training loss: 0.2687, validation loss: 0.2425
2024-05-23 17:33:37 [INFO]: Epoch 101 - training loss: 0.2675, validation loss: 0.2403
2024-05-23 17:33:38 [INFO]: Epoch 102 - training loss: 0.2672, validation loss: 0.2409
2024-05-23 17:33:39 [INFO]: Epoch 103 - training loss: 0.2676, validation loss: 0.2388
2024-05-23 17:33:40 [INFO]: Epoch 104 - training loss: 0.2698, validation loss: 0.2394
2024-05-23 17:33:40 [INFO]: Epoch 105 - training loss: 0.2661, validation loss: 0.2389
2024-05-23 17:33:41 [INFO]: Epoch 106 - training loss: 0.2682, validation loss: 0.2391
2024-05-23 17:33:42 [INFO]: Epoch 107 - training loss: 0.2650, validation loss: 0.2355
2024-05-23 17:33:43 [INFO]: Epoch 108 - training loss: 0.2687, validation loss: 0.2388
2024-05-23 17:33:43 [INFO]: Epoch 109 - training loss: 0.2672, validation loss: 0.2364
2024-05-23 17:33:44 [INFO]: Epoch 110 - training loss: 0.2667, validation loss: 0.2392
2024-05-23 17:33:45 [INFO]: Epoch 111 - training loss: 0.2624, validation loss: 0.2422
2024-05-23 17:33:45 [INFO]: Epoch 112 - training loss: 0.2635, validation loss: 0.2360
2024-05-23 17:33:46 [INFO]: Epoch 113 - training loss: 0.2656, validation loss: 0.2361
2024-05-23 17:33:47 [INFO]: Epoch 114 - training loss: 0.2661, validation loss: 0.2354
2024-05-23 17:33:47 [INFO]: Epoch 115 - training loss: 0.2653, validation loss: 0.2385
2024-05-23 17:33:48 [INFO]: Epoch 116 - training loss: 0.2623, validation loss: 0.2330
2024-05-23 17:33:49 [INFO]: Epoch 117 - training loss: 0.2630, validation loss: 0.2357
2024-05-23 17:33:49 [INFO]: Epoch 118 - training loss: 0.2649, validation loss: 0.2324
2024-05-23 17:33:50 [INFO]: Epoch 119 - training loss: 0.2614, validation loss: 0.2336
2024-05-23 17:33:51 [INFO]: Epoch 120 - training loss: 0.2603, validation loss: 0.2331
2024-05-23 17:33:51 [INFO]: Epoch 121 - training loss: 0.2615, validation loss: 0.2319
2024-05-23 17:33:52 [INFO]: Epoch 122 - training loss: 0.2613, validation loss: 0.2297
2024-05-23 17:33:53 [INFO]: Epoch 123 - training loss: 0.2620, validation loss: 0.2328
2024-05-23 17:33:53 [INFO]: Epoch 124 - training loss: 0.2587, validation loss: 0.2246
2024-05-23 17:33:54 [INFO]: Epoch 125 - training loss: 0.2602, validation loss: 0.2251
2024-05-23 17:33:55 [INFO]: Epoch 126 - training loss: 0.2604, validation loss: 0.2263
2024-05-23 17:33:56 [INFO]: Epoch 127 - training loss: 0.2603, validation loss: 0.2283
2024-05-23 17:33:56 [INFO]: Epoch 128 - training loss: 0.2575, validation loss: 0.2283
2024-05-23 17:33:57 [INFO]: Epoch 129 - training loss: 0.2583, validation loss: 0.2251
2024-05-23 17:33:58 [INFO]: Epoch 130 - training loss: 0.2592, validation loss: 0.2271
2024-05-23 17:33:58 [INFO]: Epoch 131 - training loss: 0.2600, validation loss: 0.2264
2024-05-23 17:33:59 [INFO]: Epoch 132 - training loss: 0.2579, validation loss: 0.2306
2024-05-23 17:34:00 [INFO]: Epoch 133 - training loss: 0.2568, validation loss: 0.2243
2024-05-23 17:34:00 [INFO]: Epoch 134 - training loss: 0.2554, validation loss: 0.2289
2024-05-23 17:34:01 [INFO]: Epoch 135 - training loss: 0.2566, validation loss: 0.2260
2024-05-23 17:34:02 [INFO]: Epoch 136 - training loss: 0.2571, validation loss: 0.2271
2024-05-23 17:34:02 [INFO]: Epoch 137 - training loss: 0.2536, validation loss: 0.2295
2024-05-23 17:34:03 [INFO]: Epoch 138 - training loss: 0.2548, validation loss: 0.2241
2024-05-23 17:34:04 [INFO]: Epoch 139 - training loss: 0.2581, validation loss: 0.2178
2024-05-23 17:34:05 [INFO]: Epoch 140 - training loss: 0.2535, validation loss: 0.2179
2024-05-23 17:34:05 [INFO]: Epoch 141 - training loss: 0.2548, validation loss: 0.2213
2024-05-23 17:34:06 [INFO]: Epoch 142 - training loss: 0.2543, validation loss: 0.2219
2024-05-23 17:34:07 [INFO]: Epoch 143 - training loss: 0.2562, validation loss: 0.2188
2024-05-23 17:34:08 [INFO]: Epoch 144 - training loss: 0.2554, validation loss: 0.2183
2024-05-23 17:34:08 [INFO]: Epoch 145 - training loss: 0.2541, validation loss: 0.2187
2024-05-23 17:34:09 [INFO]: Epoch 146 - training loss: 0.2539, validation loss: 0.2193
2024-05-23 17:34:10 [INFO]: Epoch 147 - training loss: 0.2558, validation loss: 0.2195
2024-05-23 17:34:10 [INFO]: Epoch 148 - training loss: 0.2533, validation loss: 0.2165
2024-05-23 17:34:11 [INFO]: Epoch 149 - training loss: 0.2510, validation loss: 0.2180
2024-05-23 17:34:12 [INFO]: Epoch 150 - training loss: 0.2509, validation loss: 0.2129
2024-05-23 17:34:12 [INFO]: Epoch 151 - training loss: 0.2532, validation loss: 0.2201
2024-05-23 17:34:13 [INFO]: Epoch 152 - training loss: 0.2499, validation loss: 0.2253
2024-05-23 17:34:14 [INFO]: Epoch 153 - training loss: 0.2532, validation loss: 0.2183
2024-05-23 17:34:14 [INFO]: Epoch 154 - training loss: 0.2520, validation loss: 0.2231
2024-05-23 17:34:15 [INFO]: Epoch 155 - training loss: 0.2537, validation loss: 0.2207
2024-05-23 17:34:16 [INFO]: Epoch 156 - training loss: 0.2519, validation loss: 0.2185
2024-05-23 17:34:17 [INFO]: Epoch 157 - training loss: 0.2513, validation loss: 0.2273
2024-05-23 17:34:17 [INFO]: Epoch 158 - training loss: 0.2508, validation loss: 0.2174
2024-05-23 17:34:18 [INFO]: Epoch 159 - training loss: 0.2512, validation loss: 0.2208
2024-05-23 17:34:19 [INFO]: Epoch 160 - training loss: 0.2513, validation loss: 0.2199
2024-05-23 17:34:19 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-05-23 17:34:19 [INFO]: Finished training. The best model is from epoch#150.
2024-05-23 17:34:19 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_0/Transformer_physionet_2012_seta/20240523_T173228/Transformer.pypots
2024-05-23 17:34:19 [INFO]: Transformer on PhysioNet-2012-seta: MAE=0.2139, MSE=0.2218
2024-05-23 17:34:19 [INFO]: Successfully saved to overlay_minibatchmasking_saved_results/round_0/Transformer_physionet_2012_seta/imputation.pkl
2024-05-23 17:34:19 [INFO]: Using the given device: cuda:0
2024-05-23 17:34:19 [INFO]: Model files will be saved to overlay_minibatchmasking_saved_results/round_0/TimesNet_physionet_2012_seta/20240523_T173419
2024-05-23 17:34:19 [INFO]: Tensorboard file will be saved to overlay_minibatchmasking_saved_results/round_0/TimesNet_physionet_2012_seta/20240523_T173419/tensorboard
2024-05-23 17:34:19 [INFO]: TimesNet initialized with the given hyperparameters, the number of trainable parameters: 21,649,573
2024-05-23 17:34:20 [INFO]: Epoch 001 - training loss: 0.4324, validation loss: 0.5703
2024-05-23 17:34:21 [INFO]: Epoch 002 - training loss: 0.4626, validation loss: 0.4004
2024-05-23 17:34:22 [INFO]: Epoch 003 - training loss: 0.3978, validation loss: 1.0628
2024-05-23 17:34:23 [INFO]: Epoch 004 - training loss: 0.4202, validation loss: 0.9828
2024-05-23 17:34:24 [INFO]: Epoch 005 - training loss: 0.3457, validation loss: 0.4448
2024-05-23 17:34:25 [INFO]: Epoch 006 - training loss: 0.3546, validation loss: 0.4137
2024-05-23 17:34:26 [INFO]: Epoch 007 - training loss: 0.3899, validation loss: 0.3720
2024-05-23 17:34:26 [INFO]: Epoch 008 - training loss: 0.3696, validation loss: 0.3242
2024-05-23 17:34:27 [INFO]: Epoch 009 - training loss: 0.3540, validation loss: 0.3340
2024-05-23 17:34:28 [INFO]: Epoch 010 - training loss: 0.3454, validation loss: 0.3061
2024-05-23 17:34:29 [INFO]: Epoch 011 - training loss: 0.4219, validation loss: 0.3268
2024-05-23 17:34:30 [INFO]: Epoch 012 - training loss: 0.3836, validation loss: 0.3154
2024-05-23 17:34:30 [INFO]: Epoch 013 - training loss: 0.3231, validation loss: 0.3026
2024-05-23 17:34:31 [INFO]: Epoch 014 - training loss: 0.3511, validation loss: 0.3030
2024-05-23 17:34:32 [INFO]: Epoch 015 - training loss: 0.3282, validation loss: 0.3306
2024-05-23 17:34:33 [INFO]: Epoch 016 - training loss: 0.3288, validation loss: 0.3063
2024-05-23 17:34:34 [INFO]: Epoch 017 - training loss: 0.3119, validation loss: 0.2924
2024-05-23 17:34:35 [INFO]: Epoch 018 - training loss: 0.3750, validation loss: 0.3149
2024-05-23 17:34:35 [INFO]: Epoch 019 - training loss: 0.3221, validation loss: 0.2952
2024-05-23 17:34:36 [INFO]: Epoch 020 - training loss: 0.2955, validation loss: 0.2873
2024-05-23 17:34:37 [INFO]: Epoch 021 - training loss: 0.2998, validation loss: 0.2881
2024-05-23 17:34:38 [INFO]: Epoch 022 - training loss: 0.3003, validation loss: 0.2839
2024-05-23 17:34:39 [INFO]: Epoch 023 - training loss: 0.3585, validation loss: 0.2894
2024-05-23 17:34:39 [INFO]: Epoch 024 - training loss: 0.3076, validation loss: 0.2843
2024-05-23 17:34:40 [INFO]: Epoch 025 - training loss: 0.3016, validation loss: 0.2779
2024-05-23 17:34:41 [INFO]: Epoch 026 - training loss: 0.3025, validation loss: 0.2806
2024-05-23 17:34:42 [INFO]: Epoch 027 - training loss: 0.3070, validation loss: 0.2807
2024-05-23 17:34:43 [INFO]: Epoch 028 - training loss: 0.3038, validation loss: 0.2747
2024-05-23 17:34:44 [INFO]: Epoch 029 - training loss: 0.3235, validation loss: 0.2772
2024-05-23 17:34:44 [INFO]: Epoch 030 - training loss: 0.3008, validation loss: 0.2904
2024-05-23 17:34:45 [INFO]: Epoch 031 - training loss: 0.3051, validation loss: 0.2852
2024-05-23 17:34:46 [INFO]: Epoch 032 - training loss: 0.3217, validation loss: 0.2833
2024-05-23 17:34:47 [INFO]: Epoch 033 - training loss: 0.3361, validation loss: 0.3233
2024-05-23 17:34:48 [INFO]: Epoch 034 - training loss: 0.3231, validation loss: 0.2804
2024-05-23 17:34:48 [INFO]: Epoch 035 - training loss: 0.2788, validation loss: 0.2765
2024-05-23 17:34:49 [INFO]: Epoch 036 - training loss: 0.3122, validation loss: 0.2882
2024-05-23 17:34:50 [INFO]: Epoch 037 - training loss: 0.2815, validation loss: 0.2919
2024-05-23 17:34:51 [INFO]: Epoch 038 - training loss: 0.3414, validation loss: 0.3208
2024-05-23 17:34:51 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-05-23 17:34:51 [INFO]: Finished training. The best model is from epoch#28.
2024-05-23 17:34:51 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_0/TimesNet_physionet_2012_seta/20240523_T173419/TimesNet.pypots
2024-05-23 17:34:51 [INFO]: TimesNet on PhysioNet-2012-seta: MAE=0.2939, MSE=0.2783
2024-05-23 17:34:51 [INFO]: Successfully saved to overlay_minibatchmasking_saved_results/round_0/TimesNet_physionet_2012_seta/imputation.pkl
2024-05-23 17:34:51 [INFO]: Using the given device: cuda:0
2024-05-23 17:34:51 [INFO]: Model files will be saved to overlay_minibatchmasking_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173451
2024-05-23 17:34:51 [INFO]: Tensorboard file will be saved to overlay_minibatchmasking_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173451/tensorboard
2024-05-23 17:34:51 [INFO]: CSDI initialized with the given hyperparameters, the number of trainable parameters: 1,694,753
2024-05-23 17:35:35 [INFO]: Epoch 001 - training loss: 0.4162, validation loss: 0.3459
2024-05-23 17:35:35 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173451/CSDI_epoch1_loss0.3458691850304604.pypots
2024-05-23 17:36:18 [INFO]: Epoch 002 - training loss: 0.3323, validation loss: 0.2878
2024-05-23 17:36:18 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173451/CSDI_epoch2_loss0.28775178641080856.pypots
2024-05-23 17:37:02 [INFO]: Epoch 003 - training loss: 0.2827, validation loss: 0.2480
2024-05-23 17:37:02 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173451/CSDI_epoch3_loss0.24795361310243608.pypots
2024-05-23 17:37:46 [INFO]: Epoch 004 - training loss: 0.2905, validation loss: 0.2329
2024-05-23 17:37:46 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173451/CSDI_epoch4_loss0.23293498829007148.pypots
2024-05-23 17:38:30 [INFO]: Epoch 005 - training loss: 0.2582, validation loss: 0.2199
2024-05-23 17:38:30 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173451/CSDI_epoch5_loss0.21990140452980994.pypots
2024-05-23 17:39:14 [INFO]: Epoch 006 - training loss: 0.2600, validation loss: 0.2186
2024-05-23 17:39:14 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173451/CSDI_epoch6_loss0.21864723041653633.pypots
2024-05-23 17:39:58 [INFO]: Epoch 007 - training loss: 0.2478, validation loss: 0.2071
2024-05-23 17:39:58 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173451/CSDI_epoch7_loss0.2070799618959427.pypots
2024-05-23 17:40:42 [INFO]: Epoch 008 - training loss: 0.2447, validation loss: 0.2120
2024-05-23 17:40:42 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173451/CSDI_epoch8_loss0.2120335005223751.pypots
2024-05-23 17:41:26 [INFO]: Epoch 009 - training loss: 0.2334, validation loss: 0.2087
2024-05-23 17:41:26 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173451/CSDI_epoch9_loss0.2086838513612747.pypots
2024-05-23 17:42:10 [INFO]: Epoch 010 - training loss: 0.2305, validation loss: 0.2024
2024-05-23 17:42:10 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173451/CSDI_epoch10_loss0.2024160660803318.pypots
2024-05-23 17:42:54 [INFO]: Epoch 011 - training loss: 0.2436, validation loss: 0.1963
2024-05-23 17:42:54 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173451/CSDI_epoch11_loss0.19629012569785118.pypots
2024-05-23 17:43:39 [INFO]: Epoch 012 - training loss: 0.2377, validation loss: 0.1988
2024-05-23 17:43:39 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173451/CSDI_epoch12_loss0.1988453522324562.pypots
2024-05-23 17:44:23 [INFO]: Epoch 013 - training loss: 0.2429, validation loss: 0.1941
2024-05-23 17:44:23 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173451/CSDI_epoch13_loss0.19407689422369004.pypots
2024-05-23 17:45:06 [INFO]: Epoch 014 - training loss: 0.2334, validation loss: 0.1929
2024-05-23 17:45:06 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173451/CSDI_epoch14_loss0.1929258167743683.pypots
2024-05-23 17:45:50 [INFO]: Epoch 015 - training loss: 0.2414, validation loss: 0.1925
2024-05-23 17:45:50 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173451/CSDI_epoch15_loss0.19253941550850867.pypots
2024-05-23 17:46:34 [INFO]: Epoch 016 - training loss: 0.2277, validation loss: 0.1943
2024-05-23 17:46:34 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173451/CSDI_epoch16_loss0.19433041736483575.pypots
2024-05-23 17:47:18 [INFO]: Epoch 017 - training loss: 0.2496, validation loss: 0.1917
2024-05-23 17:47:18 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173451/CSDI_epoch17_loss0.19171406775712968.pypots
2024-05-23 17:48:02 [INFO]: Epoch 018 - training loss: 0.2432, validation loss: 0.1923
2024-05-23 17:48:02 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173451/CSDI_epoch18_loss0.192294429987669.pypots
2024-05-23 17:48:46 [INFO]: Epoch 019 - training loss: 0.2400, validation loss: 0.1893
2024-05-23 17:48:46 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173451/CSDI_epoch19_loss0.18930671811103822.pypots
2024-05-23 17:49:30 [INFO]: Epoch 020 - training loss: 0.2313, validation loss: 0.1909
2024-05-23 17:49:30 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173451/CSDI_epoch20_loss0.19089163839817047.pypots
2024-05-23 17:50:14 [INFO]: Epoch 021 - training loss: 0.2199, validation loss: 0.1869
2024-05-23 17:50:14 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173451/CSDI_epoch21_loss0.18693671226501465.pypots
2024-05-23 17:50:58 [INFO]: Epoch 022 - training loss: 0.2308, validation loss: 0.1832
2024-05-23 17:50:58 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173451/CSDI_epoch22_loss0.18315320685505868.pypots
2024-05-23 17:51:41 [INFO]: Epoch 023 - training loss: 0.2283, validation loss: 0.1904
2024-05-23 17:51:42 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173451/CSDI_epoch23_loss0.19037227407097818.pypots
2024-05-23 17:52:25 [INFO]: Epoch 024 - training loss: 0.2252, validation loss: 0.1919
2024-05-23 17:52:25 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173451/CSDI_epoch24_loss0.19191489592194558.pypots
2024-05-23 17:53:09 [INFO]: Epoch 025 - training loss: 0.2288, validation loss: 0.1848
2024-05-23 17:53:09 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173451/CSDI_epoch25_loss0.18483550995588302.pypots
2024-05-23 17:53:53 [INFO]: Epoch 026 - training loss: 0.2357, validation loss: 0.1859
2024-05-23 17:53:53 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173451/CSDI_epoch26_loss0.18590299487113954.pypots
2024-05-23 17:54:37 [INFO]: Epoch 027 - training loss: 0.2198, validation loss: 0.1857
2024-05-23 17:54:37 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173451/CSDI_epoch27_loss0.1857389785349369.pypots
2024-05-23 17:55:21 [INFO]: Epoch 028 - training loss: 0.2273, validation loss: 0.1834
2024-05-23 17:55:21 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173451/CSDI_epoch28_loss0.18340644910931586.pypots
2024-05-23 17:56:04 [INFO]: Epoch 029 - training loss: 0.2260, validation loss: 0.1833
2024-05-23 17:56:04 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173451/CSDI_epoch29_loss0.18332357108592987.pypots
2024-05-23 17:56:48 [INFO]: Epoch 030 - training loss: 0.2234, validation loss: 0.1829
2024-05-23 17:56:48 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173451/CSDI_epoch30_loss0.18292376101017.pypots
2024-05-23 17:57:32 [INFO]: Epoch 031 - training loss: 0.2181, validation loss: 0.1833
2024-05-23 17:57:32 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173451/CSDI_epoch31_loss0.18325464874505998.pypots
2024-05-23 17:58:16 [INFO]: Epoch 032 - training loss: 0.2329, validation loss: 0.1833
2024-05-23 17:58:16 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173451/CSDI_epoch32_loss0.18332460075616835.pypots
2024-05-23 17:59:00 [INFO]: Epoch 033 - training loss: 0.2250, validation loss: 0.1828
2024-05-23 17:59:00 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173451/CSDI_epoch33_loss0.1828030101954937.pypots
2024-05-23 17:59:44 [INFO]: Epoch 034 - training loss: 0.2224, validation loss: 0.1833
2024-05-23 17:59:44 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173451/CSDI_epoch34_loss0.18334198892116546.pypots
2024-05-23 18:00:27 [INFO]: Epoch 035 - training loss: 0.2258, validation loss: 0.1813
2024-05-23 18:00:27 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173451/CSDI_epoch35_loss0.18130733519792558.pypots
2024-05-23 18:01:11 [INFO]: Epoch 036 - training loss: 0.2148, validation loss: 0.1825
2024-05-23 18:01:11 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173451/CSDI_epoch36_loss0.18253422603011132.pypots
2024-05-23 18:01:55 [INFO]: Epoch 037 - training loss: 0.2289, validation loss: 0.1806
2024-05-23 18:01:55 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173451/CSDI_epoch37_loss0.18057861328125.pypots
2024-05-23 18:02:39 [INFO]: Epoch 038 - training loss: 0.2283, validation loss: 0.1795
2024-05-23 18:02:39 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173451/CSDI_epoch38_loss0.17950396463274956.pypots
2024-05-23 18:03:23 [INFO]: Epoch 039 - training loss: 0.2174, validation loss: 0.1789
2024-05-23 18:03:23 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173451/CSDI_epoch39_loss0.17887087166309357.pypots
2024-05-23 18:04:07 [INFO]: Epoch 040 - training loss: 0.2286, validation loss: 0.1838
2024-05-23 18:04:07 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173451/CSDI_epoch40_loss0.1838146984577179.pypots
2024-05-23 18:04:51 [INFO]: Epoch 041 - training loss: 0.2153, validation loss: 0.1808
2024-05-23 18:04:51 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173451/CSDI_epoch41_loss0.1808207266032696.pypots
2024-05-23 18:05:35 [INFO]: Epoch 042 - training loss: 0.2231, validation loss: 0.1782
2024-05-23 18:05:35 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173451/CSDI_epoch42_loss0.17816514819860457.pypots
2024-05-23 18:06:19 [INFO]: Epoch 043 - training loss: 0.2162, validation loss: 0.1797
2024-05-23 18:06:19 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173451/CSDI_epoch43_loss0.1797462821006775.pypots
2024-05-23 18:07:03 [INFO]: Epoch 044 - training loss: 0.2271, validation loss: 0.1830
2024-05-23 18:07:03 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173451/CSDI_epoch44_loss0.182966910302639.pypots
2024-05-23 18:07:47 [INFO]: Epoch 045 - training loss: 0.2192, validation loss: 0.1806
2024-05-23 18:07:47 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173451/CSDI_epoch45_loss0.1806081034243107.pypots
2024-05-23 18:08:31 [INFO]: Epoch 046 - training loss: 0.2236, validation loss: 0.1804
2024-05-23 18:08:31 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173451/CSDI_epoch46_loss0.18040482476353645.pypots
2024-05-23 18:09:15 [INFO]: Epoch 047 - training loss: 0.2248, validation loss: 0.1793
2024-05-23 18:09:15 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173451/CSDI_epoch47_loss0.17932673022150994.pypots
2024-05-23 18:09:59 [INFO]: Epoch 048 - training loss: 0.2234, validation loss: 0.1774
2024-05-23 18:09:59 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173451/CSDI_epoch48_loss0.17742739692330362.pypots
2024-05-23 18:10:43 [INFO]: Epoch 049 - training loss: 0.2263, validation loss: 0.1758
2024-05-23 18:10:43 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173451/CSDI_epoch49_loss0.17578405439853667.pypots
2024-05-23 18:11:27 [INFO]: Epoch 050 - training loss: 0.2145, validation loss: 0.1765
2024-05-23 18:11:27 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173451/CSDI_epoch50_loss0.17653682827949524.pypots
2024-05-23 18:12:11 [INFO]: Epoch 051 - training loss: 0.2170, validation loss: 0.1782
2024-05-23 18:12:11 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173451/CSDI_epoch51_loss0.17817925289273262.pypots
2024-05-23 18:12:55 [INFO]: Epoch 052 - training loss: 0.2277, validation loss: 0.1758
2024-05-23 18:12:55 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173451/CSDI_epoch52_loss0.1757763147354126.pypots
2024-05-23 18:13:39 [INFO]: Epoch 053 - training loss: 0.2139, validation loss: 0.1752
2024-05-23 18:13:39 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173451/CSDI_epoch53_loss0.1752408489584923.pypots
2024-05-23 18:14:23 [INFO]: Epoch 054 - training loss: 0.2271, validation loss: 0.1796
2024-05-23 18:14:23 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173451/CSDI_epoch54_loss0.17958235517144203.pypots
2024-05-23 18:15:07 [INFO]: Epoch 055 - training loss: 0.2265, validation loss: 0.1762
2024-05-23 18:15:07 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173451/CSDI_epoch55_loss0.1761987827718258.pypots
2024-05-23 18:15:51 [INFO]: Epoch 056 - training loss: 0.2158, validation loss: 0.1750
2024-05-23 18:15:51 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173451/CSDI_epoch56_loss0.17499350532889366.pypots
2024-05-23 18:16:35 [INFO]: Epoch 057 - training loss: 0.2219, validation loss: 0.1747
2024-05-23 18:16:35 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173451/CSDI_epoch57_loss0.17466107606887818.pypots
2024-05-23 18:17:19 [INFO]: Epoch 058 - training loss: 0.2140, validation loss: 0.1741
2024-05-23 18:17:19 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173451/CSDI_epoch58_loss0.1740631267428398.pypots
2024-05-23 18:18:03 [INFO]: Epoch 059 - training loss: 0.2124, validation loss: 0.1756
2024-05-23 18:18:03 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173451/CSDI_epoch59_loss0.17564372569322587.pypots
2024-05-23 18:18:47 [INFO]: Epoch 060 - training loss: 0.2169, validation loss: 0.1754
2024-05-23 18:18:47 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173451/CSDI_epoch60_loss0.17536345347762108.pypots
2024-05-23 18:19:31 [INFO]: Epoch 061 - training loss: 0.2209, validation loss: 0.1748
2024-05-23 18:19:31 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173451/CSDI_epoch61_loss0.17482342198491096.pypots
2024-05-23 18:20:15 [INFO]: Epoch 062 - training loss: 0.2118, validation loss: 0.1751
2024-05-23 18:20:15 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173451/CSDI_epoch62_loss0.17509673312306404.pypots
2024-05-23 18:20:59 [INFO]: Epoch 063 - training loss: 0.2166, validation loss: 0.1765
2024-05-23 18:20:59 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173451/CSDI_epoch63_loss0.17651000246405602.pypots
2024-05-23 18:21:43 [INFO]: Epoch 064 - training loss: 0.2064, validation loss: 0.1736
2024-05-23 18:21:43 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173451/CSDI_epoch64_loss0.17363795414566993.pypots
2024-05-23 18:22:27 [INFO]: Epoch 065 - training loss: 0.2158, validation loss: 0.1739
2024-05-23 18:22:27 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173451/CSDI_epoch65_loss0.17386263683438302.pypots
2024-05-23 18:23:11 [INFO]: Epoch 066 - training loss: 0.2211, validation loss: 0.1733
2024-05-23 18:23:11 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173451/CSDI_epoch66_loss0.17332108095288276.pypots
2024-05-23 18:23:55 [INFO]: Epoch 067 - training loss: 0.2089, validation loss: 0.1723
2024-05-23 18:23:55 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173451/CSDI_epoch67_loss0.17231877148151398.pypots
2024-05-23 18:24:39 [INFO]: Epoch 068 - training loss: 0.2060, validation loss: 0.1737
2024-05-23 18:24:39 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173451/CSDI_epoch68_loss0.1737114742398262.pypots
2024-05-23 18:25:23 [INFO]: Epoch 069 - training loss: 0.2175, validation loss: 0.1727
2024-05-23 18:25:23 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173451/CSDI_epoch69_loss0.172725660353899.pypots
2024-05-23 18:26:07 [INFO]: Epoch 070 - training loss: 0.2171, validation loss: 0.1748
2024-05-23 18:26:07 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173451/CSDI_epoch70_loss0.17475782185792924.pypots
2024-05-23 18:26:50 [INFO]: Epoch 071 - training loss: 0.2104, validation loss: 0.1735
2024-05-23 18:26:50 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173451/CSDI_epoch71_loss0.17346782982349396.pypots
2024-05-23 18:27:34 [INFO]: Epoch 072 - training loss: 0.2106, validation loss: 0.1717
2024-05-23 18:27:34 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173451/CSDI_epoch72_loss0.17165814563632012.pypots
2024-05-23 18:28:18 [INFO]: Epoch 073 - training loss: 0.2142, validation loss: 0.1775
2024-05-23 18:28:18 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173451/CSDI_epoch73_loss0.1775494322180748.pypots
2024-05-23 18:29:02 [INFO]: Epoch 074 - training loss: 0.2040, validation loss: 0.1746
2024-05-23 18:29:02 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173451/CSDI_epoch74_loss0.174573877453804.pypots
2024-05-23 18:29:46 [INFO]: Epoch 075 - training loss: 0.2189, validation loss: 0.1723
2024-05-23 18:29:46 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173451/CSDI_epoch75_loss0.17228617295622825.pypots
2024-05-23 18:30:30 [INFO]: Epoch 076 - training loss: 0.2183, validation loss: 0.1739
2024-05-23 18:30:30 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173451/CSDI_epoch76_loss0.17391871511936188.pypots
2024-05-23 18:31:14 [INFO]: Epoch 077 - training loss: 0.2060, validation loss: 0.1723
2024-05-23 18:31:14 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173451/CSDI_epoch77_loss0.17226032987236978.pypots
2024-05-23 18:31:58 [INFO]: Epoch 078 - training loss: 0.2140, validation loss: 0.1729
2024-05-23 18:31:58 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173451/CSDI_epoch78_loss0.17291889637708663.pypots
2024-05-23 18:32:42 [INFO]: Epoch 079 - training loss: 0.2297, validation loss: 0.1739
2024-05-23 18:32:42 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173451/CSDI_epoch79_loss0.17394365593791009.pypots
2024-05-23 18:33:26 [INFO]: Epoch 080 - training loss: 0.2091, validation loss: 0.1734
2024-05-23 18:33:26 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173451/CSDI_epoch80_loss0.17335349023342134.pypots
2024-05-23 18:34:10 [INFO]: Epoch 081 - training loss: 0.2113, validation loss: 0.1713
2024-05-23 18:34:10 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173451/CSDI_epoch81_loss0.17134521380066872.pypots
2024-05-23 18:34:54 [INFO]: Epoch 082 - training loss: 0.2114, validation loss: 0.1713
2024-05-23 18:34:54 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173451/CSDI_epoch82_loss0.17125174552202224.pypots
2024-05-23 18:35:38 [INFO]: Epoch 083 - training loss: 0.2201, validation loss: 0.1720
2024-05-23 18:35:38 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173451/CSDI_epoch83_loss0.1720091760158539.pypots
2024-05-23 18:36:22 [INFO]: Epoch 084 - training loss: 0.2225, validation loss: 0.1726
2024-05-23 18:36:22 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173451/CSDI_epoch84_loss0.172641359269619.pypots
2024-05-23 18:37:05 [INFO]: Epoch 085 - training loss: 0.2119, validation loss: 0.1716
2024-05-23 18:37:05 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173451/CSDI_epoch85_loss0.1716436564922333.pypots
2024-05-23 18:37:49 [INFO]: Epoch 086 - training loss: 0.2157, validation loss: 0.1720
2024-05-23 18:37:49 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173451/CSDI_epoch86_loss0.17203288301825523.pypots
2024-05-23 18:38:33 [INFO]: Epoch 087 - training loss: 0.2139, validation loss: 0.1741
2024-05-23 18:38:33 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173451/CSDI_epoch87_loss0.17413974702358245.pypots
2024-05-23 18:39:17 [INFO]: Epoch 088 - training loss: 0.2112, validation loss: 0.1712
2024-05-23 18:39:17 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173451/CSDI_epoch88_loss0.17119138613343238.pypots
2024-05-23 18:40:01 [INFO]: Epoch 089 - training loss: 0.2136, validation loss: 0.1713
2024-05-23 18:40:01 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173451/CSDI_epoch89_loss0.17131552323698998.pypots
2024-05-23 18:40:45 [INFO]: Epoch 090 - training loss: 0.2144, validation loss: 0.1728
2024-05-23 18:40:45 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173451/CSDI_epoch90_loss0.17281174063682556.pypots
2024-05-23 18:41:28 [INFO]: Epoch 091 - training loss: 0.2242, validation loss: 0.1702
2024-05-23 18:41:29 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173451/CSDI_epoch91_loss0.17017142698168755.pypots
2024-05-23 18:42:12 [INFO]: Epoch 092 - training loss: 0.2115, validation loss: 0.1731
2024-05-23 18:42:12 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173451/CSDI_epoch92_loss0.1730769991874695.pypots
2024-05-23 18:42:56 [INFO]: Epoch 093 - training loss: 0.2201, validation loss: 0.1707
2024-05-23 18:42:56 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173451/CSDI_epoch93_loss0.17069765627384187.pypots
2024-05-23 18:43:40 [INFO]: Epoch 094 - training loss: 0.2219, validation loss: 0.1712
2024-05-23 18:43:40 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173451/CSDI_epoch94_loss0.1712123639881611.pypots
2024-05-23 18:44:24 [INFO]: Epoch 095 - training loss: 0.2148, validation loss: 0.1703
2024-05-23 18:44:24 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173451/CSDI_epoch95_loss0.17034780085086823.pypots
2024-05-23 18:45:08 [INFO]: Epoch 096 - training loss: 0.2142, validation loss: 0.1700
2024-05-23 18:45:08 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173451/CSDI_epoch96_loss0.17004655301570892.pypots
2024-05-23 18:45:52 [INFO]: Epoch 097 - training loss: 0.2148, validation loss: 0.1689
2024-05-23 18:45:52 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173451/CSDI_epoch97_loss0.16890203207731247.pypots
2024-05-23 18:46:36 [INFO]: Epoch 098 - training loss: 0.2154, validation loss: 0.1702
2024-05-23 18:46:36 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173451/CSDI_epoch98_loss0.17021825313568115.pypots
2024-05-23 18:47:20 [INFO]: Epoch 099 - training loss: 0.2179, validation loss: 0.1706
2024-05-23 18:47:20 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173451/CSDI_epoch99_loss0.17059966474771499.pypots
2024-05-23 18:48:04 [INFO]: Epoch 100 - training loss: 0.2143, validation loss: 0.1708
2024-05-23 18:48:04 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173451/CSDI_epoch100_loss0.17079659998416902.pypots
2024-05-23 18:48:48 [INFO]: Epoch 101 - training loss: 0.2207, validation loss: 0.1718
2024-05-23 18:48:48 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173451/CSDI_epoch101_loss0.17180026322603226.pypots
2024-05-23 18:49:32 [INFO]: Epoch 102 - training loss: 0.2196, validation loss: 0.1693
2024-05-23 18:49:32 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173451/CSDI_epoch102_loss0.1692671425640583.pypots
2024-05-23 18:50:16 [INFO]: Epoch 103 - training loss: 0.2081, validation loss: 0.1724
2024-05-23 18:50:16 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173451/CSDI_epoch103_loss0.17235343754291535.pypots
2024-05-23 18:51:00 [INFO]: Epoch 104 - training loss: 0.2277, validation loss: 0.1696
2024-05-23 18:51:00 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173451/CSDI_epoch104_loss0.1696017563343048.pypots
2024-05-23 18:51:44 [INFO]: Epoch 105 - training loss: 0.2065, validation loss: 0.1694
2024-05-23 18:51:44 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173451/CSDI_epoch105_loss0.16936209946870803.pypots
2024-05-23 18:52:28 [INFO]: Epoch 106 - training loss: 0.2068, validation loss: 0.1685
2024-05-23 18:52:28 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173451/CSDI_epoch106_loss0.16848329380154609.pypots
2024-05-23 18:53:12 [INFO]: Epoch 107 - training loss: 0.2123, validation loss: 0.1686
2024-05-23 18:53:12 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173451/CSDI_epoch107_loss0.16855804324150087.pypots
2024-05-23 18:53:55 [INFO]: Epoch 108 - training loss: 0.2167, validation loss: 0.1697
2024-05-23 18:53:55 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173451/CSDI_epoch108_loss0.16971340999007226.pypots
2024-05-23 18:54:39 [INFO]: Epoch 109 - training loss: 0.2225, validation loss: 0.1701
2024-05-23 18:54:39 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173451/CSDI_epoch109_loss0.1701260417699814.pypots
2024-05-23 18:55:23 [INFO]: Epoch 110 - training loss: 0.2103, validation loss: 0.1681
2024-05-23 18:55:23 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173451/CSDI_epoch110_loss0.1681365005671978.pypots
2024-05-23 18:56:07 [INFO]: Epoch 111 - training loss: 0.2148, validation loss: 0.1687
2024-05-23 18:56:07 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173451/CSDI_epoch111_loss0.1686733514070511.pypots
2024-05-23 18:56:51 [INFO]: Epoch 112 - training loss: 0.2108, validation loss: 0.1690
2024-05-23 18:56:51 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173451/CSDI_epoch112_loss0.16898784935474395.pypots
2024-05-23 18:57:35 [INFO]: Epoch 113 - training loss: 0.2000, validation loss: 0.1689
2024-05-23 18:57:35 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173451/CSDI_epoch113_loss0.1688811369240284.pypots
2024-05-23 18:58:19 [INFO]: Epoch 114 - training loss: 0.2082, validation loss: 0.1687
2024-05-23 18:58:19 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173451/CSDI_epoch114_loss0.16873746663331984.pypots
2024-05-23 18:59:03 [INFO]: Epoch 115 - training loss: 0.2205, validation loss: 0.1681
2024-05-23 18:59:03 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173451/CSDI_epoch115_loss0.1680571213364601.pypots
2024-05-23 18:59:47 [INFO]: Epoch 116 - training loss: 0.2136, validation loss: 0.1693
2024-05-23 18:59:47 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173451/CSDI_epoch116_loss0.16931132674217225.pypots
2024-05-23 19:00:31 [INFO]: Epoch 117 - training loss: 0.2110, validation loss: 0.1682
2024-05-23 19:00:31 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173451/CSDI_epoch117_loss0.1682155579328537.pypots
2024-05-23 19:01:15 [INFO]: Epoch 118 - training loss: 0.2127, validation loss: 0.1683
2024-05-23 19:01:15 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173451/CSDI_epoch118_loss0.16827537938952447.pypots
2024-05-23 19:01:59 [INFO]: Epoch 119 - training loss: 0.2088, validation loss: 0.1676
2024-05-23 19:01:59 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173451/CSDI_epoch119_loss0.1676281549036503.pypots
2024-05-23 19:02:43 [INFO]: Epoch 120 - training loss: 0.2039, validation loss: 0.1664
2024-05-23 19:02:43 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173451/CSDI_epoch120_loss0.16637009084224702.pypots
2024-05-23 19:03:27 [INFO]: Epoch 121 - training loss: 0.2132, validation loss: 0.1672
2024-05-23 19:03:27 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173451/CSDI_epoch121_loss0.16717960238456725.pypots
2024-05-23 19:04:10 [INFO]: Epoch 122 - training loss: 0.1970, validation loss: 0.1688
2024-05-23 19:04:11 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173451/CSDI_epoch122_loss0.16881835088133812.pypots
2024-05-23 19:04:55 [INFO]: Epoch 123 - training loss: 0.2103, validation loss: 0.1684
2024-05-23 19:04:55 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173451/CSDI_epoch123_loss0.16843039095401763.pypots
2024-05-23 19:05:39 [INFO]: Epoch 124 - training loss: 0.2082, validation loss: 0.1702
2024-05-23 19:05:39 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173451/CSDI_epoch124_loss0.17022778168320657.pypots
2024-05-23 19:06:23 [INFO]: Epoch 125 - training loss: 0.2083, validation loss: 0.1674
2024-05-23 19:06:23 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173451/CSDI_epoch125_loss0.16743629723787307.pypots
2024-05-23 19:07:07 [INFO]: Epoch 126 - training loss: 0.2140, validation loss: 0.1684
2024-05-23 19:07:07 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173451/CSDI_epoch126_loss0.16838683038949967.pypots
2024-05-23 19:07:51 [INFO]: Epoch 127 - training loss: 0.1936, validation loss: 0.1678
2024-05-23 19:07:51 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173451/CSDI_epoch127_loss0.16784015595912932.pypots
2024-05-23 19:08:35 [INFO]: Epoch 128 - training loss: 0.2170, validation loss: 0.1659
2024-05-23 19:08:35 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173451/CSDI_epoch128_loss0.16594951823353768.pypots
2024-05-23 19:09:19 [INFO]: Epoch 129 - training loss: 0.2190, validation loss: 0.1686
2024-05-23 19:09:19 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173451/CSDI_epoch129_loss0.16862041428685187.pypots
2024-05-23 19:10:03 [INFO]: Epoch 130 - training loss: 0.2112, validation loss: 0.1682
2024-05-23 19:10:03 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173451/CSDI_epoch130_loss0.16824926659464837.pypots
2024-05-23 19:10:47 [INFO]: Epoch 131 - training loss: 0.2086, validation loss: 0.1677
2024-05-23 19:10:47 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173451/CSDI_epoch131_loss0.1677411749958992.pypots
2024-05-23 19:11:31 [INFO]: Epoch 132 - training loss: 0.2046, validation loss: 0.1667
2024-05-23 19:11:31 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173451/CSDI_epoch132_loss0.16665229946374893.pypots
2024-05-23 19:12:15 [INFO]: Epoch 133 - training loss: 0.2031, validation loss: 0.1684
2024-05-23 19:12:15 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173451/CSDI_epoch133_loss0.1683751218020916.pypots
2024-05-23 19:12:59 [INFO]: Epoch 134 - training loss: 0.2121, validation loss: 0.1716
2024-05-23 19:12:59 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173451/CSDI_epoch134_loss0.17155471742153167.pypots
2024-05-23 19:13:43 [INFO]: Epoch 135 - training loss: 0.2092, validation loss: 0.1688
2024-05-23 19:13:43 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173451/CSDI_epoch135_loss0.16879364028573035.pypots
2024-05-23 19:14:27 [INFO]: Epoch 136 - training loss: 0.2060, validation loss: 0.1672
2024-05-23 19:14:27 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173451/CSDI_epoch136_loss0.16720869094133378.pypots
2024-05-23 19:15:11 [INFO]: Epoch 137 - training loss: 0.2142, validation loss: 0.1678
2024-05-23 19:15:11 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173451/CSDI_epoch137_loss0.16778818741440774.pypots
2024-05-23 19:15:55 [INFO]: Epoch 138 - training loss: 0.2061, validation loss: 0.1668
2024-05-23 19:15:55 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173451/CSDI_epoch138_loss0.16679080203175545.pypots
2024-05-23 19:15:55 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-05-23 19:15:55 [INFO]: Finished training. The best model is from epoch#128.
2024-05-23 19:15:55 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173451/CSDI.pypots
2024-05-23 19:23:18 [INFO]: CSDI on PhysioNet-2012-seta: MAE=0.2144, MSE=0.2414
2024-05-23 19:52:43 [INFO]: Successfully saved to overlay_minibatchmasking_saved_results/round_0/CSDI_physionet_2012_seta/imputation.pkl
2024-05-23 19:52:43 [INFO]: Using the given device: cuda:0
2024-05-23 19:52:43 [INFO]: Model files will be saved to overlay_minibatchmasking_saved_results/round_0/GPVAE_physionet_2012_seta/20240523_T195243
2024-05-23 19:52:43 [INFO]: Tensorboard file will be saved to overlay_minibatchmasking_saved_results/round_0/GPVAE_physionet_2012_seta/20240523_T195243/tensorboard
2024-05-23 19:52:43 [INFO]: GPVAE initialized with the given hyperparameters, the number of trainable parameters: 1,028,244
2024-05-23 19:52:44 [INFO]: Epoch 001 - training loss: 42358.2950, validation loss: 0.9279
2024-05-23 19:52:45 [INFO]: Epoch 002 - training loss: 24362.0462, validation loss: 0.7461
2024-05-23 19:52:46 [INFO]: Epoch 003 - training loss: 23496.4524, validation loss: 0.6901
2024-05-23 19:52:46 [INFO]: Epoch 004 - training loss: 23213.9933, validation loss: 0.6694
2024-05-23 19:52:47 [INFO]: Epoch 005 - training loss: 23075.5874, validation loss: 0.6652
2024-05-23 19:52:47 [INFO]: Epoch 006 - training loss: 22998.8833, validation loss: 0.6542
2024-05-23 19:52:48 [INFO]: Epoch 007 - training loss: 22952.0730, validation loss: 0.6554
2024-05-23 19:52:49 [INFO]: Epoch 008 - training loss: 22920.6805, validation loss: 0.6558
2024-05-23 19:52:49 [INFO]: Epoch 009 - training loss: 22899.8994, validation loss: 0.6502
2024-05-23 19:52:50 [INFO]: Epoch 010 - training loss: 22884.2905, validation loss: 0.6458
2024-05-23 19:52:50 [INFO]: Epoch 011 - training loss: 22873.1502, validation loss: 0.6524
2024-05-23 19:52:51 [INFO]: Epoch 012 - training loss: 22864.9640, validation loss: 0.6408
2024-05-23 19:52:52 [INFO]: Epoch 013 - training loss: 22858.9750, validation loss: 0.6432
2024-05-23 19:52:52 [INFO]: Epoch 014 - training loss: 22853.0339, validation loss: 0.6432
2024-05-23 19:52:53 [INFO]: Epoch 015 - training loss: 22849.2955, validation loss: 0.6335
2024-05-23 19:52:53 [INFO]: Epoch 016 - training loss: 22846.8931, validation loss: 0.6365
2024-05-23 19:52:54 [INFO]: Epoch 017 - training loss: 22842.8983, validation loss: 0.6293
2024-05-23 19:52:55 [INFO]: Epoch 018 - training loss: 22840.3818, validation loss: 0.6404
2024-05-23 19:52:55 [INFO]: Epoch 019 - training loss: 22838.0598, validation loss: 0.6257
2024-05-23 19:52:56 [INFO]: Epoch 020 - training loss: 22836.2953, validation loss: 0.6270
2024-05-23 19:52:56 [INFO]: Epoch 021 - training loss: 22835.4947, validation loss: 0.6201
2024-05-23 19:52:57 [INFO]: Epoch 022 - training loss: 22832.9539, validation loss: 0.6162
2024-05-23 19:52:58 [INFO]: Epoch 023 - training loss: 22832.3452, validation loss: 0.6179
2024-05-23 19:52:58 [INFO]: Epoch 024 - training loss: 22831.4515, validation loss: 0.6198
2024-05-23 19:52:59 [INFO]: Epoch 025 - training loss: 22831.2292, validation loss: 0.6247
2024-05-23 19:52:59 [INFO]: Epoch 026 - training loss: 22829.8788, validation loss: 0.6167
2024-05-23 19:53:00 [INFO]: Epoch 027 - training loss: 22829.1166, validation loss: 0.6117
2024-05-23 19:53:01 [INFO]: Epoch 028 - training loss: 22828.2142, validation loss: 0.6097
2024-05-23 19:53:01 [INFO]: Epoch 029 - training loss: 22827.4642, validation loss: 0.6118
2024-05-23 19:53:02 [INFO]: Epoch 030 - training loss: 22827.1732, validation loss: 0.6073
2024-05-23 19:53:02 [INFO]: Epoch 031 - training loss: 22826.2950, validation loss: 0.6085
2024-05-23 19:53:03 [INFO]: Epoch 032 - training loss: 22825.2849, validation loss: 0.6170
2024-05-23 19:53:04 [INFO]: Epoch 033 - training loss: 22824.2297, validation loss: 0.6014
2024-05-23 19:53:04 [INFO]: Epoch 034 - training loss: 22823.0015, validation loss: 0.6007
2024-05-23 19:53:05 [INFO]: Epoch 035 - training loss: 22821.9374, validation loss: 0.5970
2024-05-23 19:53:05 [INFO]: Epoch 036 - training loss: 22820.4281, validation loss: 0.5908
2024-05-23 19:53:06 [INFO]: Epoch 037 - training loss: 22819.5901, validation loss: 0.5947
2024-05-23 19:53:07 [INFO]: Epoch 038 - training loss: 22818.7448, validation loss: 0.5856
2024-05-23 19:53:07 [INFO]: Epoch 039 - training loss: 22817.6006, validation loss: 0.5964
2024-05-23 19:53:08 [INFO]: Epoch 040 - training loss: 22816.8196, validation loss: 0.5933
2024-05-23 19:53:08 [INFO]: Epoch 041 - training loss: 22816.1221, validation loss: 0.5801
2024-05-23 19:53:09 [INFO]: Epoch 042 - training loss: 22814.7435, validation loss: 0.5740
2024-05-23 19:53:10 [INFO]: Epoch 043 - training loss: 22814.3315, validation loss: 0.5775
2024-05-23 19:53:10 [INFO]: Epoch 044 - training loss: 22812.9314, validation loss: 0.5681
2024-05-23 19:53:11 [INFO]: Epoch 045 - training loss: 22812.4164, validation loss: 0.5685
2024-05-23 19:53:11 [INFO]: Epoch 046 - training loss: 22811.2777, validation loss: 0.5698
2024-05-23 19:53:12 [INFO]: Epoch 047 - training loss: 22810.5060, validation loss: 0.5841
2024-05-23 19:53:13 [INFO]: Epoch 048 - training loss: 22810.3001, validation loss: 0.5612
2024-05-23 19:53:13 [INFO]: Epoch 049 - training loss: 22808.3968, validation loss: 0.5578
2024-05-23 19:53:14 [INFO]: Epoch 050 - training loss: 22807.4085, validation loss: 0.5497
2024-05-23 19:53:14 [INFO]: Epoch 051 - training loss: 22806.3689, validation loss: 0.5436
2024-05-23 19:53:15 [INFO]: Epoch 052 - training loss: 22804.9799, validation loss: 0.5415
2024-05-23 19:53:16 [INFO]: Epoch 053 - training loss: 22804.5489, validation loss: 0.5356
2024-05-23 19:53:16 [INFO]: Epoch 054 - training loss: 22803.5702, validation loss: 0.5459
2024-05-23 19:53:17 [INFO]: Epoch 055 - training loss: 22802.9390, validation loss: 0.5490
2024-05-23 19:53:17 [INFO]: Epoch 056 - training loss: 22802.5872, validation loss: 0.5283
2024-05-23 19:53:18 [INFO]: Epoch 057 - training loss: 22802.9714, validation loss: 0.5364
2024-05-23 19:53:19 [INFO]: Epoch 058 - training loss: 22800.6731, validation loss: 0.5323
2024-05-23 19:53:19 [INFO]: Epoch 059 - training loss: 22800.6950, validation loss: 0.5256
2024-05-23 19:53:20 [INFO]: Epoch 060 - training loss: 22800.5780, validation loss: 0.5374
2024-05-23 19:53:20 [INFO]: Epoch 061 - training loss: 22799.4582, validation loss: 0.5185
2024-05-23 19:53:21 [INFO]: Epoch 062 - training loss: 22798.6604, validation loss: 0.5209
2024-05-23 19:53:22 [INFO]: Epoch 063 - training loss: 22797.6088, validation loss: 0.5155
2024-05-23 19:53:22 [INFO]: Epoch 064 - training loss: 22797.3595, validation loss: 0.5259
2024-05-23 19:53:23 [INFO]: Epoch 065 - training loss: 22797.5884, validation loss: 0.5155
2024-05-23 19:53:23 [INFO]: Epoch 066 - training loss: 22796.6804, validation loss: 0.5187
2024-05-23 19:53:24 [INFO]: Epoch 067 - training loss: 22796.3540, validation loss: 0.5203
2024-05-23 19:53:25 [INFO]: Epoch 068 - training loss: 22796.0717, validation loss: 0.5301
2024-05-23 19:53:25 [INFO]: Epoch 069 - training loss: 22795.7540, validation loss: 0.5216
2024-05-23 19:53:26 [INFO]: Epoch 070 - training loss: 22795.2526, validation loss: 0.5159
2024-05-23 19:53:26 [INFO]: Epoch 071 - training loss: 22794.9979, validation loss: 0.5164
2024-05-23 19:53:27 [INFO]: Epoch 072 - training loss: 22794.1758, validation loss: 0.5235
2024-05-23 19:53:28 [INFO]: Epoch 073 - training loss: 22795.2596, validation loss: 0.5151
2024-05-23 19:53:28 [INFO]: Epoch 074 - training loss: 22794.7448, validation loss: 0.5182
2024-05-23 19:53:29 [INFO]: Epoch 075 - training loss: 22795.4058, validation loss: 0.5312
2024-05-23 19:53:29 [INFO]: Epoch 076 - training loss: 22799.5739, validation loss: 0.5106
2024-05-23 19:53:30 [INFO]: Epoch 077 - training loss: 22795.8778, validation loss: 0.5205
2024-05-23 19:53:31 [INFO]: Epoch 078 - training loss: 22794.6263, validation loss: 0.5122
2024-05-23 19:53:31 [INFO]: Epoch 079 - training loss: 22793.9725, validation loss: 0.5174
2024-05-23 19:53:32 [INFO]: Epoch 080 - training loss: 22793.1082, validation loss: 0.5179
2024-05-23 19:53:32 [INFO]: Epoch 081 - training loss: 22792.9952, validation loss: 0.5172
2024-05-23 19:53:33 [INFO]: Epoch 082 - training loss: 22792.9455, validation loss: 0.5189
2024-05-23 19:53:34 [INFO]: Epoch 083 - training loss: 22792.6408, validation loss: 0.5172
2024-05-23 19:53:34 [INFO]: Epoch 084 - training loss: 22792.3465, validation loss: 0.5360
2024-05-23 19:53:35 [INFO]: Epoch 085 - training loss: 22793.8283, validation loss: 0.5130
2024-05-23 19:53:35 [INFO]: Epoch 086 - training loss: 22794.1410, validation loss: 0.5194
2024-05-23 19:53:35 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-05-23 19:53:35 [INFO]: Finished training. The best model is from epoch#76.
2024-05-23 19:53:35 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_0/GPVAE_physionet_2012_seta/20240523_T195243/GPVAE.pypots
2024-05-23 19:53:35 [INFO]: GP-VAE on PhysioNet-2012-seta: MAE=0.4329, MSE=0.4658
2024-05-23 19:53:36 [INFO]: Successfully saved to overlay_minibatchmasking_saved_results/round_0/GPVAE_physionet_2012_seta/imputation.pkl
2024-05-23 19:53:36 [INFO]: Using the given device: cuda:0
2024-05-23 19:53:36 [INFO]: Model files will be saved to overlay_minibatchmasking_saved_results/round_0/USGAN_physionet_2012_seta/20240523_T195336
2024-05-23 19:53:36 [INFO]: Tensorboard file will be saved to overlay_minibatchmasking_saved_results/round_0/USGAN_physionet_2012_seta/20240523_T195336/tensorboard
2024-05-23 19:53:36 [INFO]: USGAN initialized with the given hyperparameters, the number of trainable parameters: 16,010,261
2024-05-23 19:53:57 [INFO]: Epoch 001 - generator training loss: 0.5805, discriminator training loss: 0.3818, validation loss: 0.6171
2024-05-23 19:54:16 [INFO]: Epoch 002 - generator training loss: 0.4640, discriminator training loss: 0.2564, validation loss: 0.5178
2024-05-23 19:54:34 [INFO]: Epoch 003 - generator training loss: 0.4357, discriminator training loss: 0.2035, validation loss: 0.4949
2024-05-23 19:54:53 [INFO]: Epoch 004 - generator training loss: 0.4470, discriminator training loss: 0.1502, validation loss: 0.4776
2024-05-23 19:55:11 [INFO]: Epoch 005 - generator training loss: 0.4402, discriminator training loss: 0.1214, validation loss: 0.4610
2024-05-23 19:55:30 [INFO]: Epoch 006 - generator training loss: 0.4277, discriminator training loss: 0.1021, validation loss: 0.4503
2024-05-23 19:55:48 [INFO]: Epoch 007 - generator training loss: 0.4170, discriminator training loss: 0.0887, validation loss: 0.4375
2024-05-23 19:56:07 [INFO]: Epoch 008 - generator training loss: 0.4073, discriminator training loss: 0.0790, validation loss: 0.4314
2024-05-23 19:56:25 [INFO]: Epoch 009 - generator training loss: 0.3981, discriminator training loss: 0.0712, validation loss: 0.4264
2024-05-23 19:56:44 [INFO]: Epoch 010 - generator training loss: 0.3933, discriminator training loss: 0.0651, validation loss: 0.4146
2024-05-23 19:57:03 [INFO]: Epoch 011 - generator training loss: 0.3848, discriminator training loss: 0.0598, validation loss: 0.4081
2024-05-23 19:57:21 [INFO]: Epoch 012 - generator training loss: 0.3783, discriminator training loss: 0.0553, validation loss: 0.4009
2024-05-23 19:57:40 [INFO]: Epoch 013 - generator training loss: 0.3724, discriminator training loss: 0.0515, validation loss: 0.3959
2024-05-23 19:57:58 [INFO]: Epoch 014 - generator training loss: 0.3681, discriminator training loss: 0.0480, validation loss: 0.3920
2024-05-23 19:58:17 [INFO]: Epoch 015 - generator training loss: 0.3627, discriminator training loss: 0.0452, validation loss: 0.3881
2024-05-23 19:58:35 [INFO]: Epoch 016 - generator training loss: 0.3582, discriminator training loss: 0.0428, validation loss: 0.3811
2024-05-23 19:58:54 [INFO]: Epoch 017 - generator training loss: 0.3535, discriminator training loss: 0.0410, validation loss: 0.3764
2024-05-23 19:59:12 [INFO]: Epoch 018 - generator training loss: 0.3466, discriminator training loss: 0.0393, validation loss: 0.3729
2024-05-23 19:59:31 [INFO]: Epoch 019 - generator training loss: 0.3413, discriminator training loss: 0.0375, validation loss: 0.3706
2024-05-23 19:59:49 [INFO]: Epoch 020 - generator training loss: 0.3361, discriminator training loss: 0.0364, validation loss: 0.3648
2024-05-23 20:00:08 [INFO]: Epoch 021 - generator training loss: 0.3314, discriminator training loss: 0.0351, validation loss: 0.3612
2024-05-23 20:00:27 [INFO]: Epoch 022 - generator training loss: 0.3310, discriminator training loss: 0.0343, validation loss: 0.3595
2024-05-23 20:00:46 [INFO]: Epoch 023 - generator training loss: 0.3259, discriminator training loss: 0.0333, validation loss: 0.3543
2024-05-23 20:01:05 [INFO]: Epoch 024 - generator training loss: 0.3198, discriminator training loss: 0.0323, validation loss: 0.3497
2024-05-23 20:01:23 [INFO]: Epoch 025 - generator training loss: 0.3145, discriminator training loss: 0.0318, validation loss: 0.3485
2024-05-23 20:01:41 [INFO]: Epoch 026 - generator training loss: 0.3108, discriminator training loss: 0.0309, validation loss: 0.3470
2024-05-23 20:02:00 [INFO]: Epoch 027 - generator training loss: 0.3056, discriminator training loss: 0.0302, validation loss: 0.3396
2024-05-23 20:02:19 [INFO]: Epoch 028 - generator training loss: 0.3036, discriminator training loss: 0.0297, validation loss: 0.3388
2024-05-23 20:02:38 [INFO]: Epoch 029 - generator training loss: 0.3020, discriminator training loss: 0.0292, validation loss: 0.3576
2024-05-23 20:02:57 [INFO]: Epoch 030 - generator training loss: 0.3103, discriminator training loss: 0.0288, validation loss: 0.3445
2024-05-23 20:03:15 [INFO]: Epoch 031 - generator training loss: 0.2988, discriminator training loss: 0.0285, validation loss: 0.3336
2024-05-23 20:03:34 [INFO]: Epoch 032 - generator training loss: 0.2899, discriminator training loss: 0.0280, validation loss: 0.3332
2024-05-23 20:03:52 [INFO]: Epoch 033 - generator training loss: 0.2882, discriminator training loss: 0.0276, validation loss: 0.3296
2024-05-23 20:04:10 [INFO]: Epoch 034 - generator training loss: 0.2819, discriminator training loss: 0.0272, validation loss: 0.3269
2024-05-23 20:04:28 [INFO]: Epoch 035 - generator training loss: 0.2834, discriminator training loss: 0.0271, validation loss: 0.3257
2024-05-23 20:04:47 [INFO]: Epoch 036 - generator training loss: 0.2817, discriminator training loss: 0.0267, validation loss: 0.3252
2024-05-23 20:05:05 [INFO]: Epoch 037 - generator training loss: 0.2777, discriminator training loss: 0.0264, validation loss: 0.3275
2024-05-23 20:05:23 [INFO]: Epoch 038 - generator training loss: 0.2703, discriminator training loss: 0.0261, validation loss: 0.3220
2024-05-23 20:05:41 [INFO]: Epoch 039 - generator training loss: 0.2683, discriminator training loss: 0.0258, validation loss: 0.3212
2024-05-23 20:05:59 [INFO]: Epoch 040 - generator training loss: 0.2671, discriminator training loss: 0.0256, validation loss: 0.3201
2024-05-23 20:06:17 [INFO]: Epoch 041 - generator training loss: 0.2619, discriminator training loss: 0.0252, validation loss: 0.3273
2024-05-23 20:06:35 [INFO]: Epoch 042 - generator training loss: 0.2633, discriminator training loss: 0.0251, validation loss: 0.3166
2024-05-23 20:06:53 [INFO]: Epoch 043 - generator training loss: 0.2577, discriminator training loss: 0.0248, validation loss: 0.3131
2024-05-23 20:07:11 [INFO]: Epoch 044 - generator training loss: 0.2529, discriminator training loss: 0.0246, validation loss: 0.3167
2024-05-23 20:07:29 [INFO]: Epoch 045 - generator training loss: 0.2506, discriminator training loss: 0.0244, validation loss: 0.3142
2024-05-23 20:07:48 [INFO]: Epoch 046 - generator training loss: 0.2483, discriminator training loss: 0.0242, validation loss: 0.3127
2024-05-23 20:08:06 [INFO]: Epoch 047 - generator training loss: 0.2579, discriminator training loss: 0.0241, validation loss: 0.3098
2024-05-23 20:08:24 [INFO]: Epoch 048 - generator training loss: 0.2466, discriminator training loss: 0.0240, validation loss: 0.3092
2024-05-23 20:08:42 [INFO]: Epoch 049 - generator training loss: 0.2463, discriminator training loss: 0.0237, validation loss: 0.3137
2024-05-23 20:09:01 [INFO]: Epoch 050 - generator training loss: 0.2447, discriminator training loss: 0.0236, validation loss: 0.3115
2024-05-23 20:09:18 [INFO]: Epoch 051 - generator training loss: 0.2370, discriminator training loss: 0.0234, validation loss: 0.3080
2024-05-23 20:09:37 [INFO]: Epoch 052 - generator training loss: 0.2355, discriminator training loss: 0.0233, validation loss: 0.3063
2024-05-23 20:09:55 [INFO]: Epoch 053 - generator training loss: 0.2341, discriminator training loss: 0.0230, validation loss: 0.3094
2024-05-23 20:10:13 [INFO]: Epoch 054 - generator training loss: 0.2339, discriminator training loss: 0.0230, validation loss: 0.3096
2024-05-23 20:10:31 [INFO]: Epoch 055 - generator training loss: 0.2320, discriminator training loss: 0.0229, validation loss: 0.3044
2024-05-23 20:10:50 [INFO]: Epoch 056 - generator training loss: 0.2264, discriminator training loss: 0.0226, validation loss: 0.3070
2024-05-23 20:11:08 [INFO]: Epoch 057 - generator training loss: 0.2276, discriminator training loss: 0.0225, validation loss: 0.3059
2024-05-23 20:11:26 [INFO]: Epoch 058 - generator training loss: 0.2248, discriminator training loss: 0.0222, validation loss: 0.3074
2024-05-23 20:11:44 [INFO]: Epoch 059 - generator training loss: 0.2210, discriminator training loss: 0.0222, validation loss: 0.3090
2024-05-23 20:12:02 [INFO]: Epoch 060 - generator training loss: 0.2204, discriminator training loss: 0.0220, validation loss: 0.3047
2024-05-23 20:12:20 [INFO]: Epoch 061 - generator training loss: 0.2253, discriminator training loss: 0.0220, validation loss: 0.3057
2024-05-23 20:12:38 [INFO]: Epoch 062 - generator training loss: 0.2156, discriminator training loss: 0.0218, validation loss: 0.3031
2024-05-23 20:12:57 [INFO]: Epoch 063 - generator training loss: 0.2159, discriminator training loss: 0.0217, validation loss: 0.3058
2024-05-23 20:13:15 [INFO]: Epoch 064 - generator training loss: 0.2151, discriminator training loss: 0.0215, validation loss: 0.3103
2024-05-23 20:13:33 [INFO]: Epoch 065 - generator training loss: 0.2227, discriminator training loss: 0.0216, validation loss: 0.3060
2024-05-23 20:13:51 [INFO]: Epoch 066 - generator training loss: 0.2180, discriminator training loss: 0.0213, validation loss: 0.3099
2024-05-23 20:14:09 [INFO]: Epoch 067 - generator training loss: 0.2146, discriminator training loss: 0.0214, validation loss: 0.3048
2024-05-23 20:14:28 [INFO]: Epoch 068 - generator training loss: 0.2065, discriminator training loss: 0.0212, validation loss: 0.3028
2024-05-23 20:14:46 [INFO]: Epoch 069 - generator training loss: 0.2070, discriminator training loss: 0.0210, validation loss: 0.3067
2024-05-23 20:15:05 [INFO]: Epoch 070 - generator training loss: 0.2082, discriminator training loss: 0.0210, validation loss: 0.3031
2024-05-23 20:15:23 [INFO]: Epoch 071 - generator training loss: 0.2019, discriminator training loss: 0.0209, validation loss: 0.3034
2024-05-23 20:15:41 [INFO]: Epoch 072 - generator training loss: 0.2000, discriminator training loss: 0.0209, validation loss: 0.2997
2024-05-23 20:16:00 [INFO]: Epoch 073 - generator training loss: 0.1986, discriminator training loss: 0.0209, validation loss: 0.3035
2024-05-23 20:16:18 [INFO]: Epoch 074 - generator training loss: 0.2003, discriminator training loss: 0.0209, validation loss: 0.3007
2024-05-23 20:16:36 [INFO]: Epoch 075 - generator training loss: 0.2044, discriminator training loss: 0.0206, validation loss: 0.3135
2024-05-23 20:16:55 [INFO]: Epoch 076 - generator training loss: 0.2056, discriminator training loss: 0.0206, validation loss: 0.3034
2024-05-23 20:17:13 [INFO]: Epoch 077 - generator training loss: 0.1941, discriminator training loss: 0.0205, validation loss: 0.3032
2024-05-23 20:17:31 [INFO]: Epoch 078 - generator training loss: 0.1915, discriminator training loss: 0.0204, validation loss: 0.2999
2024-05-23 20:17:50 [INFO]: Epoch 079 - generator training loss: 0.1908, discriminator training loss: 0.0204, validation loss: 0.3041
2024-05-23 20:18:08 [INFO]: Epoch 080 - generator training loss: 0.1925, discriminator training loss: 0.0203, validation loss: 0.3016
2024-05-23 20:18:26 [INFO]: Epoch 081 - generator training loss: 0.1921, discriminator training loss: 0.0203, validation loss: 0.2999
2024-05-23 20:18:45 [INFO]: Epoch 082 - generator training loss: 0.1902, discriminator training loss: 0.0201, validation loss: 0.3011
2024-05-23 20:18:45 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-05-23 20:18:45 [INFO]: Finished training. The best model is from epoch#72.
2024-05-23 20:18:45 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_0/USGAN_physionet_2012_seta/20240523_T195336/USGAN.pypots
2024-05-23 20:18:47 [INFO]: US-GAN on PhysioNet-2012-seta: MAE=0.2916, MSE=0.2513
2024-05-23 20:18:57 [INFO]: Successfully saved to overlay_minibatchmasking_saved_results/round_0/USGAN_physionet_2012_seta/imputation.pkl
2024-05-23 20:18:57 [INFO]: Using the given device: cuda:0
2024-05-23 20:18:57 [INFO]: Model files will be saved to overlay_minibatchmasking_saved_results/round_0/BRITS_physionet_2012_seta/20240523_T201857
2024-05-23 20:18:57 [INFO]: Tensorboard file will be saved to overlay_minibatchmasking_saved_results/round_0/BRITS_physionet_2012_seta/20240523_T201857/tensorboard
2024-05-23 20:18:57 [INFO]: BRITS initialized with the given hyperparameters, the number of trainable parameters: 9,176,048
2024-05-23 20:19:12 [INFO]: Epoch 001 - training loss: 1.1200, validation loss: 0.5206
2024-05-23 20:19:24 [INFO]: Epoch 002 - training loss: 0.9044, validation loss: 0.4610
2024-05-23 20:19:35 [INFO]: Epoch 003 - training loss: 0.8377, validation loss: 0.4320
2024-05-23 20:19:47 [INFO]: Epoch 004 - training loss: 0.7986, validation loss: 0.4114
2024-05-23 20:19:59 [INFO]: Epoch 005 - training loss: 0.7707, validation loss: 0.3954
2024-05-23 20:20:11 [INFO]: Epoch 006 - training loss: 0.7483, validation loss: 0.3839
2024-05-23 20:20:23 [INFO]: Epoch 007 - training loss: 0.7305, validation loss: 0.3735
2024-05-23 20:20:35 [INFO]: Epoch 008 - training loss: 0.7155, validation loss: 0.3679
2024-05-23 20:20:47 [INFO]: Epoch 009 - training loss: 0.7031, validation loss: 0.3633
2024-05-23 20:20:59 [INFO]: Epoch 010 - training loss: 0.6925, validation loss: 0.3586
2024-05-23 20:21:11 [INFO]: Epoch 011 - training loss: 0.6822, validation loss: 0.3558
2024-05-23 20:21:23 [INFO]: Epoch 012 - training loss: 0.6744, validation loss: 0.3533
2024-05-23 20:21:35 [INFO]: Epoch 013 - training loss: 0.6659, validation loss: 0.3502
2024-05-23 20:21:47 [INFO]: Epoch 014 - training loss: 0.6594, validation loss: 0.3489
2024-05-23 20:21:59 [INFO]: Epoch 015 - training loss: 0.6534, validation loss: 0.3499
2024-05-23 20:22:11 [INFO]: Epoch 016 - training loss: 0.6476, validation loss: 0.3490
2024-05-23 20:22:23 [INFO]: Epoch 017 - training loss: 0.6433, validation loss: 0.3464
2024-05-23 20:22:35 [INFO]: Epoch 018 - training loss: 0.6390, validation loss: 0.3468
2024-05-23 20:22:47 [INFO]: Epoch 019 - training loss: 0.6340, validation loss: 0.3443
2024-05-23 20:22:59 [INFO]: Epoch 020 - training loss: 0.6307, validation loss: 0.3437
2024-05-23 20:23:11 [INFO]: Epoch 021 - training loss: 0.6272, validation loss: 0.3445
2024-05-23 20:23:23 [INFO]: Epoch 022 - training loss: 0.6243, validation loss: 0.3430
2024-05-23 20:23:35 [INFO]: Epoch 023 - training loss: 0.6200, validation loss: 0.3419
2024-05-23 20:23:47 [INFO]: Epoch 024 - training loss: 0.6163, validation loss: 0.3410
2024-05-23 20:23:59 [INFO]: Epoch 025 - training loss: 0.6142, validation loss: 0.3408
2024-05-23 20:24:11 [INFO]: Epoch 026 - training loss: 0.6104, validation loss: 0.3406
2024-05-23 20:24:23 [INFO]: Epoch 027 - training loss: 0.6074, validation loss: 0.3406
2024-05-23 20:24:35 [INFO]: Epoch 028 - training loss: 0.6050, validation loss: 0.3409
2024-05-23 20:24:47 [INFO]: Epoch 029 - training loss: 0.6057, validation loss: 0.3419
2024-05-23 20:24:59 [INFO]: Epoch 030 - training loss: 0.6007, validation loss: 0.3416
2024-05-23 20:25:11 [INFO]: Epoch 031 - training loss: 0.5971, validation loss: 0.3449
2024-05-23 20:25:23 [INFO]: Epoch 032 - training loss: 0.5977, validation loss: 0.3401
2024-05-23 20:25:35 [INFO]: Epoch 033 - training loss: 0.5908, validation loss: 0.3414
2024-05-23 20:25:47 [INFO]: Epoch 034 - training loss: 0.5881, validation loss: 0.3409
2024-05-23 20:25:59 [INFO]: Epoch 035 - training loss: 0.5878, validation loss: 0.3425
2024-05-23 20:26:11 [INFO]: Epoch 036 - training loss: 0.5824, validation loss: 0.3428
2024-05-23 20:26:23 [INFO]: Epoch 037 - training loss: 0.5911, validation loss: 0.3460
2024-05-23 20:26:35 [INFO]: Epoch 038 - training loss: 0.5862, validation loss: 0.3465
2024-05-23 20:26:47 [INFO]: Epoch 039 - training loss: 0.5796, validation loss: 0.3446
2024-05-23 20:26:59 [INFO]: Epoch 040 - training loss: 0.5743, validation loss: 0.3435
2024-05-23 20:27:11 [INFO]: Epoch 041 - training loss: 0.5712, validation loss: 0.3450
2024-05-23 20:27:23 [INFO]: Epoch 042 - training loss: 0.5682, validation loss: 0.3441
2024-05-23 20:27:23 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-05-23 20:27:23 [INFO]: Finished training. The best model is from epoch#32.
2024-05-23 20:27:23 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_0/BRITS_physionet_2012_seta/20240523_T201857/BRITS.pypots
2024-05-23 20:27:26 [INFO]: BRITS on PhysioNet-2012-seta: MAE=0.2525, MSE=0.2668
2024-05-23 20:27:35 [INFO]: Successfully saved to overlay_minibatchmasking_saved_results/round_0/BRITS_physionet_2012_seta/imputation.pkl
2024-05-23 20:27:35 [INFO]: Using the given device: cuda:0
2024-05-23 20:27:35 [INFO]: Model files will be saved to overlay_minibatchmasking_saved_results/round_0/MRNN_physionet_2012_seta/20240523_T202735
2024-05-23 20:27:35 [INFO]: Tensorboard file will be saved to overlay_minibatchmasking_saved_results/round_0/MRNN_physionet_2012_seta/20240523_T202735/tensorboard
2024-05-23 20:27:35 [INFO]: MRNN initialized with the given hyperparameters, the number of trainable parameters: 7,599
2024-05-23 20:27:41 [INFO]: Epoch 001 - training loss: 1.4111, validation loss: 0.9988
2024-05-23 20:27:41 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_0/MRNN_physionet_2012_seta/20240523_T202735/MRNN_epoch1_loss0.9988177865743637.pypots
2024-05-23 20:27:44 [INFO]: Epoch 002 - training loss: 0.8902, validation loss: 0.9642
2024-05-23 20:27:44 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_0/MRNN_physionet_2012_seta/20240523_T202735/MRNN_epoch2_loss0.9641747444868087.pypots
2024-05-23 20:27:47 [INFO]: Epoch 003 - training loss: 0.6795, validation loss: 0.9450
2024-05-23 20:27:47 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_0/MRNN_physionet_2012_seta/20240523_T202735/MRNN_epoch3_loss0.9449559688568115.pypots
2024-05-23 20:27:49 [INFO]: Epoch 004 - training loss: 0.6209, validation loss: 0.9321
2024-05-23 20:27:49 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_0/MRNN_physionet_2012_seta/20240523_T202735/MRNN_epoch4_loss0.9321058213710784.pypots
2024-05-23 20:27:52 [INFO]: Epoch 005 - training loss: 0.5882, validation loss: 0.9245
2024-05-23 20:27:52 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_0/MRNN_physionet_2012_seta/20240523_T202735/MRNN_epoch5_loss0.9245115607976914.pypots
2024-05-23 20:27:55 [INFO]: Epoch 006 - training loss: 0.5642, validation loss: 0.9199
2024-05-23 20:27:55 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_0/MRNN_physionet_2012_seta/20240523_T202735/MRNN_epoch6_loss0.9198730617761612.pypots
2024-05-23 20:27:58 [INFO]: Epoch 007 - training loss: 0.5406, validation loss: 0.9166
2024-05-23 20:27:58 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_0/MRNN_physionet_2012_seta/20240523_T202735/MRNN_epoch7_loss0.9166181743144989.pypots
2024-05-23 20:28:01 [INFO]: Epoch 008 - training loss: 0.5311, validation loss: 0.9140
2024-05-23 20:28:01 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_0/MRNN_physionet_2012_seta/20240523_T202735/MRNN_epoch8_loss0.9139874905347825.pypots
2024-05-23 20:28:03 [INFO]: Epoch 009 - training loss: 0.5274, validation loss: 0.9135
2024-05-23 20:28:03 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_0/MRNN_physionet_2012_seta/20240523_T202735/MRNN_epoch9_loss0.9134609371423721.pypots
2024-05-23 20:28:06 [INFO]: Epoch 010 - training loss: 0.5085, validation loss: 0.9124
2024-05-23 20:28:06 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_0/MRNN_physionet_2012_seta/20240523_T202735/MRNN_epoch10_loss0.9123621910810471.pypots
2024-05-23 20:28:09 [INFO]: Epoch 011 - training loss: 0.5004, validation loss: 0.9121
2024-05-23 20:28:09 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_0/MRNN_physionet_2012_seta/20240523_T202735/MRNN_epoch11_loss0.912124416232109.pypots
2024-05-23 20:28:12 [INFO]: Epoch 012 - training loss: 0.4949, validation loss: 0.9126
2024-05-23 20:28:12 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_0/MRNN_physionet_2012_seta/20240523_T202735/MRNN_epoch12_loss0.912621408700943.pypots
2024-05-23 20:28:14 [INFO]: Epoch 013 - training loss: 0.4933, validation loss: 0.9138
2024-05-23 20:28:14 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_0/MRNN_physionet_2012_seta/20240523_T202735/MRNN_epoch13_loss0.913755014538765.pypots
2024-05-23 20:28:17 [INFO]: Epoch 014 - training loss: 0.4796, validation loss: 0.9159
2024-05-23 20:28:17 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_0/MRNN_physionet_2012_seta/20240523_T202735/MRNN_epoch14_loss0.9158924251794816.pypots
2024-05-23 20:28:20 [INFO]: Epoch 015 - training loss: 0.4801, validation loss: 0.9159
2024-05-23 20:28:20 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_0/MRNN_physionet_2012_seta/20240523_T202735/MRNN_epoch15_loss0.9158966720104218.pypots
2024-05-23 20:28:23 [INFO]: Epoch 016 - training loss: 0.4754, validation loss: 0.9175
2024-05-23 20:28:23 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_0/MRNN_physionet_2012_seta/20240523_T202735/MRNN_epoch16_loss0.9174509525299073.pypots
2024-05-23 20:28:26 [INFO]: Epoch 017 - training loss: 0.4670, validation loss: 0.9187
2024-05-23 20:28:26 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_0/MRNN_physionet_2012_seta/20240523_T202735/MRNN_epoch17_loss0.9187334567308426.pypots
2024-05-23 20:28:28 [INFO]: Epoch 018 - training loss: 0.4719, validation loss: 0.9210
2024-05-23 20:28:28 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_0/MRNN_physionet_2012_seta/20240523_T202735/MRNN_epoch18_loss0.9209631830453873.pypots
2024-05-23 20:28:31 [INFO]: Epoch 019 - training loss: 0.4678, validation loss: 0.9205
2024-05-23 20:28:31 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_0/MRNN_physionet_2012_seta/20240523_T202735/MRNN_epoch19_loss0.9204851508140564.pypots
2024-05-23 20:28:34 [INFO]: Epoch 020 - training loss: 0.4591, validation loss: 0.9221
2024-05-23 20:28:34 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_0/MRNN_physionet_2012_seta/20240523_T202735/MRNN_epoch20_loss0.9220662295818329.pypots
2024-05-23 20:28:37 [INFO]: Epoch 021 - training loss: 0.4583, validation loss: 0.9220
2024-05-23 20:28:37 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_0/MRNN_physionet_2012_seta/20240523_T202735/MRNN_epoch21_loss0.9219536632299423.pypots
2024-05-23 20:28:37 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-05-23 20:28:37 [INFO]: Finished training. The best model is from epoch#11.
2024-05-23 20:28:37 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_0/MRNN_physionet_2012_seta/20240523_T202735/MRNN.pypots
2024-05-23 20:28:38 [INFO]: MRNN on PhysioNet-2012-seta: MAE=0.6824, MSE=0.9046
2024-05-23 20:28:42 [INFO]: Successfully saved to overlay_minibatchmasking_saved_results/round_0/MRNN_physionet_2012_seta/imputation.pkl
2024-05-23 20:28:42 [INFO]: Using the given device: cpu
2024-05-23 20:28:42 [INFO]: LOCF on PhysioNet-2012-seta: MAE=0.4109, MSE=0.5324
2024-05-23 20:28:42 [INFO]: Successfully saved to overlay_minibatchmasking_saved_results/round_0/LOCF_physionet_2012_seta/imputation.pkl
2024-05-23 20:28:42 [INFO]: Median on PhysioNet-2012-seta: MAE=0.6868, MSE=1.0189
2024-05-23 20:28:42 [INFO]: Successfully saved to overlay_minibatchmasking_saved_results/round_0/Median_physionet_2012_seta/imputation.pkl
2024-05-23 20:28:42 [INFO]: Mean on PhysioNet-2012-seta: MAE=0.7045, MSE=0.9898
2024-05-23 20:28:42 [INFO]: Successfully saved to overlay_minibatchmasking_saved_results/round_0/Mean_physionet_2012_seta/imputation.pkl
2024-05-23 20:28:42 [INFO]: Have set the random seed as 2024 for numpy and pytorch.
2024-05-23 20:28:42 [INFO]: Using the given device: cuda:0
2024-05-23 20:28:42 [INFO]: Model files will be saved to overlay_minibatchmasking_saved_results/round_1/SAITS_physionet_2012_seta/20240523_T202842
2024-05-23 20:28:42 [INFO]: Tensorboard file will be saved to overlay_minibatchmasking_saved_results/round_1/SAITS_physionet_2012_seta/20240523_T202842/tensorboard
2024-05-23 20:28:42 [INFO]: SAITS initialized with the given hyperparameters, the number of trainable parameters: 5,322,806
2024-05-23 20:28:44 [INFO]: Epoch 001 - training loss: 1.0993, validation loss: 0.4902
2024-05-23 20:28:45 [INFO]: Epoch 002 - training loss: 0.6962, validation loss: 0.4366
2024-05-23 20:28:46 [INFO]: Epoch 003 - training loss: 0.5837, validation loss: 0.3873
2024-05-23 20:28:47 [INFO]: Epoch 004 - training loss: 0.5226, validation loss: 0.3722
2024-05-23 20:28:49 [INFO]: Epoch 005 - training loss: 0.4910, validation loss: 0.3573
2024-05-23 20:28:50 [INFO]: Epoch 006 - training loss: 0.4597, validation loss: 0.3450
2024-05-23 20:28:51 [INFO]: Epoch 007 - training loss: 0.4399, validation loss: 0.3366
2024-05-23 20:28:53 [INFO]: Epoch 008 - training loss: 0.4231, validation loss: 0.3289
2024-05-23 20:28:54 [INFO]: Epoch 009 - training loss: 0.4042, validation loss: 0.3112
2024-05-23 20:28:55 [INFO]: Epoch 010 - training loss: 0.3885, validation loss: 0.3048
2024-05-23 20:28:56 [INFO]: Epoch 011 - training loss: 0.3748, validation loss: 0.2966
2024-05-23 20:28:58 [INFO]: Epoch 012 - training loss: 0.3635, validation loss: 0.2887
2024-05-23 20:28:59 [INFO]: Epoch 013 - training loss: 0.3570, validation loss: 0.2884
2024-05-23 20:29:00 [INFO]: Epoch 014 - training loss: 0.3497, validation loss: 0.2795
2024-05-23 20:29:01 [INFO]: Epoch 015 - training loss: 0.3407, validation loss: 0.2794
2024-05-23 20:29:03 [INFO]: Epoch 016 - training loss: 0.3333, validation loss: 0.2766
2024-05-23 20:29:04 [INFO]: Epoch 017 - training loss: 0.3276, validation loss: 0.2629
2024-05-23 20:29:05 [INFO]: Epoch 018 - training loss: 0.3172, validation loss: 0.2664
2024-05-23 20:29:07 [INFO]: Epoch 019 - training loss: 0.3219, validation loss: 0.2673
2024-05-23 20:29:08 [INFO]: Epoch 020 - training loss: 0.3111, validation loss: 0.2629
2024-05-23 20:29:09 [INFO]: Epoch 021 - training loss: 0.3111, validation loss: 0.2619
2024-05-23 20:29:10 [INFO]: Epoch 022 - training loss: 0.3102, validation loss: 0.2608
2024-05-23 20:29:12 [INFO]: Epoch 023 - training loss: 0.3025, validation loss: 0.2556
2024-05-23 20:29:13 [INFO]: Epoch 024 - training loss: 0.3024, validation loss: 0.2552
2024-05-23 20:29:14 [INFO]: Epoch 025 - training loss: 0.2962, validation loss: 0.2549
2024-05-23 20:29:15 [INFO]: Epoch 026 - training loss: 0.2927, validation loss: 0.2481
2024-05-23 20:29:17 [INFO]: Epoch 027 - training loss: 0.2936, validation loss: 0.2506
2024-05-23 20:29:18 [INFO]: Epoch 028 - training loss: 0.2913, validation loss: 0.2455
2024-05-23 20:29:19 [INFO]: Epoch 029 - training loss: 0.2899, validation loss: 0.2463
2024-05-23 20:29:21 [INFO]: Epoch 030 - training loss: 0.2873, validation loss: 0.2506
2024-05-23 20:29:22 [INFO]: Epoch 031 - training loss: 0.2848, validation loss: 0.2500
2024-05-23 20:29:23 [INFO]: Epoch 032 - training loss: 0.2821, validation loss: 0.2465
2024-05-23 20:29:24 [INFO]: Epoch 033 - training loss: 0.2823, validation loss: 0.2413
2024-05-23 20:29:26 [INFO]: Epoch 034 - training loss: 0.2830, validation loss: 0.2440
2024-05-23 20:29:27 [INFO]: Epoch 035 - training loss: 0.2767, validation loss: 0.2433
2024-05-23 20:29:28 [INFO]: Epoch 036 - training loss: 0.2775, validation loss: 0.2427
2024-05-23 20:29:29 [INFO]: Epoch 037 - training loss: 0.2772, validation loss: 0.2397
2024-05-23 20:29:31 [INFO]: Epoch 038 - training loss: 0.2736, validation loss: 0.2421
2024-05-23 20:29:32 [INFO]: Epoch 039 - training loss: 0.2728, validation loss: 0.2410
2024-05-23 20:29:33 [INFO]: Epoch 040 - training loss: 0.2711, validation loss: 0.2352
2024-05-23 20:29:34 [INFO]: Epoch 041 - training loss: 0.2735, validation loss: 0.2369
2024-05-23 20:29:36 [INFO]: Epoch 042 - training loss: 0.2702, validation loss: 0.2418
2024-05-23 20:29:37 [INFO]: Epoch 043 - training loss: 0.2699, validation loss: 0.2426
2024-05-23 20:29:38 [INFO]: Epoch 044 - training loss: 0.2686, validation loss: 0.2333
2024-05-23 20:29:40 [INFO]: Epoch 045 - training loss: 0.2643, validation loss: 0.2342
2024-05-23 20:29:41 [INFO]: Epoch 046 - training loss: 0.2686, validation loss: 0.2311
2024-05-23 20:29:42 [INFO]: Epoch 047 - training loss: 0.2667, validation loss: 0.2352
2024-05-23 20:29:43 [INFO]: Epoch 048 - training loss: 0.2634, validation loss: 0.2342
2024-05-23 20:29:45 [INFO]: Epoch 049 - training loss: 0.2630, validation loss: 0.2261
2024-05-23 20:29:46 [INFO]: Epoch 050 - training loss: 0.2638, validation loss: 0.2284
2024-05-23 20:29:47 [INFO]: Epoch 051 - training loss: 0.2654, validation loss: 0.2306
2024-05-23 20:29:49 [INFO]: Epoch 052 - training loss: 0.2608, validation loss: 0.2280
2024-05-23 20:29:50 [INFO]: Epoch 053 - training loss: 0.2623, validation loss: 0.2257
2024-05-23 20:29:51 [INFO]: Epoch 054 - training loss: 0.2605, validation loss: 0.2307
2024-05-23 20:29:52 [INFO]: Epoch 055 - training loss: 0.2613, validation loss: 0.2365
2024-05-23 20:29:54 [INFO]: Epoch 056 - training loss: 0.2584, validation loss: 0.2268
2024-05-23 20:29:55 [INFO]: Epoch 057 - training loss: 0.2559, validation loss: 0.2305
2024-05-23 20:29:56 [INFO]: Epoch 058 - training loss: 0.2568, validation loss: 0.2259
2024-05-23 20:29:57 [INFO]: Epoch 059 - training loss: 0.2545, validation loss: 0.2229
2024-05-23 20:29:59 [INFO]: Epoch 060 - training loss: 0.2613, validation loss: 0.2245
2024-05-23 20:30:00 [INFO]: Epoch 061 - training loss: 0.2583, validation loss: 0.2184
2024-05-23 20:30:01 [INFO]: Epoch 062 - training loss: 0.2568, validation loss: 0.2234
2024-05-23 20:30:03 [INFO]: Epoch 063 - training loss: 0.2580, validation loss: 0.2351
2024-05-23 20:30:04 [INFO]: Epoch 064 - training loss: 0.2593, validation loss: 0.2183
2024-05-23 20:30:05 [INFO]: Epoch 065 - training loss: 0.2541, validation loss: 0.2234
2024-05-23 20:30:06 [INFO]: Epoch 066 - training loss: 0.2523, validation loss: 0.2255
2024-05-23 20:30:08 [INFO]: Epoch 067 - training loss: 0.2520, validation loss: 0.2368
2024-05-23 20:30:09 [INFO]: Epoch 068 - training loss: 0.2501, validation loss: 0.2238
2024-05-23 20:30:10 [INFO]: Epoch 069 - training loss: 0.2532, validation loss: 0.2226
2024-05-23 20:30:12 [INFO]: Epoch 070 - training loss: 0.2520, validation loss: 0.2258
2024-05-23 20:30:13 [INFO]: Epoch 071 - training loss: 0.2519, validation loss: 0.2203
2024-05-23 20:30:14 [INFO]: Epoch 072 - training loss: 0.2501, validation loss: 0.2346
2024-05-23 20:30:15 [INFO]: Epoch 073 - training loss: 0.2502, validation loss: 0.2168
2024-05-23 20:30:17 [INFO]: Epoch 074 - training loss: 0.2523, validation loss: 0.2204
2024-05-23 20:30:18 [INFO]: Epoch 075 - training loss: 0.2490, validation loss: 0.2138
2024-05-23 20:30:19 [INFO]: Epoch 076 - training loss: 0.2504, validation loss: 0.2187
2024-05-23 20:30:20 [INFO]: Epoch 077 - training loss: 0.2492, validation loss: 0.2215
2024-05-23 20:30:22 [INFO]: Epoch 078 - training loss: 0.2469, validation loss: 0.2195
2024-05-23 20:30:23 [INFO]: Epoch 079 - training loss: 0.2487, validation loss: 0.2209
2024-05-23 20:30:24 [INFO]: Epoch 080 - training loss: 0.2481, validation loss: 0.2260
2024-05-23 20:30:26 [INFO]: Epoch 081 - training loss: 0.2468, validation loss: 0.2170
2024-05-23 20:30:27 [INFO]: Epoch 082 - training loss: 0.2479, validation loss: 0.2170
2024-05-23 20:30:28 [INFO]: Epoch 083 - training loss: 0.2438, validation loss: 0.2190
2024-05-23 20:30:29 [INFO]: Epoch 084 - training loss: 0.2468, validation loss: 0.2225
2024-05-23 20:30:31 [INFO]: Epoch 085 - training loss: 0.2462, validation loss: 0.2323
2024-05-23 20:30:31 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-05-23 20:30:31 [INFO]: Finished training. The best model is from epoch#75.
2024-05-23 20:30:31 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_1/SAITS_physionet_2012_seta/20240523_T202842/SAITS.pypots
2024-05-23 20:30:31 [INFO]: SAITS on PhysioNet-2012-seta: MAE=0.2063, MSE=0.2181
2024-05-23 20:30:31 [INFO]: Successfully saved to overlay_minibatchmasking_saved_results/round_1/SAITS_physionet_2012_seta/imputation.pkl
2024-05-23 20:30:31 [INFO]: Using the given device: cuda:0
2024-05-23 20:30:31 [INFO]: Model files will be saved to overlay_minibatchmasking_saved_results/round_1/Transformer_physionet_2012_seta/20240523_T203031
2024-05-23 20:30:31 [INFO]: Tensorboard file will be saved to overlay_minibatchmasking_saved_results/round_1/Transformer_physionet_2012_seta/20240523_T203031/tensorboard
2024-05-23 20:30:31 [INFO]: Transformer initialized with the given hyperparameters, the number of trainable parameters: 2,152,613
2024-05-23 20:30:32 [INFO]: Epoch 001 - training loss: 1.1244, validation loss: 0.5471
2024-05-23 20:30:33 [INFO]: Epoch 002 - training loss: 0.7360, validation loss: 0.4666
2024-05-23 20:30:33 [INFO]: Epoch 003 - training loss: 0.6357, validation loss: 0.4435
2024-05-23 20:30:34 [INFO]: Epoch 004 - training loss: 0.5892, validation loss: 0.4298
2024-05-23 20:30:35 [INFO]: Epoch 005 - training loss: 0.5487, validation loss: 0.4097
2024-05-23 20:30:36 [INFO]: Epoch 006 - training loss: 0.5192, validation loss: 0.3909
2024-05-23 20:30:36 [INFO]: Epoch 007 - training loss: 0.4985, validation loss: 0.3824
2024-05-23 20:30:37 [INFO]: Epoch 008 - training loss: 0.4828, validation loss: 0.3786
2024-05-23 20:30:38 [INFO]: Epoch 009 - training loss: 0.4627, validation loss: 0.3776
2024-05-23 20:30:38 [INFO]: Epoch 010 - training loss: 0.4517, validation loss: 0.3633
2024-05-23 20:30:39 [INFO]: Epoch 011 - training loss: 0.4414, validation loss: 0.3553
2024-05-23 20:30:40 [INFO]: Epoch 012 - training loss: 0.4395, validation loss: 0.3551
2024-05-23 20:30:40 [INFO]: Epoch 013 - training loss: 0.4202, validation loss: 0.3418
2024-05-23 20:30:41 [INFO]: Epoch 014 - training loss: 0.4126, validation loss: 0.3408
2024-05-23 20:30:42 [INFO]: Epoch 015 - training loss: 0.3994, validation loss: 0.3400
2024-05-23 20:30:43 [INFO]: Epoch 016 - training loss: 0.3890, validation loss: 0.3342
2024-05-23 20:30:43 [INFO]: Epoch 017 - training loss: 0.3943, validation loss: 0.3282
2024-05-23 20:30:44 [INFO]: Epoch 018 - training loss: 0.3842, validation loss: 0.3236
2024-05-23 20:30:45 [INFO]: Epoch 019 - training loss: 0.3761, validation loss: 0.3214
2024-05-23 20:30:45 [INFO]: Epoch 020 - training loss: 0.3749, validation loss: 0.3163
2024-05-23 20:30:46 [INFO]: Epoch 021 - training loss: 0.3681, validation loss: 0.3180
2024-05-23 20:30:47 [INFO]: Epoch 022 - training loss: 0.3628, validation loss: 0.3145
2024-05-23 20:30:47 [INFO]: Epoch 023 - training loss: 0.3609, validation loss: 0.3088
2024-05-23 20:30:48 [INFO]: Epoch 024 - training loss: 0.3555, validation loss: 0.3059
2024-05-23 20:30:49 [INFO]: Epoch 025 - training loss: 0.3555, validation loss: 0.3102
2024-05-23 20:30:49 [INFO]: Epoch 026 - training loss: 0.3486, validation loss: 0.3054
2024-05-23 20:30:50 [INFO]: Epoch 027 - training loss: 0.3450, validation loss: 0.2994
2024-05-23 20:30:51 [INFO]: Epoch 028 - training loss: 0.3404, validation loss: 0.2958
2024-05-23 20:30:52 [INFO]: Epoch 029 - training loss: 0.3390, validation loss: 0.2961
2024-05-23 20:30:52 [INFO]: Epoch 030 - training loss: 0.3403, validation loss: 0.2926
2024-05-23 20:30:53 [INFO]: Epoch 031 - training loss: 0.3347, validation loss: 0.2927
2024-05-23 20:30:54 [INFO]: Epoch 032 - training loss: 0.3330, validation loss: 0.2911
2024-05-23 20:30:54 [INFO]: Epoch 033 - training loss: 0.3324, validation loss: 0.2898
2024-05-23 20:30:55 [INFO]: Epoch 034 - training loss: 0.3316, validation loss: 0.2872
2024-05-23 20:30:56 [INFO]: Epoch 035 - training loss: 0.3238, validation loss: 0.2943
2024-05-23 20:30:56 [INFO]: Epoch 036 - training loss: 0.3276, validation loss: 0.2901
2024-05-23 20:30:57 [INFO]: Epoch 037 - training loss: 0.3232, validation loss: 0.2849
2024-05-23 20:30:58 [INFO]: Epoch 038 - training loss: 0.3239, validation loss: 0.2843
2024-05-23 20:30:59 [INFO]: Epoch 039 - training loss: 0.3145, validation loss: 0.2784
2024-05-23 20:30:59 [INFO]: Epoch 040 - training loss: 0.3196, validation loss: 0.2784
2024-05-23 20:31:00 [INFO]: Epoch 041 - training loss: 0.3164, validation loss: 0.2774
2024-05-23 20:31:01 [INFO]: Epoch 042 - training loss: 0.3145, validation loss: 0.2809
2024-05-23 20:31:01 [INFO]: Epoch 043 - training loss: 0.3138, validation loss: 0.2728
2024-05-23 20:31:02 [INFO]: Epoch 044 - training loss: 0.3132, validation loss: 0.2724
2024-05-23 20:31:03 [INFO]: Epoch 045 - training loss: 0.3095, validation loss: 0.2718
2024-05-23 20:31:03 [INFO]: Epoch 046 - training loss: 0.3094, validation loss: 0.2713
2024-05-23 20:31:04 [INFO]: Epoch 047 - training loss: 0.3084, validation loss: 0.2678
2024-05-23 20:31:05 [INFO]: Epoch 048 - training loss: 0.3058, validation loss: 0.2654
2024-05-23 20:31:06 [INFO]: Epoch 049 - training loss: 0.3096, validation loss: 0.2820
2024-05-23 20:31:06 [INFO]: Epoch 050 - training loss: 0.3051, validation loss: 0.2639
2024-05-23 20:31:07 [INFO]: Epoch 051 - training loss: 0.3040, validation loss: 0.2631
2024-05-23 20:31:08 [INFO]: Epoch 052 - training loss: 0.3032, validation loss: 0.2659
2024-05-23 20:31:08 [INFO]: Epoch 053 - training loss: 0.2993, validation loss: 0.2684
2024-05-23 20:31:09 [INFO]: Epoch 054 - training loss: 0.2996, validation loss: 0.2660
2024-05-23 20:31:10 [INFO]: Epoch 055 - training loss: 0.2973, validation loss: 0.2656
2024-05-23 20:31:10 [INFO]: Epoch 056 - training loss: 0.2996, validation loss: 0.2648
2024-05-23 20:31:11 [INFO]: Epoch 057 - training loss: 0.2960, validation loss: 0.2618
2024-05-23 20:31:12 [INFO]: Epoch 058 - training loss: 0.2967, validation loss: 0.2612
2024-05-23 20:31:12 [INFO]: Epoch 059 - training loss: 0.2981, validation loss: 0.2607
2024-05-23 20:31:13 [INFO]: Epoch 060 - training loss: 0.2938, validation loss: 0.2653
2024-05-23 20:31:14 [INFO]: Epoch 061 - training loss: 0.2945, validation loss: 0.2567
2024-05-23 20:31:15 [INFO]: Epoch 062 - training loss: 0.2922, validation loss: 0.2597
2024-05-23 20:31:15 [INFO]: Epoch 063 - training loss: 0.2932, validation loss: 0.2647
2024-05-23 20:31:16 [INFO]: Epoch 064 - training loss: 0.2941, validation loss: 0.2616
2024-05-23 20:31:17 [INFO]: Epoch 065 - training loss: 0.2936, validation loss: 0.2554
2024-05-23 20:31:17 [INFO]: Epoch 066 - training loss: 0.2896, validation loss: 0.2574
2024-05-23 20:31:18 [INFO]: Epoch 067 - training loss: 0.2878, validation loss: 0.2670
2024-05-23 20:31:19 [INFO]: Epoch 068 - training loss: 0.2859, validation loss: 0.2584
2024-05-23 20:31:19 [INFO]: Epoch 069 - training loss: 0.2880, validation loss: 0.2554
2024-05-23 20:31:20 [INFO]: Epoch 070 - training loss: 0.2890, validation loss: 0.2524
2024-05-23 20:31:21 [INFO]: Epoch 071 - training loss: 0.2868, validation loss: 0.2592
2024-05-23 20:31:22 [INFO]: Epoch 072 - training loss: 0.2855, validation loss: 0.2537
2024-05-23 20:31:22 [INFO]: Epoch 073 - training loss: 0.2868, validation loss: 0.2528
2024-05-23 20:31:23 [INFO]: Epoch 074 - training loss: 0.2818, validation loss: 0.2530
2024-05-23 20:31:24 [INFO]: Epoch 075 - training loss: 0.2833, validation loss: 0.2503
2024-05-23 20:31:24 [INFO]: Epoch 076 - training loss: 0.2833, validation loss: 0.2552
2024-05-23 20:31:25 [INFO]: Epoch 077 - training loss: 0.2806, validation loss: 0.2538
2024-05-23 20:31:26 [INFO]: Epoch 078 - training loss: 0.2831, validation loss: 0.2494
2024-05-23 20:31:26 [INFO]: Epoch 079 - training loss: 0.2826, validation loss: 0.2529
2024-05-23 20:31:27 [INFO]: Epoch 080 - training loss: 0.2790, validation loss: 0.2542
2024-05-23 20:31:28 [INFO]: Epoch 081 - training loss: 0.2828, validation loss: 0.2505
2024-05-23 20:31:29 [INFO]: Epoch 082 - training loss: 0.2796, validation loss: 0.2468
2024-05-23 20:31:29 [INFO]: Epoch 083 - training loss: 0.2785, validation loss: 0.2461
2024-05-23 20:31:30 [INFO]: Epoch 084 - training loss: 0.2787, validation loss: 0.2493
2024-05-23 20:31:31 [INFO]: Epoch 085 - training loss: 0.2793, validation loss: 0.2580
2024-05-23 20:31:31 [INFO]: Epoch 086 - training loss: 0.2805, validation loss: 0.2513
2024-05-23 20:31:32 [INFO]: Epoch 087 - training loss: 0.2759, validation loss: 0.2505
2024-05-23 20:31:33 [INFO]: Epoch 088 - training loss: 0.2772, validation loss: 0.2457
2024-05-23 20:31:33 [INFO]: Epoch 089 - training loss: 0.2761, validation loss: 0.2432
2024-05-23 20:31:34 [INFO]: Epoch 090 - training loss: 0.2754, validation loss: 0.2445
2024-05-23 20:31:35 [INFO]: Epoch 091 - training loss: 0.2748, validation loss: 0.2439
2024-05-23 20:31:36 [INFO]: Epoch 092 - training loss: 0.2733, validation loss: 0.2422
2024-05-23 20:31:36 [INFO]: Epoch 093 - training loss: 0.2759, validation loss: 0.2434
2024-05-23 20:31:37 [INFO]: Epoch 094 - training loss: 0.2740, validation loss: 0.2428
2024-05-23 20:31:38 [INFO]: Epoch 095 - training loss: 0.2742, validation loss: 0.2410
2024-05-23 20:31:38 [INFO]: Epoch 096 - training loss: 0.2729, validation loss: 0.2378
2024-05-23 20:31:39 [INFO]: Epoch 097 - training loss: 0.2708, validation loss: 0.2452
2024-05-23 20:31:40 [INFO]: Epoch 098 - training loss: 0.2723, validation loss: 0.2431
2024-05-23 20:31:40 [INFO]: Epoch 099 - training loss: 0.2717, validation loss: 0.2354
2024-05-23 20:31:41 [INFO]: Epoch 100 - training loss: 0.2713, validation loss: 0.2385
2024-05-23 20:31:42 [INFO]: Epoch 101 - training loss: 0.2710, validation loss: 0.2397
2024-05-23 20:31:42 [INFO]: Epoch 102 - training loss: 0.2707, validation loss: 0.2345
2024-05-23 20:31:43 [INFO]: Epoch 103 - training loss: 0.2680, validation loss: 0.2311
2024-05-23 20:31:44 [INFO]: Epoch 104 - training loss: 0.2693, validation loss: 0.2310
2024-05-23 20:31:45 [INFO]: Epoch 105 - training loss: 0.2680, validation loss: 0.2258
2024-05-23 20:31:45 [INFO]: Epoch 106 - training loss: 0.2684, validation loss: 0.2268
2024-05-23 20:31:46 [INFO]: Epoch 107 - training loss: 0.2681, validation loss: 0.2283
2024-05-23 20:31:47 [INFO]: Epoch 108 - training loss: 0.2670, validation loss: 0.2307
2024-05-23 20:31:47 [INFO]: Epoch 109 - training loss: 0.2649, validation loss: 0.2265
2024-05-23 20:31:48 [INFO]: Epoch 110 - training loss: 0.2660, validation loss: 0.2256
2024-05-23 20:31:49 [INFO]: Epoch 111 - training loss: 0.2674, validation loss: 0.2291
2024-05-23 20:31:49 [INFO]: Epoch 112 - training loss: 0.2658, validation loss: 0.2255
2024-05-23 20:31:50 [INFO]: Epoch 113 - training loss: 0.2683, validation loss: 0.2329
2024-05-23 20:31:51 [INFO]: Epoch 114 - training loss: 0.2644, validation loss: 0.2284
2024-05-23 20:31:52 [INFO]: Epoch 115 - training loss: 0.2625, validation loss: 0.2338
2024-05-23 20:31:52 [INFO]: Epoch 116 - training loss: 0.2627, validation loss: 0.2275
2024-05-23 20:31:53 [INFO]: Epoch 117 - training loss: 0.2629, validation loss: 0.2247
2024-05-23 20:31:54 [INFO]: Epoch 118 - training loss: 0.2636, validation loss: 0.2270
2024-05-23 20:31:54 [INFO]: Epoch 119 - training loss: 0.2620, validation loss: 0.2300
2024-05-23 20:31:55 [INFO]: Epoch 120 - training loss: 0.2630, validation loss: 0.2227
2024-05-23 20:31:56 [INFO]: Epoch 121 - training loss: 0.2605, validation loss: 0.2240
2024-05-23 20:31:56 [INFO]: Epoch 122 - training loss: 0.2609, validation loss: 0.2245
2024-05-23 20:31:57 [INFO]: Epoch 123 - training loss: 0.2605, validation loss: 0.2255
2024-05-23 20:31:58 [INFO]: Epoch 124 - training loss: 0.2615, validation loss: 0.2232
2024-05-23 20:31:59 [INFO]: Epoch 125 - training loss: 0.2626, validation loss: 0.2232
2024-05-23 20:31:59 [INFO]: Epoch 126 - training loss: 0.2596, validation loss: 0.2231
2024-05-23 20:32:00 [INFO]: Epoch 127 - training loss: 0.2624, validation loss: 0.2246
2024-05-23 20:32:01 [INFO]: Epoch 128 - training loss: 0.2599, validation loss: 0.2212
2024-05-23 20:32:01 [INFO]: Epoch 129 - training loss: 0.2594, validation loss: 0.2230
2024-05-23 20:32:02 [INFO]: Epoch 130 - training loss: 0.2599, validation loss: 0.2296
2024-05-23 20:32:03 [INFO]: Epoch 131 - training loss: 0.2611, validation loss: 0.2237
2024-05-23 20:32:03 [INFO]: Epoch 132 - training loss: 0.2585, validation loss: 0.2206
2024-05-23 20:32:04 [INFO]: Epoch 133 - training loss: 0.2569, validation loss: 0.2254
2024-05-23 20:32:05 [INFO]: Epoch 134 - training loss: 0.2594, validation loss: 0.2223
2024-05-23 20:32:06 [INFO]: Epoch 135 - training loss: 0.2570, validation loss: 0.2259
2024-05-23 20:32:06 [INFO]: Epoch 136 - training loss: 0.2554, validation loss: 0.2243
2024-05-23 20:32:07 [INFO]: Epoch 137 - training loss: 0.2570, validation loss: 0.2228
2024-05-23 20:32:08 [INFO]: Epoch 138 - training loss: 0.2580, validation loss: 0.2221
2024-05-23 20:32:08 [INFO]: Epoch 139 - training loss: 0.2553, validation loss: 0.2220
2024-05-23 20:32:09 [INFO]: Epoch 140 - training loss: 0.2550, validation loss: 0.2182
2024-05-23 20:32:10 [INFO]: Epoch 141 - training loss: 0.2534, validation loss: 0.2208
2024-05-23 20:32:10 [INFO]: Epoch 142 - training loss: 0.2538, validation loss: 0.2199
2024-05-23 20:32:11 [INFO]: Epoch 143 - training loss: 0.2541, validation loss: 0.2204
2024-05-23 20:32:12 [INFO]: Epoch 144 - training loss: 0.2546, validation loss: 0.2218
2024-05-23 20:32:13 [INFO]: Epoch 145 - training loss: 0.2543, validation loss: 0.2192
2024-05-23 20:32:13 [INFO]: Epoch 146 - training loss: 0.2536, validation loss: 0.2219
2024-05-23 20:32:14 [INFO]: Epoch 147 - training loss: 0.2575, validation loss: 0.2202
2024-05-23 20:32:15 [INFO]: Epoch 148 - training loss: 0.2554, validation loss: 0.2198
2024-05-23 20:32:15 [INFO]: Epoch 149 - training loss: 0.2535, validation loss: 0.2249
2024-05-23 20:32:16 [INFO]: Epoch 150 - training loss: 0.2580, validation loss: 0.2205
2024-05-23 20:32:16 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-05-23 20:32:16 [INFO]: Finished training. The best model is from epoch#140.
2024-05-23 20:32:16 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_1/Transformer_physionet_2012_seta/20240523_T203031/Transformer.pypots
2024-05-23 20:32:16 [INFO]: Transformer on PhysioNet-2012-seta: MAE=0.2170, MSE=0.2092
2024-05-23 20:32:16 [INFO]: Successfully saved to overlay_minibatchmasking_saved_results/round_1/Transformer_physionet_2012_seta/imputation.pkl
2024-05-23 20:32:16 [INFO]: Using the given device: cuda:0
2024-05-23 20:32:16 [INFO]: Model files will be saved to overlay_minibatchmasking_saved_results/round_1/TimesNet_physionet_2012_seta/20240523_T203216
2024-05-23 20:32:16 [INFO]: Tensorboard file will be saved to overlay_minibatchmasking_saved_results/round_1/TimesNet_physionet_2012_seta/20240523_T203216/tensorboard
2024-05-23 20:32:17 [INFO]: TimesNet initialized with the given hyperparameters, the number of trainable parameters: 21,649,573
2024-05-23 20:32:17 [INFO]: Epoch 001 - training loss: 0.4516, validation loss: 1.2025
2024-05-23 20:32:18 [INFO]: Epoch 002 - training loss: 1.2129, validation loss: 0.4140
2024-05-23 20:32:19 [INFO]: Epoch 003 - training loss: 0.4698, validation loss: 0.5129
2024-05-23 20:32:20 [INFO]: Epoch 004 - training loss: 0.5422, validation loss: 0.3384
2024-05-23 20:32:21 [INFO]: Epoch 005 - training loss: 0.3419, validation loss: 0.3515
2024-05-23 20:32:21 [INFO]: Epoch 006 - training loss: 0.3524, validation loss: 0.3354
2024-05-23 20:32:22 [INFO]: Epoch 007 - training loss: 0.3997, validation loss: 0.3625
2024-05-23 20:32:23 [INFO]: Epoch 008 - training loss: 0.3695, validation loss: 0.3328
2024-05-23 20:32:24 [INFO]: Epoch 009 - training loss: 0.4011, validation loss: 0.3275
2024-05-23 20:32:25 [INFO]: Epoch 010 - training loss: 0.3181, validation loss: 0.3155
2024-05-23 20:32:25 [INFO]: Epoch 011 - training loss: 0.3222, validation loss: 0.3104
2024-05-23 20:32:26 [INFO]: Epoch 012 - training loss: 0.3693, validation loss: 0.3293
2024-05-23 20:32:27 [INFO]: Epoch 013 - training loss: 0.3528, validation loss: 0.3806
2024-05-23 20:32:28 [INFO]: Epoch 014 - training loss: 0.3710, validation loss: 0.3126
2024-05-23 20:32:29 [INFO]: Epoch 015 - training loss: 0.3478, validation loss: 0.3384
2024-05-23 20:32:30 [INFO]: Epoch 016 - training loss: 0.2995, validation loss: 0.3058
2024-05-23 20:32:30 [INFO]: Epoch 017 - training loss: 0.3997, validation loss: 0.3026
2024-05-23 20:32:31 [INFO]: Epoch 018 - training loss: 0.3416, validation loss: 0.3133
2024-05-23 20:32:32 [INFO]: Epoch 019 - training loss: 0.3566, validation loss: 0.3095
2024-05-23 20:32:33 [INFO]: Epoch 020 - training loss: 0.3086, validation loss: 0.2991
2024-05-23 20:32:34 [INFO]: Epoch 021 - training loss: 0.3035, validation loss: 0.2892
2024-05-23 20:32:34 [INFO]: Epoch 022 - training loss: 0.3068, validation loss: 0.2842
2024-05-23 20:32:35 [INFO]: Epoch 023 - training loss: 0.3110, validation loss: 0.2988
2024-05-23 20:32:36 [INFO]: Epoch 024 - training loss: 0.3207, validation loss: 0.3019
2024-05-23 20:32:37 [INFO]: Epoch 025 - training loss: 0.3161, validation loss: 0.2940
2024-05-23 20:32:38 [INFO]: Epoch 026 - training loss: 0.3164, validation loss: 0.2866
2024-05-23 20:32:38 [INFO]: Epoch 027 - training loss: 0.3017, validation loss: 0.2808
2024-05-23 20:32:39 [INFO]: Epoch 028 - training loss: 0.3236, validation loss: 0.2795
2024-05-23 20:32:40 [INFO]: Epoch 029 - training loss: 0.3165, validation loss: 0.3547
2024-05-23 20:32:41 [INFO]: Epoch 030 - training loss: 0.3181, validation loss: 0.2898
2024-05-23 20:32:42 [INFO]: Epoch 031 - training loss: 0.3424, validation loss: 0.3263
2024-05-23 20:32:42 [INFO]: Epoch 032 - training loss: 0.3210, validation loss: 0.3078
2024-05-23 20:32:43 [INFO]: Epoch 033 - training loss: 0.3266, validation loss: 0.2842
2024-05-23 20:32:44 [INFO]: Epoch 034 - training loss: 0.3563, validation loss: 0.2896
2024-05-23 20:32:45 [INFO]: Epoch 035 - training loss: 0.3294, validation loss: 0.2827
2024-05-23 20:32:46 [INFO]: Epoch 036 - training loss: 0.3078, validation loss: 0.2772
2024-05-23 20:32:47 [INFO]: Epoch 037 - training loss: 0.2903, validation loss: 0.2891
2024-05-23 20:32:47 [INFO]: Epoch 038 - training loss: 0.2980, validation loss: 0.2854
2024-05-23 20:32:48 [INFO]: Epoch 039 - training loss: 0.2943, validation loss: 0.2954
2024-05-23 20:32:49 [INFO]: Epoch 040 - training loss: 0.2937, validation loss: 0.2917
2024-05-23 20:32:50 [INFO]: Epoch 041 - training loss: 0.2955, validation loss: 0.2796
2024-05-23 20:32:51 [INFO]: Epoch 042 - training loss: 0.3088, validation loss: 0.2755
2024-05-23 20:32:51 [INFO]: Epoch 043 - training loss: 0.2943, validation loss: 0.2748
2024-05-23 20:32:52 [INFO]: Epoch 044 - training loss: 0.3100, validation loss: 0.2747
2024-05-23 20:32:53 [INFO]: Epoch 045 - training loss: 0.3755, validation loss: 0.2731
2024-05-23 20:32:54 [INFO]: Epoch 046 - training loss: 0.2757, validation loss: 0.2749
2024-05-23 20:32:55 [INFO]: Epoch 047 - training loss: 0.3149, validation loss: 0.2782
2024-05-23 20:32:55 [INFO]: Epoch 048 - training loss: 0.2867, validation loss: 0.2705
2024-05-23 20:32:56 [INFO]: Epoch 049 - training loss: 0.2947, validation loss: 0.2668
2024-05-23 20:32:57 [INFO]: Epoch 050 - training loss: 0.2954, validation loss: 0.2892
2024-05-23 20:32:58 [INFO]: Epoch 051 - training loss: 0.2782, validation loss: 0.2705
2024-05-23 20:32:59 [INFO]: Epoch 052 - training loss: 0.2739, validation loss: 0.2902
2024-05-23 20:32:59 [INFO]: Epoch 053 - training loss: 0.3285, validation loss: 0.3118
2024-05-23 20:33:00 [INFO]: Epoch 054 - training loss: 0.3192, validation loss: 0.2865
2024-05-23 20:33:01 [INFO]: Epoch 055 - training loss: 0.3021, validation loss: 0.2919
2024-05-23 20:33:02 [INFO]: Epoch 056 - training loss: 0.3288, validation loss: 0.2734
2024-05-23 20:33:03 [INFO]: Epoch 057 - training loss: 0.3419, validation loss: 0.2782
2024-05-23 20:33:04 [INFO]: Epoch 058 - training loss: 0.3031, validation loss: 0.2669
2024-05-23 20:33:04 [INFO]: Epoch 059 - training loss: 0.2786, validation loss: 0.2760
2024-05-23 20:33:04 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-05-23 20:33:04 [INFO]: Finished training. The best model is from epoch#49.
2024-05-23 20:33:04 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_1/TimesNet_physionet_2012_seta/20240523_T203216/TimesNet.pypots
2024-05-23 20:33:04 [INFO]: TimesNet on PhysioNet-2012-seta: MAE=0.2690, MSE=0.2347
2024-05-23 20:33:05 [INFO]: Successfully saved to overlay_minibatchmasking_saved_results/round_1/TimesNet_physionet_2012_seta/imputation.pkl
2024-05-23 20:33:05 [INFO]: Using the given device: cuda:0
2024-05-23 20:33:05 [INFO]: Model files will be saved to overlay_minibatchmasking_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T203305
2024-05-23 20:33:05 [INFO]: Tensorboard file will be saved to overlay_minibatchmasking_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T203305/tensorboard
2024-05-23 20:33:05 [INFO]: CSDI initialized with the given hyperparameters, the number of trainable parameters: 1,694,753
2024-05-23 20:33:48 [INFO]: Epoch 001 - training loss: 0.4114, validation loss: 0.3432
2024-05-23 20:33:48 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T203305/CSDI_epoch1_loss0.3431821972131729.pypots
2024-05-23 20:34:32 [INFO]: Epoch 002 - training loss: 0.3226, validation loss: 0.2861
2024-05-23 20:34:32 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T203305/CSDI_epoch2_loss0.28608074933290484.pypots
2024-05-23 20:35:15 [INFO]: Epoch 003 - training loss: 0.2864, validation loss: 0.2491
2024-05-23 20:35:15 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T203305/CSDI_epoch3_loss0.2491081640124321.pypots
2024-05-23 20:35:59 [INFO]: Epoch 004 - training loss: 0.2697, validation loss: 0.2306
2024-05-23 20:35:59 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T203305/CSDI_epoch4_loss0.2305635057389736.pypots
2024-05-23 20:36:43 [INFO]: Epoch 005 - training loss: 0.2647, validation loss: 0.2205
2024-05-23 20:36:43 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T203305/CSDI_epoch5_loss0.22052484825253488.pypots
2024-05-23 20:37:27 [INFO]: Epoch 006 - training loss: 0.2554, validation loss: 0.2141
2024-05-23 20:37:27 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T203305/CSDI_epoch6_loss0.2141347773373127.pypots
2024-05-23 20:38:11 [INFO]: Epoch 007 - training loss: 0.2679, validation loss: 0.2109
2024-05-23 20:38:11 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T203305/CSDI_epoch7_loss0.21090581342577935.pypots
2024-05-23 20:38:55 [INFO]: Epoch 008 - training loss: 0.2336, validation loss: 0.2071
2024-05-23 20:38:55 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T203305/CSDI_epoch8_loss0.20713210105895996.pypots
2024-05-23 20:39:38 [INFO]: Epoch 009 - training loss: 0.2377, validation loss: 0.2045
2024-05-23 20:39:38 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T203305/CSDI_epoch9_loss0.20454260259866713.pypots
2024-05-23 20:40:22 [INFO]: Epoch 010 - training loss: 0.2567, validation loss: 0.1974
2024-05-23 20:40:22 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T203305/CSDI_epoch10_loss0.1974194623529911.pypots
2024-05-23 20:41:06 [INFO]: Epoch 011 - training loss: 0.2358, validation loss: 0.1991
2024-05-23 20:41:06 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T203305/CSDI_epoch11_loss0.19907912313938142.pypots
2024-05-23 20:41:50 [INFO]: Epoch 012 - training loss: 0.2359, validation loss: 0.1994
2024-05-23 20:41:50 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T203305/CSDI_epoch12_loss0.19937721267342567.pypots
2024-05-23 20:42:34 [INFO]: Epoch 013 - training loss: 0.2373, validation loss: 0.1963
2024-05-23 20:42:34 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T203305/CSDI_epoch13_loss0.19634872674942017.pypots
2024-05-23 20:43:18 [INFO]: Epoch 014 - training loss: 0.2367, validation loss: 0.1967
2024-05-23 20:43:18 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T203305/CSDI_epoch14_loss0.19668901711702347.pypots
2024-05-23 20:44:02 [INFO]: Epoch 015 - training loss: 0.2308, validation loss: 0.1933
2024-05-23 20:44:02 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T203305/CSDI_epoch15_loss0.19327252954244614.pypots
2024-05-23 20:44:45 [INFO]: Epoch 016 - training loss: 0.2396, validation loss: 0.1929
2024-05-23 20:44:45 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T203305/CSDI_epoch16_loss0.19294654428958893.pypots
2024-05-23 20:45:29 [INFO]: Epoch 017 - training loss: 0.2470, validation loss: 0.1907
2024-05-23 20:45:29 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T203305/CSDI_epoch17_loss0.19065016135573387.pypots
2024-05-23 20:46:13 [INFO]: Epoch 018 - training loss: 0.2256, validation loss: 0.1922
2024-05-23 20:46:13 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T203305/CSDI_epoch18_loss0.19215852692723273.pypots
2024-05-23 20:46:57 [INFO]: Epoch 019 - training loss: 0.2349, validation loss: 0.1894
2024-05-23 20:46:57 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T203305/CSDI_epoch19_loss0.18944485783576964.pypots
2024-05-23 20:47:41 [INFO]: Epoch 020 - training loss: 0.2282, validation loss: 0.1890
2024-05-23 20:47:41 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T203305/CSDI_epoch20_loss0.18898154571652412.pypots
2024-05-23 20:48:25 [INFO]: Epoch 021 - training loss: 0.2359, validation loss: 0.1895
2024-05-23 20:48:25 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T203305/CSDI_epoch21_loss0.1894697554409504.pypots
2024-05-23 20:49:09 [INFO]: Epoch 022 - training loss: 0.2276, validation loss: 0.1886
2024-05-23 20:49:09 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T203305/CSDI_epoch22_loss0.18858067840337753.pypots
2024-05-23 20:49:52 [INFO]: Epoch 023 - training loss: 0.2371, validation loss: 0.1957
2024-05-23 20:49:53 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T203305/CSDI_epoch23_loss0.19567485004663468.pypots
2024-05-23 20:50:36 [INFO]: Epoch 024 - training loss: 0.2290, validation loss: 0.1887
2024-05-23 20:50:36 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T203305/CSDI_epoch24_loss0.18869636505842208.pypots
2024-05-23 20:51:20 [INFO]: Epoch 025 - training loss: 0.2246, validation loss: 0.1878
2024-05-23 20:51:20 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T203305/CSDI_epoch25_loss0.18780077993869781.pypots
2024-05-23 20:52:04 [INFO]: Epoch 026 - training loss: 0.2249, validation loss: 0.1849
2024-05-23 20:52:04 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T203305/CSDI_epoch26_loss0.18491950035095214.pypots
2024-05-23 20:52:48 [INFO]: Epoch 027 - training loss: 0.2348, validation loss: 0.1888
2024-05-23 20:52:48 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T203305/CSDI_epoch27_loss0.18880238831043245.pypots
2024-05-23 20:53:32 [INFO]: Epoch 028 - training loss: 0.2259, validation loss: 0.1854
2024-05-23 20:53:32 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T203305/CSDI_epoch28_loss0.18540361151099205.pypots
2024-05-23 20:54:16 [INFO]: Epoch 029 - training loss: 0.2259, validation loss: 0.1863
2024-05-23 20:54:16 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T203305/CSDI_epoch29_loss0.1863141380250454.pypots
2024-05-23 20:55:00 [INFO]: Epoch 030 - training loss: 0.2263, validation loss: 0.1837
2024-05-23 20:55:00 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T203305/CSDI_epoch30_loss0.18372823521494866.pypots
2024-05-23 20:55:44 [INFO]: Epoch 031 - training loss: 0.2270, validation loss: 0.1829
2024-05-23 20:55:44 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T203305/CSDI_epoch31_loss0.18291129767894745.pypots
2024-05-23 20:56:27 [INFO]: Epoch 032 - training loss: 0.2333, validation loss: 0.1814
2024-05-23 20:56:27 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T203305/CSDI_epoch32_loss0.18137822076678276.pypots
2024-05-23 20:57:11 [INFO]: Epoch 033 - training loss: 0.2348, validation loss: 0.1863
2024-05-23 20:57:11 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T203305/CSDI_epoch33_loss0.1862611584365368.pypots
2024-05-23 20:57:55 [INFO]: Epoch 034 - training loss: 0.2230, validation loss: 0.1815
2024-05-23 20:57:55 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T203305/CSDI_epoch34_loss0.1814688503742218.pypots
2024-05-23 20:58:39 [INFO]: Epoch 035 - training loss: 0.2187, validation loss: 0.1840
2024-05-23 20:58:39 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T203305/CSDI_epoch35_loss0.18404037207365037.pypots
2024-05-23 20:59:23 [INFO]: Epoch 036 - training loss: 0.2188, validation loss: 0.1808
2024-05-23 20:59:23 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T203305/CSDI_epoch36_loss0.18078488558530809.pypots
2024-05-23 21:00:07 [INFO]: Epoch 037 - training loss: 0.2113, validation loss: 0.1810
2024-05-23 21:00:07 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T203305/CSDI_epoch37_loss0.18101688772439956.pypots
2024-05-23 21:00:51 [INFO]: Epoch 038 - training loss: 0.2214, validation loss: 0.1809
2024-05-23 21:00:51 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T203305/CSDI_epoch38_loss0.18088518902659417.pypots
2024-05-23 21:01:35 [INFO]: Epoch 039 - training loss: 0.2221, validation loss: 0.1823
2024-05-23 21:01:35 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T203305/CSDI_epoch39_loss0.18229250982403755.pypots
2024-05-23 21:02:19 [INFO]: Epoch 040 - training loss: 0.2324, validation loss: 0.1792
2024-05-23 21:02:19 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T203305/CSDI_epoch40_loss0.17919494584202766.pypots
2024-05-23 21:03:03 [INFO]: Epoch 041 - training loss: 0.2148, validation loss: 0.1870
2024-05-23 21:03:03 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T203305/CSDI_epoch41_loss0.18695870712399482.pypots
2024-05-23 21:03:47 [INFO]: Epoch 042 - training loss: 0.2246, validation loss: 0.1813
2024-05-23 21:03:47 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T203305/CSDI_epoch42_loss0.1813141368329525.pypots
2024-05-23 21:04:31 [INFO]: Epoch 043 - training loss: 0.2246, validation loss: 0.1788
2024-05-23 21:04:31 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T203305/CSDI_epoch43_loss0.1787728525698185.pypots
2024-05-23 21:05:15 [INFO]: Epoch 044 - training loss: 0.2274, validation loss: 0.1782
2024-05-23 21:05:15 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T203305/CSDI_epoch44_loss0.1781522251665592.pypots
2024-05-23 21:05:59 [INFO]: Epoch 045 - training loss: 0.2277, validation loss: 0.1796
2024-05-23 21:05:59 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T203305/CSDI_epoch45_loss0.1796168193221092.pypots
2024-05-23 21:06:43 [INFO]: Epoch 046 - training loss: 0.2247, validation loss: 0.1792
2024-05-23 21:06:43 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T203305/CSDI_epoch46_loss0.17922863438725473.pypots
2024-05-23 21:07:28 [INFO]: Epoch 047 - training loss: 0.2167, validation loss: 0.1771
2024-05-23 21:07:28 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T203305/CSDI_epoch47_loss0.17712447419762611.pypots
2024-05-23 21:08:12 [INFO]: Epoch 048 - training loss: 0.2196, validation loss: 0.1782
2024-05-23 21:08:12 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T203305/CSDI_epoch48_loss0.1781547710299492.pypots
2024-05-23 21:08:56 [INFO]: Epoch 049 - training loss: 0.2220, validation loss: 0.1775
2024-05-23 21:08:56 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T203305/CSDI_epoch49_loss0.17745634838938712.pypots
2024-05-23 21:09:40 [INFO]: Epoch 050 - training loss: 0.2203, validation loss: 0.1824
2024-05-23 21:09:40 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T203305/CSDI_epoch50_loss0.18240462318062783.pypots
2024-05-23 21:10:24 [INFO]: Epoch 051 - training loss: 0.2105, validation loss: 0.1787
2024-05-23 21:10:24 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T203305/CSDI_epoch51_loss0.17870813682675363.pypots
2024-05-23 21:11:08 [INFO]: Epoch 052 - training loss: 0.2139, validation loss: 0.1785
2024-05-23 21:11:08 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T203305/CSDI_epoch52_loss0.17849060669541358.pypots
2024-05-23 21:11:53 [INFO]: Epoch 053 - training loss: 0.2211, validation loss: 0.1801
2024-05-23 21:11:53 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T203305/CSDI_epoch53_loss0.18012386187911034.pypots
2024-05-23 21:12:37 [INFO]: Epoch 054 - training loss: 0.2168, validation loss: 0.1777
2024-05-23 21:12:37 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T203305/CSDI_epoch54_loss0.17770672217011452.pypots
2024-05-23 21:13:21 [INFO]: Epoch 055 - training loss: 0.2266, validation loss: 0.1771
2024-05-23 21:13:21 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T203305/CSDI_epoch55_loss0.17710085287690164.pypots
2024-05-23 21:14:05 [INFO]: Epoch 056 - training loss: 0.2201, validation loss: 0.1778
2024-05-23 21:14:05 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T203305/CSDI_epoch56_loss0.17780620530247687.pypots
2024-05-23 21:14:49 [INFO]: Epoch 057 - training loss: 0.2222, validation loss: 0.1757
2024-05-23 21:14:49 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T203305/CSDI_epoch57_loss0.17572575211524963.pypots
2024-05-23 21:15:33 [INFO]: Epoch 058 - training loss: 0.2188, validation loss: 0.1769
2024-05-23 21:15:33 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T203305/CSDI_epoch58_loss0.17694433480501176.pypots
2024-05-23 21:16:17 [INFO]: Epoch 059 - training loss: 0.2191, validation loss: 0.1767
2024-05-23 21:16:17 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T203305/CSDI_epoch59_loss0.17669666036963463.pypots
2024-05-23 21:17:01 [INFO]: Epoch 060 - training loss: 0.2179, validation loss: 0.1782
2024-05-23 21:17:01 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T203305/CSDI_epoch60_loss0.17815627232193948.pypots
2024-05-23 21:17:44 [INFO]: Epoch 061 - training loss: 0.2059, validation loss: 0.1831
2024-05-23 21:17:44 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T203305/CSDI_epoch61_loss0.1830724745988846.pypots
2024-05-23 21:18:28 [INFO]: Epoch 062 - training loss: 0.2211, validation loss: 0.1736
2024-05-23 21:18:28 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T203305/CSDI_epoch62_loss0.1735984705388546.pypots
2024-05-23 21:19:12 [INFO]: Epoch 063 - training loss: 0.2278, validation loss: 0.1742
2024-05-23 21:19:12 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T203305/CSDI_epoch63_loss0.17423627525568008.pypots
2024-05-23 21:19:56 [INFO]: Epoch 064 - training loss: 0.2234, validation loss: 0.1748
2024-05-23 21:19:56 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T203305/CSDI_epoch64_loss0.17478740960359573.pypots
2024-05-23 21:20:40 [INFO]: Epoch 065 - training loss: 0.2106, validation loss: 0.1750
2024-05-23 21:20:40 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T203305/CSDI_epoch65_loss0.17498960271477698.pypots
2024-05-23 21:21:24 [INFO]: Epoch 066 - training loss: 0.2156, validation loss: 0.1747
2024-05-23 21:21:24 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T203305/CSDI_epoch66_loss0.1746653936803341.pypots
2024-05-23 21:22:07 [INFO]: Epoch 067 - training loss: 0.2050, validation loss: 0.1753
2024-05-23 21:22:07 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T203305/CSDI_epoch67_loss0.1753331333398819.pypots
2024-05-23 21:22:51 [INFO]: Epoch 068 - training loss: 0.2208, validation loss: 0.1756
2024-05-23 21:22:51 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T203305/CSDI_epoch68_loss0.17558056712150574.pypots
2024-05-23 21:23:35 [INFO]: Epoch 069 - training loss: 0.2286, validation loss: 0.1738
2024-05-23 21:23:35 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T203305/CSDI_epoch69_loss0.17376840561628343.pypots
2024-05-23 21:24:19 [INFO]: Epoch 070 - training loss: 0.2186, validation loss: 0.1732
2024-05-23 21:24:19 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T203305/CSDI_epoch70_loss0.17318819090723991.pypots
2024-05-23 21:25:03 [INFO]: Epoch 071 - training loss: 0.2159, validation loss: 0.1743
2024-05-23 21:25:03 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T203305/CSDI_epoch71_loss0.17434634119272233.pypots
2024-05-23 21:25:47 [INFO]: Epoch 072 - training loss: 0.2117, validation loss: 0.1739
2024-05-23 21:25:47 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T203305/CSDI_epoch72_loss0.1738938048481941.pypots
2024-05-23 21:26:31 [INFO]: Epoch 073 - training loss: 0.2102, validation loss: 0.1754
2024-05-23 21:26:31 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T203305/CSDI_epoch73_loss0.1754281535744667.pypots
2024-05-23 21:27:15 [INFO]: Epoch 074 - training loss: 0.2115, validation loss: 0.1742
2024-05-23 21:27:15 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T203305/CSDI_epoch74_loss0.17418893799185753.pypots
2024-05-23 21:27:59 [INFO]: Epoch 075 - training loss: 0.2179, validation loss: 0.1733
2024-05-23 21:27:59 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T203305/CSDI_epoch75_loss0.17329737022519112.pypots
2024-05-23 21:28:43 [INFO]: Epoch 076 - training loss: 0.2270, validation loss: 0.1793
2024-05-23 21:28:43 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T203305/CSDI_epoch76_loss0.17928855046629905.pypots
2024-05-23 21:29:27 [INFO]: Epoch 077 - training loss: 0.2219, validation loss: 0.1722
2024-05-23 21:29:27 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T203305/CSDI_epoch77_loss0.17219725102186204.pypots
2024-05-23 21:30:10 [INFO]: Epoch 078 - training loss: 0.2119, validation loss: 0.1740
2024-05-23 21:30:11 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T203305/CSDI_epoch78_loss0.1739712156355381.pypots
2024-05-23 21:30:54 [INFO]: Epoch 079 - training loss: 0.2066, validation loss: 0.1729
2024-05-23 21:30:54 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T203305/CSDI_epoch79_loss0.17285202220082282.pypots
2024-05-23 21:31:38 [INFO]: Epoch 080 - training loss: 0.2103, validation loss: 0.1718
2024-05-23 21:31:38 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T203305/CSDI_epoch80_loss0.17184408456087114.pypots
2024-05-23 21:32:22 [INFO]: Epoch 081 - training loss: 0.2196, validation loss: 0.1726
2024-05-23 21:32:22 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T203305/CSDI_epoch81_loss0.17257680520415306.pypots
2024-05-23 21:33:06 [INFO]: Epoch 082 - training loss: 0.2196, validation loss: 0.1726
2024-05-23 21:33:06 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T203305/CSDI_epoch82_loss0.17260359302163125.pypots
2024-05-23 21:33:50 [INFO]: Epoch 083 - training loss: 0.2102, validation loss: 0.1720
2024-05-23 21:33:51 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T203305/CSDI_epoch83_loss0.1719878777861595.pypots
2024-05-23 21:34:34 [INFO]: Epoch 084 - training loss: 0.2196, validation loss: 0.1727
2024-05-23 21:34:34 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T203305/CSDI_epoch84_loss0.1726968064904213.pypots
2024-05-23 21:35:18 [INFO]: Epoch 085 - training loss: 0.2246, validation loss: 0.1770
2024-05-23 21:35:18 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T203305/CSDI_epoch85_loss0.17701162695884703.pypots
2024-05-23 21:36:02 [INFO]: Epoch 086 - training loss: 0.2099, validation loss: 0.1706
2024-05-23 21:36:02 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T203305/CSDI_epoch86_loss0.17063637897372247.pypots
2024-05-23 21:36:46 [INFO]: Epoch 087 - training loss: 0.2139, validation loss: 0.1702
2024-05-23 21:36:46 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T203305/CSDI_epoch87_loss0.17024395540356635.pypots
2024-05-23 21:37:30 [INFO]: Epoch 088 - training loss: 0.2128, validation loss: 0.1715
2024-05-23 21:37:30 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T203305/CSDI_epoch88_loss0.17146720141172409.pypots
2024-05-23 21:38:14 [INFO]: Epoch 089 - training loss: 0.2095, validation loss: 0.1718
2024-05-23 21:38:14 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T203305/CSDI_epoch89_loss0.171842709928751.pypots
2024-05-23 21:38:58 [INFO]: Epoch 090 - training loss: 0.1995, validation loss: 0.1726
2024-05-23 21:38:58 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T203305/CSDI_epoch90_loss0.17258976325392722.pypots
2024-05-23 21:39:42 [INFO]: Epoch 091 - training loss: 0.2079, validation loss: 0.1713
2024-05-23 21:39:42 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T203305/CSDI_epoch91_loss0.17128766253590583.pypots
2024-05-23 21:40:26 [INFO]: Epoch 092 - training loss: 0.2210, validation loss: 0.1700
2024-05-23 21:40:26 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T203305/CSDI_epoch92_loss0.16998815834522246.pypots
2024-05-23 21:41:10 [INFO]: Epoch 093 - training loss: 0.2154, validation loss: 0.1727
2024-05-23 21:41:10 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T203305/CSDI_epoch93_loss0.17267632782459258.pypots
2024-05-23 21:41:54 [INFO]: Epoch 094 - training loss: 0.2117, validation loss: 0.1755
2024-05-23 21:41:54 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T203305/CSDI_epoch94_loss0.17549978122115134.pypots
2024-05-23 21:42:38 [INFO]: Epoch 095 - training loss: 0.2103, validation loss: 0.1712
2024-05-23 21:42:38 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T203305/CSDI_epoch95_loss0.17116608917713166.pypots
2024-05-23 21:43:22 [INFO]: Epoch 096 - training loss: 0.2171, validation loss: 0.1740
2024-05-23 21:43:22 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T203305/CSDI_epoch96_loss0.17399973794817924.pypots
2024-05-23 21:44:06 [INFO]: Epoch 097 - training loss: 0.2119, validation loss: 0.1721
2024-05-23 21:44:06 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T203305/CSDI_epoch97_loss0.1721066989004612.pypots
2024-05-23 21:44:50 [INFO]: Epoch 098 - training loss: 0.2021, validation loss: 0.1721
2024-05-23 21:44:50 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T203305/CSDI_epoch98_loss0.17213608995079993.pypots
2024-05-23 21:45:34 [INFO]: Epoch 099 - training loss: 0.2129, validation loss: 0.1714
2024-05-23 21:45:34 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T203305/CSDI_epoch99_loss0.17136139050126076.pypots
2024-05-23 21:46:18 [INFO]: Epoch 100 - training loss: 0.2112, validation loss: 0.1708
2024-05-23 21:46:18 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T203305/CSDI_epoch100_loss0.1707887351512909.pypots
2024-05-23 21:47:02 [INFO]: Epoch 101 - training loss: 0.2116, validation loss: 0.1711
2024-05-23 21:47:02 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T203305/CSDI_epoch101_loss0.1711105413734913.pypots
2024-05-23 21:47:46 [INFO]: Epoch 102 - training loss: 0.2140, validation loss: 0.1718
2024-05-23 21:47:46 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T203305/CSDI_epoch102_loss0.17183116003870963.pypots
2024-05-23 21:47:46 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-05-23 21:47:46 [INFO]: Finished training. The best model is from epoch#92.
2024-05-23 21:47:46 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T203305/CSDI.pypots
2024-05-23 21:55:09 [INFO]: CSDI on PhysioNet-2012-seta: MAE=0.2299, MSE=0.2464
2024-05-23 22:24:34 [INFO]: Successfully saved to overlay_minibatchmasking_saved_results/round_1/CSDI_physionet_2012_seta/imputation.pkl
2024-05-23 22:24:34 [INFO]: Using the given device: cuda:0
2024-05-23 22:24:34 [INFO]: Model files will be saved to overlay_minibatchmasking_saved_results/round_1/GPVAE_physionet_2012_seta/20240523_T222434
2024-05-23 22:24:34 [INFO]: Tensorboard file will be saved to overlay_minibatchmasking_saved_results/round_1/GPVAE_physionet_2012_seta/20240523_T222434/tensorboard
2024-05-23 22:24:34 [INFO]: GPVAE initialized with the given hyperparameters, the number of trainable parameters: 1,028,244
2024-05-23 22:24:35 [INFO]: Epoch 001 - training loss: 42643.7757, validation loss: 0.9024
2024-05-23 22:24:35 [INFO]: Epoch 002 - training loss: 24422.4415, validation loss: 0.7437
2024-05-23 22:24:36 [INFO]: Epoch 003 - training loss: 23522.6077, validation loss: 0.7455
2024-05-23 22:24:36 [INFO]: Epoch 004 - training loss: 23229.0125, validation loss: 0.6879
2024-05-23 22:24:37 [INFO]: Epoch 005 - training loss: 23084.4357, validation loss: 0.6677
2024-05-23 22:24:37 [INFO]: Epoch 006 - training loss: 23003.3495, validation loss: 0.6605
2024-05-23 22:24:38 [INFO]: Epoch 007 - training loss: 22954.9524, validation loss: 0.6670
2024-05-23 22:24:39 [INFO]: Epoch 008 - training loss: 22923.0278, validation loss: 0.6587
2024-05-23 22:24:39 [INFO]: Epoch 009 - training loss: 22900.9260, validation loss: 0.6644
2024-05-23 22:24:40 [INFO]: Epoch 010 - training loss: 22885.4271, validation loss: 0.6554
2024-05-23 22:24:40 [INFO]: Epoch 011 - training loss: 22874.3032, validation loss: 0.6590
2024-05-23 22:24:41 [INFO]: Epoch 012 - training loss: 22866.2155, validation loss: 0.6699
2024-05-23 22:24:41 [INFO]: Epoch 013 - training loss: 22859.4220, validation loss: 0.6524
2024-05-23 22:24:42 [INFO]: Epoch 014 - training loss: 22854.1536, validation loss: 0.6468
2024-05-23 22:24:42 [INFO]: Epoch 015 - training loss: 22850.6692, validation loss: 0.6399
2024-05-23 22:24:43 [INFO]: Epoch 016 - training loss: 22848.5906, validation loss: 0.6375
2024-05-23 22:24:44 [INFO]: Epoch 017 - training loss: 22844.0409, validation loss: 0.6641
2024-05-23 22:24:44 [INFO]: Epoch 018 - training loss: 22841.7884, validation loss: 0.6321
2024-05-23 22:24:45 [INFO]: Epoch 019 - training loss: 22838.6900, validation loss: 0.6315
2024-05-23 22:24:45 [INFO]: Epoch 020 - training loss: 22836.3117, validation loss: 0.6255
2024-05-23 22:24:46 [INFO]: Epoch 021 - training loss: 22834.8331, validation loss: 0.6235
2024-05-23 22:24:46 [INFO]: Epoch 022 - training loss: 22833.1700, validation loss: 0.6326
2024-05-23 22:24:47 [INFO]: Epoch 023 - training loss: 22832.9842, validation loss: 0.6176
2024-05-23 22:24:47 [INFO]: Epoch 024 - training loss: 22833.7132, validation loss: 0.6208
2024-05-23 22:24:48 [INFO]: Epoch 025 - training loss: 22831.1148, validation loss: 0.6205
2024-05-23 22:24:49 [INFO]: Epoch 026 - training loss: 22829.5263, validation loss: 0.6168
2024-05-23 22:24:49 [INFO]: Epoch 027 - training loss: 22828.7307, validation loss: 0.6146
2024-05-23 22:24:50 [INFO]: Epoch 028 - training loss: 22828.7547, validation loss: 0.6114
2024-05-23 22:24:50 [INFO]: Epoch 029 - training loss: 22828.1097, validation loss: 0.6259
2024-05-23 22:24:51 [INFO]: Epoch 030 - training loss: 22828.8169, validation loss: 0.6112
2024-05-23 22:24:51 [INFO]: Epoch 031 - training loss: 22826.6548, validation loss: 0.6122
2024-05-23 22:24:52 [INFO]: Epoch 032 - training loss: 22825.7833, validation loss: 0.6109
2024-05-23 22:24:52 [INFO]: Epoch 033 - training loss: 22824.7688, validation loss: 0.6162
2024-05-23 22:24:53 [INFO]: Epoch 034 - training loss: 22824.9764, validation loss: 0.6126
2024-05-23 22:24:54 [INFO]: Epoch 035 - training loss: 22823.2058, validation loss: 0.6071
2024-05-23 22:24:54 [INFO]: Epoch 036 - training loss: 22821.9406, validation loss: 0.5967
2024-05-23 22:24:55 [INFO]: Epoch 037 - training loss: 22820.5993, validation loss: 0.5842
2024-05-23 22:24:55 [INFO]: Epoch 038 - training loss: 22818.4794, validation loss: 0.5853
2024-05-23 22:24:56 [INFO]: Epoch 039 - training loss: 22816.8851, validation loss: 0.5708
2024-05-23 22:24:56 [INFO]: Epoch 040 - training loss: 22814.4492, validation loss: 0.5635
2024-05-23 22:24:57 [INFO]: Epoch 041 - training loss: 22813.0897, validation loss: 0.5594
2024-05-23 22:24:57 [INFO]: Epoch 042 - training loss: 22811.5737, validation loss: 0.5575
2024-05-23 22:24:58 [INFO]: Epoch 043 - training loss: 22810.4046, validation loss: 0.5582
2024-05-23 22:24:59 [INFO]: Epoch 044 - training loss: 22809.5614, validation loss: 0.5635
2024-05-23 22:24:59 [INFO]: Epoch 045 - training loss: 22809.0849, validation loss: 0.5550
2024-05-23 22:25:00 [INFO]: Epoch 046 - training loss: 22808.1840, validation loss: 0.5473
2024-05-23 22:25:00 [INFO]: Epoch 047 - training loss: 22808.0822, validation loss: 0.5561
2024-05-23 22:25:01 [INFO]: Epoch 048 - training loss: 22806.4988, validation loss: 0.5485
2024-05-23 22:25:01 [INFO]: Epoch 049 - training loss: 22806.3960, validation loss: 0.5461
2024-05-23 22:25:02 [INFO]: Epoch 050 - training loss: 22805.1911, validation loss: 0.5422
2024-05-23 22:25:02 [INFO]: Epoch 051 - training loss: 22805.0365, validation loss: 0.5402
2024-05-23 22:25:03 [INFO]: Epoch 052 - training loss: 22804.8927, validation loss: 0.5500
2024-05-23 22:25:04 [INFO]: Epoch 053 - training loss: 22804.2987, validation loss: 0.5397
2024-05-23 22:25:04 [INFO]: Epoch 054 - training loss: 22803.0568, validation loss: 0.5388
2024-05-23 22:25:05 [INFO]: Epoch 055 - training loss: 22803.8197, validation loss: 0.5331
2024-05-23 22:25:05 [INFO]: Epoch 056 - training loss: 22801.6865, validation loss: 0.5317
2024-05-23 22:25:06 [INFO]: Epoch 057 - training loss: 22801.2060, validation loss: 0.5262
2024-05-23 22:25:06 [INFO]: Epoch 058 - training loss: 22801.3422, validation loss: 0.5402
2024-05-23 22:25:07 [INFO]: Epoch 059 - training loss: 22800.4434, validation loss: 0.5271
2024-05-23 22:25:07 [INFO]: Epoch 060 - training loss: 22800.0516, validation loss: 0.5328
2024-05-23 22:25:08 [INFO]: Epoch 061 - training loss: 22799.7872, validation loss: 0.5332
2024-05-23 22:25:09 [INFO]: Epoch 062 - training loss: 22799.4691, validation loss: 0.5320
2024-05-23 22:25:09 [INFO]: Epoch 063 - training loss: 22799.8320, validation loss: 0.5200
2024-05-23 22:25:10 [INFO]: Epoch 064 - training loss: 22798.3503, validation loss: 0.5226
2024-05-23 22:25:10 [INFO]: Epoch 065 - training loss: 22797.9314, validation loss: 0.5217
2024-05-23 22:25:11 [INFO]: Epoch 066 - training loss: 22797.1830, validation loss: 0.5157
2024-05-23 22:25:11 [INFO]: Epoch 067 - training loss: 22797.0012, validation loss: 0.5179
2024-05-23 22:25:12 [INFO]: Epoch 068 - training loss: 22797.0210, validation loss: 0.5183
2024-05-23 22:25:12 [INFO]: Epoch 069 - training loss: 22796.4935, validation loss: 0.5122
2024-05-23 22:25:13 [INFO]: Epoch 070 - training loss: 22796.0476, validation loss: 0.5276
2024-05-23 22:25:14 [INFO]: Epoch 071 - training loss: 22796.0240, validation loss: 0.5179
2024-05-23 22:25:14 [INFO]: Epoch 072 - training loss: 22795.1623, validation loss: 0.5158
2024-05-23 22:25:15 [INFO]: Epoch 073 - training loss: 22795.5457, validation loss: 0.5168
2024-05-23 22:25:15 [INFO]: Epoch 074 - training loss: 22795.3366, validation loss: 0.5311
2024-05-23 22:25:16 [INFO]: Epoch 075 - training loss: 22796.8323, validation loss: 0.5167
2024-05-23 22:25:16 [INFO]: Epoch 076 - training loss: 22796.5758, validation loss: 0.5267
2024-05-23 22:25:17 [INFO]: Epoch 077 - training loss: 22795.8593, validation loss: 0.5154
2024-05-23 22:25:17 [INFO]: Epoch 078 - training loss: 22794.3591, validation loss: 0.5138
2024-05-23 22:25:18 [INFO]: Epoch 079 - training loss: 22793.7311, validation loss: 0.5146
2024-05-23 22:25:18 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-05-23 22:25:18 [INFO]: Finished training. The best model is from epoch#69.
2024-05-23 22:25:18 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_1/GPVAE_physionet_2012_seta/20240523_T222434/GPVAE.pypots
2024-05-23 22:25:18 [INFO]: GP-VAE on PhysioNet-2012-seta: MAE=0.4330, MSE=0.4579
2024-05-23 22:25:18 [INFO]: Successfully saved to overlay_minibatchmasking_saved_results/round_1/GPVAE_physionet_2012_seta/imputation.pkl
2024-05-23 22:25:18 [INFO]: Using the given device: cuda:0
2024-05-23 22:25:18 [INFO]: Model files will be saved to overlay_minibatchmasking_saved_results/round_1/USGAN_physionet_2012_seta/20240523_T222518
2024-05-23 22:25:18 [INFO]: Tensorboard file will be saved to overlay_minibatchmasking_saved_results/round_1/USGAN_physionet_2012_seta/20240523_T222518/tensorboard
2024-05-23 22:25:19 [INFO]: USGAN initialized with the given hyperparameters, the number of trainable parameters: 16,010,261
2024-05-23 22:25:39 [INFO]: Epoch 001 - generator training loss: 0.6044, discriminator training loss: 0.3728, validation loss: 0.6211
2024-05-23 22:25:58 [INFO]: Epoch 002 - generator training loss: 0.4782, discriminator training loss: 0.2557, validation loss: 0.5210
2024-05-23 22:26:16 [INFO]: Epoch 003 - generator training loss: 0.4362, discriminator training loss: 0.2050, validation loss: 0.5037
2024-05-23 22:26:34 [INFO]: Epoch 004 - generator training loss: 0.4500, discriminator training loss: 0.1500, validation loss: 0.4913
2024-05-23 22:26:52 [INFO]: Epoch 005 - generator training loss: 0.4427, discriminator training loss: 0.1204, validation loss: 0.4720
2024-05-23 22:27:10 [INFO]: Epoch 006 - generator training loss: 0.4317, discriminator training loss: 0.1017, validation loss: 0.4553
2024-05-23 22:27:28 [INFO]: Epoch 007 - generator training loss: 0.4224, discriminator training loss: 0.0882, validation loss: 0.4462
2024-05-23 22:27:46 [INFO]: Epoch 008 - generator training loss: 0.4119, discriminator training loss: 0.0781, validation loss: 0.4412
2024-05-23 22:28:04 [INFO]: Epoch 009 - generator training loss: 0.4055, discriminator training loss: 0.0705, validation loss: 0.4298
2024-05-23 22:28:22 [INFO]: Epoch 010 - generator training loss: 0.3995, discriminator training loss: 0.0644, validation loss: 0.4168
2024-05-23 22:28:40 [INFO]: Epoch 011 - generator training loss: 0.3927, discriminator training loss: 0.0592, validation loss: 0.4144
2024-05-23 22:28:58 [INFO]: Epoch 012 - generator training loss: 0.3877, discriminator training loss: 0.0551, validation loss: 0.4063
2024-05-23 22:29:16 [INFO]: Epoch 013 - generator training loss: 0.3816, discriminator training loss: 0.0513, validation loss: 0.3998
2024-05-23 22:29:34 [INFO]: Epoch 014 - generator training loss: 0.3758, discriminator training loss: 0.0480, validation loss: 0.3963
2024-05-23 22:29:52 [INFO]: Epoch 015 - generator training loss: 0.3699, discriminator training loss: 0.0452, validation loss: 0.3908
2024-05-23 22:30:11 [INFO]: Epoch 016 - generator training loss: 0.3650, discriminator training loss: 0.0431, validation loss: 0.3949
2024-05-23 22:30:29 [INFO]: Epoch 017 - generator training loss: 0.3601, discriminator training loss: 0.0411, validation loss: 0.3826
2024-05-23 22:30:47 [INFO]: Epoch 018 - generator training loss: 0.3543, discriminator training loss: 0.0394, validation loss: 0.3811
2024-05-23 22:31:05 [INFO]: Epoch 019 - generator training loss: 0.3481, discriminator training loss: 0.0379, validation loss: 0.3747
2024-05-23 22:31:23 [INFO]: Epoch 020 - generator training loss: 0.3429, discriminator training loss: 0.0366, validation loss: 0.3715
2024-05-23 22:31:41 [INFO]: Epoch 021 - generator training loss: 0.3378, discriminator training loss: 0.0354, validation loss: 0.3719
2024-05-23 22:31:59 [INFO]: Epoch 022 - generator training loss: 0.3345, discriminator training loss: 0.0344, validation loss: 0.3625
2024-05-23 22:32:17 [INFO]: Epoch 023 - generator training loss: 0.3278, discriminator training loss: 0.0335, validation loss: 0.3603
2024-05-23 22:32:35 [INFO]: Epoch 024 - generator training loss: 0.3361, discriminator training loss: 0.0329, validation loss: 0.3605
2024-05-23 22:32:53 [INFO]: Epoch 025 - generator training loss: 0.3241, discriminator training loss: 0.0322, validation loss: 0.3556
2024-05-23 22:33:11 [INFO]: Epoch 026 - generator training loss: 0.3154, discriminator training loss: 0.0314, validation loss: 0.3518
2024-05-23 22:33:29 [INFO]: Epoch 027 - generator training loss: 0.3101, discriminator training loss: 0.0306, validation loss: 0.3462
2024-05-23 22:33:47 [INFO]: Epoch 028 - generator training loss: 0.3027, discriminator training loss: 0.0302, validation loss: 0.3441
2024-05-23 22:34:05 [INFO]: Epoch 029 - generator training loss: 0.3002, discriminator training loss: 0.0297, validation loss: 0.3389
2024-05-23 22:34:23 [INFO]: Epoch 030 - generator training loss: 0.2984, discriminator training loss: 0.0293, validation loss: 0.3370
2024-05-23 22:34:41 [INFO]: Epoch 031 - generator training loss: 0.2947, discriminator training loss: 0.0287, validation loss: 0.3415
2024-05-23 22:35:00 [INFO]: Epoch 032 - generator training loss: 0.2921, discriminator training loss: 0.0284, validation loss: 0.3352
2024-05-23 22:35:18 [INFO]: Epoch 033 - generator training loss: 0.2887, discriminator training loss: 0.0281, validation loss: 0.3358
2024-05-23 22:35:36 [INFO]: Epoch 034 - generator training loss: 0.2941, discriminator training loss: 0.0278, validation loss: 0.3332
2024-05-23 22:35:54 [INFO]: Epoch 035 - generator training loss: 0.2849, discriminator training loss: 0.0274, validation loss: 0.3266
2024-05-23 22:36:12 [INFO]: Epoch 036 - generator training loss: 0.2768, discriminator training loss: 0.0271, validation loss: 0.3259
2024-05-23 22:36:30 [INFO]: Epoch 037 - generator training loss: 0.2718, discriminator training loss: 0.0268, validation loss: 0.3237
2024-05-23 22:36:48 [INFO]: Epoch 038 - generator training loss: 0.2695, discriminator training loss: 0.0264, validation loss: 0.3212
2024-05-23 22:37:06 [INFO]: Epoch 039 - generator training loss: 0.2697, discriminator training loss: 0.0262, validation loss: 0.3221
2024-05-23 22:37:24 [INFO]: Epoch 040 - generator training loss: 0.2663, discriminator training loss: 0.0259, validation loss: 0.3187
2024-05-23 22:37:42 [INFO]: Epoch 041 - generator training loss: 0.2686, discriminator training loss: 0.0258, validation loss: 0.3152
2024-05-23 22:38:00 [INFO]: Epoch 042 - generator training loss: 0.2640, discriminator training loss: 0.0255, validation loss: 0.3201
2024-05-23 22:38:19 [INFO]: Epoch 043 - generator training loss: 0.2584, discriminator training loss: 0.0252, validation loss: 0.3198
2024-05-23 22:38:37 [INFO]: Epoch 044 - generator training loss: 0.2569, discriminator training loss: 0.0250, validation loss: 0.3168
2024-05-23 22:38:55 [INFO]: Epoch 045 - generator training loss: 0.2538, discriminator training loss: 0.0245, validation loss: 0.3196
2024-05-23 22:39:13 [INFO]: Epoch 046 - generator training loss: 0.2545, discriminator training loss: 0.0246, validation loss: 0.3132
2024-05-23 22:39:31 [INFO]: Epoch 047 - generator training loss: 0.2504, discriminator training loss: 0.0243, validation loss: 0.3113
2024-05-23 22:39:49 [INFO]: Epoch 048 - generator training loss: 0.2441, discriminator training loss: 0.0243, validation loss: 0.3091
2024-05-23 22:40:07 [INFO]: Epoch 049 - generator training loss: 0.2420, discriminator training loss: 0.0240, validation loss: 0.3117
2024-05-23 22:40:25 [INFO]: Epoch 050 - generator training loss: 0.2410, discriminator training loss: 0.0239, validation loss: 0.3115
2024-05-23 22:40:43 [INFO]: Epoch 051 - generator training loss: 0.2507, discriminator training loss: 0.0236, validation loss: 0.3121
2024-05-23 22:41:01 [INFO]: Epoch 052 - generator training loss: 0.2473, discriminator training loss: 0.0236, validation loss: 0.3102
2024-05-23 22:41:19 [INFO]: Epoch 053 - generator training loss: 0.2414, discriminator training loss: 0.0233, validation loss: 0.3162
2024-05-23 22:41:37 [INFO]: Epoch 054 - generator training loss: 0.2364, discriminator training loss: 0.0232, validation loss: 0.3134
2024-05-23 22:41:55 [INFO]: Epoch 055 - generator training loss: 0.2448, discriminator training loss: 0.0232, validation loss: 0.3077
2024-05-23 22:42:13 [INFO]: Epoch 056 - generator training loss: 0.2336, discriminator training loss: 0.0229, validation loss: 0.3088
2024-05-23 22:42:31 [INFO]: Epoch 057 - generator training loss: 0.2303, discriminator training loss: 0.0228, validation loss: 0.3087
2024-05-23 22:42:49 [INFO]: Epoch 058 - generator training loss: 0.2257, discriminator training loss: 0.0228, validation loss: 0.3072
2024-05-23 22:43:08 [INFO]: Epoch 059 - generator training loss: 0.2222, discriminator training loss: 0.0226, validation loss: 0.3062
2024-05-23 22:43:25 [INFO]: Epoch 060 - generator training loss: 0.2194, discriminator training loss: 0.0225, validation loss: 0.3067
2024-05-23 22:43:44 [INFO]: Epoch 061 - generator training loss: 0.2217, discriminator training loss: 0.0227, validation loss: 0.3072
2024-05-23 22:44:02 [INFO]: Epoch 062 - generator training loss: 0.2191, discriminator training loss: 0.0223, validation loss: 0.3083
2024-05-23 22:44:20 [INFO]: Epoch 063 - generator training loss: 0.2164, discriminator training loss: 0.0221, validation loss: 0.3052
2024-05-23 22:44:38 [INFO]: Epoch 064 - generator training loss: 0.2174, discriminator training loss: 0.0222, validation loss: 0.3056
2024-05-23 22:44:56 [INFO]: Epoch 065 - generator training loss: 0.2138, discriminator training loss: 0.0220, validation loss: 0.3080
2024-05-23 22:45:14 [INFO]: Epoch 066 - generator training loss: 0.2130, discriminator training loss: 0.0218, validation loss: 0.3102
2024-05-23 22:45:32 [INFO]: Epoch 067 - generator training loss: 0.2129, discriminator training loss: 0.0219, validation loss: 0.3091
2024-05-23 22:45:50 [INFO]: Epoch 068 - generator training loss: 0.2138, discriminator training loss: 0.0218, validation loss: 0.3069
2024-05-23 22:46:08 [INFO]: Epoch 069 - generator training loss: 0.2100, discriminator training loss: 0.0218, validation loss: 0.3094
2024-05-23 22:46:26 [INFO]: Epoch 070 - generator training loss: 0.2086, discriminator training loss: 0.0216, validation loss: 0.3081
2024-05-23 22:46:44 [INFO]: Epoch 071 - generator training loss: 0.2042, discriminator training loss: 0.0216, validation loss: 0.3092
2024-05-23 22:47:02 [INFO]: Epoch 072 - generator training loss: 0.2010, discriminator training loss: 0.0213, validation loss: 0.3073
2024-05-23 22:47:20 [INFO]: Epoch 073 - generator training loss: 0.2116, discriminator training loss: 0.0214, validation loss: 0.3144
2024-05-23 22:47:20 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-05-23 22:47:20 [INFO]: Finished training. The best model is from epoch#63.
2024-05-23 22:47:21 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_1/USGAN_physionet_2012_seta/20240523_T222518/USGAN.pypots
2024-05-23 22:47:23 [INFO]: US-GAN on PhysioNet-2012-seta: MAE=0.2960, MSE=0.2558
2024-05-23 22:47:33 [INFO]: Successfully saved to overlay_minibatchmasking_saved_results/round_1/USGAN_physionet_2012_seta/imputation.pkl
2024-05-23 22:47:33 [INFO]: Using the given device: cuda:0
2024-05-23 22:47:33 [INFO]: Model files will be saved to overlay_minibatchmasking_saved_results/round_1/BRITS_physionet_2012_seta/20240523_T224733
2024-05-23 22:47:33 [INFO]: Tensorboard file will be saved to overlay_minibatchmasking_saved_results/round_1/BRITS_physionet_2012_seta/20240523_T224733/tensorboard
2024-05-23 22:47:33 [INFO]: BRITS initialized with the given hyperparameters, the number of trainable parameters: 9,176,048
2024-05-23 22:47:47 [INFO]: Epoch 001 - training loss: 1.1071, validation loss: 0.5272
2024-05-23 22:47:59 [INFO]: Epoch 002 - training loss: 0.9054, validation loss: 0.4665
2024-05-23 22:48:11 [INFO]: Epoch 003 - training loss: 0.8419, validation loss: 0.4334
2024-05-23 22:48:23 [INFO]: Epoch 004 - training loss: 0.8033, validation loss: 0.4119
2024-05-23 22:48:35 [INFO]: Epoch 005 - training loss: 0.7718, validation loss: 0.3954
2024-05-23 22:48:46 [INFO]: Epoch 006 - training loss: 0.7483, validation loss: 0.3830
2024-05-23 22:48:58 [INFO]: Epoch 007 - training loss: 0.7287, validation loss: 0.3729
2024-05-23 22:49:10 [INFO]: Epoch 008 - training loss: 0.7138, validation loss: 0.3672
2024-05-23 22:49:22 [INFO]: Epoch 009 - training loss: 0.7018, validation loss: 0.3627
2024-05-23 22:49:34 [INFO]: Epoch 010 - training loss: 0.6894, validation loss: 0.3573
2024-05-23 22:49:46 [INFO]: Epoch 011 - training loss: 0.6801, validation loss: 0.3548
2024-05-23 22:49:57 [INFO]: Epoch 012 - training loss: 0.6722, validation loss: 0.3540
2024-05-23 22:50:09 [INFO]: Epoch 013 - training loss: 0.6651, validation loss: 0.3524
2024-05-23 22:50:21 [INFO]: Epoch 014 - training loss: 0.6587, validation loss: 0.3496
2024-05-23 22:50:33 [INFO]: Epoch 015 - training loss: 0.6525, validation loss: 0.3501
2024-05-23 22:50:45 [INFO]: Epoch 016 - training loss: 0.6524, validation loss: 0.3499
2024-05-23 22:50:56 [INFO]: Epoch 017 - training loss: 0.6457, validation loss: 0.3468
2024-05-23 22:51:08 [INFO]: Epoch 018 - training loss: 0.6414, validation loss: 0.3463
2024-05-23 22:51:20 [INFO]: Epoch 019 - training loss: 0.6374, validation loss: 0.3447
2024-05-23 22:51:32 [INFO]: Epoch 020 - training loss: 0.6330, validation loss: 0.3432
2024-05-23 22:51:44 [INFO]: Epoch 021 - training loss: 0.6282, validation loss: 0.3434
2024-05-23 22:51:56 [INFO]: Epoch 022 - training loss: 0.6244, validation loss: 0.3396
2024-05-23 22:52:07 [INFO]: Epoch 023 - training loss: 0.6214, validation loss: 0.3402
2024-05-23 22:52:19 [INFO]: Epoch 024 - training loss: 0.6177, validation loss: 0.3398
2024-05-23 22:52:31 [INFO]: Epoch 025 - training loss: 0.6143, validation loss: 0.3375
2024-05-23 22:52:43 [INFO]: Epoch 026 - training loss: 0.6131, validation loss: 0.3404
2024-05-23 22:52:55 [INFO]: Epoch 027 - training loss: 0.6104, validation loss: 0.3371
2024-05-23 22:53:07 [INFO]: Epoch 028 - training loss: 0.6051, validation loss: 0.3357
2024-05-23 22:53:18 [INFO]: Epoch 029 - training loss: 0.6026, validation loss: 0.3372
2024-05-23 22:53:30 [INFO]: Epoch 030 - training loss: 0.5988, validation loss: 0.3389
2024-05-23 22:53:42 [INFO]: Epoch 031 - training loss: 0.5959, validation loss: 0.3366
2024-05-23 22:53:54 [INFO]: Epoch 032 - training loss: 0.5935, validation loss: 0.3371
2024-05-23 22:54:06 [INFO]: Epoch 033 - training loss: 0.5909, validation loss: 0.3367
2024-05-23 22:54:18 [INFO]: Epoch 034 - training loss: 0.5874, validation loss: 0.3363
2024-05-23 22:54:29 [INFO]: Epoch 035 - training loss: 0.5851, validation loss: 0.3374
2024-05-23 22:54:41 [INFO]: Epoch 036 - training loss: 0.5817, validation loss: 0.3385
2024-05-23 22:54:53 [INFO]: Epoch 037 - training loss: 0.5824, validation loss: 0.3405
2024-05-23 22:55:05 [INFO]: Epoch 038 - training loss: 0.5790, validation loss: 0.3380
2024-05-23 22:55:05 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-05-23 22:55:05 [INFO]: Finished training. The best model is from epoch#28.
2024-05-23 22:55:05 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_1/BRITS_physionet_2012_seta/20240523_T224733/BRITS.pypots
2024-05-23 22:55:07 [INFO]: BRITS on PhysioNet-2012-seta: MAE=0.2531, MSE=0.2641
2024-05-23 22:55:17 [INFO]: Successfully saved to overlay_minibatchmasking_saved_results/round_1/BRITS_physionet_2012_seta/imputation.pkl
2024-05-23 22:55:17 [INFO]: Using the given device: cuda:0
2024-05-23 22:55:17 [INFO]: Model files will be saved to overlay_minibatchmasking_saved_results/round_1/MRNN_physionet_2012_seta/20240523_T225517
2024-05-23 22:55:17 [INFO]: Tensorboard file will be saved to overlay_minibatchmasking_saved_results/round_1/MRNN_physionet_2012_seta/20240523_T225517/tensorboard
2024-05-23 22:55:17 [INFO]: MRNN initialized with the given hyperparameters, the number of trainable parameters: 7,599
2024-05-23 22:55:22 [INFO]: Epoch 001 - training loss: 1.3048, validation loss: 0.9937
2024-05-23 22:55:22 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_1/MRNN_physionet_2012_seta/20240523_T225517/MRNN_epoch1_loss0.9937272399663926.pypots
2024-05-23 22:55:25 [INFO]: Epoch 002 - training loss: 0.8335, validation loss: 0.9630
2024-05-23 22:55:25 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_1/MRNN_physionet_2012_seta/20240523_T225517/MRNN_epoch2_loss0.962967362999916.pypots
2024-05-23 22:55:28 [INFO]: Epoch 003 - training loss: 0.6549, validation loss: 0.9410
2024-05-23 22:55:28 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_1/MRNN_physionet_2012_seta/20240523_T225517/MRNN_epoch3_loss0.9409643203020096.pypots
2024-05-23 22:55:31 [INFO]: Epoch 004 - training loss: 0.6040, validation loss: 0.9273
2024-05-23 22:55:31 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_1/MRNN_physionet_2012_seta/20240523_T225517/MRNN_epoch4_loss0.9272888481616974.pypots
2024-05-23 22:55:33 [INFO]: Epoch 005 - training loss: 0.5790, validation loss: 0.9210
2024-05-23 22:55:33 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_1/MRNN_physionet_2012_seta/20240523_T225517/MRNN_epoch5_loss0.9210471481084823.pypots
2024-05-23 22:55:36 [INFO]: Epoch 006 - training loss: 0.5625, validation loss: 0.9205
2024-05-23 22:55:36 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_1/MRNN_physionet_2012_seta/20240523_T225517/MRNN_epoch6_loss0.9205324620008468.pypots
2024-05-23 22:55:39 [INFO]: Epoch 007 - training loss: 0.5391, validation loss: 0.9134
2024-05-23 22:55:39 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_1/MRNN_physionet_2012_seta/20240523_T225517/MRNN_epoch7_loss0.9133548557758331.pypots
2024-05-23 22:55:42 [INFO]: Epoch 008 - training loss: 0.5253, validation loss: 0.9110
2024-05-23 22:55:42 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_1/MRNN_physionet_2012_seta/20240523_T225517/MRNN_epoch8_loss0.9110315829515457.pypots
2024-05-23 22:55:44 [INFO]: Epoch 009 - training loss: 0.5155, validation loss: 0.9095
2024-05-23 22:55:44 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_1/MRNN_physionet_2012_seta/20240523_T225517/MRNN_epoch9_loss0.9094581007957458.pypots
2024-05-23 22:55:47 [INFO]: Epoch 010 - training loss: 0.5119, validation loss: 0.9099
2024-05-23 22:55:47 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_1/MRNN_physionet_2012_seta/20240523_T225517/MRNN_epoch10_loss0.9099286258220672.pypots
2024-05-23 22:55:50 [INFO]: Epoch 011 - training loss: 0.5058, validation loss: 0.9093
2024-05-23 22:55:50 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_1/MRNN_physionet_2012_seta/20240523_T225517/MRNN_epoch11_loss0.9093229174613953.pypots
2024-05-23 22:55:53 [INFO]: Epoch 012 - training loss: 0.4928, validation loss: 0.9097
2024-05-23 22:55:53 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_1/MRNN_physionet_2012_seta/20240523_T225517/MRNN_epoch12_loss0.9097108483314514.pypots
2024-05-23 22:55:55 [INFO]: Epoch 013 - training loss: 0.4885, validation loss: 0.9103
2024-05-23 22:55:55 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_1/MRNN_physionet_2012_seta/20240523_T225517/MRNN_epoch13_loss0.9103025883436203.pypots
2024-05-23 22:55:58 [INFO]: Epoch 014 - training loss: 0.4784, validation loss: 0.9119
2024-05-23 22:55:58 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_1/MRNN_physionet_2012_seta/20240523_T225517/MRNN_epoch14_loss0.9118653565645218.pypots
2024-05-23 22:56:01 [INFO]: Epoch 015 - training loss: 0.4745, validation loss: 0.9140
2024-05-23 22:56:01 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_1/MRNN_physionet_2012_seta/20240523_T225517/MRNN_epoch15_loss0.9140032857656479.pypots
2024-05-23 22:56:04 [INFO]: Epoch 016 - training loss: 0.4802, validation loss: 0.9173
2024-05-23 22:56:04 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_1/MRNN_physionet_2012_seta/20240523_T225517/MRNN_epoch16_loss0.9172683745622635.pypots
2024-05-23 22:56:06 [INFO]: Epoch 017 - training loss: 0.4722, validation loss: 0.9186
2024-05-23 22:56:06 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_1/MRNN_physionet_2012_seta/20240523_T225517/MRNN_epoch17_loss0.918633708357811.pypots
2024-05-23 22:56:09 [INFO]: Epoch 018 - training loss: 0.4728, validation loss: 0.9200
2024-05-23 22:56:09 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_1/MRNN_physionet_2012_seta/20240523_T225517/MRNN_epoch18_loss0.9199566394090652.pypots
2024-05-23 22:56:12 [INFO]: Epoch 019 - training loss: 0.4644, validation loss: 0.9217
2024-05-23 22:56:12 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_1/MRNN_physionet_2012_seta/20240523_T225517/MRNN_epoch19_loss0.92172369658947.pypots
2024-05-23 22:56:15 [INFO]: Epoch 020 - training loss: 0.4612, validation loss: 0.9221
2024-05-23 22:56:15 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_1/MRNN_physionet_2012_seta/20240523_T225517/MRNN_epoch20_loss0.9221114575862884.pypots
2024-05-23 22:56:17 [INFO]: Epoch 021 - training loss: 0.4578, validation loss: 0.9240
2024-05-23 22:56:17 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_1/MRNN_physionet_2012_seta/20240523_T225517/MRNN_epoch21_loss0.9239753097295761.pypots
2024-05-23 22:56:17 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-05-23 22:56:17 [INFO]: Finished training. The best model is from epoch#11.
2024-05-23 22:56:17 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_1/MRNN_physionet_2012_seta/20240523_T225517/MRNN.pypots
2024-05-23 22:56:18 [INFO]: MRNN on PhysioNet-2012-seta: MAE=0.6819, MSE=0.9073
2024-05-23 22:56:23 [INFO]: Successfully saved to overlay_minibatchmasking_saved_results/round_1/MRNN_physionet_2012_seta/imputation.pkl
2024-05-23 22:56:23 [INFO]: Using the given device: cpu
2024-05-23 22:56:23 [INFO]: LOCF on PhysioNet-2012-seta: MAE=0.4109, MSE=0.5324
2024-05-23 22:56:23 [INFO]: Successfully created the given path "saved_results/round_1/LOCF_physionet_2012_seta".
2024-05-23 22:56:23 [INFO]: Successfully saved to overlay_minibatchmasking_saved_results/round_1/LOCF_physionet_2012_seta/imputation.pkl
2024-05-23 22:56:23 [INFO]: Median on PhysioNet-2012-seta: MAE=0.6868, MSE=1.0189
2024-05-23 22:56:23 [INFO]: Successfully created the given path "saved_results/round_1/Median_physionet_2012_seta".
2024-05-23 22:56:23 [INFO]: Successfully saved to overlay_minibatchmasking_saved_results/round_1/Median_physionet_2012_seta/imputation.pkl
2024-05-23 22:56:23 [INFO]: Mean on PhysioNet-2012-seta: MAE=0.7045, MSE=0.9898
2024-05-23 22:56:23 [INFO]: Successfully created the given path "saved_results/round_1/Mean_physionet_2012_seta".
2024-05-23 22:56:23 [INFO]: Successfully saved to overlay_minibatchmasking_saved_results/round_1/Mean_physionet_2012_seta/imputation.pkl
2024-05-23 22:56:23 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
2024-05-23 22:56:23 [INFO]: Using the given device: cuda:0
2024-05-23 22:56:23 [INFO]: Model files will be saved to overlay_minibatchmasking_saved_results/round_2/SAITS_physionet_2012_seta/20240523_T225623
2024-05-23 22:56:23 [INFO]: Tensorboard file will be saved to overlay_minibatchmasking_saved_results/round_2/SAITS_physionet_2012_seta/20240523_T225623/tensorboard
2024-05-23 22:56:23 [INFO]: SAITS initialized with the given hyperparameters, the number of trainable parameters: 5,322,806
2024-05-23 22:56:24 [INFO]: Epoch 001 - training loss: 1.0568, validation loss: 0.4853
2024-05-23 22:56:26 [INFO]: Epoch 002 - training loss: 0.6763, validation loss: 0.4255
2024-05-23 22:56:27 [INFO]: Epoch 003 - training loss: 0.5718, validation loss: 0.3961
2024-05-23 22:56:28 [INFO]: Epoch 004 - training loss: 0.5191, validation loss: 0.3764
2024-05-23 22:56:29 [INFO]: Epoch 005 - training loss: 0.4806, validation loss: 0.3613
2024-05-23 22:56:31 [INFO]: Epoch 006 - training loss: 0.4565, validation loss: 0.3467
2024-05-23 22:56:32 [INFO]: Epoch 007 - training loss: 0.4396, validation loss: 0.3368
2024-05-23 22:56:33 [INFO]: Epoch 008 - training loss: 0.4231, validation loss: 0.3295
2024-05-23 22:56:34 [INFO]: Epoch 009 - training loss: 0.4028, validation loss: 0.3151
2024-05-23 22:56:36 [INFO]: Epoch 010 - training loss: 0.3855, validation loss: 0.3005
2024-05-23 22:56:37 [INFO]: Epoch 011 - training loss: 0.3743, validation loss: 0.2967
2024-05-23 22:56:38 [INFO]: Epoch 012 - training loss: 0.3662, validation loss: 0.2935
2024-05-23 22:56:40 [INFO]: Epoch 013 - training loss: 0.3586, validation loss: 0.2960
2024-05-23 22:56:41 [INFO]: Epoch 014 - training loss: 0.3521, validation loss: 0.2823
2024-05-23 22:56:42 [INFO]: Epoch 015 - training loss: 0.3409, validation loss: 0.2818
2024-05-23 22:56:43 [INFO]: Epoch 016 - training loss: 0.3400, validation loss: 0.2740
2024-05-23 22:56:45 [INFO]: Epoch 017 - training loss: 0.3314, validation loss: 0.2766
2024-05-23 22:56:46 [INFO]: Epoch 018 - training loss: 0.3229, validation loss: 0.2721
2024-05-23 22:56:47 [INFO]: Epoch 019 - training loss: 0.3225, validation loss: 0.2649
2024-05-23 22:56:48 [INFO]: Epoch 020 - training loss: 0.3137, validation loss: 0.2586
2024-05-23 22:56:50 [INFO]: Epoch 021 - training loss: 0.3072, validation loss: 0.2613
2024-05-23 22:56:51 [INFO]: Epoch 022 - training loss: 0.3071, validation loss: 0.2569
2024-05-23 22:56:52 [INFO]: Epoch 023 - training loss: 0.3025, validation loss: 0.2556
2024-05-23 22:56:53 [INFO]: Epoch 024 - training loss: 0.3008, validation loss: 0.2588
2024-05-23 22:56:55 [INFO]: Epoch 025 - training loss: 0.2945, validation loss: 0.2595
2024-05-23 22:56:56 [INFO]: Epoch 026 - training loss: 0.2940, validation loss: 0.2533
2024-05-23 22:56:57 [INFO]: Epoch 027 - training loss: 0.2939, validation loss: 0.2482
2024-05-23 22:56:59 [INFO]: Epoch 028 - training loss: 0.2904, validation loss: 0.2447
2024-05-23 22:57:00 [INFO]: Epoch 029 - training loss: 0.2891, validation loss: 0.2471
2024-05-23 22:57:01 [INFO]: Epoch 030 - training loss: 0.2847, validation loss: 0.2438
2024-05-23 22:57:02 [INFO]: Epoch 031 - training loss: 0.2825, validation loss: 0.2453
2024-05-23 22:57:04 [INFO]: Epoch 032 - training loss: 0.2850, validation loss: 0.2403
2024-05-23 22:57:05 [INFO]: Epoch 033 - training loss: 0.2813, validation loss: 0.2404
2024-05-23 22:57:06 [INFO]: Epoch 034 - training loss: 0.2777, validation loss: 0.2427
2024-05-23 22:57:07 [INFO]: Epoch 035 - training loss: 0.2771, validation loss: 0.2307
2024-05-23 22:57:09 [INFO]: Epoch 036 - training loss: 0.2749, validation loss: 0.2371
2024-05-23 22:57:10 [INFO]: Epoch 037 - training loss: 0.2729, validation loss: 0.2360
2024-05-23 22:57:11 [INFO]: Epoch 038 - training loss: 0.2751, validation loss: 0.2375
2024-05-23 22:57:12 [INFO]: Epoch 039 - training loss: 0.2726, validation loss: 0.2370
2024-05-23 22:57:14 [INFO]: Epoch 040 - training loss: 0.2744, validation loss: 0.2302
2024-05-23 22:57:15 [INFO]: Epoch 041 - training loss: 0.2706, validation loss: 0.2392
2024-05-23 22:57:16 [INFO]: Epoch 042 - training loss: 0.2692, validation loss: 0.2398
2024-05-23 22:57:17 [INFO]: Epoch 043 - training loss: 0.2671, validation loss: 0.2266
2024-05-23 22:57:19 [INFO]: Epoch 044 - training loss: 0.2658, validation loss: 0.2300
2024-05-23 22:57:20 [INFO]: Epoch 045 - training loss: 0.2659, validation loss: 0.2327
2024-05-23 22:57:21 [INFO]: Epoch 046 - training loss: 0.2657, validation loss: 0.2367
2024-05-23 22:57:23 [INFO]: Epoch 047 - training loss: 0.2664, validation loss: 0.2236
2024-05-23 22:57:24 [INFO]: Epoch 048 - training loss: 0.2632, validation loss: 0.2280
2024-05-23 22:57:25 [INFO]: Epoch 049 - training loss: 0.2641, validation loss: 0.2259
2024-05-23 22:57:26 [INFO]: Epoch 050 - training loss: 0.2608, validation loss: 0.2258
2024-05-23 22:57:28 [INFO]: Epoch 051 - training loss: 0.2618, validation loss: 0.2277
2024-05-23 22:57:29 [INFO]: Epoch 052 - training loss: 0.2611, validation loss: 0.2269
2024-05-23 22:57:30 [INFO]: Epoch 053 - training loss: 0.2600, validation loss: 0.2319
2024-05-23 22:57:31 [INFO]: Epoch 054 - training loss: 0.2592, validation loss: 0.2306
2024-05-23 22:57:33 [INFO]: Epoch 055 - training loss: 0.2597, validation loss: 0.2317
2024-05-23 22:57:34 [INFO]: Epoch 056 - training loss: 0.2589, validation loss: 0.2214
2024-05-23 22:57:35 [INFO]: Epoch 057 - training loss: 0.2629, validation loss: 0.2232
2024-05-23 22:57:36 [INFO]: Epoch 058 - training loss: 0.2560, validation loss: 0.2314
2024-05-23 22:57:38 [INFO]: Epoch 059 - training loss: 0.2542, validation loss: 0.2342
2024-05-23 22:57:39 [INFO]: Epoch 060 - training loss: 0.2519, validation loss: 0.2208
2024-05-23 22:57:40 [INFO]: Epoch 061 - training loss: 0.2565, validation loss: 0.2705
2024-05-23 22:57:42 [INFO]: Epoch 062 - training loss: 0.2530, validation loss: 0.2302
2024-05-23 22:57:43 [INFO]: Epoch 063 - training loss: 0.2549, validation loss: 0.2245
2024-05-23 22:57:44 [INFO]: Epoch 064 - training loss: 0.2530, validation loss: 0.2229
2024-05-23 22:57:45 [INFO]: Epoch 065 - training loss: 0.2502, validation loss: 0.2244
2024-05-23 22:57:47 [INFO]: Epoch 066 - training loss: 0.2505, validation loss: 0.2327
2024-05-23 22:57:48 [INFO]: Epoch 067 - training loss: 0.2511, validation loss: 0.2233
2024-05-23 22:57:49 [INFO]: Epoch 068 - training loss: 0.2533, validation loss: 0.2234
2024-05-23 22:57:50 [INFO]: Epoch 069 - training loss: 0.2514, validation loss: 0.2258
2024-05-23 22:57:52 [INFO]: Epoch 070 - training loss: 0.2490, validation loss: 0.2191
2024-05-23 22:57:53 [INFO]: Epoch 071 - training loss: 0.2491, validation loss: 0.2215
2024-05-23 22:57:54 [INFO]: Epoch 072 - training loss: 0.2505, validation loss: 0.2219
2024-05-23 22:57:55 [INFO]: Epoch 073 - training loss: 0.2504, validation loss: 0.2259
2024-05-23 22:57:57 [INFO]: Epoch 074 - training loss: 0.2494, validation loss: 0.2365
2024-05-23 22:57:58 [INFO]: Epoch 075 - training loss: 0.2494, validation loss: 0.2236
2024-05-23 22:57:59 [INFO]: Epoch 076 - training loss: 0.2483, validation loss: 0.2291
2024-05-23 22:58:01 [INFO]: Epoch 077 - training loss: 0.2472, validation loss: 0.2199
2024-05-23 22:58:02 [INFO]: Epoch 078 - training loss: 0.2468, validation loss: 0.2191
2024-05-23 22:58:03 [INFO]: Epoch 079 - training loss: 0.2490, validation loss: 0.2201
2024-05-23 22:58:04 [INFO]: Epoch 080 - training loss: 0.2456, validation loss: 0.2247
2024-05-23 22:58:06 [INFO]: Epoch 081 - training loss: 0.2449, validation loss: 0.2246
2024-05-23 22:58:07 [INFO]: Epoch 082 - training loss: 0.2440, validation loss: 0.2113
2024-05-23 22:58:08 [INFO]: Epoch 083 - training loss: 0.2435, validation loss: 0.2165
2024-05-23 22:58:09 [INFO]: Epoch 084 - training loss: 0.2450, validation loss: 0.2158
2024-05-23 22:58:11 [INFO]: Epoch 085 - training loss: 0.2412, validation loss: 0.2298
2024-05-23 22:58:12 [INFO]: Epoch 086 - training loss: 0.2422, validation loss: 0.2182
2024-05-23 22:58:13 [INFO]: Epoch 087 - training loss: 0.2442, validation loss: 0.2176
2024-05-23 22:58:15 [INFO]: Epoch 088 - training loss: 0.2439, validation loss: 0.2203
2024-05-23 22:58:16 [INFO]: Epoch 089 - training loss: 0.2407, validation loss: 0.2166
2024-05-23 22:58:17 [INFO]: Epoch 090 - training loss: 0.2421, validation loss: 0.2264
2024-05-23 22:58:18 [INFO]: Epoch 091 - training loss: 0.2406, validation loss: 0.2153
2024-05-23 22:58:20 [INFO]: Epoch 092 - training loss: 0.2442, validation loss: 0.2162
2024-05-23 22:58:20 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-05-23 22:58:20 [INFO]: Finished training. The best model is from epoch#82.
2024-05-23 22:58:20 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_2/SAITS_physionet_2012_seta/20240523_T225623/SAITS.pypots
2024-05-23 22:58:20 [INFO]: SAITS on PhysioNet-2012-seta: MAE=0.2033, MSE=0.2277
2024-05-23 22:58:20 [INFO]: Successfully saved to overlay_minibatchmasking_saved_results/round_2/SAITS_physionet_2012_seta/imputation.pkl
2024-05-23 22:58:20 [INFO]: Using the given device: cuda:0
2024-05-23 22:58:20 [INFO]: Model files will be saved to overlay_minibatchmasking_saved_results/round_2/Transformer_physionet_2012_seta/20240523_T225820
2024-05-23 22:58:20 [INFO]: Tensorboard file will be saved to overlay_minibatchmasking_saved_results/round_2/Transformer_physionet_2012_seta/20240523_T225820/tensorboard
2024-05-23 22:58:20 [INFO]: Transformer initialized with the given hyperparameters, the number of trainable parameters: 2,152,613
2024-05-23 22:58:21 [INFO]: Epoch 001 - training loss: 1.1693, validation loss: 0.5612
2024-05-23 22:58:22 [INFO]: Epoch 002 - training loss: 0.7568, validation loss: 0.4842
2024-05-23 22:58:22 [INFO]: Epoch 003 - training loss: 0.6327, validation loss: 0.4536
2024-05-23 22:58:23 [INFO]: Epoch 004 - training loss: 0.5869, validation loss: 0.4496
2024-05-23 22:58:24 [INFO]: Epoch 005 - training loss: 0.5476, validation loss: 0.4130
2024-05-23 22:58:24 [INFO]: Epoch 006 - training loss: 0.5150, validation loss: 0.3978
2024-05-23 22:58:25 [INFO]: Epoch 007 - training loss: 0.4990, validation loss: 0.3876
2024-05-23 22:58:26 [INFO]: Epoch 008 - training loss: 0.4771, validation loss: 0.3822
2024-05-23 22:58:26 [INFO]: Epoch 009 - training loss: 0.4681, validation loss: 0.3735
2024-05-23 22:58:27 [INFO]: Epoch 010 - training loss: 0.4560, validation loss: 0.3669
2024-05-23 22:58:28 [INFO]: Epoch 011 - training loss: 0.4432, validation loss: 0.3612
2024-05-23 22:58:28 [INFO]: Epoch 012 - training loss: 0.4350, validation loss: 0.3630
2024-05-23 22:58:29 [INFO]: Epoch 013 - training loss: 0.4172, validation loss: 0.3491
2024-05-23 22:58:30 [INFO]: Epoch 014 - training loss: 0.4121, validation loss: 0.3411
2024-05-23 22:58:31 [INFO]: Epoch 015 - training loss: 0.4026, validation loss: 0.3435
2024-05-23 22:58:31 [INFO]: Epoch 016 - training loss: 0.3928, validation loss: 0.3292
2024-05-23 22:58:32 [INFO]: Epoch 017 - training loss: 0.3832, validation loss: 0.3299
2024-05-23 22:58:33 [INFO]: Epoch 018 - training loss: 0.3824, validation loss: 0.3319
2024-05-23 22:58:33 [INFO]: Epoch 019 - training loss: 0.3812, validation loss: 0.3248
2024-05-23 22:58:34 [INFO]: Epoch 020 - training loss: 0.3772, validation loss: 0.3249
2024-05-23 22:58:35 [INFO]: Epoch 021 - training loss: 0.3672, validation loss: 0.3222
2024-05-23 22:58:35 [INFO]: Epoch 022 - training loss: 0.3632, validation loss: 0.3184
2024-05-23 22:58:36 [INFO]: Epoch 023 - training loss: 0.3611, validation loss: 0.3198
2024-05-23 22:58:37 [INFO]: Epoch 024 - training loss: 0.3568, validation loss: 0.3134
2024-05-23 22:58:37 [INFO]: Epoch 025 - training loss: 0.3504, validation loss: 0.3105
2024-05-23 22:58:38 [INFO]: Epoch 026 - training loss: 0.3464, validation loss: 0.3101
2024-05-23 22:58:39 [INFO]: Epoch 027 - training loss: 0.3511, validation loss: 0.3096
2024-05-23 22:58:40 [INFO]: Epoch 028 - training loss: 0.3430, validation loss: 0.2994
2024-05-23 22:58:40 [INFO]: Epoch 029 - training loss: 0.3406, validation loss: 0.3039
2024-05-23 22:58:41 [INFO]: Epoch 030 - training loss: 0.3365, validation loss: 0.2967
2024-05-23 22:58:42 [INFO]: Epoch 031 - training loss: 0.3361, validation loss: 0.2957
2024-05-23 22:58:42 [INFO]: Epoch 032 - training loss: 0.3323, validation loss: 0.2976
2024-05-23 22:58:43 [INFO]: Epoch 033 - training loss: 0.3315, validation loss: 0.2944
2024-05-23 22:58:44 [INFO]: Epoch 034 - training loss: 0.3305, validation loss: 0.2881
2024-05-23 22:58:44 [INFO]: Epoch 035 - training loss: 0.3280, validation loss: 0.2903
2024-05-23 22:58:45 [INFO]: Epoch 036 - training loss: 0.3247, validation loss: 0.2889
2024-05-23 22:58:46 [INFO]: Epoch 037 - training loss: 0.3203, validation loss: 0.2877
2024-05-23 22:58:46 [INFO]: Epoch 038 - training loss: 0.3246, validation loss: 0.2879
2024-05-23 22:58:47 [INFO]: Epoch 039 - training loss: 0.3184, validation loss: 0.2815
2024-05-23 22:58:48 [INFO]: Epoch 040 - training loss: 0.3161, validation loss: 0.2841
2024-05-23 22:58:49 [INFO]: Epoch 041 - training loss: 0.3147, validation loss: 0.2824
2024-05-23 22:58:49 [INFO]: Epoch 042 - training loss: 0.3154, validation loss: 0.2797
2024-05-23 22:58:50 [INFO]: Epoch 043 - training loss: 0.3165, validation loss: 0.2810
2024-05-23 22:58:51 [INFO]: Epoch 044 - training loss: 0.3113, validation loss: 0.2807
2024-05-23 22:58:51 [INFO]: Epoch 045 - training loss: 0.3080, validation loss: 0.2814
2024-05-23 22:58:52 [INFO]: Epoch 046 - training loss: 0.3100, validation loss: 0.2798
2024-05-23 22:58:53 [INFO]: Epoch 047 - training loss: 0.3082, validation loss: 0.2737
2024-05-23 22:58:53 [INFO]: Epoch 048 - training loss: 0.3094, validation loss: 0.2765
2024-05-23 22:58:54 [INFO]: Epoch 049 - training loss: 0.3057, validation loss: 0.2712
2024-05-23 22:58:55 [INFO]: Epoch 050 - training loss: 0.3055, validation loss: 0.2758
2024-05-23 22:58:56 [INFO]: Epoch 051 - training loss: 0.3036, validation loss: 0.2737
2024-05-23 22:58:56 [INFO]: Epoch 052 - training loss: 0.3039, validation loss: 0.2729
2024-05-23 22:58:57 [INFO]: Epoch 053 - training loss: 0.3030, validation loss: 0.2682
2024-05-23 22:58:58 [INFO]: Epoch 054 - training loss: 0.3005, validation loss: 0.2656
2024-05-23 22:58:58 [INFO]: Epoch 055 - training loss: 0.2982, validation loss: 0.2706
2024-05-23 22:58:59 [INFO]: Epoch 056 - training loss: 0.3044, validation loss: 0.2667
2024-05-23 22:59:00 [INFO]: Epoch 057 - training loss: 0.2977, validation loss: 0.2632
2024-05-23 22:59:00 [INFO]: Epoch 058 - training loss: 0.2963, validation loss: 0.2610
2024-05-23 22:59:01 [INFO]: Epoch 059 - training loss: 0.2977, validation loss: 0.2642
2024-05-23 22:59:02 [INFO]: Epoch 060 - training loss: 0.2958, validation loss: 0.2606
2024-05-23 22:59:03 [INFO]: Epoch 061 - training loss: 0.2954, validation loss: 0.2630
2024-05-23 22:59:03 [INFO]: Epoch 062 - training loss: 0.2949, validation loss: 0.2629
2024-05-23 22:59:04 [INFO]: Epoch 063 - training loss: 0.2941, validation loss: 0.2585
2024-05-23 22:59:05 [INFO]: Epoch 064 - training loss: 0.2932, validation loss: 0.2557
2024-05-23 22:59:05 [INFO]: Epoch 065 - training loss: 0.2927, validation loss: 0.2562
2024-05-23 22:59:06 [INFO]: Epoch 066 - training loss: 0.2883, validation loss: 0.2557
2024-05-23 22:59:07 [INFO]: Epoch 067 - training loss: 0.2910, validation loss: 0.2499
2024-05-23 22:59:07 [INFO]: Epoch 068 - training loss: 0.2881, validation loss: 0.2519
2024-05-23 22:59:08 [INFO]: Epoch 069 - training loss: 0.2917, validation loss: 0.2518
2024-05-23 22:59:09 [INFO]: Epoch 070 - training loss: 0.2866, validation loss: 0.2535
2024-05-23 22:59:09 [INFO]: Epoch 071 - training loss: 0.2865, validation loss: 0.2526
2024-05-23 22:59:10 [INFO]: Epoch 072 - training loss: 0.2852, validation loss: 0.2515
2024-05-23 22:59:11 [INFO]: Epoch 073 - training loss: 0.2848, validation loss: 0.2561
2024-05-23 22:59:11 [INFO]: Epoch 074 - training loss: 0.2834, validation loss: 0.2503
2024-05-23 22:59:12 [INFO]: Epoch 075 - training loss: 0.2839, validation loss: 0.2507
2024-05-23 22:59:13 [INFO]: Epoch 076 - training loss: 0.2845, validation loss: 0.2521
2024-05-23 22:59:14 [INFO]: Epoch 077 - training loss: 0.2842, validation loss: 0.2503
2024-05-23 22:59:14 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-05-23 22:59:14 [INFO]: Finished training. The best model is from epoch#67.
2024-05-23 22:59:14 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_2/Transformer_physionet_2012_seta/20240523_T225820/Transformer.pypots
2024-05-23 22:59:14 [INFO]: Transformer on PhysioNet-2012-seta: MAE=0.2258, MSE=0.2283
2024-05-23 22:59:14 [INFO]: Successfully saved to overlay_minibatchmasking_saved_results/round_2/Transformer_physionet_2012_seta/imputation.pkl
2024-05-23 22:59:14 [INFO]: Using the given device: cuda:0
2024-05-23 22:59:14 [INFO]: Model files will be saved to overlay_minibatchmasking_saved_results/round_2/TimesNet_physionet_2012_seta/20240523_T225914
2024-05-23 22:59:14 [INFO]: Tensorboard file will be saved to overlay_minibatchmasking_saved_results/round_2/TimesNet_physionet_2012_seta/20240523_T225914/tensorboard
2024-05-23 22:59:14 [INFO]: TimesNet initialized with the given hyperparameters, the number of trainable parameters: 21,649,573
2024-05-23 22:59:15 [INFO]: Epoch 001 - training loss: 0.5028, validation loss: 0.6641
2024-05-23 22:59:16 [INFO]: Epoch 002 - training loss: 0.4467, validation loss: 0.4604
2024-05-23 22:59:16 [INFO]: Epoch 003 - training loss: 0.4639, validation loss: 0.6686
2024-05-23 22:59:17 [INFO]: Epoch 004 - training loss: 0.4955, validation loss: 0.6021
2024-05-23 22:59:18 [INFO]: Epoch 005 - training loss: 0.4551, validation loss: 0.3722
2024-05-23 22:59:19 [INFO]: Epoch 006 - training loss: 0.3563, validation loss: 0.3739
2024-05-23 22:59:20 [INFO]: Epoch 007 - training loss: 0.3731, validation loss: 0.4607
2024-05-23 22:59:20 [INFO]: Epoch 008 - training loss: 0.3341, validation loss: 0.4101
2024-05-23 22:59:21 [INFO]: Epoch 009 - training loss: 0.3718, validation loss: 0.3247
2024-05-23 22:59:22 [INFO]: Epoch 010 - training loss: 0.3465, validation loss: 0.3095
2024-05-23 22:59:23 [INFO]: Epoch 011 - training loss: 0.3429, validation loss: 0.3089
2024-05-23 22:59:24 [INFO]: Epoch 012 - training loss: 0.3110, validation loss: 0.3063
2024-05-23 22:59:24 [INFO]: Epoch 013 - training loss: 0.3177, validation loss: 0.3060
2024-05-23 22:59:25 [INFO]: Epoch 014 - training loss: 0.3072, validation loss: 0.2872
2024-05-23 22:59:26 [INFO]: Epoch 015 - training loss: 0.3821, validation loss: 0.3177
2024-05-23 22:59:27 [INFO]: Epoch 016 - training loss: 0.3260, validation loss: 0.3090
2024-05-23 22:59:28 [INFO]: Epoch 017 - training loss: 0.3418, validation loss: 0.2919
2024-05-23 22:59:28 [INFO]: Epoch 018 - training loss: 0.3558, validation loss: 0.2922
2024-05-23 22:59:29 [INFO]: Epoch 019 - training loss: 0.3188, validation loss: 0.2854
2024-05-23 22:59:30 [INFO]: Epoch 020 - training loss: 0.4034, validation loss: 0.2835
2024-05-23 22:59:31 [INFO]: Epoch 021 - training loss: 0.3273, validation loss: 0.2979
2024-05-23 22:59:32 [INFO]: Epoch 022 - training loss: 0.3412, validation loss: 0.2912
2024-05-23 22:59:32 [INFO]: Epoch 023 - training loss: 0.3289, validation loss: 0.3280
2024-05-23 22:59:33 [INFO]: Epoch 024 - training loss: 0.3690, validation loss: 0.3145
2024-05-23 22:59:34 [INFO]: Epoch 025 - training loss: 0.3621, validation loss: 0.3734
2024-05-23 22:59:35 [INFO]: Epoch 026 - training loss: 0.3408, validation loss: 0.2902
2024-05-23 22:59:36 [INFO]: Epoch 027 - training loss: 0.3280, validation loss: 0.2859
2024-05-23 22:59:36 [INFO]: Epoch 028 - training loss: 0.3084, validation loss: 0.2807
2024-05-23 22:59:37 [INFO]: Epoch 029 - training loss: 0.3080, validation loss: 0.2771
2024-05-23 22:59:38 [INFO]: Epoch 030 - training loss: 0.3007, validation loss: 0.2837
2024-05-23 22:59:39 [INFO]: Epoch 031 - training loss: 0.3018, validation loss: 0.2921
2024-05-23 22:59:40 [INFO]: Epoch 032 - training loss: 0.3749, validation loss: 0.2852
2024-05-23 22:59:40 [INFO]: Epoch 033 - training loss: 0.3089, validation loss: 0.2744
2024-05-23 22:59:41 [INFO]: Epoch 034 - training loss: 0.3831, validation loss: 0.2781
2024-05-23 22:59:42 [INFO]: Epoch 035 - training loss: 0.2883, validation loss: 0.2852
2024-05-23 22:59:43 [INFO]: Epoch 036 - training loss: 0.3782, validation loss: 0.2708
2024-05-23 22:59:44 [INFO]: Epoch 037 - training loss: 0.3580, validation loss: 0.2741
2024-05-23 22:59:44 [INFO]: Epoch 038 - training loss: 0.3095, validation loss: 0.2758
2024-05-23 22:59:45 [INFO]: Epoch 039 - training loss: 0.3100, validation loss: 0.2777
2024-05-23 22:59:46 [INFO]: Epoch 040 - training loss: 0.2911, validation loss: 0.2698
2024-05-23 22:59:47 [INFO]: Epoch 041 - training loss: 0.3121, validation loss: 0.2953
2024-05-23 22:59:48 [INFO]: Epoch 042 - training loss: 0.2831, validation loss: 0.2863
2024-05-23 22:59:48 [INFO]: Epoch 043 - training loss: 0.3215, validation loss: 0.2772
2024-05-23 22:59:49 [INFO]: Epoch 044 - training loss: 0.3041, validation loss: 0.2803
2024-05-23 22:59:50 [INFO]: Epoch 045 - training loss: 0.2970, validation loss: 0.2810
2024-05-23 22:59:51 [INFO]: Epoch 046 - training loss: 0.2966, validation loss: 0.2689
2024-05-23 22:59:52 [INFO]: Epoch 047 - training loss: 0.2819, validation loss: 0.2713
2024-05-23 22:59:52 [INFO]: Epoch 048 - training loss: 0.3003, validation loss: 0.2859
2024-05-23 22:59:53 [INFO]: Epoch 049 - training loss: 0.2911, validation loss: 0.2855
2024-05-23 22:59:54 [INFO]: Epoch 050 - training loss: 0.3098, validation loss: 0.3219
2024-05-23 22:59:55 [INFO]: Epoch 051 - training loss: 0.3412, validation loss: 0.2750
2024-05-23 22:59:56 [INFO]: Epoch 052 - training loss: 0.3071, validation loss: 0.2758
2024-05-23 22:59:56 [INFO]: Epoch 053 - training loss: 0.3070, validation loss: 0.2749
2024-05-23 22:59:57 [INFO]: Epoch 054 - training loss: 0.3211, validation loss: 0.2690
2024-05-23 22:59:58 [INFO]: Epoch 055 - training loss: 0.3066, validation loss: 0.2689
2024-05-23 22:59:59 [INFO]: Epoch 056 - training loss: 0.3188, validation loss: 0.2668
2024-05-23 23:00:00 [INFO]: Epoch 057 - training loss: 0.2706, validation loss: 0.2631
2024-05-23 23:00:00 [INFO]: Epoch 058 - training loss: 0.2950, validation loss: 0.2687
2024-05-23 23:00:01 [INFO]: Epoch 059 - training loss: 0.3016, validation loss: 0.2711
2024-05-23 23:00:02 [INFO]: Epoch 060 - training loss: 0.2910, validation loss: 0.2729
2024-05-23 23:00:03 [INFO]: Epoch 061 - training loss: 0.2625, validation loss: 0.2710
2024-05-23 23:00:04 [INFO]: Epoch 062 - training loss: 0.2875, validation loss: 0.2664
2024-05-23 23:00:04 [INFO]: Epoch 063 - training loss: 0.2955, validation loss: 0.2633
2024-05-23 23:00:05 [INFO]: Epoch 064 - training loss: 0.2852, validation loss: 0.2668
2024-05-23 23:00:06 [INFO]: Epoch 065 - training loss: 0.2607, validation loss: 0.2678
2024-05-23 23:00:07 [INFO]: Epoch 066 - training loss: 0.2845, validation loss: 0.2611
2024-05-23 23:00:08 [INFO]: Epoch 067 - training loss: 0.3018, validation loss: 0.2645
2024-05-23 23:00:08 [INFO]: Epoch 068 - training loss: 0.2891, validation loss: 0.2711
2024-05-23 23:00:09 [INFO]: Epoch 069 - training loss: 0.2768, validation loss: 0.2623
2024-05-23 23:00:10 [INFO]: Epoch 070 - training loss: 0.2590, validation loss: 0.2633
2024-05-23 23:00:11 [INFO]: Epoch 071 - training loss: 0.3077, validation loss: 0.2719
2024-05-23 23:00:12 [INFO]: Epoch 072 - training loss: 0.2899, validation loss: 0.2774
2024-05-23 23:00:12 [INFO]: Epoch 073 - training loss: 0.3174, validation loss: 0.2687
2024-05-23 23:00:13 [INFO]: Epoch 074 - training loss: 0.2941, validation loss: 0.2637
2024-05-23 23:00:14 [INFO]: Epoch 075 - training loss: 0.2883, validation loss: 0.2679
2024-05-23 23:00:15 [INFO]: Epoch 076 - training loss: 0.2970, validation loss: 0.2652
2024-05-23 23:00:15 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-05-23 23:00:15 [INFO]: Finished training. The best model is from epoch#66.
2024-05-23 23:00:15 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_2/TimesNet_physionet_2012_seta/20240523_T225914/TimesNet.pypots
2024-05-23 23:00:15 [INFO]: TimesNet on PhysioNet-2012-seta: MAE=0.2647, MSE=0.2295
2024-05-23 23:00:15 [INFO]: Successfully saved to overlay_minibatchmasking_saved_results/round_2/TimesNet_physionet_2012_seta/imputation.pkl
2024-05-23 23:00:15 [INFO]: Using the given device: cuda:0
2024-05-23 23:00:15 [INFO]: Model files will be saved to overlay_minibatchmasking_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T230015
2024-05-23 23:00:15 [INFO]: Tensorboard file will be saved to overlay_minibatchmasking_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T230015/tensorboard
2024-05-23 23:00:15 [INFO]: CSDI initialized with the given hyperparameters, the number of trainable parameters: 1,694,753
2024-05-23 23:00:59 [INFO]: Epoch 001 - training loss: 0.4182, validation loss: 0.3474
2024-05-23 23:00:59 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T230015/CSDI_epoch1_loss0.34735760390758513.pypots
2024-05-23 23:01:42 [INFO]: Epoch 002 - training loss: 0.3283, validation loss: 0.2947
2024-05-23 23:01:42 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T230015/CSDI_epoch2_loss0.29472204446792605.pypots
2024-05-23 23:02:26 [INFO]: Epoch 003 - training loss: 0.2950, validation loss: 0.2507
2024-05-23 23:02:26 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T230015/CSDI_epoch3_loss0.25065172761678695.pypots
2024-05-23 23:03:09 [INFO]: Epoch 004 - training loss: 0.2536, validation loss: 0.2300
2024-05-23 23:03:09 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T230015/CSDI_epoch4_loss0.23000307753682137.pypots
2024-05-23 23:03:53 [INFO]: Epoch 005 - training loss: 0.2522, validation loss: 0.2173
2024-05-23 23:03:53 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T230015/CSDI_epoch5_loss0.21734988018870355.pypots
2024-05-23 23:04:37 [INFO]: Epoch 006 - training loss: 0.2588, validation loss: 0.2222
2024-05-23 23:04:37 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T230015/CSDI_epoch6_loss0.22222135365009307.pypots
2024-05-23 23:05:21 [INFO]: Epoch 007 - training loss: 0.2432, validation loss: 0.2127
2024-05-23 23:05:21 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T230015/CSDI_epoch7_loss0.2126649171113968.pypots
2024-05-23 23:06:05 [INFO]: Epoch 008 - training loss: 0.2478, validation loss: 0.2060
2024-05-23 23:06:05 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T230015/CSDI_epoch8_loss0.20595598816871644.pypots
2024-05-23 23:06:48 [INFO]: Epoch 009 - training loss: 0.2502, validation loss: 0.2099
2024-05-23 23:06:48 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T230015/CSDI_epoch9_loss0.20986094251275061.pypots
2024-05-23 23:07:32 [INFO]: Epoch 010 - training loss: 0.2472, validation loss: 0.2011
2024-05-23 23:07:32 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T230015/CSDI_epoch10_loss0.2010653868317604.pypots
2024-05-23 23:08:16 [INFO]: Epoch 011 - training loss: 0.2356, validation loss: 0.2006
2024-05-23 23:08:16 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T230015/CSDI_epoch11_loss0.2005513295531273.pypots
2024-05-23 23:09:00 [INFO]: Epoch 012 - training loss: 0.2430, validation loss: 0.2044
2024-05-23 23:09:00 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T230015/CSDI_epoch12_loss0.2043503262102604.pypots
2024-05-23 23:09:44 [INFO]: Epoch 013 - training loss: 0.2357, validation loss: 0.1978
2024-05-23 23:09:44 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T230015/CSDI_epoch13_loss0.19775217697024344.pypots
2024-05-23 23:10:28 [INFO]: Epoch 014 - training loss: 0.2385, validation loss: 0.1951
2024-05-23 23:10:28 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T230015/CSDI_epoch14_loss0.19513802751898765.pypots
2024-05-23 23:11:11 [INFO]: Epoch 015 - training loss: 0.2426, validation loss: 0.2011
2024-05-23 23:11:11 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T230015/CSDI_epoch15_loss0.20111837685108186.pypots
2024-05-23 23:11:55 [INFO]: Epoch 016 - training loss: 0.2413, validation loss: 0.1950
2024-05-23 23:11:55 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T230015/CSDI_epoch16_loss0.19503199309110641.pypots
2024-05-23 23:12:39 [INFO]: Epoch 017 - training loss: 0.2235, validation loss: 0.1940
2024-05-23 23:12:39 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T230015/CSDI_epoch17_loss0.19398400112986564.pypots
2024-05-23 23:13:23 [INFO]: Epoch 018 - training loss: 0.2346, validation loss: 0.1905
2024-05-23 23:13:23 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T230015/CSDI_epoch18_loss0.19054018184542656.pypots
2024-05-23 23:14:07 [INFO]: Epoch 019 - training loss: 0.2383, validation loss: 0.1917
2024-05-23 23:14:07 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T230015/CSDI_epoch19_loss0.19166327863931656.pypots
2024-05-23 23:14:50 [INFO]: Epoch 020 - training loss: 0.2338, validation loss: 0.1939
2024-05-23 23:14:50 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T230015/CSDI_epoch20_loss0.19388348534703254.pypots
2024-05-23 23:15:34 [INFO]: Epoch 021 - training loss: 0.2351, validation loss: 0.1905
2024-05-23 23:15:34 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T230015/CSDI_epoch21_loss0.19053430929780008.pypots
2024-05-23 23:16:18 [INFO]: Epoch 022 - training loss: 0.2393, validation loss: 0.1951
2024-05-23 23:16:18 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T230015/CSDI_epoch22_loss0.1951019249856472.pypots
2024-05-23 23:17:02 [INFO]: Epoch 023 - training loss: 0.2304, validation loss: 0.1894
2024-05-23 23:17:02 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T230015/CSDI_epoch23_loss0.18936977460980414.pypots
2024-05-23 23:17:46 [INFO]: Epoch 024 - training loss: 0.2237, validation loss: 0.1884
2024-05-23 23:17:46 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T230015/CSDI_epoch24_loss0.18840721771121025.pypots
2024-05-23 23:18:29 [INFO]: Epoch 025 - training loss: 0.2292, validation loss: 0.1873
2024-05-23 23:18:29 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T230015/CSDI_epoch25_loss0.18732878863811492.pypots
2024-05-23 23:19:13 [INFO]: Epoch 026 - training loss: 0.2203, validation loss: 0.1846
2024-05-23 23:19:13 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T230015/CSDI_epoch26_loss0.18457344695925712.pypots
2024-05-23 23:19:57 [INFO]: Epoch 027 - training loss: 0.2238, validation loss: 0.1859
2024-05-23 23:19:57 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T230015/CSDI_epoch27_loss0.1858741208910942.pypots
2024-05-23 23:20:41 [INFO]: Epoch 028 - training loss: 0.2270, validation loss: 0.1841
2024-05-23 23:20:41 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T230015/CSDI_epoch28_loss0.18413611501455307.pypots
2024-05-23 23:21:25 [INFO]: Epoch 029 - training loss: 0.2259, validation loss: 0.1829
2024-05-23 23:21:25 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T230015/CSDI_epoch29_loss0.18285648971796037.pypots
2024-05-23 23:22:09 [INFO]: Epoch 030 - training loss: 0.2315, validation loss: 0.1856
2024-05-23 23:22:09 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T230015/CSDI_epoch30_loss0.18561085239052771.pypots
2024-05-23 23:22:53 [INFO]: Epoch 031 - training loss: 0.2238, validation loss: 0.1846
2024-05-23 23:22:53 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T230015/CSDI_epoch31_loss0.18463656529784203.pypots
2024-05-23 23:23:37 [INFO]: Epoch 032 - training loss: 0.2240, validation loss: 0.1802
2024-05-23 23:23:37 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T230015/CSDI_epoch32_loss0.18022849932312965.pypots
2024-05-23 23:24:21 [INFO]: Epoch 033 - training loss: 0.2287, validation loss: 0.1828
2024-05-23 23:24:21 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T230015/CSDI_epoch33_loss0.18275030702352524.pypots
2024-05-23 23:25:05 [INFO]: Epoch 034 - training loss: 0.2343, validation loss: 0.1805
2024-05-23 23:25:05 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T230015/CSDI_epoch34_loss0.18054997622966767.pypots
2024-05-23 23:25:48 [INFO]: Epoch 035 - training loss: 0.2354, validation loss: 0.1823
2024-05-23 23:25:48 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T230015/CSDI_epoch35_loss0.18228833824396135.pypots
2024-05-23 23:26:32 [INFO]: Epoch 036 - training loss: 0.2359, validation loss: 0.1804
2024-05-23 23:26:32 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T230015/CSDI_epoch36_loss0.18041801676154137.pypots
2024-05-23 23:27:16 [INFO]: Epoch 037 - training loss: 0.2282, validation loss: 0.1845
2024-05-23 23:27:16 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T230015/CSDI_epoch37_loss0.18446918055415154.pypots
2024-05-23 23:28:00 [INFO]: Epoch 038 - training loss: 0.2199, validation loss: 0.1804
2024-05-23 23:28:00 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T230015/CSDI_epoch38_loss0.18042533621191978.pypots
2024-05-23 23:28:44 [INFO]: Epoch 039 - training loss: 0.2340, validation loss: 0.1805
2024-05-23 23:28:44 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T230015/CSDI_epoch39_loss0.18047160878777505.pypots
2024-05-23 23:29:28 [INFO]: Epoch 040 - training loss: 0.2213, validation loss: 0.1815
2024-05-23 23:29:28 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T230015/CSDI_epoch40_loss0.18146443665027617.pypots
2024-05-23 23:30:11 [INFO]: Epoch 041 - training loss: 0.2286, validation loss: 0.1827
2024-05-23 23:30:11 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T230015/CSDI_epoch41_loss0.18266505673527716.pypots
2024-05-23 23:30:55 [INFO]: Epoch 042 - training loss: 0.2114, validation loss: 0.1830
2024-05-23 23:30:55 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T230015/CSDI_epoch42_loss0.18302645534276962.pypots
2024-05-23 23:30:55 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-05-23 23:30:55 [INFO]: Finished training. The best model is from epoch#32.
2024-05-23 23:30:55 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T230015/CSDI.pypots
2024-05-23 23:38:15 [INFO]: CSDI on PhysioNet-2012-seta: MAE=0.2395, MSE=0.3699
2024-05-24 00:07:34 [INFO]: Successfully saved to overlay_minibatchmasking_saved_results/round_2/CSDI_physionet_2012_seta/imputation.pkl
2024-05-24 00:07:34 [INFO]: Using the given device: cuda:0
2024-05-24 00:07:34 [INFO]: Model files will be saved to overlay_minibatchmasking_saved_results/round_2/GPVAE_physionet_2012_seta/20240524_T000734
2024-05-24 00:07:34 [INFO]: Tensorboard file will be saved to overlay_minibatchmasking_saved_results/round_2/GPVAE_physionet_2012_seta/20240524_T000734/tensorboard
2024-05-24 00:07:34 [INFO]: GPVAE initialized with the given hyperparameters, the number of trainable parameters: 1,028,244
2024-05-24 00:07:35 [INFO]: Epoch 001 - training loss: 42149.2004, validation loss: 0.8975
2024-05-24 00:07:35 [INFO]: Epoch 002 - training loss: 24383.3937, validation loss: 0.7466
2024-05-24 00:07:36 [INFO]: Epoch 003 - training loss: 23508.6864, validation loss: 0.7074
2024-05-24 00:07:36 [INFO]: Epoch 004 - training loss: 23219.3969, validation loss: 0.6774
2024-05-24 00:07:37 [INFO]: Epoch 005 - training loss: 23078.0747, validation loss: 0.6679
2024-05-24 00:07:38 [INFO]: Epoch 006 - training loss: 22997.7972, validation loss: 0.6583
2024-05-24 00:07:38 [INFO]: Epoch 007 - training loss: 22950.0876, validation loss: 0.6515
2024-05-24 00:07:39 [INFO]: Epoch 008 - training loss: 22918.2817, validation loss: 0.6555
2024-05-24 00:07:39 [INFO]: Epoch 009 - training loss: 22897.3233, validation loss: 0.6498
2024-05-24 00:07:40 [INFO]: Epoch 010 - training loss: 22882.8643, validation loss: 0.6483
2024-05-24 00:07:40 [INFO]: Epoch 011 - training loss: 22872.2455, validation loss: 0.6535
2024-05-24 00:07:41 [INFO]: Epoch 012 - training loss: 22864.0710, validation loss: 0.6430
2024-05-24 00:07:41 [INFO]: Epoch 013 - training loss: 22858.1442, validation loss: 0.6410
2024-05-24 00:07:42 [INFO]: Epoch 014 - training loss: 22853.4184, validation loss: 0.6398
2024-05-24 00:07:43 [INFO]: Epoch 015 - training loss: 22849.7063, validation loss: 0.6417
2024-05-24 00:07:43 [INFO]: Epoch 016 - training loss: 22845.7917, validation loss: 0.6332
2024-05-24 00:07:44 [INFO]: Epoch 017 - training loss: 22843.6200, validation loss: 0.6365
2024-05-24 00:07:44 [INFO]: Epoch 018 - training loss: 22840.5012, validation loss: 0.6328
2024-05-24 00:07:45 [INFO]: Epoch 019 - training loss: 22838.2781, validation loss: 0.6265
2024-05-24 00:07:45 [INFO]: Epoch 020 - training loss: 22835.9321, validation loss: 0.6213
2024-05-24 00:07:46 [INFO]: Epoch 021 - training loss: 22834.6354, validation loss: 0.6179
2024-05-24 00:07:46 [INFO]: Epoch 022 - training loss: 22833.1592, validation loss: 0.6192
2024-05-24 00:07:47 [INFO]: Epoch 023 - training loss: 22832.2789, validation loss: 0.6153
2024-05-24 00:07:48 [INFO]: Epoch 024 - training loss: 22831.0129, validation loss: 0.6182
2024-05-24 00:07:48 [INFO]: Epoch 025 - training loss: 22830.4910, validation loss: 0.6164
2024-05-24 00:07:49 [INFO]: Epoch 026 - training loss: 22830.2672, validation loss: 0.6128
2024-05-24 00:07:49 [INFO]: Epoch 027 - training loss: 22828.7992, validation loss: 0.6096
2024-05-24 00:07:50 [INFO]: Epoch 028 - training loss: 22828.3040, validation loss: 0.6248
2024-05-24 00:07:50 [INFO]: Epoch 029 - training loss: 22828.3500, validation loss: 0.6127
2024-05-24 00:07:51 [INFO]: Epoch 030 - training loss: 22827.0311, validation loss: 0.6137
2024-05-24 00:07:51 [INFO]: Epoch 031 - training loss: 22825.7653, validation loss: 0.6113
2024-05-24 00:07:52 [INFO]: Epoch 032 - training loss: 22824.9278, validation loss: 0.6170
2024-05-24 00:07:53 [INFO]: Epoch 033 - training loss: 22824.5181, validation loss: 0.6050
2024-05-24 00:07:53 [INFO]: Epoch 034 - training loss: 22823.1412, validation loss: 0.6046
2024-05-24 00:07:54 [INFO]: Epoch 035 - training loss: 22821.7603, validation loss: 0.5987
2024-05-24 00:07:54 [INFO]: Epoch 036 - training loss: 22820.1398, validation loss: 0.5911
2024-05-24 00:07:55 [INFO]: Epoch 037 - training loss: 22818.9652, validation loss: 0.6101
2024-05-24 00:07:55 [INFO]: Epoch 038 - training loss: 22817.6975, validation loss: 0.5851
2024-05-24 00:07:56 [INFO]: Epoch 039 - training loss: 22816.5356, validation loss: 0.5753
2024-05-24 00:07:57 [INFO]: Epoch 040 - training loss: 22814.9648, validation loss: 0.5739
2024-05-24 00:07:57 [INFO]: Epoch 041 - training loss: 22814.5390, validation loss: 0.5749
2024-05-24 00:07:58 [INFO]: Epoch 042 - training loss: 22813.0301, validation loss: 0.5782
2024-05-24 00:07:58 [INFO]: Epoch 043 - training loss: 22811.9195, validation loss: 0.5703
2024-05-24 00:07:59 [INFO]: Epoch 044 - training loss: 22811.4057, validation loss: 0.5693
2024-05-24 00:07:59 [INFO]: Epoch 045 - training loss: 22810.3567, validation loss: 0.5618
2024-05-24 00:08:00 [INFO]: Epoch 046 - training loss: 22809.1107, validation loss: 0.5620
2024-05-24 00:08:00 [INFO]: Epoch 047 - training loss: 22807.8816, validation loss: 0.5523
2024-05-24 00:08:01 [INFO]: Epoch 048 - training loss: 22807.3925, validation loss: 0.5540
2024-05-24 00:08:02 [INFO]: Epoch 049 - training loss: 22806.5505, validation loss: 0.5486
2024-05-24 00:08:02 [INFO]: Epoch 050 - training loss: 22805.8963, validation loss: 0.5662
2024-05-24 00:08:03 [INFO]: Epoch 051 - training loss: 22806.6817, validation loss: 0.5522
2024-05-24 00:08:03 [INFO]: Epoch 052 - training loss: 22805.5617, validation loss: 0.5498
2024-05-24 00:08:04 [INFO]: Epoch 053 - training loss: 22804.2304, validation loss: 0.5482
2024-05-24 00:08:04 [INFO]: Epoch 054 - training loss: 22803.4938, validation loss: 0.5553
2024-05-24 00:08:05 [INFO]: Epoch 055 - training loss: 22803.8680, validation loss: 0.5372
2024-05-24 00:08:05 [INFO]: Epoch 056 - training loss: 22803.7007, validation loss: 0.5402
2024-05-24 00:08:06 [INFO]: Epoch 057 - training loss: 22802.2116, validation loss: 0.5293
2024-05-24 00:08:07 [INFO]: Epoch 058 - training loss: 22801.7228, validation loss: 0.5329
2024-05-24 00:08:07 [INFO]: Epoch 059 - training loss: 22800.6537, validation loss: 0.5273
2024-05-24 00:08:08 [INFO]: Epoch 060 - training loss: 22801.1124, validation loss: 0.5298
2024-05-24 00:08:08 [INFO]: Epoch 061 - training loss: 22799.5091, validation loss: 0.5267
2024-05-24 00:08:09 [INFO]: Epoch 062 - training loss: 22799.5391, validation loss: 0.5212
2024-05-24 00:08:09 [INFO]: Epoch 063 - training loss: 22798.2247, validation loss: 0.5182
2024-05-24 00:08:10 [INFO]: Epoch 064 - training loss: 22798.0011, validation loss: 0.5381
2024-05-24 00:08:11 [INFO]: Epoch 065 - training loss: 22801.9443, validation loss: 0.5169
2024-05-24 00:08:11 [INFO]: Epoch 066 - training loss: 22799.0140, validation loss: 0.5224
2024-05-24 00:08:12 [INFO]: Epoch 067 - training loss: 22798.0488, validation loss: 0.5121
2024-05-24 00:08:12 [INFO]: Epoch 068 - training loss: 22796.9684, validation loss: 0.5254
2024-05-24 00:08:13 [INFO]: Epoch 069 - training loss: 22796.5748, validation loss: 0.5094
2024-05-24 00:08:13 [INFO]: Epoch 070 - training loss: 22795.9036, validation loss: 0.5093
2024-05-24 00:08:14 [INFO]: Epoch 071 - training loss: 22795.5942, validation loss: 0.5110
2024-05-24 00:08:14 [INFO]: Epoch 072 - training loss: 22794.9086, validation loss: 0.5099
2024-05-24 00:08:15 [INFO]: Epoch 073 - training loss: 22794.9574, validation loss: 0.5283
2024-05-24 00:08:16 [INFO]: Epoch 074 - training loss: 22794.3844, validation loss: 0.5229
2024-05-24 00:08:16 [INFO]: Epoch 075 - training loss: 22794.0041, validation loss: 0.5151
2024-05-24 00:08:17 [INFO]: Epoch 076 - training loss: 22794.1368, validation loss: 0.5186
2024-05-24 00:08:17 [INFO]: Epoch 077 - training loss: 22793.6332, validation loss: 0.5141
2024-05-24 00:08:18 [INFO]: Epoch 078 - training loss: 22793.4475, validation loss: 0.5112
2024-05-24 00:08:18 [INFO]: Epoch 079 - training loss: 22793.5247, validation loss: 0.5098
2024-05-24 00:08:19 [INFO]: Epoch 080 - training loss: 22794.1285, validation loss: 0.5055
2024-05-24 00:08:19 [INFO]: Epoch 081 - training loss: 22793.6717, validation loss: 0.5242
2024-05-24 00:08:20 [INFO]: Epoch 082 - training loss: 22793.8049, validation loss: 0.5040
2024-05-24 00:08:21 [INFO]: Epoch 083 - training loss: 22793.8310, validation loss: 0.5120
2024-05-24 00:08:21 [INFO]: Epoch 084 - training loss: 22792.8731, validation loss: 0.5134
2024-05-24 00:08:22 [INFO]: Epoch 085 - training loss: 22792.5275, validation loss: 0.5127
2024-05-24 00:08:22 [INFO]: Epoch 086 - training loss: 22792.3189, validation loss: 0.5044
2024-05-24 00:08:23 [INFO]: Epoch 087 - training loss: 22792.3983, validation loss: 0.5154
2024-05-24 00:08:23 [INFO]: Epoch 088 - training loss: 22791.8840, validation loss: 0.5039
2024-05-24 00:08:24 [INFO]: Epoch 089 - training loss: 22792.0338, validation loss: 0.5105
2024-05-24 00:08:24 [INFO]: Epoch 090 - training loss: 22791.0402, validation loss: 0.5073
2024-05-24 00:08:25 [INFO]: Epoch 091 - training loss: 22790.9386, validation loss: 0.5094
2024-05-24 00:08:26 [INFO]: Epoch 092 - training loss: 22791.2977, validation loss: 0.5100
2024-05-24 00:08:26 [INFO]: Epoch 093 - training loss: 22791.0636, validation loss: 0.5076
2024-05-24 00:08:27 [INFO]: Epoch 094 - training loss: 22790.0116, validation loss: 0.5095
2024-05-24 00:08:27 [INFO]: Epoch 095 - training loss: 22790.3564, validation loss: 0.5060
2024-05-24 00:08:28 [INFO]: Epoch 096 - training loss: 22789.8719, validation loss: 0.5087
2024-05-24 00:08:28 [INFO]: Epoch 097 - training loss: 22790.3733, validation loss: 0.5051
2024-05-24 00:08:29 [INFO]: Epoch 098 - training loss: 22789.6927, validation loss: 0.5101
2024-05-24 00:08:29 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-05-24 00:08:29 [INFO]: Finished training. The best model is from epoch#88.
2024-05-24 00:08:29 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_2/GPVAE_physionet_2012_seta/20240524_T000734/GPVAE.pypots
2024-05-24 00:08:29 [INFO]: GP-VAE on PhysioNet-2012-seta: MAE=0.4248, MSE=0.4452
2024-05-24 00:08:29 [INFO]: Successfully saved to overlay_minibatchmasking_saved_results/round_2/GPVAE_physionet_2012_seta/imputation.pkl
2024-05-24 00:08:29 [INFO]: Using the given device: cuda:0
2024-05-24 00:08:29 [INFO]: Model files will be saved to overlay_minibatchmasking_saved_results/round_2/USGAN_physionet_2012_seta/20240524_T000829
2024-05-24 00:08:29 [INFO]: Tensorboard file will be saved to overlay_minibatchmasking_saved_results/round_2/USGAN_physionet_2012_seta/20240524_T000829/tensorboard
2024-05-24 00:08:29 [INFO]: USGAN initialized with the given hyperparameters, the number of trainable parameters: 16,010,261
2024-05-24 00:08:50 [INFO]: Epoch 001 - generator training loss: 0.5974, discriminator training loss: 0.3787, validation loss: 0.6175
2024-05-24 00:09:08 [INFO]: Epoch 002 - generator training loss: 0.4683, discriminator training loss: 0.2582, validation loss: 0.5217
2024-05-24 00:09:27 [INFO]: Epoch 003 - generator training loss: 0.4265, discriminator training loss: 0.2086, validation loss: 0.5058
2024-05-24 00:09:45 [INFO]: Epoch 004 - generator training loss: 0.4493, discriminator training loss: 0.1547, validation loss: 0.4929
2024-05-24 00:10:03 [INFO]: Epoch 005 - generator training loss: 0.4419, discriminator training loss: 0.1246, validation loss: 0.4716
2024-05-24 00:10:21 [INFO]: Epoch 006 - generator training loss: 0.4329, discriminator training loss: 0.1043, validation loss: 0.4623
2024-05-24 00:10:39 [INFO]: Epoch 007 - generator training loss: 0.4209, discriminator training loss: 0.0901, validation loss: 0.4477
2024-05-24 00:10:57 [INFO]: Epoch 008 - generator training loss: 0.4093, discriminator training loss: 0.0801, validation loss: 0.4336
2024-05-24 00:11:15 [INFO]: Epoch 009 - generator training loss: 0.4015, discriminator training loss: 0.0721, validation loss: 0.4298
2024-05-24 00:11:33 [INFO]: Epoch 010 - generator training loss: 0.3915, discriminator training loss: 0.0653, validation loss: 0.4208
2024-05-24 00:11:51 [INFO]: Epoch 011 - generator training loss: 0.3851, discriminator training loss: 0.0599, validation loss: 0.4132
2024-05-24 00:12:09 [INFO]: Epoch 012 - generator training loss: 0.3795, discriminator training loss: 0.0553, validation loss: 0.4070
2024-05-24 00:12:27 [INFO]: Epoch 013 - generator training loss: 0.3745, discriminator training loss: 0.0514, validation loss: 0.3996
2024-05-24 00:12:45 [INFO]: Epoch 014 - generator training loss: 0.3704, discriminator training loss: 0.0482, validation loss: 0.3937
2024-05-24 00:13:03 [INFO]: Epoch 015 - generator training loss: 0.3624, discriminator training loss: 0.0453, validation loss: 0.3898
2024-05-24 00:13:22 [INFO]: Epoch 016 - generator training loss: 0.3592, discriminator training loss: 0.0429, validation loss: 0.3869
2024-05-24 00:13:40 [INFO]: Epoch 017 - generator training loss: 0.3526, discriminator training loss: 0.0408, validation loss: 0.3824
2024-05-24 00:13:58 [INFO]: Epoch 018 - generator training loss: 0.3489, discriminator training loss: 0.0392, validation loss: 0.3803
2024-05-24 00:14:16 [INFO]: Epoch 019 - generator training loss: 0.3422, discriminator training loss: 0.0376, validation loss: 0.3764
2024-05-24 00:14:34 [INFO]: Epoch 020 - generator training loss: 0.3387, discriminator training loss: 0.0362, validation loss: 0.3707
2024-05-24 00:14:52 [INFO]: Epoch 021 - generator training loss: 0.3333, discriminator training loss: 0.0351, validation loss: 0.3677
2024-05-24 00:15:10 [INFO]: Epoch 022 - generator training loss: 0.3290, discriminator training loss: 0.0342, validation loss: 0.3625
2024-05-24 00:15:28 [INFO]: Epoch 023 - generator training loss: 0.3265, discriminator training loss: 0.0333, validation loss: 0.3621
2024-05-24 00:15:46 [INFO]: Epoch 024 - generator training loss: 0.3224, discriminator training loss: 0.0323, validation loss: 0.3570
2024-05-24 00:16:04 [INFO]: Epoch 025 - generator training loss: 0.3176, discriminator training loss: 0.0317, validation loss: 0.3545
2024-05-24 00:16:22 [INFO]: Epoch 026 - generator training loss: 0.3132, discriminator training loss: 0.0308, validation loss: 0.3521
2024-05-24 00:16:40 [INFO]: Epoch 027 - generator training loss: 0.3112, discriminator training loss: 0.0303, validation loss: 0.3564
2024-05-24 00:16:58 [INFO]: Epoch 028 - generator training loss: 0.3094, discriminator training loss: 0.0297, validation loss: 0.3496
2024-05-24 00:17:16 [INFO]: Epoch 029 - generator training loss: 0.3030, discriminator training loss: 0.0292, validation loss: 0.3457
2024-05-24 00:17:35 [INFO]: Epoch 030 - generator training loss: 0.2996, discriminator training loss: 0.0288, validation loss: 0.3428
2024-05-24 00:17:53 [INFO]: Epoch 031 - generator training loss: 0.2954, discriminator training loss: 0.0282, validation loss: 0.3403
2024-05-24 00:18:11 [INFO]: Epoch 032 - generator training loss: 0.2910, discriminator training loss: 0.0278, validation loss: 0.3451
2024-05-24 00:18:29 [INFO]: Epoch 033 - generator training loss: 0.2891, discriminator training loss: 0.0275, validation loss: 0.3393
2024-05-24 00:18:47 [INFO]: Epoch 034 - generator training loss: 0.2867, discriminator training loss: 0.0270, validation loss: 0.3360
2024-05-24 00:19:05 [INFO]: Epoch 035 - generator training loss: 0.2883, discriminator training loss: 0.0267, validation loss: 0.3317
2024-05-24 00:19:23 [INFO]: Epoch 036 - generator training loss: 0.2821, discriminator training loss: 0.0262, validation loss: 0.3251
2024-05-24 00:19:41 [INFO]: Epoch 037 - generator training loss: 0.2811, discriminator training loss: 0.0261, validation loss: 0.3271
2024-05-24 00:19:59 [INFO]: Epoch 038 - generator training loss: 0.2829, discriminator training loss: 0.0260, validation loss: 0.3251
2024-05-24 00:20:17 [INFO]: Epoch 039 - generator training loss: 0.2730, discriminator training loss: 0.0257, validation loss: 0.3299
2024-05-24 00:20:35 [INFO]: Epoch 040 - generator training loss: 0.2705, discriminator training loss: 0.0254, validation loss: 0.3310
2024-05-24 00:20:53 [INFO]: Epoch 041 - generator training loss: 0.2717, discriminator training loss: 0.0252, validation loss: 0.3209
2024-05-24 00:21:12 [INFO]: Epoch 042 - generator training loss: 0.2603, discriminator training loss: 0.0249, validation loss: 0.3190
2024-05-24 00:21:30 [INFO]: Epoch 043 - generator training loss: 0.2605, discriminator training loss: 0.0245, validation loss: 0.3165
2024-05-24 00:21:48 [INFO]: Epoch 044 - generator training loss: 0.2577, discriminator training loss: 0.0246, validation loss: 0.3226
2024-05-24 00:22:06 [INFO]: Epoch 045 - generator training loss: 0.2558, discriminator training loss: 0.0243, validation loss: 0.3137
2024-05-24 00:22:24 [INFO]: Epoch 046 - generator training loss: 0.2526, discriminator training loss: 0.0240, validation loss: 0.3154
2024-05-24 00:22:42 [INFO]: Epoch 047 - generator training loss: 0.2523, discriminator training loss: 0.0240, validation loss: 0.3145
2024-05-24 00:23:00 [INFO]: Epoch 048 - generator training loss: 0.2502, discriminator training loss: 0.0240, validation loss: 0.3142
2024-05-24 00:23:18 [INFO]: Epoch 049 - generator training loss: 0.2483, discriminator training loss: 0.0236, validation loss: 0.3128
2024-05-24 00:23:36 [INFO]: Epoch 050 - generator training loss: 0.2470, discriminator training loss: 0.0237, validation loss: 0.3134
2024-05-24 00:23:54 [INFO]: Epoch 051 - generator training loss: 0.2422, discriminator training loss: 0.0234, validation loss: 0.3114
2024-05-24 00:24:12 [INFO]: Epoch 052 - generator training loss: 0.2419, discriminator training loss: 0.0232, validation loss: 0.3190
2024-05-24 00:24:30 [INFO]: Epoch 053 - generator training loss: 0.2413, discriminator training loss: 0.0234, validation loss: 0.3164
2024-05-24 00:24:48 [INFO]: Epoch 054 - generator training loss: 0.2378, discriminator training loss: 0.0231, validation loss: 0.3119
2024-05-24 00:25:06 [INFO]: Epoch 055 - generator training loss: 0.2332, discriminator training loss: 0.0229, validation loss: 0.3104
2024-05-24 00:25:25 [INFO]: Epoch 056 - generator training loss: 0.2353, discriminator training loss: 0.0226, validation loss: 0.3104
2024-05-24 00:25:43 [INFO]: Epoch 057 - generator training loss: 0.2349, discriminator training loss: 0.0228, validation loss: 0.3130
2024-05-24 00:26:01 [INFO]: Epoch 058 - generator training loss: 0.2321, discriminator training loss: 0.0226, validation loss: 0.3118
2024-05-24 00:26:19 [INFO]: Epoch 059 - generator training loss: 0.2259, discriminator training loss: 0.0224, validation loss: 0.3127
2024-05-24 00:26:37 [INFO]: Epoch 060 - generator training loss: 0.2233, discriminator training loss: 0.0224, validation loss: 0.3081
2024-05-24 00:26:55 [INFO]: Epoch 061 - generator training loss: 0.2196, discriminator training loss: 0.0220, validation loss: 0.3097
2024-05-24 00:27:13 [INFO]: Epoch 062 - generator training loss: 0.2176, discriminator training loss: 0.0220, validation loss: 0.3084
2024-05-24 00:27:31 [INFO]: Epoch 063 - generator training loss: 0.2275, discriminator training loss: 0.0219, validation loss: 0.3118
2024-05-24 00:27:49 [INFO]: Epoch 064 - generator training loss: 0.2238, discriminator training loss: 0.0218, validation loss: 0.3136
2024-05-24 00:28:07 [INFO]: Epoch 065 - generator training loss: 0.2223, discriminator training loss: 0.0218, validation loss: 0.3165
2024-05-24 00:28:25 [INFO]: Epoch 066 - generator training loss: 0.2213, discriminator training loss: 0.0216, validation loss: 0.3084
2024-05-24 00:28:43 [INFO]: Epoch 067 - generator training loss: 0.2132, discriminator training loss: 0.0215, validation loss: 0.3233
2024-05-24 00:29:01 [INFO]: Epoch 068 - generator training loss: 0.2196, discriminator training loss: 0.0215, validation loss: 0.3090
2024-05-24 00:29:19 [INFO]: Epoch 069 - generator training loss: 0.2159, discriminator training loss: 0.0213, validation loss: 0.3107
2024-05-24 00:29:38 [INFO]: Epoch 070 - generator training loss: 0.2159, discriminator training loss: 0.0212, validation loss: 0.3114
2024-05-24 00:29:38 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-05-24 00:29:38 [INFO]: Finished training. The best model is from epoch#60.
2024-05-24 00:29:38 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_2/USGAN_physionet_2012_seta/20240524_T000829/USGAN.pypots
2024-05-24 00:29:40 [INFO]: US-GAN on PhysioNet-2012-seta: MAE=0.2959, MSE=0.2622
2024-05-24 00:29:50 [INFO]: Successfully saved to overlay_minibatchmasking_saved_results/round_2/USGAN_physionet_2012_seta/imputation.pkl
2024-05-24 00:29:50 [INFO]: Using the given device: cuda:0
2024-05-24 00:29:50 [INFO]: Model files will be saved to overlay_minibatchmasking_saved_results/round_2/BRITS_physionet_2012_seta/20240524_T002950
2024-05-24 00:29:50 [INFO]: Tensorboard file will be saved to overlay_minibatchmasking_saved_results/round_2/BRITS_physionet_2012_seta/20240524_T002950/tensorboard
2024-05-24 00:29:50 [INFO]: BRITS initialized with the given hyperparameters, the number of trainable parameters: 9,176,048
2024-05-24 00:30:04 [INFO]: Epoch 001 - training loss: 1.1181, validation loss: 0.5209
2024-05-24 00:30:16 [INFO]: Epoch 002 - training loss: 0.9084, validation loss: 0.4645
2024-05-24 00:30:28 [INFO]: Epoch 003 - training loss: 0.8427, validation loss: 0.4347
2024-05-24 00:30:40 [INFO]: Epoch 004 - training loss: 0.8041, validation loss: 0.4145
2024-05-24 00:30:52 [INFO]: Epoch 005 - training loss: 0.7743, validation loss: 0.3990
2024-05-24 00:31:03 [INFO]: Epoch 006 - training loss: 0.7515, validation loss: 0.3855
2024-05-24 00:31:15 [INFO]: Epoch 007 - training loss: 0.7326, validation loss: 0.3745
2024-05-24 00:31:27 [INFO]: Epoch 008 - training loss: 0.7155, validation loss: 0.3673
2024-05-24 00:31:39 [INFO]: Epoch 009 - training loss: 0.7031, validation loss: 0.3627
2024-05-24 00:31:50 [INFO]: Epoch 010 - training loss: 0.6912, validation loss: 0.3593
2024-05-24 00:32:02 [INFO]: Epoch 011 - training loss: 0.6809, validation loss: 0.3580
2024-05-24 00:32:14 [INFO]: Epoch 012 - training loss: 0.6729, validation loss: 0.3549
2024-05-24 00:32:26 [INFO]: Epoch 013 - training loss: 0.6653, validation loss: 0.3553
2024-05-24 00:32:38 [INFO]: Epoch 014 - training loss: 0.6583, validation loss: 0.3533
2024-05-24 00:32:49 [INFO]: Epoch 015 - training loss: 0.6534, validation loss: 0.3505
2024-05-24 00:33:01 [INFO]: Epoch 016 - training loss: 0.6476, validation loss: 0.3492
2024-05-24 00:33:13 [INFO]: Epoch 017 - training loss: 0.6431, validation loss: 0.3499
2024-05-24 00:33:25 [INFO]: Epoch 018 - training loss: 0.6387, validation loss: 0.3465
2024-05-24 00:33:37 [INFO]: Epoch 019 - training loss: 0.6350, validation loss: 0.3484
2024-05-24 00:33:48 [INFO]: Epoch 020 - training loss: 0.6323, validation loss: 0.3470
2024-05-24 00:34:00 [INFO]: Epoch 021 - training loss: 0.6280, validation loss: 0.3443
2024-05-24 00:34:12 [INFO]: Epoch 022 - training loss: 0.6251, validation loss: 0.3435
2024-05-24 00:34:24 [INFO]: Epoch 023 - training loss: 0.6211, validation loss: 0.3437
2024-05-24 00:34:36 [INFO]: Epoch 024 - training loss: 0.6184, validation loss: 0.3425
2024-05-24 00:34:48 [INFO]: Epoch 025 - training loss: 0.6161, validation loss: 0.3428
2024-05-24 00:34:59 [INFO]: Epoch 026 - training loss: 0.6146, validation loss: 0.3440
2024-05-24 00:35:11 [INFO]: Epoch 027 - training loss: 0.6108, validation loss: 0.3427
2024-05-24 00:35:23 [INFO]: Epoch 028 - training loss: 0.6080, validation loss: 0.3411
2024-05-24 00:35:35 [INFO]: Epoch 029 - training loss: 0.6036, validation loss: 0.3400
2024-05-24 00:35:46 [INFO]: Epoch 030 - training loss: 0.5997, validation loss: 0.3393
2024-05-24 00:35:58 [INFO]: Epoch 031 - training loss: 0.5975, validation loss: 0.3407
2024-05-24 00:36:10 [INFO]: Epoch 032 - training loss: 0.5946, validation loss: 0.3381
2024-05-24 00:36:22 [INFO]: Epoch 033 - training loss: 0.5916, validation loss: 0.3407
2024-05-24 00:36:34 [INFO]: Epoch 034 - training loss: 0.5890, validation loss: 0.3412
2024-05-24 00:36:45 [INFO]: Epoch 035 - training loss: 0.5862, validation loss: 0.3403
2024-05-24 00:36:57 [INFO]: Epoch 036 - training loss: 0.5840, validation loss: 0.3412
2024-05-24 00:37:09 [INFO]: Epoch 037 - training loss: 0.5809, validation loss: 0.3414
2024-05-24 00:37:21 [INFO]: Epoch 038 - training loss: 0.5776, validation loss: 0.3411
2024-05-24 00:37:32 [INFO]: Epoch 039 - training loss: 0.5751, validation loss: 0.3439
2024-05-24 00:37:44 [INFO]: Epoch 040 - training loss: 0.5729, validation loss: 0.3419
2024-05-24 00:37:56 [INFO]: Epoch 041 - training loss: 0.5696, validation loss: 0.3435
2024-05-24 00:38:08 [INFO]: Epoch 042 - training loss: 0.5681, validation loss: 0.3452
2024-05-24 00:38:08 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-05-24 00:38:08 [INFO]: Finished training. The best model is from epoch#32.
2024-05-24 00:38:08 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_2/BRITS_physionet_2012_seta/20240524_T002950/BRITS.pypots
2024-05-24 00:38:10 [INFO]: BRITS on PhysioNet-2012-seta: MAE=0.2559, MSE=0.2660
2024-05-24 00:38:20 [INFO]: Successfully saved to overlay_minibatchmasking_saved_results/round_2/BRITS_physionet_2012_seta/imputation.pkl
2024-05-24 00:38:20 [INFO]: Using the given device: cuda:0
2024-05-24 00:38:20 [INFO]: Model files will be saved to overlay_minibatchmasking_saved_results/round_2/MRNN_physionet_2012_seta/20240524_T003820
2024-05-24 00:38:20 [INFO]: Tensorboard file will be saved to overlay_minibatchmasking_saved_results/round_2/MRNN_physionet_2012_seta/20240524_T003820/tensorboard
2024-05-24 00:38:20 [INFO]: MRNN initialized with the given hyperparameters, the number of trainable parameters: 7,599
2024-05-24 00:38:26 [INFO]: Epoch 001 - training loss: 1.1295, validation loss: 0.9969
2024-05-24 00:38:26 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_2/MRNN_physionet_2012_seta/20240524_T003820/MRNN_epoch1_loss0.9969234675168991.pypots
2024-05-24 00:38:28 [INFO]: Epoch 002 - training loss: 0.7058, validation loss: 0.9745
2024-05-24 00:38:28 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_2/MRNN_physionet_2012_seta/20240524_T003820/MRNN_epoch2_loss0.9745120704174042.pypots
2024-05-24 00:38:31 [INFO]: Epoch 003 - training loss: 0.6214, validation loss: 0.9446
2024-05-24 00:38:31 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_2/MRNN_physionet_2012_seta/20240524_T003820/MRNN_epoch3_loss0.9445986717939376.pypots
2024-05-24 00:38:34 [INFO]: Epoch 004 - training loss: 0.5713, validation loss: 0.9288
2024-05-24 00:38:34 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_2/MRNN_physionet_2012_seta/20240524_T003820/MRNN_epoch4_loss0.9288031995296478.pypots
2024-05-24 00:38:37 [INFO]: Epoch 005 - training loss: 0.5494, validation loss: 0.9218
2024-05-24 00:38:37 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_2/MRNN_physionet_2012_seta/20240524_T003820/MRNN_epoch5_loss0.9218145817518234.pypots
2024-05-24 00:38:39 [INFO]: Epoch 006 - training loss: 0.5255, validation loss: 0.9163
2024-05-24 00:38:39 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_2/MRNN_physionet_2012_seta/20240524_T003820/MRNN_epoch6_loss0.9162919133901596.pypots
2024-05-24 00:38:42 [INFO]: Epoch 007 - training loss: 0.5150, validation loss: 0.9131
2024-05-24 00:38:42 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_2/MRNN_physionet_2012_seta/20240524_T003820/MRNN_epoch7_loss0.9130960136651993.pypots
2024-05-24 00:38:45 [INFO]: Epoch 008 - training loss: 0.5047, validation loss: 0.9111
2024-05-24 00:38:45 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_2/MRNN_physionet_2012_seta/20240524_T003820/MRNN_epoch8_loss0.9110545307397843.pypots
2024-05-24 00:38:48 [INFO]: Epoch 009 - training loss: 0.4934, validation loss: 0.9098
2024-05-24 00:38:48 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_2/MRNN_physionet_2012_seta/20240524_T003820/MRNN_epoch9_loss0.9097710460424423.pypots
2024-05-24 00:38:50 [INFO]: Epoch 010 - training loss: 0.4893, validation loss: 0.9108
2024-05-24 00:38:50 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_2/MRNN_physionet_2012_seta/20240524_T003820/MRNN_epoch10_loss0.9107845366001129.pypots
2024-05-24 00:38:53 [INFO]: Epoch 011 - training loss: 0.4810, validation loss: 0.9112
2024-05-24 00:38:53 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_2/MRNN_physionet_2012_seta/20240524_T003820/MRNN_epoch11_loss0.9112311929464341.pypots
2024-05-24 00:38:56 [INFO]: Epoch 012 - training loss: 0.4794, validation loss: 0.9107
2024-05-24 00:38:56 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_2/MRNN_physionet_2012_seta/20240524_T003820/MRNN_epoch12_loss0.9106639266014099.pypots
2024-05-24 00:38:59 [INFO]: Epoch 013 - training loss: 0.4724, validation loss: 0.9124
2024-05-24 00:38:59 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_2/MRNN_physionet_2012_seta/20240524_T003820/MRNN_epoch13_loss0.9124460041522979.pypots
2024-05-24 00:39:01 [INFO]: Epoch 014 - training loss: 0.4731, validation loss: 0.9137
2024-05-24 00:39:01 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_2/MRNN_physionet_2012_seta/20240524_T003820/MRNN_epoch14_loss0.9137453883886337.pypots
2024-05-24 00:39:04 [INFO]: Epoch 015 - training loss: 0.4655, validation loss: 0.9166
2024-05-24 00:39:04 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_2/MRNN_physionet_2012_seta/20240524_T003820/MRNN_epoch15_loss0.9165965616703033.pypots
2024-05-24 00:39:07 [INFO]: Epoch 016 - training loss: 0.4583, validation loss: 0.9169
2024-05-24 00:39:07 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_2/MRNN_physionet_2012_seta/20240524_T003820/MRNN_epoch16_loss0.91685191988945.pypots
2024-05-24 00:39:10 [INFO]: Epoch 017 - training loss: 0.4591, validation loss: 0.9186
2024-05-24 00:39:10 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_2/MRNN_physionet_2012_seta/20240524_T003820/MRNN_epoch17_loss0.9186051249504089.pypots
2024-05-24 00:39:12 [INFO]: Epoch 018 - training loss: 0.4701, validation loss: 0.9220
2024-05-24 00:39:12 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_2/MRNN_physionet_2012_seta/20240524_T003820/MRNN_epoch18_loss0.9220147490501404.pypots
2024-05-24 00:39:15 [INFO]: Epoch 019 - training loss: 0.4582, validation loss: 0.9210
2024-05-24 00:39:15 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_2/MRNN_physionet_2012_seta/20240524_T003820/MRNN_epoch19_loss0.9210487306118011.pypots
2024-05-24 00:39:15 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-05-24 00:39:15 [INFO]: Finished training. The best model is from epoch#9.
2024-05-24 00:39:15 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_2/MRNN_physionet_2012_seta/20240524_T003820/MRNN.pypots
2024-05-24 00:39:16 [INFO]: MRNN on PhysioNet-2012-seta: MAE=0.6828, MSE=0.9060
2024-05-24 00:39:20 [INFO]: Successfully saved to overlay_minibatchmasking_saved_results/round_2/MRNN_physionet_2012_seta/imputation.pkl
2024-05-24 00:39:20 [INFO]: Using the given device: cpu
2024-05-24 00:39:20 [INFO]: LOCF on PhysioNet-2012-seta: MAE=0.4109, MSE=0.5324
2024-05-24 00:39:20 [INFO]: Successfully created the given path "saved_results/round_2/LOCF_physionet_2012_seta".
2024-05-24 00:39:20 [INFO]: Successfully saved to overlay_minibatchmasking_saved_results/round_2/LOCF_physionet_2012_seta/imputation.pkl
2024-05-24 00:39:20 [INFO]: Median on PhysioNet-2012-seta: MAE=0.6868, MSE=1.0189
2024-05-24 00:39:21 [INFO]: Successfully created the given path "saved_results/round_2/Median_physionet_2012_seta".
2024-05-24 00:39:21 [INFO]: Successfully saved to overlay_minibatchmasking_saved_results/round_2/Median_physionet_2012_seta/imputation.pkl
2024-05-24 00:39:21 [INFO]: Mean on PhysioNet-2012-seta: MAE=0.7045, MSE=0.9898
2024-05-24 00:39:21 [INFO]: Successfully created the given path "saved_results/round_2/Mean_physionet_2012_seta".
2024-05-24 00:39:21 [INFO]: Successfully saved to overlay_minibatchmasking_saved_results/round_2/Mean_physionet_2012_seta/imputation.pkl
2024-05-24 00:39:21 [INFO]: Have set the random seed as 2026 for numpy and pytorch.
2024-05-24 00:39:21 [INFO]: Using the given device: cuda:0
2024-05-24 00:39:21 [INFO]: Model files will be saved to overlay_minibatchmasking_saved_results/round_3/SAITS_physionet_2012_seta/20240524_T003921
2024-05-24 00:39:21 [INFO]: Tensorboard file will be saved to overlay_minibatchmasking_saved_results/round_3/SAITS_physionet_2012_seta/20240524_T003921/tensorboard
2024-05-24 00:39:21 [INFO]: SAITS initialized with the given hyperparameters, the number of trainable parameters: 5,322,806
2024-05-24 00:39:22 [INFO]: Epoch 001 - training loss: 1.1261, validation loss: 0.4961
2024-05-24 00:39:23 [INFO]: Epoch 002 - training loss: 0.7243, validation loss: 0.4411
2024-05-24 00:39:25 [INFO]: Epoch 003 - training loss: 0.5978, validation loss: 0.4077
2024-05-24 00:39:26 [INFO]: Epoch 004 - training loss: 0.5316, validation loss: 0.3844
2024-05-24 00:39:27 [INFO]: Epoch 005 - training loss: 0.4928, validation loss: 0.3742
2024-05-24 00:39:28 [INFO]: Epoch 006 - training loss: 0.4650, validation loss: 0.3539
2024-05-24 00:39:30 [INFO]: Epoch 007 - training loss: 0.4427, validation loss: 0.3539
2024-05-24 00:39:31 [INFO]: Epoch 008 - training loss: 0.4165, validation loss: 0.3302
2024-05-24 00:39:32 [INFO]: Epoch 009 - training loss: 0.4025, validation loss: 0.3184
2024-05-24 00:39:33 [INFO]: Epoch 010 - training loss: 0.3876, validation loss: 0.3061
2024-05-24 00:39:35 [INFO]: Epoch 011 - training loss: 0.3750, validation loss: 0.2971
2024-05-24 00:39:36 [INFO]: Epoch 012 - training loss: 0.3677, validation loss: 0.2917
2024-05-24 00:39:37 [INFO]: Epoch 013 - training loss: 0.3494, validation loss: 0.2850
2024-05-24 00:39:38 [INFO]: Epoch 014 - training loss: 0.3459, validation loss: 0.2868
2024-05-24 00:39:40 [INFO]: Epoch 015 - training loss: 0.3401, validation loss: 0.2777
2024-05-24 00:39:42 [INFO]: Epoch 016 - training loss: 0.3345, validation loss: 0.2779
2024-05-24 00:39:43 [INFO]: Epoch 017 - training loss: 0.3246, validation loss: 0.2735
2024-05-24 00:39:44 [INFO]: Epoch 018 - training loss: 0.3218, validation loss: 0.2682
2024-05-24 00:39:45 [INFO]: Epoch 019 - training loss: 0.3151, validation loss: 0.2677
2024-05-24 00:39:47 [INFO]: Epoch 020 - training loss: 0.3146, validation loss: 0.2611
2024-05-24 00:39:48 [INFO]: Epoch 021 - training loss: 0.3058, validation loss: 0.2575
2024-05-24 00:39:49 [INFO]: Epoch 022 - training loss: 0.3058, validation loss: 0.2593
2024-05-24 00:39:50 [INFO]: Epoch 023 - training loss: 0.3064, validation loss: 0.2610
2024-05-24 00:39:52 [INFO]: Epoch 024 - training loss: 0.2980, validation loss: 0.2519
2024-05-24 00:39:53 [INFO]: Epoch 025 - training loss: 0.2983, validation loss: 0.2481
2024-05-24 00:39:54 [INFO]: Epoch 026 - training loss: 0.2922, validation loss: 0.2510
2024-05-24 00:39:55 [INFO]: Epoch 027 - training loss: 0.2919, validation loss: 0.2485
2024-05-24 00:39:57 [INFO]: Epoch 028 - training loss: 0.2909, validation loss: 0.2444
2024-05-24 00:39:58 [INFO]: Epoch 029 - training loss: 0.2865, validation loss: 0.2484
2024-05-24 00:39:59 [INFO]: Epoch 030 - training loss: 0.2872, validation loss: 0.2459
2024-05-24 00:40:01 [INFO]: Epoch 031 - training loss: 0.2846, validation loss: 0.2421
2024-05-24 00:40:02 [INFO]: Epoch 032 - training loss: 0.2823, validation loss: 0.2433
2024-05-24 00:40:03 [INFO]: Epoch 033 - training loss: 0.2794, validation loss: 0.2417
2024-05-24 00:40:04 [INFO]: Epoch 034 - training loss: 0.2794, validation loss: 0.2412
2024-05-24 00:40:06 [INFO]: Epoch 035 - training loss: 0.2774, validation loss: 0.2394
2024-05-24 00:40:07 [INFO]: Epoch 036 - training loss: 0.2782, validation loss: 0.2353
2024-05-24 00:40:08 [INFO]: Epoch 037 - training loss: 0.2743, validation loss: 0.2373
2024-05-24 00:40:09 [INFO]: Epoch 038 - training loss: 0.2771, validation loss: 0.2348
2024-05-24 00:40:11 [INFO]: Epoch 039 - training loss: 0.2736, validation loss: 0.2392
2024-05-24 00:40:12 [INFO]: Epoch 040 - training loss: 0.2747, validation loss: 0.2354
2024-05-24 00:40:13 [INFO]: Epoch 041 - training loss: 0.2735, validation loss: 0.2398
2024-05-24 00:40:14 [INFO]: Epoch 042 - training loss: 0.2739, validation loss: 0.2313
2024-05-24 00:40:16 [INFO]: Epoch 043 - training loss: 0.2696, validation loss: 0.2353
2024-05-24 00:40:17 [INFO]: Epoch 044 - training loss: 0.2674, validation loss: 0.2299
2024-05-24 00:40:18 [INFO]: Epoch 045 - training loss: 0.2702, validation loss: 0.2281
2024-05-24 00:40:19 [INFO]: Epoch 046 - training loss: 0.2671, validation loss: 0.2286
2024-05-24 00:40:21 [INFO]: Epoch 047 - training loss: 0.2677, validation loss: 0.2283
2024-05-24 00:40:22 [INFO]: Epoch 048 - training loss: 0.2674, validation loss: 0.2379
2024-05-24 00:40:23 [INFO]: Epoch 049 - training loss: 0.2671, validation loss: 0.2292
2024-05-24 00:40:25 [INFO]: Epoch 050 - training loss: 0.2631, validation loss: 0.2272
2024-05-24 00:40:26 [INFO]: Epoch 051 - training loss: 0.2632, validation loss: 0.2278
2024-05-24 00:40:27 [INFO]: Epoch 052 - training loss: 0.2623, validation loss: 0.2269
2024-05-24 00:40:28 [INFO]: Epoch 053 - training loss: 0.2615, validation loss: 0.2318
2024-05-24 00:40:30 [INFO]: Epoch 054 - training loss: 0.2598, validation loss: 0.2345
2024-05-24 00:40:31 [INFO]: Epoch 055 - training loss: 0.2611, validation loss: 0.2260
2024-05-24 00:40:32 [INFO]: Epoch 056 - training loss: 0.2588, validation loss: 0.2230
2024-05-24 00:40:33 [INFO]: Epoch 057 - training loss: 0.2588, validation loss: 0.2262
2024-05-24 00:40:35 [INFO]: Epoch 058 - training loss: 0.2584, validation loss: 0.2365
2024-05-24 00:40:36 [INFO]: Epoch 059 - training loss: 0.2579, validation loss: 0.2280
2024-05-24 00:40:37 [INFO]: Epoch 060 - training loss: 0.2577, validation loss: 0.2222
2024-05-24 00:40:39 [INFO]: Epoch 061 - training loss: 0.2563, validation loss: 0.2262
2024-05-24 00:40:40 [INFO]: Epoch 062 - training loss: 0.2579, validation loss: 0.2284
2024-05-24 00:40:41 [INFO]: Epoch 063 - training loss: 0.2594, validation loss: 0.2176
2024-05-24 00:40:42 [INFO]: Epoch 064 - training loss: 0.2570, validation loss: 0.2226
2024-05-24 00:40:44 [INFO]: Epoch 065 - training loss: 0.2565, validation loss: 0.2192
2024-05-24 00:40:45 [INFO]: Epoch 066 - training loss: 0.2528, validation loss: 0.2275
2024-05-24 00:40:46 [INFO]: Epoch 067 - training loss: 0.2510, validation loss: 0.2232
2024-05-24 00:40:47 [INFO]: Epoch 068 - training loss: 0.2543, validation loss: 0.2261
2024-05-24 00:40:49 [INFO]: Epoch 069 - training loss: 0.2524, validation loss: 0.2178
2024-05-24 00:40:50 [INFO]: Epoch 070 - training loss: 0.2541, validation loss: 0.2415
2024-05-24 00:40:51 [INFO]: Epoch 071 - training loss: 0.2534, validation loss: 0.2277
2024-05-24 00:40:53 [INFO]: Epoch 072 - training loss: 0.2497, validation loss: 0.2298
2024-05-24 00:40:54 [INFO]: Epoch 073 - training loss: 0.2494, validation loss: 0.2365
2024-05-24 00:40:54 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-05-24 00:40:54 [INFO]: Finished training. The best model is from epoch#63.
2024-05-24 00:40:54 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_3/SAITS_physionet_2012_seta/20240524_T003921/SAITS.pypots
2024-05-24 00:40:54 [INFO]: SAITS on PhysioNet-2012-seta: MAE=0.2073, MSE=0.2275
2024-05-24 00:40:54 [INFO]: Successfully saved to overlay_minibatchmasking_saved_results/round_3/SAITS_physionet_2012_seta/imputation.pkl
2024-05-24 00:40:54 [INFO]: Using the given device: cuda:0
2024-05-24 00:40:54 [INFO]: Model files will be saved to overlay_minibatchmasking_saved_results/round_3/Transformer_physionet_2012_seta/20240524_T004054
2024-05-24 00:40:54 [INFO]: Tensorboard file will be saved to overlay_minibatchmasking_saved_results/round_3/Transformer_physionet_2012_seta/20240524_T004054/tensorboard
2024-05-24 00:40:54 [INFO]: Transformer initialized with the given hyperparameters, the number of trainable parameters: 2,152,613
2024-05-24 00:40:55 [INFO]: Epoch 001 - training loss: 1.1747, validation loss: 0.5616
2024-05-24 00:40:56 [INFO]: Epoch 002 - training loss: 0.7391, validation loss: 0.4739
2024-05-24 00:40:57 [INFO]: Epoch 003 - training loss: 0.6407, validation loss: 0.4537
2024-05-24 00:40:57 [INFO]: Epoch 004 - training loss: 0.5911, validation loss: 0.4283
2024-05-24 00:40:58 [INFO]: Epoch 005 - training loss: 0.5444, validation loss: 0.4131
2024-05-24 00:40:59 [INFO]: Epoch 006 - training loss: 0.5193, validation loss: 0.4029
2024-05-24 00:40:59 [INFO]: Epoch 007 - training loss: 0.4971, validation loss: 0.3909
2024-05-24 00:41:00 [INFO]: Epoch 008 - training loss: 0.4713, validation loss: 0.3819
2024-05-24 00:41:01 [INFO]: Epoch 009 - training loss: 0.4729, validation loss: 0.3683
2024-05-24 00:41:01 [INFO]: Epoch 010 - training loss: 0.4488, validation loss: 0.3614
2024-05-24 00:41:02 [INFO]: Epoch 011 - training loss: 0.4375, validation loss: 0.3537
2024-05-24 00:41:03 [INFO]: Epoch 012 - training loss: 0.4288, validation loss: 0.3509
2024-05-24 00:41:03 [INFO]: Epoch 013 - training loss: 0.4227, validation loss: 0.3508
2024-05-24 00:41:04 [INFO]: Epoch 014 - training loss: 0.4118, validation loss: 0.3369
2024-05-24 00:41:05 [INFO]: Epoch 015 - training loss: 0.4044, validation loss: 0.3333
2024-05-24 00:41:05 [INFO]: Epoch 016 - training loss: 0.3939, validation loss: 0.3267
2024-05-24 00:41:06 [INFO]: Epoch 017 - training loss: 0.3908, validation loss: 0.3237
2024-05-24 00:41:07 [INFO]: Epoch 018 - training loss: 0.3851, validation loss: 0.3173
2024-05-24 00:41:08 [INFO]: Epoch 019 - training loss: 0.3760, validation loss: 0.3158
2024-05-24 00:41:08 [INFO]: Epoch 020 - training loss: 0.3756, validation loss: 0.3125
2024-05-24 00:41:09 [INFO]: Epoch 021 - training loss: 0.3646, validation loss: 0.3136
2024-05-24 00:41:10 [INFO]: Epoch 022 - training loss: 0.3603, validation loss: 0.3116
2024-05-24 00:41:10 [INFO]: Epoch 023 - training loss: 0.3599, validation loss: 0.3077
2024-05-24 00:41:11 [INFO]: Epoch 024 - training loss: 0.3558, validation loss: 0.3122
2024-05-24 00:41:12 [INFO]: Epoch 025 - training loss: 0.3544, validation loss: 0.3024
2024-05-24 00:41:12 [INFO]: Epoch 026 - training loss: 0.3463, validation loss: 0.3063
2024-05-24 00:41:13 [INFO]: Epoch 027 - training loss: 0.3454, validation loss: 0.3022
2024-05-24 00:41:14 [INFO]: Epoch 028 - training loss: 0.3433, validation loss: 0.2938
2024-05-24 00:41:14 [INFO]: Epoch 029 - training loss: 0.3383, validation loss: 0.2944
2024-05-24 00:41:15 [INFO]: Epoch 030 - training loss: 0.3391, validation loss: 0.2909
2024-05-24 00:41:16 [INFO]: Epoch 031 - training loss: 0.3340, validation loss: 0.2966
2024-05-24 00:41:17 [INFO]: Epoch 032 - training loss: 0.3325, validation loss: 0.2909
2024-05-24 00:41:17 [INFO]: Epoch 033 - training loss: 0.3334, validation loss: 0.2923
2024-05-24 00:41:18 [INFO]: Epoch 034 - training loss: 0.3274, validation loss: 0.2857
2024-05-24 00:41:19 [INFO]: Epoch 035 - training loss: 0.3262, validation loss: 0.2872
2024-05-24 00:41:19 [INFO]: Epoch 036 - training loss: 0.3271, validation loss: 0.2871
2024-05-24 00:41:20 [INFO]: Epoch 037 - training loss: 0.3231, validation loss: 0.2891
2024-05-24 00:41:21 [INFO]: Epoch 038 - training loss: 0.3270, validation loss: 0.2827
2024-05-24 00:41:21 [INFO]: Epoch 039 - training loss: 0.3177, validation loss: 0.2821
2024-05-24 00:41:22 [INFO]: Epoch 040 - training loss: 0.3169, validation loss: 0.2808
2024-05-24 00:41:23 [INFO]: Epoch 041 - training loss: 0.3182, validation loss: 0.2758
2024-05-24 00:41:23 [INFO]: Epoch 042 - training loss: 0.3133, validation loss: 0.2754
2024-05-24 00:41:24 [INFO]: Epoch 043 - training loss: 0.3114, validation loss: 0.2754
2024-05-24 00:41:25 [INFO]: Epoch 044 - training loss: 0.3114, validation loss: 0.2776
2024-05-24 00:41:25 [INFO]: Epoch 045 - training loss: 0.3113, validation loss: 0.2756
2024-05-24 00:41:26 [INFO]: Epoch 046 - training loss: 0.3088, validation loss: 0.2740
2024-05-24 00:41:27 [INFO]: Epoch 047 - training loss: 0.3086, validation loss: 0.2734
2024-05-24 00:41:28 [INFO]: Epoch 048 - training loss: 0.3079, validation loss: 0.2680
2024-05-24 00:41:28 [INFO]: Epoch 049 - training loss: 0.3059, validation loss: 0.2677
2024-05-24 00:41:29 [INFO]: Epoch 050 - training loss: 0.3039, validation loss: 0.2674
2024-05-24 00:41:30 [INFO]: Epoch 051 - training loss: 0.3017, validation loss: 0.2672
2024-05-24 00:41:30 [INFO]: Epoch 052 - training loss: 0.3013, validation loss: 0.2703
2024-05-24 00:41:31 [INFO]: Epoch 053 - training loss: 0.2988, validation loss: 0.2673
2024-05-24 00:41:32 [INFO]: Epoch 054 - training loss: 0.3003, validation loss: 0.2649
2024-05-24 00:41:32 [INFO]: Epoch 055 - training loss: 0.2978, validation loss: 0.2670
2024-05-24 00:41:33 [INFO]: Epoch 056 - training loss: 0.2984, validation loss: 0.2653
2024-05-24 00:41:34 [INFO]: Epoch 057 - training loss: 0.2963, validation loss: 0.2632
2024-05-24 00:41:35 [INFO]: Epoch 058 - training loss: 0.2950, validation loss: 0.2630
2024-05-24 00:41:35 [INFO]: Epoch 059 - training loss: 0.2954, validation loss: 0.2617
2024-05-24 00:41:36 [INFO]: Epoch 060 - training loss: 0.2932, validation loss: 0.2606
2024-05-24 00:41:37 [INFO]: Epoch 061 - training loss: 0.2926, validation loss: 0.2570
2024-05-24 00:41:37 [INFO]: Epoch 062 - training loss: 0.2907, validation loss: 0.2589
2024-05-24 00:41:38 [INFO]: Epoch 063 - training loss: 0.2897, validation loss: 0.2573
2024-05-24 00:41:39 [INFO]: Epoch 064 - training loss: 0.2909, validation loss: 0.2566
2024-05-24 00:41:39 [INFO]: Epoch 065 - training loss: 0.2905, validation loss: 0.2574
2024-05-24 00:41:40 [INFO]: Epoch 066 - training loss: 0.2892, validation loss: 0.2543
2024-05-24 00:41:41 [INFO]: Epoch 067 - training loss: 0.2877, validation loss: 0.2515
2024-05-24 00:41:41 [INFO]: Epoch 068 - training loss: 0.2879, validation loss: 0.2517
2024-05-24 00:41:42 [INFO]: Epoch 069 - training loss: 0.2890, validation loss: 0.2497
2024-05-24 00:41:43 [INFO]: Epoch 070 - training loss: 0.2865, validation loss: 0.2519
2024-05-24 00:41:43 [INFO]: Epoch 071 - training loss: 0.2831, validation loss: 0.2534
2024-05-24 00:41:44 [INFO]: Epoch 072 - training loss: 0.2822, validation loss: 0.2474
2024-05-24 00:41:45 [INFO]: Epoch 073 - training loss: 0.2884, validation loss: 0.2506
2024-05-24 00:41:46 [INFO]: Epoch 074 - training loss: 0.2843, validation loss: 0.2544
2024-05-24 00:41:46 [INFO]: Epoch 075 - training loss: 0.2850, validation loss: 0.2471
2024-05-24 00:41:47 [INFO]: Epoch 076 - training loss: 0.2826, validation loss: 0.2473
2024-05-24 00:41:48 [INFO]: Epoch 077 - training loss: 0.2821, validation loss: 0.2480
2024-05-24 00:41:48 [INFO]: Epoch 078 - training loss: 0.2813, validation loss: 0.2456
2024-05-24 00:41:49 [INFO]: Epoch 079 - training loss: 0.2814, validation loss: 0.2486
2024-05-24 00:41:50 [INFO]: Epoch 080 - training loss: 0.2791, validation loss: 0.2448
2024-05-24 00:41:50 [INFO]: Epoch 081 - training loss: 0.2784, validation loss: 0.2446
2024-05-24 00:41:51 [INFO]: Epoch 082 - training loss: 0.2808, validation loss: 0.2431
2024-05-24 00:41:52 [INFO]: Epoch 083 - training loss: 0.2820, validation loss: 0.2421
2024-05-24 00:41:52 [INFO]: Epoch 084 - training loss: 0.2754, validation loss: 0.2423
2024-05-24 00:41:53 [INFO]: Epoch 085 - training loss: 0.2762, validation loss: 0.2450
2024-05-24 00:41:54 [INFO]: Epoch 086 - training loss: 0.2761, validation loss: 0.2384
2024-05-24 00:41:54 [INFO]: Epoch 087 - training loss: 0.2789, validation loss: 0.2383
2024-05-24 00:41:55 [INFO]: Epoch 088 - training loss: 0.2743, validation loss: 0.2423
2024-05-24 00:41:56 [INFO]: Epoch 089 - training loss: 0.2754, validation loss: 0.2405
2024-05-24 00:41:57 [INFO]: Epoch 090 - training loss: 0.2746, validation loss: 0.2476
2024-05-24 00:41:57 [INFO]: Epoch 091 - training loss: 0.2752, validation loss: 0.2439
2024-05-24 00:41:58 [INFO]: Epoch 092 - training loss: 0.2763, validation loss: 0.2410
2024-05-24 00:41:59 [INFO]: Epoch 093 - training loss: 0.2711, validation loss: 0.2387
2024-05-24 00:41:59 [INFO]: Epoch 094 - training loss: 0.2711, validation loss: 0.2417
2024-05-24 00:42:00 [INFO]: Epoch 095 - training loss: 0.2724, validation loss: 0.2397
2024-05-24 00:42:01 [INFO]: Epoch 096 - training loss: 0.2722, validation loss: 0.2411
2024-05-24 00:42:01 [INFO]: Epoch 097 - training loss: 0.2699, validation loss: 0.2399
2024-05-24 00:42:01 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-05-24 00:42:01 [INFO]: Finished training. The best model is from epoch#87.
2024-05-24 00:42:01 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_3/Transformer_physionet_2012_seta/20240524_T004054/Transformer.pypots
2024-05-24 00:42:01 [INFO]: Transformer on PhysioNet-2012-seta: MAE=0.2192, MSE=0.2218
2024-05-24 00:42:02 [INFO]: Successfully saved to overlay_minibatchmasking_saved_results/round_3/Transformer_physionet_2012_seta/imputation.pkl
2024-05-24 00:42:02 [INFO]: Using the given device: cuda:0
2024-05-24 00:42:02 [INFO]: Model files will be saved to overlay_minibatchmasking_saved_results/round_3/TimesNet_physionet_2012_seta/20240524_T004202
2024-05-24 00:42:02 [INFO]: Tensorboard file will be saved to overlay_minibatchmasking_saved_results/round_3/TimesNet_physionet_2012_seta/20240524_T004202/tensorboard
2024-05-24 00:42:02 [INFO]: TimesNet initialized with the given hyperparameters, the number of trainable parameters: 21,649,573
2024-05-24 00:42:03 [INFO]: Epoch 001 - training loss: 0.5000, validation loss: 1.0034
2024-05-24 00:42:03 [INFO]: Epoch 002 - training loss: 0.7137, validation loss: 0.4726
2024-05-24 00:42:04 [INFO]: Epoch 003 - training loss: 0.4654, validation loss: 0.3941
2024-05-24 00:42:05 [INFO]: Epoch 004 - training loss: 0.4590, validation loss: 0.3458
2024-05-24 00:42:06 [INFO]: Epoch 005 - training loss: 0.3467, validation loss: 0.4284
2024-05-24 00:42:07 [INFO]: Epoch 006 - training loss: 0.4589, validation loss: 0.4711
2024-05-24 00:42:07 [INFO]: Epoch 007 - training loss: 0.3574, validation loss: 0.4476
2024-05-24 00:42:08 [INFO]: Epoch 008 - training loss: 0.3864, validation loss: 0.3348
2024-05-24 00:42:09 [INFO]: Epoch 009 - training loss: 0.3520, validation loss: 0.3299
2024-05-24 00:42:10 [INFO]: Epoch 010 - training loss: 0.3237, validation loss: 0.3432
2024-05-24 00:42:11 [INFO]: Epoch 011 - training loss: 0.3201, validation loss: 0.3134
2024-05-24 00:42:11 [INFO]: Epoch 012 - training loss: 0.3488, validation loss: 0.3199
2024-05-24 00:42:12 [INFO]: Epoch 013 - training loss: 0.3214, validation loss: 0.3097
2024-05-24 00:42:13 [INFO]: Epoch 014 - training loss: 0.3569, validation loss: 0.3180
2024-05-24 00:42:14 [INFO]: Epoch 015 - training loss: 0.3249, validation loss: 0.3105
2024-05-24 00:42:15 [INFO]: Epoch 016 - training loss: 0.3256, validation loss: 0.3086
2024-05-24 00:42:15 [INFO]: Epoch 017 - training loss: 0.3104, validation loss: 0.3000
2024-05-24 00:42:16 [INFO]: Epoch 018 - training loss: 0.3057, validation loss: 0.2965
2024-05-24 00:42:17 [INFO]: Epoch 019 - training loss: 0.3491, validation loss: 0.3230
2024-05-24 00:42:18 [INFO]: Epoch 020 - training loss: 0.3375, validation loss: 0.3120
2024-05-24 00:42:19 [INFO]: Epoch 021 - training loss: 0.3154, validation loss: 0.3019
2024-05-24 00:42:19 [INFO]: Epoch 022 - training loss: 0.3098, validation loss: 0.2967
2024-05-24 00:42:20 [INFO]: Epoch 023 - training loss: 0.3193, validation loss: 0.2995
2024-05-24 00:42:21 [INFO]: Epoch 024 - training loss: 0.3225, validation loss: 0.2859
2024-05-24 00:42:22 [INFO]: Epoch 025 - training loss: 0.3116, validation loss: 0.2966
2024-05-24 00:42:23 [INFO]: Epoch 026 - training loss: 0.3497, validation loss: 0.3436
2024-05-24 00:42:23 [INFO]: Epoch 027 - training loss: 0.3397, validation loss: 0.2888
2024-05-24 00:42:24 [INFO]: Epoch 028 - training loss: 0.3070, validation loss: 0.3026
2024-05-24 00:42:25 [INFO]: Epoch 029 - training loss: 0.3011, validation loss: 0.2861
2024-05-24 00:42:26 [INFO]: Epoch 030 - training loss: 0.3549, validation loss: 0.2901
2024-05-24 00:42:27 [INFO]: Epoch 031 - training loss: 0.3240, validation loss: 0.2929
2024-05-24 00:42:27 [INFO]: Epoch 032 - training loss: 0.2956, validation loss: 0.2850
2024-05-24 00:42:28 [INFO]: Epoch 033 - training loss: 0.2873, validation loss: 0.2802
2024-05-24 00:42:29 [INFO]: Epoch 034 - training loss: 0.3034, validation loss: 0.3098
2024-05-24 00:42:30 [INFO]: Epoch 035 - training loss: 0.3056, validation loss: 0.2822
2024-05-24 00:42:31 [INFO]: Epoch 036 - training loss: 0.2972, validation loss: 0.2862
2024-05-24 00:42:31 [INFO]: Epoch 037 - training loss: 0.2960, validation loss: 0.2743
2024-05-24 00:42:32 [INFO]: Epoch 038 - training loss: 0.3301, validation loss: 0.2930
2024-05-24 00:42:33 [INFO]: Epoch 039 - training loss: 0.2983, validation loss: 0.2851
2024-05-24 00:42:34 [INFO]: Epoch 040 - training loss: 0.3070, validation loss: 0.2760
2024-05-24 00:42:35 [INFO]: Epoch 041 - training loss: 0.3139, validation loss: 0.2803
2024-05-24 00:42:35 [INFO]: Epoch 042 - training loss: 0.3015, validation loss: 0.2770
2024-05-24 00:42:36 [INFO]: Epoch 043 - training loss: 0.2981, validation loss: 0.2794
2024-05-24 00:42:37 [INFO]: Epoch 044 - training loss: 0.2877, validation loss: 0.2853
2024-05-24 00:42:38 [INFO]: Epoch 045 - training loss: 0.2968, validation loss: 0.2774
2024-05-24 00:42:39 [INFO]: Epoch 046 - training loss: 0.3341, validation loss: 0.2785
2024-05-24 00:42:39 [INFO]: Epoch 047 - training loss: 0.2737, validation loss: 0.2668
2024-05-24 00:42:40 [INFO]: Epoch 048 - training loss: 0.2830, validation loss: 0.2684
2024-05-24 00:42:41 [INFO]: Epoch 049 - training loss: 0.2905, validation loss: 0.2786
2024-05-24 00:42:42 [INFO]: Epoch 050 - training loss: 0.2960, validation loss: 0.2721
2024-05-24 00:42:43 [INFO]: Epoch 051 - training loss: 0.2755, validation loss: 0.2787
2024-05-24 00:42:43 [INFO]: Epoch 052 - training loss: 0.2902, validation loss: 0.2852
2024-05-24 00:42:44 [INFO]: Epoch 053 - training loss: 0.2848, validation loss: 0.2797
2024-05-24 00:42:45 [INFO]: Epoch 054 - training loss: 0.2959, validation loss: 0.2746
2024-05-24 00:42:46 [INFO]: Epoch 055 - training loss: 0.2863, validation loss: 0.2701
2024-05-24 00:42:47 [INFO]: Epoch 056 - training loss: 0.3279, validation loss: 0.2736
2024-05-24 00:42:47 [INFO]: Epoch 057 - training loss: 0.3065, validation loss: 0.2663
2024-05-24 00:42:48 [INFO]: Epoch 058 - training loss: 0.2967, validation loss: 0.2719
2024-05-24 00:42:49 [INFO]: Epoch 059 - training loss: 0.2738, validation loss: 0.2797
2024-05-24 00:42:50 [INFO]: Epoch 060 - training loss: 0.2824, validation loss: 0.2695
2024-05-24 00:42:51 [INFO]: Epoch 061 - training loss: 0.2735, validation loss: 0.2708
2024-05-24 00:42:51 [INFO]: Epoch 062 - training loss: 0.2899, validation loss: 0.2685
2024-05-24 00:42:52 [INFO]: Epoch 063 - training loss: 0.2723, validation loss: 0.2748
2024-05-24 00:42:53 [INFO]: Epoch 064 - training loss: 0.2897, validation loss: 0.2727
2024-05-24 00:42:54 [INFO]: Epoch 065 - training loss: 0.3082, validation loss: 0.2748
2024-05-24 00:42:55 [INFO]: Epoch 066 - training loss: 0.2728, validation loss: 0.2767
2024-05-24 00:42:55 [INFO]: Epoch 067 - training loss: 0.2670, validation loss: 0.2700
2024-05-24 00:42:55 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-05-24 00:42:55 [INFO]: Finished training. The best model is from epoch#57.
2024-05-24 00:42:55 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_3/TimesNet_physionet_2012_seta/20240524_T004202/TimesNet.pypots
2024-05-24 00:42:56 [INFO]: TimesNet on PhysioNet-2012-seta: MAE=0.2666, MSE=0.2332
2024-05-24 00:42:56 [INFO]: Successfully saved to overlay_minibatchmasking_saved_results/round_3/TimesNet_physionet_2012_seta/imputation.pkl
2024-05-24 00:42:56 [INFO]: Using the given device: cuda:0
2024-05-24 00:42:56 [INFO]: Model files will be saved to overlay_minibatchmasking_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T004256
2024-05-24 00:42:56 [INFO]: Tensorboard file will be saved to overlay_minibatchmasking_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T004256/tensorboard
2024-05-24 00:42:56 [INFO]: CSDI initialized with the given hyperparameters, the number of trainable parameters: 1,694,753
2024-05-24 00:43:39 [INFO]: Epoch 001 - training loss: 0.4253, validation loss: 0.3397
2024-05-24 00:43:39 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T004256/CSDI_epoch1_loss0.3396843418478966.pypots
2024-05-24 00:44:23 [INFO]: Epoch 002 - training loss: 0.3307, validation loss: 0.3059
2024-05-24 00:44:23 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T004256/CSDI_epoch2_loss0.30594209879636763.pypots
2024-05-24 00:45:06 [INFO]: Epoch 003 - training loss: 0.2872, validation loss: 0.2477
2024-05-24 00:45:07 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T004256/CSDI_epoch3_loss0.24773741737008095.pypots
2024-05-24 00:45:50 [INFO]: Epoch 004 - training loss: 0.2634, validation loss: 0.2291
2024-05-24 00:45:50 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T004256/CSDI_epoch4_loss0.2291007786989212.pypots
2024-05-24 00:46:34 [INFO]: Epoch 005 - training loss: 0.2690, validation loss: 0.2189
2024-05-24 00:46:34 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T004256/CSDI_epoch5_loss0.21890708953142166.pypots
2024-05-24 00:47:18 [INFO]: Epoch 006 - training loss: 0.2428, validation loss: 0.2158
2024-05-24 00:47:18 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T004256/CSDI_epoch6_loss0.21582343503832818.pypots
2024-05-24 00:48:02 [INFO]: Epoch 007 - training loss: 0.2402, validation loss: 0.2105
2024-05-24 00:48:02 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T004256/CSDI_epoch7_loss0.21046825498342514.pypots
2024-05-24 00:48:46 [INFO]: Epoch 008 - training loss: 0.2467, validation loss: 0.2048
2024-05-24 00:48:46 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T004256/CSDI_epoch8_loss0.204780150949955.pypots
2024-05-24 00:49:30 [INFO]: Epoch 009 - training loss: 0.2321, validation loss: 0.2057
2024-05-24 00:49:30 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T004256/CSDI_epoch9_loss0.20565389841794968.pypots
2024-05-24 00:50:14 [INFO]: Epoch 010 - training loss: 0.2342, validation loss: 0.2009
2024-05-24 00:50:14 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T004256/CSDI_epoch10_loss0.20085212960839272.pypots
2024-05-24 00:50:58 [INFO]: Epoch 011 - training loss: 0.2366, validation loss: 0.2012
2024-05-24 00:50:58 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T004256/CSDI_epoch11_loss0.20119635686278342.pypots
2024-05-24 00:51:42 [INFO]: Epoch 012 - training loss: 0.2395, validation loss: 0.1958
2024-05-24 00:51:42 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T004256/CSDI_epoch12_loss0.19581682309508325.pypots
2024-05-24 00:52:26 [INFO]: Epoch 013 - training loss: 0.2401, validation loss: 0.1961
2024-05-24 00:52:26 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T004256/CSDI_epoch13_loss0.1960962936282158.pypots
2024-05-24 00:53:10 [INFO]: Epoch 014 - training loss: 0.2382, validation loss: 0.1950
2024-05-24 00:53:10 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T004256/CSDI_epoch14_loss0.19499080404639244.pypots
2024-05-24 00:53:54 [INFO]: Epoch 015 - training loss: 0.2354, validation loss: 0.1968
2024-05-24 00:53:54 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T004256/CSDI_epoch15_loss0.19675053730607034.pypots
2024-05-24 00:54:37 [INFO]: Epoch 016 - training loss: 0.2256, validation loss: 0.1889
2024-05-24 00:54:37 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T004256/CSDI_epoch16_loss0.18889527395367622.pypots
2024-05-24 00:55:21 [INFO]: Epoch 017 - training loss: 0.2311, validation loss: 0.1924
2024-05-24 00:55:21 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T004256/CSDI_epoch17_loss0.192381539195776.pypots
2024-05-24 00:56:05 [INFO]: Epoch 018 - training loss: 0.2387, validation loss: 0.1877
2024-05-24 00:56:05 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T004256/CSDI_epoch18_loss0.18769411742687225.pypots
2024-05-24 00:56:49 [INFO]: Epoch 019 - training loss: 0.2356, validation loss: 0.1882
2024-05-24 00:56:49 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T004256/CSDI_epoch19_loss0.18817529529333116.pypots
2024-05-24 00:57:33 [INFO]: Epoch 020 - training loss: 0.2305, validation loss: 0.1884
2024-05-24 00:57:33 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T004256/CSDI_epoch20_loss0.1883579857647419.pypots
2024-05-24 00:58:16 [INFO]: Epoch 021 - training loss: 0.2349, validation loss: 0.1891
2024-05-24 00:58:16 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T004256/CSDI_epoch21_loss0.1891372174024582.pypots
2024-05-24 00:59:00 [INFO]: Epoch 022 - training loss: 0.2295, validation loss: 0.1887
2024-05-24 00:59:00 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T004256/CSDI_epoch22_loss0.18870674595236778.pypots
2024-05-24 00:59:44 [INFO]: Epoch 023 - training loss: 0.2298, validation loss: 0.1896
2024-05-24 00:59:44 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T004256/CSDI_epoch23_loss0.18962346389889717.pypots
2024-05-24 01:00:28 [INFO]: Epoch 024 - training loss: 0.2327, validation loss: 0.1889
2024-05-24 07:34:01 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T004256/CSDI_epoch24_loss0.1889443464577198.pypots
2024-05-24 07:34:45 [INFO]: Epoch 025 - training loss: 0.2335, validation loss: 0.1884
2024-05-24 07:34:45 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T004256/CSDI_epoch25_loss0.18836022764444352.pypots
2024-05-24 07:35:29 [INFO]: Epoch 026 - training loss: 0.2198, validation loss: 0.1883
2024-05-24 07:35:29 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T004256/CSDI_epoch26_loss0.1882624566555023.pypots
2024-05-24 07:36:13 [INFO]: Epoch 027 - training loss: 0.2260, validation loss: 0.1841
2024-05-24 07:36:13 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T004256/CSDI_epoch27_loss0.18406296372413636.pypots
2024-05-24 07:36:57 [INFO]: Epoch 028 - training loss: 0.2112, validation loss: 0.1831
2024-05-24 07:36:57 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T004256/CSDI_epoch28_loss0.18308247327804567.pypots
2024-05-24 07:37:41 [INFO]: Epoch 029 - training loss: 0.2244, validation loss: 0.1853
2024-05-24 07:37:41 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T004256/CSDI_epoch29_loss0.18525330051779748.pypots
2024-05-24 07:38:25 [INFO]: Epoch 030 - training loss: 0.2380, validation loss: 0.1841
2024-05-24 07:38:25 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T004256/CSDI_epoch30_loss0.184099043905735.pypots
2024-05-24 07:39:09 [INFO]: Epoch 031 - training loss: 0.2233, validation loss: 0.1818
2024-05-24 07:39:09 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T004256/CSDI_epoch31_loss0.18177314475178719.pypots
2024-05-24 07:39:53 [INFO]: Epoch 032 - training loss: 0.2334, validation loss: 0.1839
2024-05-24 07:39:53 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T004256/CSDI_epoch32_loss0.18387725502252578.pypots
2024-05-24 07:40:37 [INFO]: Epoch 033 - training loss: 0.2289, validation loss: 0.1817
2024-05-24 07:40:37 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T004256/CSDI_epoch33_loss0.18173449710011483.pypots
2024-05-24 07:41:21 [INFO]: Epoch 034 - training loss: 0.2244, validation loss: 0.1810
2024-05-24 07:41:21 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T004256/CSDI_epoch34_loss0.1810038059949875.pypots
2024-05-24 07:42:05 [INFO]: Epoch 035 - training loss: 0.2228, validation loss: 0.1797
2024-05-24 07:42:05 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T004256/CSDI_epoch35_loss0.1796862244606018.pypots
2024-05-24 07:42:49 [INFO]: Epoch 036 - training loss: 0.2246, validation loss: 0.1842
2024-05-24 07:42:49 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T004256/CSDI_epoch36_loss0.18418804183602333.pypots
2024-05-24 07:43:33 [INFO]: Epoch 037 - training loss: 0.2190, validation loss: 0.1834
2024-05-24 07:43:33 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T004256/CSDI_epoch37_loss0.18338889479637147.pypots
2024-05-24 07:44:17 [INFO]: Epoch 038 - training loss: 0.2172, validation loss: 0.1808
2024-05-24 07:44:17 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T004256/CSDI_epoch38_loss0.1807962492108345.pypots
2024-05-24 07:45:01 [INFO]: Epoch 039 - training loss: 0.2228, validation loss: 0.1818
2024-05-24 07:45:01 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T004256/CSDI_epoch39_loss0.18184003233909607.pypots
2024-05-24 07:45:45 [INFO]: Epoch 040 - training loss: 0.2270, validation loss: 0.1810
2024-05-24 07:45:45 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T004256/CSDI_epoch40_loss0.18104584589600564.pypots
2024-05-24 07:46:28 [INFO]: Epoch 041 - training loss: 0.2196, validation loss: 0.1820
2024-05-24 07:46:28 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T004256/CSDI_epoch41_loss0.18201878443360328.pypots
2024-05-24 07:47:12 [INFO]: Epoch 042 - training loss: 0.2308, validation loss: 0.1795
2024-05-24 07:47:12 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T004256/CSDI_epoch42_loss0.17949194833636284.pypots
2024-05-24 07:47:56 [INFO]: Epoch 043 - training loss: 0.2239, validation loss: 0.1817
2024-05-24 07:47:56 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T004256/CSDI_epoch43_loss0.18165341913700103.pypots
2024-05-24 07:48:40 [INFO]: Epoch 044 - training loss: 0.2135, validation loss: 0.1804
2024-05-24 07:48:40 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T004256/CSDI_epoch44_loss0.18041598573327064.pypots
2024-05-24 07:49:24 [INFO]: Epoch 045 - training loss: 0.2156, validation loss: 0.1804
2024-05-24 07:49:24 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T004256/CSDI_epoch45_loss0.18044217228889464.pypots
2024-05-24 07:50:08 [INFO]: Epoch 046 - training loss: 0.2212, validation loss: 0.1823
2024-05-24 07:50:08 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T004256/CSDI_epoch46_loss0.18231632560491562.pypots
2024-05-24 07:50:51 [INFO]: Epoch 047 - training loss: 0.2188, validation loss: 0.1780
2024-05-24 07:50:51 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T004256/CSDI_epoch47_loss0.17803846970200538.pypots
2024-05-24 07:51:35 [INFO]: Epoch 048 - training loss: 0.2156, validation loss: 0.1773
2024-05-24 07:51:35 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T004256/CSDI_epoch48_loss0.17730706557631493.pypots
2024-05-24 07:52:19 [INFO]: Epoch 049 - training loss: 0.2212, validation loss: 0.1764
2024-05-24 07:52:19 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T004256/CSDI_epoch49_loss0.17643535882234573.pypots
2024-05-24 07:53:03 [INFO]: Epoch 050 - training loss: 0.2111, validation loss: 0.1782
2024-05-24 07:53:03 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T004256/CSDI_epoch50_loss0.17823310941457748.pypots
2024-05-24 07:53:47 [INFO]: Epoch 051 - training loss: 0.2225, validation loss: 0.1778
2024-05-24 07:53:47 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T004256/CSDI_epoch51_loss0.17778957784175872.pypots
2024-05-24 07:54:30 [INFO]: Epoch 052 - training loss: 0.2228, validation loss: 0.1772
2024-05-24 07:54:30 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T004256/CSDI_epoch52_loss0.17721284180879593.pypots
2024-05-24 07:55:14 [INFO]: Epoch 053 - training loss: 0.2214, validation loss: 0.1793
2024-05-24 07:55:14 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T004256/CSDI_epoch53_loss0.17929065153002738.pypots
2024-05-24 07:55:58 [INFO]: Epoch 054 - training loss: 0.2264, validation loss: 0.1774
2024-05-24 07:55:58 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T004256/CSDI_epoch54_loss0.1774408996105194.pypots
2024-05-24 07:56:42 [INFO]: Epoch 055 - training loss: 0.2181, validation loss: 0.1789
2024-05-24 07:56:42 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T004256/CSDI_epoch55_loss0.17887186110019684.pypots
2024-05-24 07:57:26 [INFO]: Epoch 056 - training loss: 0.2156, validation loss: 0.1760
2024-05-24 07:57:26 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T004256/CSDI_epoch56_loss0.17598907127976418.pypots
2024-05-24 07:58:10 [INFO]: Epoch 057 - training loss: 0.2232, validation loss: 0.1780
2024-05-24 07:58:10 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T004256/CSDI_epoch57_loss0.178012415766716.pypots
2024-05-24 07:58:53 [INFO]: Epoch 058 - training loss: 0.2127, validation loss: 0.1781
2024-05-24 07:58:53 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T004256/CSDI_epoch58_loss0.17809298187494277.pypots
2024-05-24 07:59:37 [INFO]: Epoch 059 - training loss: 0.2141, validation loss: 0.1761
2024-05-24 07:59:37 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T004256/CSDI_epoch59_loss0.17609380036592484.pypots
2024-05-24 08:00:21 [INFO]: Epoch 060 - training loss: 0.2153, validation loss: 0.1754
2024-05-24 08:00:21 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T004256/CSDI_epoch60_loss0.1754081964492798.pypots
2024-05-24 08:01:05 [INFO]: Epoch 061 - training loss: 0.2222, validation loss: 0.1757
2024-05-24 08:01:05 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T004256/CSDI_epoch61_loss0.17574862390756607.pypots
2024-05-24 08:01:49 [INFO]: Epoch 062 - training loss: 0.2253, validation loss: 0.1754
2024-05-24 08:01:49 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T004256/CSDI_epoch62_loss0.1753706641495228.pypots
2024-05-24 08:02:33 [INFO]: Epoch 063 - training loss: 0.2237, validation loss: 0.1741
2024-05-24 08:02:33 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T004256/CSDI_epoch63_loss0.17410412505269052.pypots
2024-05-24 08:03:17 [INFO]: Epoch 064 - training loss: 0.2068, validation loss: 0.1740
2024-05-24 08:03:17 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T004256/CSDI_epoch64_loss0.17400677427649497.pypots
2024-05-24 08:04:01 [INFO]: Epoch 065 - training loss: 0.2186, validation loss: 0.1731
2024-05-24 08:04:01 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T004256/CSDI_epoch65_loss0.17307059988379478.pypots
2024-05-24 08:04:45 [INFO]: Epoch 066 - training loss: 0.2046, validation loss: 0.1746
2024-05-24 08:04:45 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T004256/CSDI_epoch66_loss0.17461508587002755.pypots
2024-05-24 08:05:29 [INFO]: Epoch 067 - training loss: 0.2184, validation loss: 0.1732
2024-05-24 08:05:29 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T004256/CSDI_epoch67_loss0.17324874252080918.pypots
2024-05-24 08:06:13 [INFO]: Epoch 068 - training loss: 0.2246, validation loss: 0.1735
2024-05-24 08:06:13 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T004256/CSDI_epoch68_loss0.17353118434548379.pypots
2024-05-24 08:06:56 [INFO]: Epoch 069 - training loss: 0.2141, validation loss: 0.1746
2024-05-24 08:06:56 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T004256/CSDI_epoch69_loss0.1746358796954155.pypots
2024-05-24 08:07:40 [INFO]: Epoch 070 - training loss: 0.2185, validation loss: 0.1739
2024-05-24 08:07:40 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T004256/CSDI_epoch70_loss0.17388001829385757.pypots
2024-05-24 08:08:24 [INFO]: Epoch 071 - training loss: 0.2127, validation loss: 0.1755
2024-05-24 08:08:24 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T004256/CSDI_epoch71_loss0.1755126141011715.pypots
2024-05-24 08:09:08 [INFO]: Epoch 072 - training loss: 0.2131, validation loss: 0.1723
2024-05-24 08:09:08 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T004256/CSDI_epoch72_loss0.17232310846447946.pypots
2024-05-24 08:09:52 [INFO]: Epoch 073 - training loss: 0.2137, validation loss: 0.1741
2024-05-24 08:09:52 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T004256/CSDI_epoch73_loss0.17414344772696494.pypots
2024-05-24 08:10:35 [INFO]: Epoch 074 - training loss: 0.2185, validation loss: 0.1748
2024-05-24 08:10:35 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T004256/CSDI_epoch74_loss0.17483962699770927.pypots
2024-05-24 08:11:19 [INFO]: Epoch 075 - training loss: 0.2105, validation loss: 0.1724
2024-05-24 08:11:19 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T004256/CSDI_epoch75_loss0.17240022718906403.pypots
2024-05-24 08:12:03 [INFO]: Epoch 076 - training loss: 0.2121, validation loss: 0.1737
2024-05-24 08:12:03 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T004256/CSDI_epoch76_loss0.17374663725495337.pypots
2024-05-24 08:12:47 [INFO]: Epoch 077 - training loss: 0.2084, validation loss: 0.1752
2024-05-24 08:12:47 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T004256/CSDI_epoch77_loss0.17515545040369035.pypots
2024-05-24 08:13:31 [INFO]: Epoch 078 - training loss: 0.2167, validation loss: 0.1758
2024-05-24 08:13:31 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T004256/CSDI_epoch78_loss0.175766196846962.pypots
2024-05-24 08:14:15 [INFO]: Epoch 079 - training loss: 0.2114, validation loss: 0.1730
2024-05-24 08:14:15 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T004256/CSDI_epoch79_loss0.1730048179626465.pypots
2024-05-24 08:14:59 [INFO]: Epoch 080 - training loss: 0.2133, validation loss: 0.1708
2024-05-24 08:14:59 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T004256/CSDI_epoch80_loss0.17079442366957664.pypots
2024-05-24 08:15:43 [INFO]: Epoch 081 - training loss: 0.2085, validation loss: 0.1748
2024-05-24 08:15:43 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T004256/CSDI_epoch81_loss0.17484685629606248.pypots
2024-05-24 08:16:27 [INFO]: Epoch 082 - training loss: 0.2126, validation loss: 0.1732
2024-05-24 08:16:27 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T004256/CSDI_epoch82_loss0.17317392081022262.pypots
2024-05-24 08:17:11 [INFO]: Epoch 083 - training loss: 0.2178, validation loss: 0.1756
2024-05-24 08:17:11 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T004256/CSDI_epoch83_loss0.1756076619029045.pypots
2024-05-24 08:17:54 [INFO]: Epoch 084 - training loss: 0.2194, validation loss: 0.1741
2024-05-24 08:17:54 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T004256/CSDI_epoch84_loss0.17413775399327278.pypots
2024-05-24 08:18:38 [INFO]: Epoch 085 - training loss: 0.2249, validation loss: 0.1753
2024-05-24 08:18:38 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T004256/CSDI_epoch85_loss0.17526910826563835.pypots
2024-05-24 08:19:22 [INFO]: Epoch 086 - training loss: 0.2169, validation loss: 0.1711
2024-05-24 08:19:22 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T004256/CSDI_epoch86_loss0.1711317293345928.pypots
2024-05-24 08:20:06 [INFO]: Epoch 087 - training loss: 0.2225, validation loss: 0.1711
2024-05-24 08:20:06 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T004256/CSDI_epoch87_loss0.17110842168331147.pypots
2024-05-24 08:20:50 [INFO]: Epoch 088 - training loss: 0.2180, validation loss: 0.1706
2024-05-24 08:20:50 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T004256/CSDI_epoch88_loss0.1705548234283924.pypots
2024-05-24 08:21:34 [INFO]: Epoch 089 - training loss: 0.2233, validation loss: 0.1733
2024-05-24 08:21:34 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T004256/CSDI_epoch89_loss0.17328750863671302.pypots
2024-05-24 08:22:17 [INFO]: Epoch 090 - training loss: 0.2104, validation loss: 0.1722
2024-05-24 08:22:17 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T004256/CSDI_epoch90_loss0.1721658192574978.pypots
2024-05-24 08:23:01 [INFO]: Epoch 091 - training loss: 0.2087, validation loss: 0.1719
2024-05-24 08:23:01 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T004256/CSDI_epoch91_loss0.17194658294320106.pypots
2024-05-24 08:23:45 [INFO]: Epoch 092 - training loss: 0.2221, validation loss: 0.1701
2024-05-24 08:23:45 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T004256/CSDI_epoch92_loss0.17007458880543708.pypots
2024-05-24 08:24:29 [INFO]: Epoch 093 - training loss: 0.2157, validation loss: 0.1708
2024-05-24 08:24:29 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T004256/CSDI_epoch93_loss0.1707848884165287.pypots
2024-05-24 08:25:13 [INFO]: Epoch 094 - training loss: 0.2071, validation loss: 0.1707
2024-05-24 08:25:13 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T004256/CSDI_epoch94_loss0.17070006355643272.pypots
2024-05-24 08:25:56 [INFO]: Epoch 095 - training loss: 0.2183, validation loss: 0.1690
2024-05-24 08:25:57 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T004256/CSDI_epoch95_loss0.1689704842865467.pypots
2024-05-24 08:26:40 [INFO]: Epoch 096 - training loss: 0.2224, validation loss: 0.1718
2024-05-24 08:26:40 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T004256/CSDI_epoch96_loss0.1718405991792679.pypots
2024-05-24 08:27:24 [INFO]: Epoch 097 - training loss: 0.2173, validation loss: 0.1698
2024-05-24 08:27:24 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T004256/CSDI_epoch97_loss0.16980609446763992.pypots
2024-05-24 08:28:08 [INFO]: Epoch 098 - training loss: 0.2183, validation loss: 0.1697
2024-05-24 08:28:08 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T004256/CSDI_epoch98_loss0.1697340801358223.pypots
2024-05-24 08:28:52 [INFO]: Epoch 099 - training loss: 0.2134, validation loss: 0.1733
2024-05-24 08:28:52 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T004256/CSDI_epoch99_loss0.17332784309983254.pypots
2024-05-24 08:29:36 [INFO]: Epoch 100 - training loss: 0.2124, validation loss: 0.1718
2024-05-24 08:29:36 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T004256/CSDI_epoch100_loss0.1718216545879841.pypots
2024-05-24 08:30:19 [INFO]: Epoch 101 - training loss: 0.2104, validation loss: 0.1717
2024-05-24 08:30:19 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T004256/CSDI_epoch101_loss0.17174383774399757.pypots
2024-05-24 08:31:03 [INFO]: Epoch 102 - training loss: 0.2130, validation loss: 0.1712
2024-05-24 08:31:03 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T004256/CSDI_epoch102_loss0.17121357843279839.pypots
2024-05-24 08:31:47 [INFO]: Epoch 103 - training loss: 0.2206, validation loss: 0.1724
2024-05-24 08:31:47 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T004256/CSDI_epoch103_loss0.17243389412760735.pypots
2024-05-24 08:32:31 [INFO]: Epoch 104 - training loss: 0.2132, validation loss: 0.1679
2024-05-24 08:32:31 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T004256/CSDI_epoch104_loss0.16792945563793182.pypots
2024-05-24 08:33:15 [INFO]: Epoch 105 - training loss: 0.2121, validation loss: 0.1746
2024-05-24 08:33:15 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T004256/CSDI_epoch105_loss0.17458350509405135.pypots
2024-05-24 08:33:59 [INFO]: Epoch 106 - training loss: 0.2164, validation loss: 0.1702
2024-05-24 08:33:59 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T004256/CSDI_epoch106_loss0.1702367402613163.pypots
2024-05-24 08:34:42 [INFO]: Epoch 107 - training loss: 0.2055, validation loss: 0.1703
2024-05-24 08:34:43 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T004256/CSDI_epoch107_loss0.17032938897609712.pypots
2024-05-24 08:35:26 [INFO]: Epoch 108 - training loss: 0.2185, validation loss: 0.1707
2024-05-24 08:35:26 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T004256/CSDI_epoch108_loss0.1706913158297539.pypots
2024-05-24 08:36:10 [INFO]: Epoch 109 - training loss: 0.2104, validation loss: 0.1733
2024-05-24 08:36:10 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T004256/CSDI_epoch109_loss0.17334718704223634.pypots
2024-05-24 08:36:54 [INFO]: Epoch 110 - training loss: 0.2250, validation loss: 0.1697
2024-05-24 08:36:54 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T004256/CSDI_epoch110_loss0.1696811243891716.pypots
2024-05-24 08:37:38 [INFO]: Epoch 111 - training loss: 0.2171, validation loss: 0.1687
2024-05-24 08:37:38 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T004256/CSDI_epoch111_loss0.1687031641602516.pypots
2024-05-24 08:38:22 [INFO]: Epoch 112 - training loss: 0.2122, validation loss: 0.1702
2024-05-24 08:38:22 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T004256/CSDI_epoch112_loss0.17018519714474678.pypots
2024-05-24 08:39:06 [INFO]: Epoch 113 - training loss: 0.2124, validation loss: 0.1697
2024-05-24 08:39:06 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T004256/CSDI_epoch113_loss0.16971597969532012.pypots
2024-05-24 08:39:49 [INFO]: Epoch 114 - training loss: 0.2210, validation loss: 0.1686
2024-05-24 08:39:49 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T004256/CSDI_epoch114_loss0.16856421157717705.pypots
2024-05-24 08:39:49 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-05-24 08:39:50 [INFO]: Finished training. The best model is from epoch#104.
2024-05-24 08:39:50 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T004256/CSDI.pypots
2024-05-24 08:47:11 [INFO]: CSDI on PhysioNet-2012-seta: MAE=0.2152, MSE=0.2359
2024-05-24 09:16:33 [INFO]: Successfully saved to overlay_minibatchmasking_saved_results/round_3/CSDI_physionet_2012_seta/imputation.pkl
2024-05-24 09:16:33 [INFO]: Using the given device: cuda:0
2024-05-24 09:16:33 [INFO]: Model files will be saved to overlay_minibatchmasking_saved_results/round_3/GPVAE_physionet_2012_seta/20240524_T091633
2024-05-24 09:16:33 [INFO]: Tensorboard file will be saved to overlay_minibatchmasking_saved_results/round_3/GPVAE_physionet_2012_seta/20240524_T091633/tensorboard
2024-05-24 09:16:33 [INFO]: GPVAE initialized with the given hyperparameters, the number of trainable parameters: 1,028,244
2024-05-24 09:16:34 [INFO]: Epoch 001 - training loss: 42232.3678, validation loss: 0.9383
2024-05-24 09:16:34 [INFO]: Epoch 002 - training loss: 24453.2961, validation loss: 0.7450
2024-05-24 09:16:35 [INFO]: Epoch 003 - training loss: 23508.1134, validation loss: 0.6829
2024-05-24 09:16:35 [INFO]: Epoch 004 - training loss: 23217.0657, validation loss: 0.6622
2024-05-24 09:16:36 [INFO]: Epoch 005 - training loss: 23077.0253, validation loss: 0.6605
2024-05-24 09:16:36 [INFO]: Epoch 006 - training loss: 22999.5405, validation loss: 0.6610
2024-05-24 09:16:37 [INFO]: Epoch 007 - training loss: 22951.0979, validation loss: 0.6530
2024-05-24 09:16:38 [INFO]: Epoch 008 - training loss: 22920.3651, validation loss: 0.6721
2024-05-24 09:16:38 [INFO]: Epoch 009 - training loss: 22899.6006, validation loss: 0.6560
2024-05-24 09:16:39 [INFO]: Epoch 010 - training loss: 22884.3342, validation loss: 0.6553
2024-05-24 09:16:39 [INFO]: Epoch 011 - training loss: 22873.8753, validation loss: 0.6521
2024-05-24 09:16:40 [INFO]: Epoch 012 - training loss: 22865.2517, validation loss: 0.6455
2024-05-24 09:16:40 [INFO]: Epoch 013 - training loss: 22858.8091, validation loss: 0.6461
2024-05-24 09:16:41 [INFO]: Epoch 014 - training loss: 22854.2069, validation loss: 0.6464
2024-05-24 09:16:42 [INFO]: Epoch 015 - training loss: 22849.7001, validation loss: 0.6433
2024-05-24 09:16:42 [INFO]: Epoch 016 - training loss: 22846.6000, validation loss: 0.6469
2024-05-24 09:16:43 [INFO]: Epoch 017 - training loss: 22843.6315, validation loss: 0.6324
2024-05-24 09:16:43 [INFO]: Epoch 018 - training loss: 22840.8155, validation loss: 0.6301
2024-05-24 09:16:44 [INFO]: Epoch 019 - training loss: 22838.6685, validation loss: 0.6278
2024-05-24 09:16:44 [INFO]: Epoch 020 - training loss: 22836.8463, validation loss: 0.6242
2024-05-24 09:16:45 [INFO]: Epoch 021 - training loss: 22835.1125, validation loss: 0.6215
2024-05-24 09:16:46 [INFO]: Epoch 022 - training loss: 22833.3815, validation loss: 0.6207
2024-05-24 09:16:46 [INFO]: Epoch 023 - training loss: 22832.7755, validation loss: 0.6191
2024-05-24 09:16:47 [INFO]: Epoch 024 - training loss: 22831.1667, validation loss: 0.6168
2024-05-24 09:16:47 [INFO]: Epoch 025 - training loss: 22830.0954, validation loss: 0.6112
2024-05-24 09:16:48 [INFO]: Epoch 026 - training loss: 22829.5902, validation loss: 0.6131
2024-05-24 09:16:48 [INFO]: Epoch 027 - training loss: 22828.4363, validation loss: 0.6127
2024-05-24 09:16:49 [INFO]: Epoch 028 - training loss: 22827.3011, validation loss: 0.6155
2024-05-24 09:16:50 [INFO]: Epoch 029 - training loss: 22826.5613, validation loss: 0.6107
2024-05-24 09:16:50 [INFO]: Epoch 030 - training loss: 22828.1794, validation loss: 0.6070
2024-05-24 09:16:51 [INFO]: Epoch 031 - training loss: 22823.8666, validation loss: 0.5979
2024-05-24 09:16:51 [INFO]: Epoch 032 - training loss: 22822.4799, validation loss: 0.5885
2024-05-24 09:16:52 [INFO]: Epoch 033 - training loss: 22820.8132, validation loss: 0.5978
2024-05-24 09:16:53 [INFO]: Epoch 034 - training loss: 22819.9727, validation loss: 0.5832
2024-05-24 09:16:53 [INFO]: Epoch 035 - training loss: 22819.1982, validation loss: 0.5831
2024-05-24 09:16:54 [INFO]: Epoch 036 - training loss: 22817.6783, validation loss: 0.5791
2024-05-24 09:16:54 [INFO]: Epoch 037 - training loss: 22816.3612, validation loss: 0.5852
2024-05-24 09:16:55 [INFO]: Epoch 038 - training loss: 22815.6120, validation loss: 0.5776
2024-05-24 09:16:55 [INFO]: Epoch 039 - training loss: 22814.5635, validation loss: 0.5744
2024-05-24 09:16:56 [INFO]: Epoch 040 - training loss: 22814.0737, validation loss: 0.5794
2024-05-24 09:16:57 [INFO]: Epoch 041 - training loss: 22813.6604, validation loss: 0.5762
2024-05-24 09:16:57 [INFO]: Epoch 042 - training loss: 22813.1004, validation loss: 0.5769
2024-05-24 09:16:58 [INFO]: Epoch 043 - training loss: 22812.8769, validation loss: 0.5889
2024-05-24 09:16:58 [INFO]: Epoch 044 - training loss: 22813.0687, validation loss: 0.5807
2024-05-24 09:16:59 [INFO]: Epoch 045 - training loss: 22812.0635, validation loss: 0.5760
2024-05-24 09:16:59 [INFO]: Epoch 046 - training loss: 22811.3760, validation loss: 0.5797
2024-05-24 09:17:00 [INFO]: Epoch 047 - training loss: 22810.5957, validation loss: 0.5729
2024-05-24 09:17:01 [INFO]: Epoch 048 - training loss: 22810.0809, validation loss: 0.5760
2024-05-24 09:17:01 [INFO]: Epoch 049 - training loss: 22809.6846, validation loss: 0.5781
2024-05-24 09:17:02 [INFO]: Epoch 050 - training loss: 22809.2736, validation loss: 0.5920
2024-05-24 09:17:02 [INFO]: Epoch 051 - training loss: 22808.9098, validation loss: 0.5643
2024-05-24 09:17:03 [INFO]: Epoch 052 - training loss: 22808.1254, validation loss: 0.5594
2024-05-24 09:17:03 [INFO]: Epoch 053 - training loss: 22806.7050, validation loss: 0.5557
2024-05-24 09:17:04 [INFO]: Epoch 054 - training loss: 22806.6536, validation loss: 0.5518
2024-05-24 09:17:05 [INFO]: Epoch 055 - training loss: 22805.0741, validation loss: 0.5519
2024-05-24 09:17:05 [INFO]: Epoch 056 - training loss: 22804.9490, validation loss: 0.5532
2024-05-24 09:17:06 [INFO]: Epoch 057 - training loss: 22807.1484, validation loss: 0.5497
2024-05-24 09:17:06 [INFO]: Epoch 058 - training loss: 22805.4491, validation loss: 0.5536
2024-05-24 09:17:07 [INFO]: Epoch 059 - training loss: 22803.0442, validation loss: 0.5395
2024-05-24 09:17:07 [INFO]: Epoch 060 - training loss: 22802.5222, validation loss: 0.5407
2024-05-24 09:17:08 [INFO]: Epoch 061 - training loss: 22801.7869, validation loss: 0.5323
2024-05-24 09:17:09 [INFO]: Epoch 062 - training loss: 22801.5115, validation loss: 0.5309
2024-05-24 09:17:09 [INFO]: Epoch 063 - training loss: 22801.1318, validation loss: 0.5324
2024-05-24 09:17:10 [INFO]: Epoch 064 - training loss: 22800.0807, validation loss: 0.5356
2024-05-24 09:17:10 [INFO]: Epoch 065 - training loss: 22799.4140, validation loss: 0.5678
2024-05-24 09:17:11 [INFO]: Epoch 066 - training loss: 22801.1689, validation loss: 0.5266
2024-05-24 09:17:11 [INFO]: Epoch 067 - training loss: 22799.4678, validation loss: 0.5275
2024-05-24 09:17:12 [INFO]: Epoch 068 - training loss: 22798.0363, validation loss: 0.5255
2024-05-24 09:17:13 [INFO]: Epoch 069 - training loss: 22798.4148, validation loss: 0.5228
2024-05-24 09:17:13 [INFO]: Epoch 070 - training loss: 22797.3354, validation loss: 0.5204
2024-05-24 09:17:14 [INFO]: Epoch 071 - training loss: 22797.2912, validation loss: 0.5197
2024-05-24 09:17:14 [INFO]: Epoch 072 - training loss: 22796.4886, validation loss: 0.5188
2024-05-24 09:17:15 [INFO]: Epoch 073 - training loss: 22795.9769, validation loss: 0.5189
2024-05-24 09:17:15 [INFO]: Epoch 074 - training loss: 22795.5960, validation loss: 0.5169
2024-05-24 09:17:16 [INFO]: Epoch 075 - training loss: 22795.3463, validation loss: 0.5142
2024-05-24 09:17:17 [INFO]: Epoch 076 - training loss: 22794.7635, validation loss: 0.5141
2024-05-24 09:17:17 [INFO]: Epoch 077 - training loss: 22793.9783, validation loss: 0.5131
2024-05-24 09:17:18 [INFO]: Epoch 078 - training loss: 22793.7184, validation loss: 0.5129
2024-05-24 09:17:18 [INFO]: Epoch 079 - training loss: 22793.7736, validation loss: 0.5115
2024-05-24 09:17:19 [INFO]: Epoch 080 - training loss: 22793.6956, validation loss: 0.5134
2024-05-24 09:17:19 [INFO]: Epoch 081 - training loss: 22793.1158, validation loss: 0.5081
2024-05-24 09:17:20 [INFO]: Epoch 082 - training loss: 22792.5116, validation loss: 0.5126
2024-05-24 09:17:21 [INFO]: Epoch 083 - training loss: 22793.0896, validation loss: 0.5074
2024-05-24 09:17:21 [INFO]: Epoch 084 - training loss: 22792.7949, validation loss: 0.5070
2024-05-24 09:17:22 [INFO]: Epoch 085 - training loss: 22792.1741, validation loss: 0.5052
2024-05-24 09:17:22 [INFO]: Epoch 086 - training loss: 22791.9569, validation loss: 0.5083
2024-05-24 09:17:23 [INFO]: Epoch 087 - training loss: 22791.4430, validation loss: 0.5053
2024-05-24 09:17:23 [INFO]: Epoch 088 - training loss: 22791.5711, validation loss: 0.5159
2024-05-24 09:17:24 [INFO]: Epoch 089 - training loss: 22791.2212, validation loss: 0.5052
2024-05-24 09:17:25 [INFO]: Epoch 090 - training loss: 22791.2490, validation loss: 0.5206
2024-05-24 09:17:25 [INFO]: Epoch 091 - training loss: 22792.6448, validation loss: 0.5076
2024-05-24 09:17:26 [INFO]: Epoch 092 - training loss: 22793.7161, validation loss: 0.5082
2024-05-24 09:17:26 [INFO]: Epoch 093 - training loss: 22793.3025, validation loss: 0.5108
2024-05-24 09:17:27 [INFO]: Epoch 094 - training loss: 22792.2722, validation loss: 0.5039
2024-05-24 09:17:27 [INFO]: Epoch 095 - training loss: 22790.8317, validation loss: 0.5062
2024-05-24 09:17:28 [INFO]: Epoch 096 - training loss: 22790.1074, validation loss: 0.5088
2024-05-24 09:17:28 [INFO]: Epoch 097 - training loss: 22789.4911, validation loss: 0.5021
2024-05-24 09:17:29 [INFO]: Epoch 098 - training loss: 22789.5423, validation loss: 0.5020
2024-05-24 09:17:30 [INFO]: Epoch 099 - training loss: 22789.5587, validation loss: 0.5066
2024-05-24 09:17:30 [INFO]: Epoch 100 - training loss: 22789.1217, validation loss: 0.5047
2024-05-24 09:17:31 [INFO]: Epoch 101 - training loss: 22789.7496, validation loss: 0.5014
2024-05-24 09:17:31 [INFO]: Epoch 102 - training loss: 22790.4404, validation loss: 0.5011
2024-05-24 09:17:32 [INFO]: Epoch 103 - training loss: 22788.9133, validation loss: 0.5067
2024-05-24 09:17:32 [INFO]: Epoch 104 - training loss: 22789.2610, validation loss: 0.5002
2024-05-24 09:17:33 [INFO]: Epoch 105 - training loss: 22788.7296, validation loss: 0.5055
2024-05-24 09:17:34 [INFO]: Epoch 106 - training loss: 22788.3714, validation loss: 0.5105
2024-05-24 09:17:34 [INFO]: Epoch 107 - training loss: 22789.8685, validation loss: 0.5078
2024-05-24 09:17:35 [INFO]: Epoch 108 - training loss: 22790.0194, validation loss: 0.5057
2024-05-24 09:17:35 [INFO]: Epoch 109 - training loss: 22789.3056, validation loss: 0.5007
2024-05-24 09:17:36 [INFO]: Epoch 110 - training loss: 22788.5941, validation loss: 0.4970
2024-05-24 09:17:37 [INFO]: Epoch 111 - training loss: 22788.1221, validation loss: 0.4974
2024-05-24 09:17:37 [INFO]: Epoch 112 - training loss: 22787.7650, validation loss: 0.4929
2024-05-24 09:17:38 [INFO]: Epoch 113 - training loss: 22787.0714, validation loss: 0.5023
2024-05-24 09:17:38 [INFO]: Epoch 114 - training loss: 22786.7867, validation loss: 0.4984
2024-05-24 09:17:39 [INFO]: Epoch 115 - training loss: 22787.0316, validation loss: 0.4939
2024-05-24 09:17:39 [INFO]: Epoch 116 - training loss: 22787.1342, validation loss: 0.4891
2024-05-24 09:17:40 [INFO]: Epoch 117 - training loss: 22787.1006, validation loss: 0.4993
2024-05-24 09:17:41 [INFO]: Epoch 118 - training loss: 22787.0088, validation loss: 0.4878
2024-05-24 09:17:41 [INFO]: Epoch 119 - training loss: 22787.1074, validation loss: 0.4992
2024-05-24 09:17:42 [INFO]: Epoch 120 - training loss: 22786.3890, validation loss: 0.4891
2024-05-24 09:17:42 [INFO]: Epoch 121 - training loss: 22786.0414, validation loss: 0.4935
2024-05-24 09:17:43 [INFO]: Epoch 122 - training loss: 22786.8941, validation loss: 0.4918
2024-05-24 09:17:43 [INFO]: Epoch 123 - training loss: 22786.8738, validation loss: 0.4939
2024-05-24 09:17:44 [INFO]: Epoch 124 - training loss: 22786.4146, validation loss: 0.4936
2024-05-24 09:17:45 [INFO]: Epoch 125 - training loss: 22785.3046, validation loss: 0.4962
2024-05-24 09:17:45 [INFO]: Epoch 126 - training loss: 22785.8498, validation loss: 0.4957
2024-05-24 09:17:46 [INFO]: Epoch 127 - training loss: 22785.6104, validation loss: 0.4948
2024-05-24 09:17:46 [INFO]: Epoch 128 - training loss: 22785.8434, validation loss: 0.4972
2024-05-24 09:17:46 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-05-24 09:17:46 [INFO]: Finished training. The best model is from epoch#118.
2024-05-24 09:17:46 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_3/GPVAE_physionet_2012_seta/20240524_T091633/GPVAE.pypots
2024-05-24 09:17:46 [INFO]: GP-VAE on PhysioNet-2012-seta: MAE=0.4160, MSE=0.4449
2024-05-24 09:17:47 [INFO]: Successfully saved to overlay_minibatchmasking_saved_results/round_3/GPVAE_physionet_2012_seta/imputation.pkl
2024-05-24 09:17:47 [INFO]: Using the given device: cuda:0
2024-05-24 09:17:47 [INFO]: Model files will be saved to overlay_minibatchmasking_saved_results/round_3/USGAN_physionet_2012_seta/20240524_T091747
2024-05-24 09:17:47 [INFO]: Tensorboard file will be saved to overlay_minibatchmasking_saved_results/round_3/USGAN_physionet_2012_seta/20240524_T091747/tensorboard
2024-05-24 09:17:47 [INFO]: USGAN initialized with the given hyperparameters, the number of trainable parameters: 16,010,261
2024-05-24 09:18:08 [INFO]: Epoch 001 - generator training loss: 0.6113, discriminator training loss: 0.3775, validation loss: 0.6181
2024-05-24 09:18:26 [INFO]: Epoch 002 - generator training loss: 0.4737, discriminator training loss: 0.2565, validation loss: 0.5225
2024-05-24 09:18:44 [INFO]: Epoch 003 - generator training loss: 0.4410, discriminator training loss: 0.2069, validation loss: 0.5005
2024-05-24 09:19:02 [INFO]: Epoch 004 - generator training loss: 0.4508, discriminator training loss: 0.1557, validation loss: 0.4853
2024-05-24 09:19:20 [INFO]: Epoch 005 - generator training loss: 0.4419, discriminator training loss: 0.1242, validation loss: 0.4710
2024-05-24 09:19:38 [INFO]: Epoch 006 - generator training loss: 0.4350, discriminator training loss: 0.1042, validation loss: 0.4533
2024-05-24 09:19:56 [INFO]: Epoch 007 - generator training loss: 0.4213, discriminator training loss: 0.0902, validation loss: 0.4419
2024-05-24 09:20:14 [INFO]: Epoch 008 - generator training loss: 0.4103, discriminator training loss: 0.0803, validation loss: 0.4277
2024-05-24 09:20:32 [INFO]: Epoch 009 - generator training loss: 0.4025, discriminator training loss: 0.0728, validation loss: 0.4176
2024-05-24 09:20:50 [INFO]: Epoch 010 - generator training loss: 0.3969, discriminator training loss: 0.0661, validation loss: 0.4135
2024-05-24 09:21:09 [INFO]: Epoch 011 - generator training loss: 0.3919, discriminator training loss: 0.0609, validation loss: 0.4081
2024-05-24 09:21:27 [INFO]: Epoch 012 - generator training loss: 0.3858, discriminator training loss: 0.0563, validation loss: 0.4029
2024-05-24 09:21:45 [INFO]: Epoch 013 - generator training loss: 0.3822, discriminator training loss: 0.0526, validation loss: 0.3977
2024-05-24 09:22:03 [INFO]: Epoch 014 - generator training loss: 0.3778, discriminator training loss: 0.0493, validation loss: 0.3939
2024-05-24 09:22:21 [INFO]: Epoch 015 - generator training loss: 0.3716, discriminator training loss: 0.0464, validation loss: 0.3898
2024-05-24 09:22:39 [INFO]: Epoch 016 - generator training loss: 0.3679, discriminator training loss: 0.0440, validation loss: 0.3830
2024-05-24 09:22:57 [INFO]: Epoch 017 - generator training loss: 0.3650, discriminator training loss: 0.0419, validation loss: 0.3800
2024-05-24 09:23:15 [INFO]: Epoch 018 - generator training loss: 0.3582, discriminator training loss: 0.0399, validation loss: 0.3773
2024-05-24 09:23:33 [INFO]: Epoch 019 - generator training loss: 0.3551, discriminator training loss: 0.0385, validation loss: 0.3755
2024-05-24 09:23:52 [INFO]: Epoch 020 - generator training loss: 0.3493, discriminator training loss: 0.0370, validation loss: 0.3701
2024-05-24 09:24:10 [INFO]: Epoch 021 - generator training loss: 0.3466, discriminator training loss: 0.0357, validation loss: 0.3651
2024-05-24 09:24:28 [INFO]: Epoch 022 - generator training loss: 0.3432, discriminator training loss: 0.0347, validation loss: 0.3624
2024-05-24 09:24:46 [INFO]: Epoch 023 - generator training loss: 0.3367, discriminator training loss: 0.0339, validation loss: 0.3579
2024-05-24 09:25:04 [INFO]: Epoch 024 - generator training loss: 0.3320, discriminator training loss: 0.0330, validation loss: 0.3596
2024-05-24 09:25:22 [INFO]: Epoch 025 - generator training loss: 0.3447, discriminator training loss: 0.0324, validation loss: 0.3616
2024-05-24 09:25:40 [INFO]: Epoch 026 - generator training loss: 0.3309, discriminator training loss: 0.0316, validation loss: 0.3562
2024-05-24 09:25:58 [INFO]: Epoch 027 - generator training loss: 0.3237, discriminator training loss: 0.0310, validation loss: 0.3524
2024-05-24 09:26:16 [INFO]: Epoch 028 - generator training loss: 0.3165, discriminator training loss: 0.0303, validation loss: 0.3478
2024-05-24 09:26:34 [INFO]: Epoch 029 - generator training loss: 0.3150, discriminator training loss: 0.0297, validation loss: 0.3456
2024-05-24 09:26:52 [INFO]: Epoch 030 - generator training loss: 0.3098, discriminator training loss: 0.0295, validation loss: 0.3476
2024-05-24 09:27:10 [INFO]: Epoch 031 - generator training loss: 0.3065, discriminator training loss: 0.0288, validation loss: 0.3392
2024-05-24 09:27:29 [INFO]: Epoch 032 - generator training loss: 0.3007, discriminator training loss: 0.0285, validation loss: 0.3323
2024-05-24 09:27:47 [INFO]: Epoch 033 - generator training loss: 0.2951, discriminator training loss: 0.0280, validation loss: 0.3335
2024-05-24 09:28:05 [INFO]: Epoch 034 - generator training loss: 0.2928, discriminator training loss: 0.0277, validation loss: 0.3290
2024-05-24 09:28:23 [INFO]: Epoch 035 - generator training loss: 0.2955, discriminator training loss: 0.0274, validation loss: 0.3359
2024-05-24 09:28:41 [INFO]: Epoch 036 - generator training loss: 0.2944, discriminator training loss: 0.0270, validation loss: 0.3463
2024-05-24 09:28:59 [INFO]: Epoch 037 - generator training loss: 0.2965, discriminator training loss: 0.0269, validation loss: 0.3273
2024-05-24 09:29:17 [INFO]: Epoch 038 - generator training loss: 0.2849, discriminator training loss: 0.0264, validation loss: 0.3200
2024-05-24 09:29:35 [INFO]: Epoch 039 - generator training loss: 0.2768, discriminator training loss: 0.0261, validation loss: 0.3178
2024-05-24 09:29:53 [INFO]: Epoch 040 - generator training loss: 0.2757, discriminator training loss: 0.0260, validation loss: 0.3206
2024-05-24 09:30:11 [INFO]: Epoch 041 - generator training loss: 0.2705, discriminator training loss: 0.0256, validation loss: 0.3185
2024-05-24 09:30:29 [INFO]: Epoch 042 - generator training loss: 0.2674, discriminator training loss: 0.0255, validation loss: 0.3195
2024-05-24 09:30:47 [INFO]: Epoch 043 - generator training loss: 0.2667, discriminator training loss: 0.0253, validation loss: 0.3119
2024-05-24 09:31:05 [INFO]: Epoch 044 - generator training loss: 0.2652, discriminator training loss: 0.0250, validation loss: 0.3122
2024-05-24 09:31:23 [INFO]: Epoch 045 - generator training loss: 0.2607, discriminator training loss: 0.0250, validation loss: 0.3103
2024-05-24 09:31:42 [INFO]: Epoch 046 - generator training loss: 0.2561, discriminator training loss: 0.0247, validation loss: 0.3078
2024-05-24 09:32:00 [INFO]: Epoch 047 - generator training loss: 0.2597, discriminator training loss: 0.0245, validation loss: 0.3095
2024-05-24 09:32:18 [INFO]: Epoch 048 - generator training loss: 0.2554, discriminator training loss: 0.0242, validation loss: 0.3074
2024-05-24 09:32:36 [INFO]: Epoch 049 - generator training loss: 0.2498, discriminator training loss: 0.0240, validation loss: 0.3076
2024-05-24 09:32:54 [INFO]: Epoch 050 - generator training loss: 0.2499, discriminator training loss: 0.0239, validation loss: 0.3072
2024-05-24 09:33:12 [INFO]: Epoch 051 - generator training loss: 0.2487, discriminator training loss: 0.0239, validation loss: 0.3041
2024-05-24 09:33:30 [INFO]: Epoch 052 - generator training loss: 0.2431, discriminator training loss: 0.0236, validation loss: 0.3126
2024-05-24 09:33:48 [INFO]: Epoch 053 - generator training loss: 0.2390, discriminator training loss: 0.0235, validation loss: 0.3009
2024-05-24 09:34:06 [INFO]: Epoch 054 - generator training loss: 0.2352, discriminator training loss: 0.0233, validation loss: 0.3082
2024-05-24 09:34:25 [INFO]: Epoch 055 - generator training loss: 0.2335, discriminator training loss: 0.0231, validation loss: 0.3067
2024-05-24 09:34:43 [INFO]: Epoch 056 - generator training loss: 0.2306, discriminator training loss: 0.0230, validation loss: 0.3050
2024-05-24 09:35:01 [INFO]: Epoch 057 - generator training loss: 0.2321, discriminator training loss: 0.0228, validation loss: 0.3041
2024-05-24 09:35:19 [INFO]: Epoch 058 - generator training loss: 0.2290, discriminator training loss: 0.0227, validation loss: 0.3034
2024-05-24 09:35:37 [INFO]: Epoch 059 - generator training loss: 0.2234, discriminator training loss: 0.0227, validation loss: 0.3027
2024-05-24 09:35:55 [INFO]: Epoch 060 - generator training loss: 0.2210, discriminator training loss: 0.0224, validation loss: 0.3025
2024-05-24 09:36:13 [INFO]: Epoch 061 - generator training loss: 0.2236, discriminator training loss: 0.0223, validation loss: 0.3050
2024-05-24 09:36:31 [INFO]: Epoch 062 - generator training loss: 0.2242, discriminator training loss: 0.0221, validation loss: 0.3078
2024-05-24 09:36:49 [INFO]: Epoch 063 - generator training loss: 0.2235, discriminator training loss: 0.0221, validation loss: 0.3057
2024-05-24 09:36:49 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-05-24 09:36:49 [INFO]: Finished training. The best model is from epoch#53.
2024-05-24 09:36:49 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_3/USGAN_physionet_2012_seta/20240524_T091747/USGAN.pypots
2024-05-24 09:36:52 [INFO]: US-GAN on PhysioNet-2012-seta: MAE=0.2964, MSE=0.2730
2024-05-24 09:37:01 [INFO]: Successfully saved to overlay_minibatchmasking_saved_results/round_3/USGAN_physionet_2012_seta/imputation.pkl
2024-05-24 09:37:01 [INFO]: Using the given device: cuda:0
2024-05-24 09:37:01 [INFO]: Model files will be saved to overlay_minibatchmasking_saved_results/round_3/BRITS_physionet_2012_seta/20240524_T093701
2024-05-24 09:37:01 [INFO]: Tensorboard file will be saved to overlay_minibatchmasking_saved_results/round_3/BRITS_physionet_2012_seta/20240524_T093701/tensorboard
2024-05-24 09:37:01 [INFO]: BRITS initialized with the given hyperparameters, the number of trainable parameters: 9,176,048
2024-05-24 09:37:16 [INFO]: Epoch 001 - training loss: 1.1271, validation loss: 0.5305
2024-05-24 09:37:28 [INFO]: Epoch 002 - training loss: 0.9168, validation loss: 0.4722
2024-05-24 09:37:40 [INFO]: Epoch 003 - training loss: 0.8548, validation loss: 0.4457
2024-05-24 09:37:51 [INFO]: Epoch 004 - training loss: 0.8184, validation loss: 0.4265
2024-05-24 09:38:03 [INFO]: Epoch 005 - training loss: 0.7900, validation loss: 0.4085
2024-05-24 09:38:15 [INFO]: Epoch 006 - training loss: 0.7646, validation loss: 0.3948
2024-05-24 09:38:27 [INFO]: Epoch 007 - training loss: 0.7446, validation loss: 0.3833
2024-05-24 09:38:39 [INFO]: Epoch 008 - training loss: 0.7266, validation loss: 0.3744
2024-05-24 09:38:51 [INFO]: Epoch 009 - training loss: 0.7119, validation loss: 0.3677
2024-05-24 09:39:02 [INFO]: Epoch 010 - training loss: 0.6997, validation loss: 0.3610
2024-05-24 09:39:14 [INFO]: Epoch 011 - training loss: 0.6888, validation loss: 0.3584
2024-05-24 09:39:26 [INFO]: Epoch 012 - training loss: 0.6799, validation loss: 0.3562
2024-05-24 09:39:38 [INFO]: Epoch 013 - training loss: 0.6709, validation loss: 0.3542
2024-05-24 09:39:50 [INFO]: Epoch 014 - training loss: 0.6640, validation loss: 0.3541
2024-05-24 09:40:02 [INFO]: Epoch 015 - training loss: 0.6568, validation loss: 0.3500
2024-05-24 09:40:13 [INFO]: Epoch 016 - training loss: 0.6519, validation loss: 0.3483
2024-05-24 09:40:25 [INFO]: Epoch 017 - training loss: 0.6462, validation loss: 0.3467
2024-05-24 09:40:37 [INFO]: Epoch 018 - training loss: 0.6419, validation loss: 0.3467
2024-05-24 09:40:49 [INFO]: Epoch 019 - training loss: 0.6374, validation loss: 0.3465
2024-05-24 09:41:01 [INFO]: Epoch 020 - training loss: 0.6328, validation loss: 0.3441
2024-05-24 09:41:12 [INFO]: Epoch 021 - training loss: 0.6297, validation loss: 0.3431
2024-05-24 09:41:24 [INFO]: Epoch 022 - training loss: 0.6247, validation loss: 0.3429
2024-05-24 09:41:36 [INFO]: Epoch 023 - training loss: 0.6219, validation loss: 0.3410
2024-05-24 09:41:48 [INFO]: Epoch 024 - training loss: 0.6175, validation loss: 0.3405
2024-05-24 09:42:00 [INFO]: Epoch 025 - training loss: 0.6149, validation loss: 0.3402
2024-05-24 09:42:11 [INFO]: Epoch 026 - training loss: 0.6123, validation loss: 0.3387
2024-05-24 09:42:23 [INFO]: Epoch 027 - training loss: 0.6081, validation loss: 0.3385
2024-05-24 09:42:35 [INFO]: Epoch 028 - training loss: 0.6053, validation loss: 0.3378
2024-05-24 09:42:47 [INFO]: Epoch 029 - training loss: 0.6035, validation loss: 0.3389
2024-05-24 09:42:59 [INFO]: Epoch 030 - training loss: 0.6022, validation loss: 0.3392
2024-05-24 09:43:11 [INFO]: Epoch 031 - training loss: 0.5967, validation loss: 0.3382
2024-05-24 09:43:22 [INFO]: Epoch 032 - training loss: 0.5941, validation loss: 0.3374
2024-05-24 09:43:34 [INFO]: Epoch 033 - training loss: 0.5911, validation loss: 0.3401
2024-05-24 09:43:46 [INFO]: Epoch 034 - training loss: 0.5886, validation loss: 0.3378
2024-05-24 09:43:58 [INFO]: Epoch 035 - training loss: 0.5859, validation loss: 0.3420
2024-05-24 09:44:10 [INFO]: Epoch 036 - training loss: 0.5853, validation loss: 0.3412
2024-05-24 09:44:22 [INFO]: Epoch 037 - training loss: 0.5801, validation loss: 0.3403
2024-05-24 09:44:35 [INFO]: Epoch 038 - training loss: 0.5772, validation loss: 0.3393
2024-05-24 09:44:47 [INFO]: Epoch 039 - training loss: 0.5739, validation loss: 0.3417
2024-05-24 09:45:00 [INFO]: Epoch 040 - training loss: 0.5710, validation loss: 0.3425
2024-05-24 09:45:12 [INFO]: Epoch 041 - training loss: 0.5687, validation loss: 0.3436
2024-05-24 09:45:25 [INFO]: Epoch 042 - training loss: 0.5653, validation loss: 0.3432
2024-05-24 09:45:25 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-05-24 09:45:25 [INFO]: Finished training. The best model is from epoch#32.
2024-05-24 09:45:25 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_3/BRITS_physionet_2012_seta/20240524_T093701/BRITS.pypots
2024-05-24 09:45:28 [INFO]: BRITS on PhysioNet-2012-seta: MAE=0.2531, MSE=0.2634
2024-05-24 09:45:37 [INFO]: Successfully saved to overlay_minibatchmasking_saved_results/round_3/BRITS_physionet_2012_seta/imputation.pkl
2024-05-24 09:45:37 [INFO]: Using the given device: cuda:0
2024-05-24 09:45:37 [INFO]: Model files will be saved to overlay_minibatchmasking_saved_results/round_3/MRNN_physionet_2012_seta/20240524_T094537
2024-05-24 09:45:37 [INFO]: Tensorboard file will be saved to overlay_minibatchmasking_saved_results/round_3/MRNN_physionet_2012_seta/20240524_T094537/tensorboard
2024-05-24 09:45:37 [INFO]: MRNN initialized with the given hyperparameters, the number of trainable parameters: 7,599
2024-05-24 09:45:43 [INFO]: Epoch 001 - training loss: 1.1493, validation loss: 0.9964
2024-05-24 09:45:43 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_3/MRNN_physionet_2012_seta/20240524_T094537/MRNN_epoch1_loss0.9963644415140152.pypots
2024-05-24 09:45:46 [INFO]: Epoch 002 - training loss: 0.7197, validation loss: 0.9653
2024-05-24 09:45:46 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_3/MRNN_physionet_2012_seta/20240524_T094537/MRNN_epoch2_loss0.9653181433677673.pypots
2024-05-24 09:45:49 [INFO]: Epoch 003 - training loss: 0.6124, validation loss: 0.9402
2024-05-24 09:45:49 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_3/MRNN_physionet_2012_seta/20240524_T094537/MRNN_epoch3_loss0.9402163118124008.pypots
2024-05-24 09:45:52 [INFO]: Epoch 004 - training loss: 0.5784, validation loss: 0.9283
2024-05-24 09:45:52 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_3/MRNN_physionet_2012_seta/20240524_T094537/MRNN_epoch4_loss0.9282898962497711.pypots
2024-05-24 09:45:55 [INFO]: Epoch 005 - training loss: 0.5496, validation loss: 0.9221
2024-05-24 09:45:55 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_3/MRNN_physionet_2012_seta/20240524_T094537/MRNN_epoch5_loss0.9220715969800949.pypots
2024-05-24 09:45:58 [INFO]: Epoch 006 - training loss: 0.5246, validation loss: 0.9173
2024-05-24 09:45:58 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_3/MRNN_physionet_2012_seta/20240524_T094537/MRNN_epoch6_loss0.9173056066036225.pypots
2024-05-24 09:46:01 [INFO]: Epoch 007 - training loss: 0.5170, validation loss: 0.9141
2024-05-24 09:46:01 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_3/MRNN_physionet_2012_seta/20240524_T094537/MRNN_epoch7_loss0.9141401737928391.pypots
2024-05-24 09:46:04 [INFO]: Epoch 008 - training loss: 0.5053, validation loss: 0.9115
2024-05-24 09:46:04 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_3/MRNN_physionet_2012_seta/20240524_T094537/MRNN_epoch8_loss0.9114815980195999.pypots
2024-05-24 09:46:07 [INFO]: Epoch 009 - training loss: 0.5075, validation loss: 0.9107
2024-05-24 09:46:07 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_3/MRNN_physionet_2012_seta/20240524_T094537/MRNN_epoch9_loss0.9106704741716385.pypots
2024-05-24 09:46:09 [INFO]: Epoch 010 - training loss: 0.4909, validation loss: 0.9092
2024-05-24 09:46:09 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_3/MRNN_physionet_2012_seta/20240524_T094537/MRNN_epoch10_loss0.9091546535491943.pypots
2024-05-24 09:46:12 [INFO]: Epoch 011 - training loss: 0.4890, validation loss: 0.9089
2024-05-24 09:46:12 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_3/MRNN_physionet_2012_seta/20240524_T094537/MRNN_epoch11_loss0.9088969439268112.pypots
2024-05-24 09:46:15 [INFO]: Epoch 012 - training loss: 0.4810, validation loss: 0.9101
2024-05-24 09:46:15 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_3/MRNN_physionet_2012_seta/20240524_T094537/MRNN_epoch12_loss0.9101487815380096.pypots
2024-05-24 09:46:18 [INFO]: Epoch 013 - training loss: 0.4868, validation loss: 0.9150
2024-05-24 09:46:18 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_3/MRNN_physionet_2012_seta/20240524_T094537/MRNN_epoch13_loss0.9149565368890762.pypots
2024-05-24 09:46:21 [INFO]: Epoch 014 - training loss: 0.4769, validation loss: 0.9117
2024-05-24 09:46:21 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_3/MRNN_physionet_2012_seta/20240524_T094537/MRNN_epoch14_loss0.9116649955511094.pypots
2024-05-24 09:46:24 [INFO]: Epoch 015 - training loss: 0.4666, validation loss: 0.9131
2024-05-24 09:46:24 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_3/MRNN_physionet_2012_seta/20240524_T094537/MRNN_epoch15_loss0.9131206661462784.pypots
2024-05-24 09:46:27 [INFO]: Epoch 016 - training loss: 0.4682, validation loss: 0.9145
2024-05-24 09:46:27 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_3/MRNN_physionet_2012_seta/20240524_T094537/MRNN_epoch16_loss0.9144623041152954.pypots
2024-05-24 09:46:30 [INFO]: Epoch 017 - training loss: 0.4676, validation loss: 0.9162
2024-05-24 09:46:30 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_3/MRNN_physionet_2012_seta/20240524_T094537/MRNN_epoch17_loss0.9162164896726608.pypots
2024-05-24 09:46:32 [INFO]: Epoch 018 - training loss: 0.4616, validation loss: 0.9184
2024-05-24 09:46:32 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_3/MRNN_physionet_2012_seta/20240524_T094537/MRNN_epoch18_loss0.9184110283851623.pypots
2024-05-24 09:46:35 [INFO]: Epoch 019 - training loss: 0.4596, validation loss: 0.9193
2024-05-24 09:46:35 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_3/MRNN_physionet_2012_seta/20240524_T094537/MRNN_epoch19_loss0.9193175077438355.pypots
2024-05-24 09:46:38 [INFO]: Epoch 020 - training loss: 0.4589, validation loss: 0.9205
2024-05-24 09:46:38 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_3/MRNN_physionet_2012_seta/20240524_T094537/MRNN_epoch20_loss0.9204551488161087.pypots
2024-05-24 09:46:41 [INFO]: Epoch 021 - training loss: 0.4569, validation loss: 0.9215
2024-05-24 09:46:41 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_3/MRNN_physionet_2012_seta/20240524_T094537/MRNN_epoch21_loss0.9215465128421784.pypots
2024-05-24 09:46:41 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-05-24 09:46:41 [INFO]: Finished training. The best model is from epoch#11.
2024-05-24 09:46:41 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_3/MRNN_physionet_2012_seta/20240524_T094537/MRNN.pypots
2024-05-24 09:46:42 [INFO]: MRNN on PhysioNet-2012-seta: MAE=0.6816, MSE=0.9052
2024-05-24 09:46:46 [INFO]: Successfully saved to overlay_minibatchmasking_saved_results/round_3/MRNN_physionet_2012_seta/imputation.pkl
2024-05-24 09:46:46 [INFO]: Using the given device: cpu
2024-05-24 09:46:46 [INFO]: LOCF on PhysioNet-2012-seta: MAE=0.4109, MSE=0.5324
2024-05-24 09:46:46 [INFO]: Successfully created the given path "saved_results/round_3/LOCF_physionet_2012_seta".
2024-05-24 09:46:46 [INFO]: Successfully saved to overlay_minibatchmasking_saved_results/round_3/LOCF_physionet_2012_seta/imputation.pkl
2024-05-24 09:46:46 [INFO]: Median on PhysioNet-2012-seta: MAE=0.6868, MSE=1.0189
2024-05-24 09:46:46 [INFO]: Successfully created the given path "saved_results/round_3/Median_physionet_2012_seta".
2024-05-24 09:46:46 [INFO]: Successfully saved to overlay_minibatchmasking_saved_results/round_3/Median_physionet_2012_seta/imputation.pkl
2024-05-24 09:46:46 [INFO]: Mean on PhysioNet-2012-seta: MAE=0.7045, MSE=0.9898
2024-05-24 09:46:46 [INFO]: Successfully created the given path "saved_results/round_3/Mean_physionet_2012_seta".
2024-05-24 09:46:46 [INFO]: Successfully saved to overlay_minibatchmasking_saved_results/round_3/Mean_physionet_2012_seta/imputation.pkl
2024-05-24 09:46:46 [INFO]: Have set the random seed as 2027 for numpy and pytorch.
2024-05-24 09:46:47 [INFO]: Using the given device: cuda:0
2024-05-24 09:46:47 [INFO]: Model files will be saved to overlay_minibatchmasking_saved_results/round_4/SAITS_physionet_2012_seta/20240524_T094647
2024-05-24 09:46:47 [INFO]: Tensorboard file will be saved to overlay_minibatchmasking_saved_results/round_4/SAITS_physionet_2012_seta/20240524_T094647/tensorboard
2024-05-24 09:46:47 [INFO]: SAITS initialized with the given hyperparameters, the number of trainable parameters: 5,322,806
2024-05-24 09:46:48 [INFO]: Epoch 001 - training loss: 1.0751, validation loss: 0.4849
2024-05-24 09:46:49 [INFO]: Epoch 002 - training loss: 0.6645, validation loss: 0.4214
2024-05-24 09:46:51 [INFO]: Epoch 003 - training loss: 0.5785, validation loss: 0.4061
2024-05-24 09:46:52 [INFO]: Epoch 004 - training loss: 0.5218, validation loss: 0.3732
2024-05-24 09:46:53 [INFO]: Epoch 005 - training loss: 0.4846, validation loss: 0.3575
2024-05-24 09:46:55 [INFO]: Epoch 006 - training loss: 0.4611, validation loss: 0.3467
2024-05-24 09:46:56 [INFO]: Epoch 007 - training loss: 0.4420, validation loss: 0.3358
2024-05-24 09:46:57 [INFO]: Epoch 008 - training loss: 0.4192, validation loss: 0.3240
2024-05-24 09:46:58 [INFO]: Epoch 009 - training loss: 0.3997, validation loss: 0.3080
2024-05-24 09:47:00 [INFO]: Epoch 010 - training loss: 0.3900, validation loss: 0.2998
2024-05-24 09:47:01 [INFO]: Epoch 011 - training loss: 0.3788, validation loss: 0.2933
2024-05-24 09:47:02 [INFO]: Epoch 012 - training loss: 0.3690, validation loss: 0.2912
2024-05-24 09:47:03 [INFO]: Epoch 013 - training loss: 0.3562, validation loss: 0.2827
2024-05-24 09:47:05 [INFO]: Epoch 014 - training loss: 0.3492, validation loss: 0.2808
2024-05-24 09:47:06 [INFO]: Epoch 015 - training loss: 0.3347, validation loss: 0.2773
2024-05-24 09:47:07 [INFO]: Epoch 016 - training loss: 0.3327, validation loss: 0.2740
2024-05-24 09:47:08 [INFO]: Epoch 017 - training loss: 0.3319, validation loss: 0.2669
2024-05-24 09:47:10 [INFO]: Epoch 018 - training loss: 0.3207, validation loss: 0.2650
2024-05-24 09:47:11 [INFO]: Epoch 019 - training loss: 0.3163, validation loss: 0.2646
2024-05-24 09:47:12 [INFO]: Epoch 020 - training loss: 0.3119, validation loss: 0.2607
2024-05-24 09:47:13 [INFO]: Epoch 021 - training loss: 0.3077, validation loss: 0.2535
2024-05-24 09:47:15 [INFO]: Epoch 022 - training loss: 0.3074, validation loss: 0.2519
2024-05-24 09:47:16 [INFO]: Epoch 023 - training loss: 0.3017, validation loss: 0.2539
2024-05-24 09:47:17 [INFO]: Epoch 024 - training loss: 0.2985, validation loss: 0.2510
2024-05-24 09:47:19 [INFO]: Epoch 025 - training loss: 0.2976, validation loss: 0.2488
2024-05-24 09:47:20 [INFO]: Epoch 026 - training loss: 0.2909, validation loss: 0.2491
2024-05-24 09:47:21 [INFO]: Epoch 027 - training loss: 0.2905, validation loss: 0.2455
2024-05-24 09:47:22 [INFO]: Epoch 028 - training loss: 0.2884, validation loss: 0.2418
2024-05-24 09:47:24 [INFO]: Epoch 029 - training loss: 0.2834, validation loss: 0.2411
2024-05-24 09:47:25 [INFO]: Epoch 030 - training loss: 0.2818, validation loss: 0.2417
2024-05-24 09:47:26 [INFO]: Epoch 031 - training loss: 0.2816, validation loss: 0.2466
2024-05-24 09:47:27 [INFO]: Epoch 032 - training loss: 0.2815, validation loss: 0.2374
2024-05-24 09:47:29 [INFO]: Epoch 033 - training loss: 0.2801, validation loss: 0.2380
2024-05-24 09:47:30 [INFO]: Epoch 034 - training loss: 0.2786, validation loss: 0.2355
2024-05-24 09:47:31 [INFO]: Epoch 035 - training loss: 0.2755, validation loss: 0.2334
2024-05-24 09:47:32 [INFO]: Epoch 036 - training loss: 0.2738, validation loss: 0.2410
2024-05-24 09:47:34 [INFO]: Epoch 037 - training loss: 0.2743, validation loss: 0.2334
2024-05-24 09:47:35 [INFO]: Epoch 038 - training loss: 0.2701, validation loss: 0.2275
2024-05-24 09:47:36 [INFO]: Epoch 039 - training loss: 0.2723, validation loss: 0.2309
2024-05-24 09:47:37 [INFO]: Epoch 040 - training loss: 0.2712, validation loss: 0.2290
2024-05-24 09:47:39 [INFO]: Epoch 041 - training loss: 0.2711, validation loss: 0.2336
2024-05-24 09:47:40 [INFO]: Epoch 042 - training loss: 0.2675, validation loss: 0.2331
2024-05-24 09:47:41 [INFO]: Epoch 043 - training loss: 0.2651, validation loss: 0.2431
2024-05-24 09:47:43 [INFO]: Epoch 044 - training loss: 0.2675, validation loss: 0.2261
2024-05-24 09:47:44 [INFO]: Epoch 045 - training loss: 0.2643, validation loss: 0.2363
2024-05-24 09:47:45 [INFO]: Epoch 046 - training loss: 0.2646, validation loss: 0.2243
2024-05-24 09:47:46 [INFO]: Epoch 047 - training loss: 0.2616, validation loss: 0.2348
2024-05-24 09:47:48 [INFO]: Epoch 048 - training loss: 0.2613, validation loss: 0.2217
2024-05-24 09:47:49 [INFO]: Epoch 049 - training loss: 0.2626, validation loss: 0.2281
2024-05-24 09:47:50 [INFO]: Epoch 050 - training loss: 0.2604, validation loss: 0.2178
2024-05-24 09:47:51 [INFO]: Epoch 051 - training loss: 0.2586, validation loss: 0.2186
2024-05-24 09:47:53 [INFO]: Epoch 052 - training loss: 0.2585, validation loss: 0.2286
2024-05-24 09:47:54 [INFO]: Epoch 053 - training loss: 0.2588, validation loss: 0.2376
2024-05-24 09:47:55 [INFO]: Epoch 054 - training loss: 0.2559, validation loss: 0.2256
2024-05-24 09:47:56 [INFO]: Epoch 055 - training loss: 0.2564, validation loss: 0.2213
2024-05-24 09:47:58 [INFO]: Epoch 056 - training loss: 0.2565, validation loss: 0.2368
2024-05-24 09:47:59 [INFO]: Epoch 057 - training loss: 0.2572, validation loss: 0.2264
2024-05-24 09:48:00 [INFO]: Epoch 058 - training loss: 0.2572, validation loss: 0.2179
2024-05-24 09:48:02 [INFO]: Epoch 059 - training loss: 0.2546, validation loss: 0.2266
2024-05-24 09:48:03 [INFO]: Epoch 060 - training loss: 0.2544, validation loss: 0.2255
2024-05-24 09:48:03 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-05-24 09:48:03 [INFO]: Finished training. The best model is from epoch#50.
2024-05-24 09:48:03 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_4/SAITS_physionet_2012_seta/20240524_T094647/SAITS.pypots
2024-05-24 09:48:03 [INFO]: SAITS on PhysioNet-2012-seta: MAE=0.2067, MSE=0.2298
2024-05-24 09:48:03 [INFO]: Successfully saved to overlay_minibatchmasking_saved_results/round_4/SAITS_physionet_2012_seta/imputation.pkl
2024-05-24 09:48:03 [INFO]: Using the given device: cuda:0
2024-05-24 09:48:03 [INFO]: Model files will be saved to overlay_minibatchmasking_saved_results/round_4/Transformer_physionet_2012_seta/20240524_T094803
2024-05-24 09:48:03 [INFO]: Tensorboard file will be saved to overlay_minibatchmasking_saved_results/round_4/Transformer_physionet_2012_seta/20240524_T094803/tensorboard
2024-05-24 09:48:03 [INFO]: Transformer initialized with the given hyperparameters, the number of trainable parameters: 2,152,613
2024-05-24 09:48:04 [INFO]: Epoch 001 - training loss: 1.1866, validation loss: 0.5558
2024-05-24 09:48:05 [INFO]: Epoch 002 - training loss: 0.7517, validation loss: 0.4788
2024-05-24 09:48:05 [INFO]: Epoch 003 - training loss: 0.6452, validation loss: 0.4569
2024-05-24 09:48:06 [INFO]: Epoch 004 - training loss: 0.5889, validation loss: 0.4311
2024-05-24 09:48:07 [INFO]: Epoch 005 - training loss: 0.5564, validation loss: 0.4098
2024-05-24 09:48:08 [INFO]: Epoch 006 - training loss: 0.5179, validation loss: 0.3890
2024-05-24 09:48:08 [INFO]: Epoch 007 - training loss: 0.5073, validation loss: 0.3992
2024-05-24 09:48:09 [INFO]: Epoch 008 - training loss: 0.4823, validation loss: 0.3742
2024-05-24 09:48:10 [INFO]: Epoch 009 - training loss: 0.4678, validation loss: 0.3677
2024-05-24 09:48:10 [INFO]: Epoch 010 - training loss: 0.4519, validation loss: 0.3667
2024-05-24 09:48:11 [INFO]: Epoch 011 - training loss: 0.4426, validation loss: 0.3558
2024-05-24 09:48:12 [INFO]: Epoch 012 - training loss: 0.4274, validation loss: 0.3496
2024-05-24 09:48:12 [INFO]: Epoch 013 - training loss: 0.4213, validation loss: 0.3463
2024-05-24 09:48:13 [INFO]: Epoch 014 - training loss: 0.4117, validation loss: 0.3408
2024-05-24 09:48:14 [INFO]: Epoch 015 - training loss: 0.4068, validation loss: 0.3348
2024-05-24 09:48:15 [INFO]: Epoch 016 - training loss: 0.3984, validation loss: 0.3338
2024-05-24 09:48:15 [INFO]: Epoch 017 - training loss: 0.3877, validation loss: 0.3287
2024-05-24 09:48:16 [INFO]: Epoch 018 - training loss: 0.3856, validation loss: 0.3263
2024-05-24 09:48:17 [INFO]: Epoch 019 - training loss: 0.3802, validation loss: 0.3244
2024-05-24 09:48:17 [INFO]: Epoch 020 - training loss: 0.3692, validation loss: 0.3182
2024-05-24 09:48:18 [INFO]: Epoch 021 - training loss: 0.3717, validation loss: 0.3135
2024-05-24 09:48:19 [INFO]: Epoch 022 - training loss: 0.3621, validation loss: 0.3151
2024-05-24 09:48:20 [INFO]: Epoch 023 - training loss: 0.3580, validation loss: 0.3086
2024-05-24 09:48:20 [INFO]: Epoch 024 - training loss: 0.3568, validation loss: 0.3038
2024-05-24 09:48:21 [INFO]: Epoch 025 - training loss: 0.3481, validation loss: 0.3049
2024-05-24 09:48:22 [INFO]: Epoch 026 - training loss: 0.3481, validation loss: 0.3064
2024-05-24 09:48:22 [INFO]: Epoch 027 - training loss: 0.3436, validation loss: 0.2964
2024-05-24 09:48:23 [INFO]: Epoch 028 - training loss: 0.3407, validation loss: 0.2995
2024-05-24 09:48:24 [INFO]: Epoch 029 - training loss: 0.3393, validation loss: 0.2979
2024-05-24 09:48:24 [INFO]: Epoch 030 - training loss: 0.3401, validation loss: 0.2967
2024-05-24 09:48:25 [INFO]: Epoch 031 - training loss: 0.3362, validation loss: 0.2939
2024-05-24 09:48:26 [INFO]: Epoch 032 - training loss: 0.3297, validation loss: 0.2921
2024-05-24 09:48:26 [INFO]: Epoch 033 - training loss: 0.3310, validation loss: 0.2885
2024-05-24 09:48:27 [INFO]: Epoch 034 - training loss: 0.3252, validation loss: 0.2861
2024-05-24 09:48:28 [INFO]: Epoch 035 - training loss: 0.3282, validation loss: 0.2878
2024-05-24 09:48:29 [INFO]: Epoch 036 - training loss: 0.3248, validation loss: 0.2859
2024-05-24 09:48:29 [INFO]: Epoch 037 - training loss: 0.3207, validation loss: 0.2828
2024-05-24 09:48:30 [INFO]: Epoch 038 - training loss: 0.3185, validation loss: 0.2811
2024-05-24 09:48:31 [INFO]: Epoch 039 - training loss: 0.3163, validation loss: 0.2794
2024-05-24 09:48:31 [INFO]: Epoch 040 - training loss: 0.3171, validation loss: 0.2782
2024-05-24 09:48:32 [INFO]: Epoch 041 - training loss: 0.3161, validation loss: 0.2799
2024-05-24 09:48:33 [INFO]: Epoch 042 - training loss: 0.3117, validation loss: 0.2769
2024-05-24 09:48:33 [INFO]: Epoch 043 - training loss: 0.3110, validation loss: 0.2752
2024-05-24 09:48:34 [INFO]: Epoch 044 - training loss: 0.3121, validation loss: 0.2710
2024-05-24 09:48:35 [INFO]: Epoch 045 - training loss: 0.3094, validation loss: 0.2751
2024-05-24 09:48:36 [INFO]: Epoch 046 - training loss: 0.3088, validation loss: 0.2725
2024-05-24 09:48:36 [INFO]: Epoch 047 - training loss: 0.3081, validation loss: 0.2672
2024-05-24 09:48:37 [INFO]: Epoch 048 - training loss: 0.3073, validation loss: 0.2732
2024-05-24 09:48:38 [INFO]: Epoch 049 - training loss: 0.3056, validation loss: 0.2687
2024-05-24 09:48:38 [INFO]: Epoch 050 - training loss: 0.3062, validation loss: 0.2711
2024-05-24 09:48:39 [INFO]: Epoch 051 - training loss: 0.3037, validation loss: 0.2679
2024-05-24 09:48:40 [INFO]: Epoch 052 - training loss: 0.2996, validation loss: 0.2640
2024-05-24 09:48:40 [INFO]: Epoch 053 - training loss: 0.2986, validation loss: 0.2674
2024-05-24 09:48:41 [INFO]: Epoch 054 - training loss: 0.3000, validation loss: 0.2657
2024-05-24 09:48:42 [INFO]: Epoch 055 - training loss: 0.2984, validation loss: 0.2650
2024-05-24 09:48:43 [INFO]: Epoch 056 - training loss: 0.2970, validation loss: 0.2643
2024-05-24 09:48:43 [INFO]: Epoch 057 - training loss: 0.2972, validation loss: 0.2655
2024-05-24 09:48:44 [INFO]: Epoch 058 - training loss: 0.2995, validation loss: 0.2659
2024-05-24 09:48:45 [INFO]: Epoch 059 - training loss: 0.2932, validation loss: 0.2647
2024-05-24 09:48:45 [INFO]: Epoch 060 - training loss: 0.2954, validation loss: 0.2638
2024-05-24 09:48:46 [INFO]: Epoch 061 - training loss: 0.2943, validation loss: 0.2581
2024-05-24 09:48:47 [INFO]: Epoch 062 - training loss: 0.2904, validation loss: 0.2592
2024-05-24 09:48:47 [INFO]: Epoch 063 - training loss: 0.2974, validation loss: 0.2603
2024-05-24 09:48:48 [INFO]: Epoch 064 - training loss: 0.2928, validation loss: 0.2606
2024-05-24 09:48:49 [INFO]: Epoch 065 - training loss: 0.2870, validation loss: 0.2589
2024-05-24 09:48:49 [INFO]: Epoch 066 - training loss: 0.2904, validation loss: 0.2595
2024-05-24 09:48:50 [INFO]: Epoch 067 - training loss: 0.2881, validation loss: 0.2595
2024-05-24 09:48:51 [INFO]: Epoch 068 - training loss: 0.2884, validation loss: 0.2584
2024-05-24 09:48:52 [INFO]: Epoch 069 - training loss: 0.2875, validation loss: 0.2556
2024-05-24 09:48:52 [INFO]: Epoch 070 - training loss: 0.2859, validation loss: 0.2581
2024-05-24 09:48:53 [INFO]: Epoch 071 - training loss: 0.2867, validation loss: 0.2523
2024-05-24 09:48:54 [INFO]: Epoch 072 - training loss: 0.2868, validation loss: 0.2537
2024-05-24 09:48:54 [INFO]: Epoch 073 - training loss: 0.2830, validation loss: 0.2520
2024-05-24 09:48:55 [INFO]: Epoch 074 - training loss: 0.2828, validation loss: 0.2589
2024-05-24 09:48:56 [INFO]: Epoch 075 - training loss: 0.2831, validation loss: 0.2546
2024-05-24 09:48:56 [INFO]: Epoch 076 - training loss: 0.2815, validation loss: 0.2543
2024-05-24 09:48:57 [INFO]: Epoch 077 - training loss: 0.2825, validation loss: 0.2492
2024-05-24 09:48:58 [INFO]: Epoch 078 - training loss: 0.2807, validation loss: 0.2479
2024-05-24 09:48:59 [INFO]: Epoch 079 - training loss: 0.2815, validation loss: 0.2493
2024-05-24 09:48:59 [INFO]: Epoch 080 - training loss: 0.2813, validation loss: 0.2477
2024-05-24 09:49:00 [INFO]: Epoch 081 - training loss: 0.2798, validation loss: 0.2497
2024-05-24 09:49:01 [INFO]: Epoch 082 - training loss: 0.2808, validation loss: 0.2515
2024-05-24 09:49:01 [INFO]: Epoch 083 - training loss: 0.2796, validation loss: 0.2495
2024-05-24 09:49:02 [INFO]: Epoch 084 - training loss: 0.2776, validation loss: 0.2467
2024-05-24 09:49:03 [INFO]: Epoch 085 - training loss: 0.2764, validation loss: 0.2487
2024-05-24 09:49:03 [INFO]: Epoch 086 - training loss: 0.2766, validation loss: 0.2419
2024-05-24 09:49:04 [INFO]: Epoch 087 - training loss: 0.2742, validation loss: 0.2471
2024-05-24 09:49:05 [INFO]: Epoch 088 - training loss: 0.2730, validation loss: 0.2404
2024-05-24 09:49:05 [INFO]: Epoch 089 - training loss: 0.2741, validation loss: 0.2428
2024-05-24 09:49:06 [INFO]: Epoch 090 - training loss: 0.2720, validation loss: 0.2441
2024-05-24 09:49:07 [INFO]: Epoch 091 - training loss: 0.2759, validation loss: 0.2467
2024-05-24 09:49:07 [INFO]: Epoch 092 - training loss: 0.2734, validation loss: 0.2485
2024-05-24 09:49:08 [INFO]: Epoch 093 - training loss: 0.2741, validation loss: 0.2420
2024-05-24 09:49:09 [INFO]: Epoch 094 - training loss: 0.2705, validation loss: 0.2433
2024-05-24 09:49:10 [INFO]: Epoch 095 - training loss: 0.2733, validation loss: 0.2412
2024-05-24 09:49:10 [INFO]: Epoch 096 - training loss: 0.2726, validation loss: 0.2446
2024-05-24 09:49:11 [INFO]: Epoch 097 - training loss: 0.2702, validation loss: 0.2392
2024-05-24 09:49:12 [INFO]: Epoch 098 - training loss: 0.2711, validation loss: 0.2388
2024-05-24 09:49:12 [INFO]: Epoch 099 - training loss: 0.2685, validation loss: 0.2411
2024-05-24 09:49:13 [INFO]: Epoch 100 - training loss: 0.2716, validation loss: 0.2355
2024-05-24 09:49:14 [INFO]: Epoch 101 - training loss: 0.2710, validation loss: 0.2341
2024-05-24 09:49:14 [INFO]: Epoch 102 - training loss: 0.2699, validation loss: 0.2375
2024-05-24 09:49:15 [INFO]: Epoch 103 - training loss: 0.2669, validation loss: 0.2290
2024-05-24 09:49:16 [INFO]: Epoch 104 - training loss: 0.2666, validation loss: 0.2349
2024-05-24 09:49:16 [INFO]: Epoch 105 - training loss: 0.2664, validation loss: 0.2362
2024-05-24 09:49:17 [INFO]: Epoch 106 - training loss: 0.2665, validation loss: 0.2352
2024-05-24 09:49:18 [INFO]: Epoch 107 - training loss: 0.2643, validation loss: 0.2317
2024-05-24 09:49:19 [INFO]: Epoch 108 - training loss: 0.2664, validation loss: 0.2287
2024-05-24 09:49:19 [INFO]: Epoch 109 - training loss: 0.2661, validation loss: 0.2380
2024-05-24 09:49:20 [INFO]: Epoch 110 - training loss: 0.2659, validation loss: 0.2288
2024-05-24 09:49:21 [INFO]: Epoch 111 - training loss: 0.2647, validation loss: 0.2275
2024-05-24 09:49:21 [INFO]: Epoch 112 - training loss: 0.2642, validation loss: 0.2314
2024-05-24 09:49:22 [INFO]: Epoch 113 - training loss: 0.2650, validation loss: 0.2278
2024-05-24 09:49:23 [INFO]: Epoch 114 - training loss: 0.2644, validation loss: 0.2348
2024-05-24 09:49:23 [INFO]: Epoch 115 - training loss: 0.2631, validation loss: 0.2317
2024-05-24 09:49:24 [INFO]: Epoch 116 - training loss: 0.2646, validation loss: 0.2316
2024-05-24 09:49:25 [INFO]: Epoch 117 - training loss: 0.2618, validation loss: 0.2300
2024-05-24 09:49:26 [INFO]: Epoch 118 - training loss: 0.2631, validation loss: 0.2316
2024-05-24 09:49:26 [INFO]: Epoch 119 - training loss: 0.2591, validation loss: 0.2303
2024-05-24 09:49:27 [INFO]: Epoch 120 - training loss: 0.2606, validation loss: 0.2324
2024-05-24 09:49:28 [INFO]: Epoch 121 - training loss: 0.2602, validation loss: 0.2314
2024-05-24 09:49:28 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-05-24 09:49:28 [INFO]: Finished training. The best model is from epoch#111.
2024-05-24 09:49:28 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_4/Transformer_physionet_2012_seta/20240524_T094803/Transformer.pypots
2024-05-24 09:49:28 [INFO]: Transformer on PhysioNet-2012-seta: MAE=0.2182, MSE=0.2307
2024-05-24 09:49:28 [INFO]: Successfully saved to overlay_minibatchmasking_saved_results/round_4/Transformer_physionet_2012_seta/imputation.pkl
2024-05-24 09:49:28 [INFO]: Using the given device: cuda:0
2024-05-24 09:49:28 [INFO]: Model files will be saved to overlay_minibatchmasking_saved_results/round_4/TimesNet_physionet_2012_seta/20240524_T094928
2024-05-24 09:49:28 [INFO]: Tensorboard file will be saved to overlay_minibatchmasking_saved_results/round_4/TimesNet_physionet_2012_seta/20240524_T094928/tensorboard
2024-05-24 09:49:28 [INFO]: TimesNet initialized with the given hyperparameters, the number of trainable parameters: 21,649,573
2024-05-24 09:49:29 [INFO]: Epoch 001 - training loss: 0.4304, validation loss: 0.7303
2024-05-24 09:49:30 [INFO]: Epoch 002 - training loss: 0.3631, validation loss: 0.8228
2024-05-24 09:49:30 [INFO]: Epoch 003 - training loss: 0.5642, validation loss: 0.7800
2024-05-24 09:49:31 [INFO]: Epoch 004 - training loss: 0.4873, validation loss: 0.3943
2024-05-24 09:49:32 [INFO]: Epoch 005 - training loss: 0.7385, validation loss: 0.3691
2024-05-24 09:49:33 [INFO]: Epoch 006 - training loss: 0.4826, validation loss: 0.3350
2024-05-24 09:49:34 [INFO]: Epoch 007 - training loss: 0.3728, validation loss: 0.3329
2024-05-24 09:49:34 [INFO]: Epoch 008 - training loss: 0.3785, validation loss: 0.3469
2024-05-24 09:49:35 [INFO]: Epoch 009 - training loss: 0.3591, validation loss: 0.3211
2024-05-24 09:49:36 [INFO]: Epoch 010 - training loss: 0.3140, validation loss: 0.3159
2024-05-24 09:49:37 [INFO]: Epoch 011 - training loss: 0.3415, validation loss: 0.3255
2024-05-24 09:49:38 [INFO]: Epoch 012 - training loss: 0.3573, validation loss: 0.3129
2024-05-24 09:49:38 [INFO]: Epoch 013 - training loss: 0.3474, validation loss: 0.3187
2024-05-24 09:49:39 [INFO]: Epoch 014 - training loss: 0.3217, validation loss: 0.3104
2024-05-24 09:49:40 [INFO]: Epoch 015 - training loss: 0.3238, validation loss: 0.3055
2024-05-24 09:49:41 [INFO]: Epoch 016 - training loss: 0.3226, validation loss: 0.3031
2024-05-24 09:49:42 [INFO]: Epoch 017 - training loss: 0.2996, validation loss: 0.2990
2024-05-24 09:49:42 [INFO]: Epoch 018 - training loss: 0.3154, validation loss: 0.2915
2024-05-24 09:49:43 [INFO]: Epoch 019 - training loss: 0.3191, validation loss: 0.2890
2024-05-24 09:49:44 [INFO]: Epoch 020 - training loss: 0.4105, validation loss: 0.2865
2024-05-24 09:49:45 [INFO]: Epoch 021 - training loss: 0.3213, validation loss: 0.2903
2024-05-24 09:49:46 [INFO]: Epoch 022 - training loss: 0.3073, validation loss: 0.2823
2024-05-24 09:49:46 [INFO]: Epoch 023 - training loss: 0.3301, validation loss: 0.2887
2024-05-24 09:49:47 [INFO]: Epoch 024 - training loss: 0.3966, validation loss: 0.3002
2024-05-24 09:49:48 [INFO]: Epoch 025 - training loss: 0.3044, validation loss: 0.2865
2024-05-24 09:49:49 [INFO]: Epoch 026 - training loss: 0.3063, validation loss: 0.2856
2024-05-24 09:49:50 [INFO]: Epoch 027 - training loss: 0.2992, validation loss: 0.2769
2024-05-24 09:49:50 [INFO]: Epoch 028 - training loss: 0.3123, validation loss: 0.2793
2024-05-24 09:49:51 [INFO]: Epoch 029 - training loss: 0.2931, validation loss: 0.2775
2024-05-24 09:49:52 [INFO]: Epoch 030 - training loss: 0.3126, validation loss: 0.2832
2024-05-24 09:49:53 [INFO]: Epoch 031 - training loss: 0.3184, validation loss: 0.2782
2024-05-24 09:49:54 [INFO]: Epoch 032 - training loss: 0.2934, validation loss: 0.2725
2024-05-24 09:49:54 [INFO]: Epoch 033 - training loss: 0.3666, validation loss: 0.2765
2024-05-24 09:49:55 [INFO]: Epoch 034 - training loss: 0.2810, validation loss: 0.2802
2024-05-24 09:49:56 [INFO]: Epoch 035 - training loss: 0.3103, validation loss: 0.2831
2024-05-24 09:49:57 [INFO]: Epoch 036 - training loss: 0.3009, validation loss: 0.2857
2024-05-24 09:49:58 [INFO]: Epoch 037 - training loss: 0.3051, validation loss: 0.2886
2024-05-24 09:49:58 [INFO]: Epoch 038 - training loss: 0.2962, validation loss: 0.2735
2024-05-24 09:49:59 [INFO]: Epoch 039 - training loss: 0.2911, validation loss: 0.2717
2024-05-24 09:50:00 [INFO]: Epoch 040 - training loss: 0.2847, validation loss: 0.2904
2024-05-24 09:50:01 [INFO]: Epoch 041 - training loss: 0.3896, validation loss: 0.2780
2024-05-24 09:50:02 [INFO]: Epoch 042 - training loss: 0.3560, validation loss: 0.2676
2024-05-24 09:50:02 [INFO]: Epoch 043 - training loss: 0.2754, validation loss: 0.2782
2024-05-24 09:50:03 [INFO]: Epoch 044 - training loss: 0.3029, validation loss: 0.2691
2024-05-24 09:50:04 [INFO]: Epoch 045 - training loss: 0.3144, validation loss: 0.2771
2024-05-24 09:50:05 [INFO]: Epoch 046 - training loss: 0.3010, validation loss: 0.2747
2024-05-24 09:50:06 [INFO]: Epoch 047 - training loss: 0.2874, validation loss: 0.2696
2024-05-24 09:50:06 [INFO]: Epoch 048 - training loss: 0.3035, validation loss: 0.2673
2024-05-24 09:50:07 [INFO]: Epoch 049 - training loss: 0.2815, validation loss: 0.2843
2024-05-24 09:50:08 [INFO]: Epoch 050 - training loss: 0.2763, validation loss: 0.2776
2024-05-24 09:50:09 [INFO]: Epoch 051 - training loss: 0.2942, validation loss: 0.2670
2024-05-24 09:50:10 [INFO]: Epoch 052 - training loss: 0.2708, validation loss: 0.2752
2024-05-24 09:50:10 [INFO]: Epoch 053 - training loss: 0.2839, validation loss: 0.2707
2024-05-24 09:50:11 [INFO]: Epoch 054 - training loss: 0.2582, validation loss: 0.2651
2024-05-24 09:50:12 [INFO]: Epoch 055 - training loss: 0.2698, validation loss: 0.2743
2024-05-24 09:50:13 [INFO]: Epoch 056 - training loss: 0.2951, validation loss: 0.2737
2024-05-24 09:50:14 [INFO]: Epoch 057 - training loss: 0.2777, validation loss: 0.2664
2024-05-24 09:50:14 [INFO]: Epoch 058 - training loss: 0.2690, validation loss: 0.2643
2024-05-24 09:50:15 [INFO]: Epoch 059 - training loss: 0.2824, validation loss: 0.2700
2024-05-24 09:50:16 [INFO]: Epoch 060 - training loss: 0.2724, validation loss: 0.2601
2024-05-24 09:50:17 [INFO]: Epoch 061 - training loss: 0.2929, validation loss: 0.2663
2024-05-24 09:50:18 [INFO]: Epoch 062 - training loss: 0.2912, validation loss: 0.2660
2024-05-24 09:50:18 [INFO]: Epoch 063 - training loss: 0.2823, validation loss: 0.2657
2024-05-24 09:50:19 [INFO]: Epoch 064 - training loss: 0.3018, validation loss: 0.2691
2024-05-24 09:50:20 [INFO]: Epoch 065 - training loss: 0.2861, validation loss: 0.2629
2024-05-24 09:50:21 [INFO]: Epoch 066 - training loss: 0.2775, validation loss: 0.2702
2024-05-24 09:50:22 [INFO]: Epoch 067 - training loss: 0.2601, validation loss: 0.2619
2024-05-24 09:50:22 [INFO]: Epoch 068 - training loss: 0.2845, validation loss: 0.2704
2024-05-24 09:50:23 [INFO]: Epoch 069 - training loss: 0.2877, validation loss: 0.2644
2024-05-24 09:50:24 [INFO]: Epoch 070 - training loss: 0.3000, validation loss: 0.2671
2024-05-24 09:50:24 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-05-24 09:50:24 [INFO]: Finished training. The best model is from epoch#60.
2024-05-24 09:50:24 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_4/TimesNet_physionet_2012_seta/20240524_T094928/TimesNet.pypots
2024-05-24 09:50:24 [INFO]: TimesNet on PhysioNet-2012-seta: MAE=0.2694, MSE=0.2368
2024-05-24 09:50:24 [INFO]: Successfully saved to overlay_minibatchmasking_saved_results/round_4/TimesNet_physionet_2012_seta/imputation.pkl
2024-05-24 09:50:24 [INFO]: Using the given device: cuda:0
2024-05-24 09:50:24 [INFO]: Model files will be saved to overlay_minibatchmasking_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T095024
2024-05-24 09:50:24 [INFO]: Tensorboard file will be saved to overlay_minibatchmasking_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T095024/tensorboard
2024-05-24 09:50:24 [INFO]: CSDI initialized with the given hyperparameters, the number of trainable parameters: 1,694,753
2024-05-24 09:51:08 [INFO]: Epoch 001 - training loss: 0.4134, validation loss: 0.3355
2024-05-24 09:51:08 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T095024/CSDI_epoch1_loss0.33551464825868604.pypots
2024-05-24 09:51:51 [INFO]: Epoch 002 - training loss: 0.3316, validation loss: 0.3115
2024-05-24 09:51:51 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T095024/CSDI_epoch2_loss0.3114656314253807.pypots
2024-05-24 09:52:35 [INFO]: Epoch 003 - training loss: 0.2974, validation loss: 0.2644
2024-05-24 09:52:35 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T095024/CSDI_epoch3_loss0.26441420391201975.pypots
2024-05-24 09:53:19 [INFO]: Epoch 004 - training loss: 0.2813, validation loss: 0.2366
2024-05-24 09:53:19 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T095024/CSDI_epoch4_loss0.23660203963518142.pypots
2024-05-24 09:54:03 [INFO]: Epoch 005 - training loss: 0.2613, validation loss: 0.2274
2024-05-24 09:54:03 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T095024/CSDI_epoch5_loss0.2274083115160465.pypots
2024-05-24 09:54:47 [INFO]: Epoch 006 - training loss: 0.2518, validation loss: 0.2163
2024-05-24 09:54:47 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T095024/CSDI_epoch6_loss0.21628958806395532.pypots
2024-05-24 09:55:31 [INFO]: Epoch 007 - training loss: 0.2560, validation loss: 0.2118
2024-05-24 09:55:31 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T095024/CSDI_epoch7_loss0.21175724714994432.pypots
2024-05-24 09:56:15 [INFO]: Epoch 008 - training loss: 0.2499, validation loss: 0.2117
2024-05-24 09:56:15 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T095024/CSDI_epoch8_loss0.21165816038846968.pypots
2024-05-24 09:56:59 [INFO]: Epoch 009 - training loss: 0.2302, validation loss: 0.2038
2024-05-24 09:56:59 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T095024/CSDI_epoch9_loss0.2037980079650879.pypots
2024-05-24 09:57:43 [INFO]: Epoch 010 - training loss: 0.2548, validation loss: 0.2043
2024-05-24 09:57:43 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T095024/CSDI_epoch10_loss0.20430971011519433.pypots
2024-05-24 09:58:27 [INFO]: Epoch 011 - training loss: 0.2434, validation loss: 0.2004
2024-05-24 09:58:27 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T095024/CSDI_epoch11_loss0.2003883294761181.pypots
2024-05-24 09:59:11 [INFO]: Epoch 012 - training loss: 0.2451, validation loss: 0.1974
2024-05-24 09:59:11 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T095024/CSDI_epoch12_loss0.19740428179502487.pypots
2024-05-24 09:59:55 [INFO]: Epoch 013 - training loss: 0.2509, validation loss: 0.1953
2024-05-24 09:59:55 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T095024/CSDI_epoch13_loss0.1953356109559536.pypots
2024-05-24 10:00:39 [INFO]: Epoch 014 - training loss: 0.2386, validation loss: 0.1976
2024-05-24 10:00:39 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T095024/CSDI_epoch14_loss0.1975661560893059.pypots
2024-05-24 10:01:23 [INFO]: Epoch 015 - training loss: 0.2279, validation loss: 0.1921
2024-05-24 10:01:23 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T095024/CSDI_epoch15_loss0.19207010567188262.pypots
2024-05-24 10:02:07 [INFO]: Epoch 016 - training loss: 0.2380, validation loss: 0.1912
2024-05-24 10:02:07 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T095024/CSDI_epoch16_loss0.1911787860095501.pypots
2024-05-24 10:02:50 [INFO]: Epoch 017 - training loss: 0.2267, validation loss: 0.1945
2024-05-24 10:02:50 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T095024/CSDI_epoch17_loss0.19453770741820336.pypots
2024-05-24 10:03:34 [INFO]: Epoch 018 - training loss: 0.2293, validation loss: 0.1925
2024-05-24 10:03:34 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T095024/CSDI_epoch18_loss0.19253319948911668.pypots
2024-05-24 10:04:18 [INFO]: Epoch 019 - training loss: 0.2191, validation loss: 0.1896
2024-05-24 10:04:18 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T095024/CSDI_epoch19_loss0.189634258300066.pypots
2024-05-24 10:05:02 [INFO]: Epoch 020 - training loss: 0.2333, validation loss: 0.1919
2024-05-24 10:05:02 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T095024/CSDI_epoch20_loss0.1918592259287834.pypots
2024-05-24 10:05:46 [INFO]: Epoch 021 - training loss: 0.2379, validation loss: 0.1898
2024-05-24 10:05:46 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T095024/CSDI_epoch21_loss0.18979217782616614.pypots
2024-05-24 10:06:30 [INFO]: Epoch 022 - training loss: 0.2264, validation loss: 0.1861
2024-05-24 10:06:30 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T095024/CSDI_epoch22_loss0.1861208662390709.pypots
2024-05-24 10:07:14 [INFO]: Epoch 023 - training loss: 0.2311, validation loss: 0.1896
2024-05-24 10:07:14 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T095024/CSDI_epoch23_loss0.18959970027208328.pypots
2024-05-24 10:07:58 [INFO]: Epoch 024 - training loss: 0.2260, validation loss: 0.1896
2024-05-24 10:07:58 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T095024/CSDI_epoch24_loss0.18964386135339736.pypots
2024-05-24 10:08:42 [INFO]: Epoch 025 - training loss: 0.2349, validation loss: 0.1853
2024-05-24 10:08:42 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T095024/CSDI_epoch25_loss0.18525809198617935.pypots
2024-05-24 10:09:26 [INFO]: Epoch 026 - training loss: 0.2277, validation loss: 0.1882
2024-05-24 10:09:26 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T095024/CSDI_epoch26_loss0.18823298215866088.pypots
2024-05-24 10:10:10 [INFO]: Epoch 027 - training loss: 0.2264, validation loss: 0.1886
2024-05-24 10:10:10 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T095024/CSDI_epoch27_loss0.18862037658691405.pypots
2024-05-24 10:10:54 [INFO]: Epoch 028 - training loss: 0.2367, validation loss: 0.1856
2024-05-24 10:10:54 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T095024/CSDI_epoch28_loss0.18556923791766167.pypots
2024-05-24 10:11:38 [INFO]: Epoch 029 - training loss: 0.2285, validation loss: 0.1866
2024-05-24 10:11:38 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T095024/CSDI_epoch29_loss0.18660920709371567.pypots
2024-05-24 10:12:22 [INFO]: Epoch 030 - training loss: 0.2171, validation loss: 0.1880
2024-05-24 10:12:22 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T095024/CSDI_epoch30_loss0.18800001218914986.pypots
2024-05-24 10:13:06 [INFO]: Epoch 031 - training loss: 0.2273, validation loss: 0.1844
2024-05-24 10:13:06 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T095024/CSDI_epoch31_loss0.18443817645311356.pypots
2024-05-24 10:13:50 [INFO]: Epoch 032 - training loss: 0.2212, validation loss: 0.1813
2024-05-24 10:13:50 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T095024/CSDI_epoch32_loss0.181299140304327.pypots
2024-05-24 10:14:34 [INFO]: Epoch 033 - training loss: 0.2277, validation loss: 0.1816
2024-05-24 10:14:34 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T095024/CSDI_epoch33_loss0.18160384073853492.pypots
2024-05-24 10:15:18 [INFO]: Epoch 034 - training loss: 0.2290, validation loss: 0.1808
2024-05-24 10:15:18 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T095024/CSDI_epoch34_loss0.18075604662299155.pypots
2024-05-24 10:16:02 [INFO]: Epoch 035 - training loss: 0.2256, validation loss: 0.1809
2024-05-24 10:16:02 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T095024/CSDI_epoch35_loss0.18085395097732543.pypots
2024-05-24 10:16:46 [INFO]: Epoch 036 - training loss: 0.2130, validation loss: 0.1810
2024-05-24 10:16:46 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T095024/CSDI_epoch36_loss0.1809792049229145.pypots
2024-05-24 10:17:30 [INFO]: Epoch 037 - training loss: 0.2255, validation loss: 0.1811
2024-05-24 10:17:30 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T095024/CSDI_epoch37_loss0.18110808059573175.pypots
2024-05-24 10:18:14 [INFO]: Epoch 038 - training loss: 0.2193, validation loss: 0.1811
2024-05-24 10:18:14 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T095024/CSDI_epoch38_loss0.18114344477653505.pypots
2024-05-24 10:18:58 [INFO]: Epoch 039 - training loss: 0.2258, validation loss: 0.1808
2024-05-24 10:18:58 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T095024/CSDI_epoch39_loss0.18079129680991174.pypots
2024-05-24 10:19:42 [INFO]: Epoch 040 - training loss: 0.2203, validation loss: 0.1814
2024-05-24 10:19:42 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T095024/CSDI_epoch40_loss0.18141645565629005.pypots
2024-05-24 10:20:26 [INFO]: Epoch 041 - training loss: 0.2233, validation loss: 0.1776
2024-05-24 10:20:26 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T095024/CSDI_epoch41_loss0.17759601399302483.pypots
2024-05-24 10:21:10 [INFO]: Epoch 042 - training loss: 0.2125, validation loss: 0.1806
2024-05-24 10:21:10 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T095024/CSDI_epoch42_loss0.18064206913113595.pypots
2024-05-24 10:21:54 [INFO]: Epoch 043 - training loss: 0.2198, validation loss: 0.1808
2024-05-24 10:21:54 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T095024/CSDI_epoch43_loss0.18079444766044617.pypots
2024-05-24 10:22:38 [INFO]: Epoch 044 - training loss: 0.2209, validation loss: 0.1798
2024-05-24 10:22:38 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T095024/CSDI_epoch44_loss0.17977015152573586.pypots
2024-05-24 10:23:22 [INFO]: Epoch 045 - training loss: 0.2209, validation loss: 0.1802
2024-05-24 10:23:22 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T095024/CSDI_epoch45_loss0.18018622025847436.pypots
2024-05-24 10:24:06 [INFO]: Epoch 046 - training loss: 0.2066, validation loss: 0.1809
2024-05-24 10:24:06 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T095024/CSDI_epoch46_loss0.1808516725897789.pypots
2024-05-24 10:24:50 [INFO]: Epoch 047 - training loss: 0.2079, validation loss: 0.1808
2024-05-24 10:24:50 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T095024/CSDI_epoch47_loss0.18081920892000197.pypots
2024-05-24 10:25:34 [INFO]: Epoch 048 - training loss: 0.2194, validation loss: 0.1781
2024-05-24 10:25:34 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T095024/CSDI_epoch48_loss0.17811098843812942.pypots
2024-05-24 10:26:17 [INFO]: Epoch 049 - training loss: 0.2303, validation loss: 0.1807
2024-05-24 10:26:17 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T095024/CSDI_epoch49_loss0.18070971891283988.pypots
2024-05-24 10:27:01 [INFO]: Epoch 050 - training loss: 0.2259, validation loss: 0.1782
2024-05-24 10:27:01 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T095024/CSDI_epoch50_loss0.17819325104355813.pypots
2024-05-24 10:27:45 [INFO]: Epoch 051 - training loss: 0.2149, validation loss: 0.1811
2024-05-24 10:27:45 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T095024/CSDI_epoch51_loss0.18111043572425842.pypots
2024-05-24 10:27:45 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-05-24 10:27:45 [INFO]: Finished training. The best model is from epoch#41.
2024-05-24 10:27:45 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T095024/CSDI.pypots
2024-05-24 10:35:06 [INFO]: CSDI on PhysioNet-2012-seta: MAE=0.2333, MSE=0.3019
2024-05-24 11:04:27 [INFO]: Successfully saved to overlay_minibatchmasking_saved_results/round_4/CSDI_physionet_2012_seta/imputation.pkl
2024-05-24 11:04:27 [INFO]: Using the given device: cuda:0
2024-05-24 11:04:27 [INFO]: Model files will be saved to overlay_minibatchmasking_saved_results/round_4/GPVAE_physionet_2012_seta/20240524_T110427
2024-05-24 11:04:27 [INFO]: Tensorboard file will be saved to overlay_minibatchmasking_saved_results/round_4/GPVAE_physionet_2012_seta/20240524_T110427/tensorboard
2024-05-24 11:04:27 [INFO]: GPVAE initialized with the given hyperparameters, the number of trainable parameters: 1,028,244
2024-05-24 11:04:28 [INFO]: Epoch 001 - training loss: 42062.1276, validation loss: 0.9444
2024-05-24 11:04:29 [INFO]: Epoch 002 - training loss: 24407.2325, validation loss: 0.7447
2024-05-24 11:04:29 [INFO]: Epoch 003 - training loss: 23512.7095, validation loss: 0.6974
2024-05-24 11:04:30 [INFO]: Epoch 004 - training loss: 23220.3264, validation loss: 0.6649
2024-05-24 11:04:30 [INFO]: Epoch 005 - training loss: 23078.5504, validation loss: 0.6677
2024-05-24 11:04:31 [INFO]: Epoch 006 - training loss: 22998.1972, validation loss: 0.6539
2024-05-24 11:04:31 [INFO]: Epoch 007 - training loss: 22950.4262, validation loss: 0.6501
2024-05-24 11:04:32 [INFO]: Epoch 008 - training loss: 22919.1375, validation loss: 0.6507
2024-05-24 11:04:32 [INFO]: Epoch 009 - training loss: 22897.8799, validation loss: 0.6522
2024-05-24 11:04:33 [INFO]: Epoch 010 - training loss: 22883.2903, validation loss: 0.6516
2024-05-24 11:04:34 [INFO]: Epoch 011 - training loss: 22872.3100, validation loss: 0.6450
2024-05-24 11:04:34 [INFO]: Epoch 012 - training loss: 22864.5771, validation loss: 0.6460
2024-05-24 11:04:35 [INFO]: Epoch 013 - training loss: 22858.7940, validation loss: 0.6581
2024-05-24 11:04:35 [INFO]: Epoch 014 - training loss: 22853.2043, validation loss: 0.6480
2024-05-24 11:04:36 [INFO]: Epoch 015 - training loss: 22849.1483, validation loss: 0.6486
2024-05-24 11:04:36 [INFO]: Epoch 016 - training loss: 22845.8864, validation loss: 0.6342
2024-05-24 11:04:37 [INFO]: Epoch 017 - training loss: 22844.0453, validation loss: 0.6388
2024-05-24 11:04:37 [INFO]: Epoch 018 - training loss: 22841.5815, validation loss: 0.6338
2024-05-24 11:04:38 [INFO]: Epoch 019 - training loss: 22838.6475, validation loss: 0.6525
2024-05-24 11:04:39 [INFO]: Epoch 020 - training loss: 22837.4874, validation loss: 0.6287
2024-05-24 11:04:39 [INFO]: Epoch 021 - training loss: 22834.6881, validation loss: 0.6217
2024-05-24 11:04:40 [INFO]: Epoch 022 - training loss: 22833.3209, validation loss: 0.6140
2024-05-24 11:04:40 [INFO]: Epoch 023 - training loss: 22832.0753, validation loss: 0.6134
2024-05-24 11:04:41 [INFO]: Epoch 024 - training loss: 22831.5730, validation loss: 0.6176
2024-05-24 11:04:41 [INFO]: Epoch 025 - training loss: 22830.2557, validation loss: 0.6179
2024-05-24 11:04:42 [INFO]: Epoch 026 - training loss: 22830.2611, validation loss: 0.6092
2024-05-24 11:04:42 [INFO]: Epoch 027 - training loss: 22828.4991, validation loss: 0.6162
2024-05-24 11:04:43 [INFO]: Epoch 028 - training loss: 22827.3143, validation loss: 0.6126
2024-05-24 11:04:44 [INFO]: Epoch 029 - training loss: 22828.0701, validation loss: 0.6060
2024-05-24 11:04:44 [INFO]: Epoch 030 - training loss: 22826.8011, validation loss: 0.6015
2024-05-24 11:04:45 [INFO]: Epoch 031 - training loss: 22825.9585, validation loss: 0.6028
2024-05-24 11:04:45 [INFO]: Epoch 032 - training loss: 22825.2277, validation loss: 0.6054
2024-05-24 11:04:46 [INFO]: Epoch 033 - training loss: 22824.9319, validation loss: 0.5976
2024-05-24 11:04:46 [INFO]: Epoch 034 - training loss: 22823.7825, validation loss: 0.5959
2024-05-24 11:04:47 [INFO]: Epoch 035 - training loss: 22822.1852, validation loss: 0.5868
2024-05-24 11:04:47 [INFO]: Epoch 036 - training loss: 22820.9171, validation loss: 0.5812
2024-05-24 11:04:48 [INFO]: Epoch 037 - training loss: 22819.4711, validation loss: 0.5806
2024-05-24 11:04:49 [INFO]: Epoch 038 - training loss: 22817.8326, validation loss: 0.5756
2024-05-24 11:04:49 [INFO]: Epoch 039 - training loss: 22816.2932, validation loss: 0.5762
2024-05-24 11:04:50 [INFO]: Epoch 040 - training loss: 22814.6597, validation loss: 0.5668
2024-05-24 11:04:50 [INFO]: Epoch 041 - training loss: 22812.7594, validation loss: 0.5566
2024-05-24 11:04:51 [INFO]: Epoch 042 - training loss: 22811.7671, validation loss: 0.5511
2024-05-24 11:04:51 [INFO]: Epoch 043 - training loss: 22811.2257, validation loss: 0.5529
2024-05-24 11:04:52 [INFO]: Epoch 044 - training loss: 22809.3747, validation loss: 0.5593
2024-05-24 11:04:52 [INFO]: Epoch 045 - training loss: 22808.9579, validation loss: 0.5431
2024-05-24 11:04:53 [INFO]: Epoch 046 - training loss: 22808.2431, validation loss: 0.5476
2024-05-24 11:04:53 [INFO]: Epoch 047 - training loss: 22806.7975, validation loss: 0.5463
2024-05-24 11:04:54 [INFO]: Epoch 048 - training loss: 22805.5627, validation loss: 0.5409
2024-05-24 11:04:55 [INFO]: Epoch 049 - training loss: 22804.4351, validation loss: 0.5361
2024-05-24 11:04:55 [INFO]: Epoch 050 - training loss: 22804.3225, validation loss: 0.5420
2024-05-24 11:04:56 [INFO]: Epoch 051 - training loss: 22803.1483, validation loss: 0.5312
2024-05-24 11:04:56 [INFO]: Epoch 052 - training loss: 22803.3255, validation loss: 0.5322
2024-05-24 11:04:57 [INFO]: Epoch 053 - training loss: 22802.2257, validation loss: 0.5346
2024-05-24 11:04:57 [INFO]: Epoch 054 - training loss: 22801.8365, validation loss: 0.5256
2024-05-24 11:04:58 [INFO]: Epoch 055 - training loss: 22801.7247, validation loss: 0.5322
2024-05-24 11:04:58 [INFO]: Epoch 056 - training loss: 22800.8268, validation loss: 0.5270
2024-05-24 11:04:59 [INFO]: Epoch 057 - training loss: 22801.1175, validation loss: 0.5241
2024-05-24 11:05:00 [INFO]: Epoch 058 - training loss: 22799.7189, validation loss: 0.5345
2024-05-24 11:05:00 [INFO]: Epoch 059 - training loss: 22799.0980, validation loss: 0.5362
2024-05-24 11:05:01 [INFO]: Epoch 060 - training loss: 22800.7435, validation loss: 0.5179
2024-05-24 11:05:01 [INFO]: Epoch 061 - training loss: 22801.0543, validation loss: 0.5296
2024-05-24 11:05:02 [INFO]: Epoch 062 - training loss: 22799.5517, validation loss: 0.5188
2024-05-24 11:05:02 [INFO]: Epoch 063 - training loss: 22798.4002, validation loss: 0.5194
2024-05-24 11:05:03 [INFO]: Epoch 064 - training loss: 22797.5298, validation loss: 0.5132
2024-05-24 11:05:03 [INFO]: Epoch 065 - training loss: 22797.3490, validation loss: 0.5206
2024-05-24 11:05:04 [INFO]: Epoch 066 - training loss: 22797.0318, validation loss: 0.5137
2024-05-24 11:05:05 [INFO]: Epoch 067 - training loss: 22796.3969, validation loss: 0.5456
2024-05-24 11:05:05 [INFO]: Epoch 068 - training loss: 22801.1121, validation loss: 0.5142
2024-05-24 11:05:06 [INFO]: Epoch 069 - training loss: 22796.9985, validation loss: 0.5152
2024-05-24 11:05:06 [INFO]: Epoch 070 - training loss: 22795.1724, validation loss: 0.5072
2024-05-24 11:05:07 [INFO]: Epoch 071 - training loss: 22794.6830, validation loss: 0.5164
2024-05-24 11:05:07 [INFO]: Epoch 072 - training loss: 22794.4567, validation loss: 0.5110
2024-05-24 11:05:08 [INFO]: Epoch 073 - training loss: 22794.1141, validation loss: 0.5124
2024-05-24 11:05:08 [INFO]: Epoch 074 - training loss: 22794.1504, validation loss: 0.5120
2024-05-24 11:05:09 [INFO]: Epoch 075 - training loss: 22794.1316, validation loss: 0.5149
2024-05-24 11:05:10 [INFO]: Epoch 076 - training loss: 22793.6130, validation loss: 0.5105
2024-05-24 11:05:10 [INFO]: Epoch 077 - training loss: 22793.0072, validation loss: 0.5143
2024-05-24 11:05:11 [INFO]: Epoch 078 - training loss: 22793.0020, validation loss: 0.5157
2024-05-24 11:05:11 [INFO]: Epoch 079 - training loss: 22792.9893, validation loss: 0.5076
2024-05-24 11:05:12 [INFO]: Epoch 080 - training loss: 22792.6516, validation loss: 0.5075
2024-05-24 11:05:12 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-05-24 11:05:12 [INFO]: Finished training. The best model is from epoch#70.
2024-05-24 11:05:12 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_4/GPVAE_physionet_2012_seta/20240524_T110427/GPVAE.pypots
2024-05-24 11:05:12 [INFO]: GP-VAE on PhysioNet-2012-seta: MAE=0.4271, MSE=0.4515
2024-05-24 11:05:12 [INFO]: Successfully saved to overlay_minibatchmasking_saved_results/round_4/GPVAE_physionet_2012_seta/imputation.pkl
2024-05-24 11:05:12 [INFO]: Using the given device: cuda:0
2024-05-24 11:05:12 [INFO]: Model files will be saved to overlay_minibatchmasking_saved_results/round_4/USGAN_physionet_2012_seta/20240524_T110512
2024-05-24 11:05:12 [INFO]: Tensorboard file will be saved to overlay_minibatchmasking_saved_results/round_4/USGAN_physionet_2012_seta/20240524_T110512/tensorboard
2024-05-24 11:05:12 [INFO]: USGAN initialized with the given hyperparameters, the number of trainable parameters: 16,010,261
2024-05-24 11:05:33 [INFO]: Epoch 001 - generator training loss: 0.5896, discriminator training loss: 0.3769, validation loss: 0.6214
2024-05-24 11:05:51 [INFO]: Epoch 002 - generator training loss: 0.4768, discriminator training loss: 0.2585, validation loss: 0.5285
2024-05-24 11:06:09 [INFO]: Epoch 003 - generator training loss: 0.4331, discriminator training loss: 0.2123, validation loss: 0.5065
2024-05-24 11:06:27 [INFO]: Epoch 004 - generator training loss: 0.4502, discriminator training loss: 0.1585, validation loss: 0.4878
2024-05-24 11:06:46 [INFO]: Epoch 005 - generator training loss: 0.4472, discriminator training loss: 0.1266, validation loss: 0.4721
2024-05-24 11:07:04 [INFO]: Epoch 006 - generator training loss: 0.4312, discriminator training loss: 0.1069, validation loss: 0.4568
2024-05-24 11:07:22 [INFO]: Epoch 007 - generator training loss: 0.4205, discriminator training loss: 0.0920, validation loss: 0.4443
2024-05-24 11:07:40 [INFO]: Epoch 008 - generator training loss: 0.4122, discriminator training loss: 0.0816, validation loss: 0.4346
2024-05-24 11:07:58 [INFO]: Epoch 009 - generator training loss: 0.3996, discriminator training loss: 0.0737, validation loss: 0.4243
2024-05-24 11:08:16 [INFO]: Epoch 010 - generator training loss: 0.3915, discriminator training loss: 0.0673, validation loss: 0.4187
2024-05-24 11:08:34 [INFO]: Epoch 011 - generator training loss: 0.3872, discriminator training loss: 0.0619, validation loss: 0.4069
2024-05-24 11:08:52 [INFO]: Epoch 012 - generator training loss: 0.3805, discriminator training loss: 0.0571, validation loss: 0.4056
2024-05-24 11:09:10 [INFO]: Epoch 013 - generator training loss: 0.3737, discriminator training loss: 0.0532, validation loss: 0.3964
2024-05-24 11:09:28 [INFO]: Epoch 014 - generator training loss: 0.3692, discriminator training loss: 0.0497, validation loss: 0.3907
2024-05-24 11:09:46 [INFO]: Epoch 015 - generator training loss: 0.3645, discriminator training loss: 0.0468, validation loss: 0.3864
2024-05-24 11:10:04 [INFO]: Epoch 016 - generator training loss: 0.3582, discriminator training loss: 0.0443, validation loss: 0.3827
2024-05-24 11:10:23 [INFO]: Epoch 017 - generator training loss: 0.3535, discriminator training loss: 0.0423, validation loss: 0.3751
2024-05-24 11:10:41 [INFO]: Epoch 018 - generator training loss: 0.3483, discriminator training loss: 0.0404, validation loss: 0.3712
2024-05-24 11:10:59 [INFO]: Epoch 019 - generator training loss: 0.3428, discriminator training loss: 0.0387, validation loss: 0.3659
2024-05-24 11:11:17 [INFO]: Epoch 020 - generator training loss: 0.3391, discriminator training loss: 0.0373, validation loss: 0.3608
2024-05-24 11:11:35 [INFO]: Epoch 021 - generator training loss: 0.3335, discriminator training loss: 0.0361, validation loss: 0.3551
2024-05-24 11:11:53 [INFO]: Epoch 022 - generator training loss: 0.3296, discriminator training loss: 0.0349, validation loss: 0.3542
2024-05-24 11:12:11 [INFO]: Epoch 023 - generator training loss: 0.3232, discriminator training loss: 0.0341, validation loss: 0.3506
2024-05-24 11:12:29 [INFO]: Epoch 024 - generator training loss: 0.3196, discriminator training loss: 0.0332, validation loss: 0.3426
2024-05-24 11:12:47 [INFO]: Epoch 025 - generator training loss: 0.3146, discriminator training loss: 0.0324, validation loss: 0.3486
2024-05-24 11:13:06 [INFO]: Epoch 026 - generator training loss: 0.3109, discriminator training loss: 0.0318, validation loss: 0.3456
2024-05-24 11:13:24 [INFO]: Epoch 027 - generator training loss: 0.3109, discriminator training loss: 0.0314, validation loss: 0.3390
2024-05-24 11:13:42 [INFO]: Epoch 028 - generator training loss: 0.3052, discriminator training loss: 0.0306, validation loss: 0.3350
2024-05-24 11:14:00 [INFO]: Epoch 029 - generator training loss: 0.2990, discriminator training loss: 0.0301, validation loss: 0.3331
2024-05-24 11:14:18 [INFO]: Epoch 030 - generator training loss: 0.2946, discriminator training loss: 0.0295, validation loss: 0.3288
2024-05-24 11:14:36 [INFO]: Epoch 031 - generator training loss: 0.2966, discriminator training loss: 0.0291, validation loss: 0.3386
2024-05-24 11:14:54 [INFO]: Epoch 032 - generator training loss: 0.2996, discriminator training loss: 0.0287, validation loss: 0.3319
2024-05-24 11:15:12 [INFO]: Epoch 033 - generator training loss: 0.2854, discriminator training loss: 0.0282, validation loss: 0.3288
2024-05-24 11:15:31 [INFO]: Epoch 034 - generator training loss: 0.2833, discriminator training loss: 0.0280, validation loss: 0.3236
2024-05-24 11:15:49 [INFO]: Epoch 035 - generator training loss: 0.2781, discriminator training loss: 0.0275, validation loss: 0.3196
2024-05-24 11:16:07 [INFO]: Epoch 036 - generator training loss: 0.2725, discriminator training loss: 0.0272, validation loss: 0.3206
2024-05-24 11:16:25 [INFO]: Epoch 037 - generator training loss: 0.2697, discriminator training loss: 0.0269, validation loss: 0.3245
2024-05-24 11:16:43 [INFO]: Epoch 038 - generator training loss: 0.2685, discriminator training loss: 0.0265, validation loss: 0.3170
2024-05-24 11:17:01 [INFO]: Epoch 039 - generator training loss: 0.2658, discriminator training loss: 0.0262, validation loss: 0.3162
2024-05-24 11:17:19 [INFO]: Epoch 040 - generator training loss: 0.2609, discriminator training loss: 0.0261, validation loss: 0.3173
2024-05-24 11:17:38 [INFO]: Epoch 041 - generator training loss: 0.2639, discriminator training loss: 0.0257, validation loss: 0.3184
2024-05-24 11:17:56 [INFO]: Epoch 042 - generator training loss: 0.2591, discriminator training loss: 0.0257, validation loss: 0.3109
2024-05-24 11:18:14 [INFO]: Epoch 043 - generator training loss: 0.2526, discriminator training loss: 0.0252, validation loss: 0.3134
2024-05-24 11:18:32 [INFO]: Epoch 044 - generator training loss: 0.2557, discriminator training loss: 0.0252, validation loss: 0.3092
2024-05-24 11:18:50 [INFO]: Epoch 045 - generator training loss: 0.2530, discriminator training loss: 0.0249, validation loss: 0.3122
2024-05-24 11:19:08 [INFO]: Epoch 046 - generator training loss: 0.2499, discriminator training loss: 0.0247, validation loss: 0.3127
2024-05-24 11:19:26 [INFO]: Epoch 047 - generator training loss: 0.2492, discriminator training loss: 0.0243, validation loss: 0.3052
2024-05-24 11:19:44 [INFO]: Epoch 048 - generator training loss: 0.2411, discriminator training loss: 0.0242, validation loss: 0.3055
2024-05-24 11:20:03 [INFO]: Epoch 049 - generator training loss: 0.2370, discriminator training loss: 0.0239, validation loss: 0.3106
2024-05-24 11:20:21 [INFO]: Epoch 050 - generator training loss: 0.2354, discriminator training loss: 0.0238, validation loss: 0.3020
2024-05-24 11:20:39 [INFO]: Epoch 051 - generator training loss: 0.2361, discriminator training loss: 0.0237, validation loss: 0.3093
2024-05-24 11:20:57 [INFO]: Epoch 052 - generator training loss: 0.2321, discriminator training loss: 0.0236, validation loss: 0.3136
2024-05-24 11:21:15 [INFO]: Epoch 053 - generator training loss: 0.2349, discriminator training loss: 0.0234, validation loss: 0.3070
2024-05-24 11:21:33 [INFO]: Epoch 054 - generator training loss: 0.2313, discriminator training loss: 0.0233, validation loss: 0.3024
2024-05-24 11:21:51 [INFO]: Epoch 055 - generator training loss: 0.2295, discriminator training loss: 0.0232, validation loss: 0.3052
2024-05-24 11:22:09 [INFO]: Epoch 056 - generator training loss: 0.2297, discriminator training loss: 0.0231, validation loss: 0.3039
2024-05-24 11:22:28 [INFO]: Epoch 057 - generator training loss: 0.2258, discriminator training loss: 0.0231, validation loss: 0.3040
2024-05-24 11:22:46 [INFO]: Epoch 058 - generator training loss: 0.2217, discriminator training loss: 0.0228, validation loss: 0.3057
2024-05-24 11:23:04 [INFO]: Epoch 059 - generator training loss: 0.2186, discriminator training loss: 0.0229, validation loss: 0.3058
2024-05-24 11:23:22 [INFO]: Epoch 060 - generator training loss: 0.2192, discriminator training loss: 0.0229, validation loss: 0.3072
2024-05-24 11:23:22 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-05-24 11:23:22 [INFO]: Finished training. The best model is from epoch#50.
2024-05-24 11:23:22 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_4/USGAN_physionet_2012_seta/20240524_T110512/USGAN.pypots
2024-05-24 11:23:25 [INFO]: US-GAN on PhysioNet-2012-seta: MAE=0.2967, MSE=0.2617
2024-05-24 11:23:34 [INFO]: Successfully saved to overlay_minibatchmasking_saved_results/round_4/USGAN_physionet_2012_seta/imputation.pkl
2024-05-24 11:23:34 [INFO]: Using the given device: cuda:0
2024-05-24 11:23:34 [INFO]: Model files will be saved to overlay_minibatchmasking_saved_results/round_4/BRITS_physionet_2012_seta/20240524_T112334
2024-05-24 11:23:34 [INFO]: Tensorboard file will be saved to overlay_minibatchmasking_saved_results/round_4/BRITS_physionet_2012_seta/20240524_T112334/tensorboard
2024-05-24 11:23:34 [INFO]: BRITS initialized with the given hyperparameters, the number of trainable parameters: 9,176,048
2024-05-24 11:23:49 [INFO]: Epoch 001 - training loss: 1.1337, validation loss: 0.5320
2024-05-24 11:24:01 [INFO]: Epoch 002 - training loss: 0.9175, validation loss: 0.4737
2024-05-24 11:24:13 [INFO]: Epoch 003 - training loss: 0.8537, validation loss: 0.4475
2024-05-24 11:24:24 [INFO]: Epoch 004 - training loss: 0.8150, validation loss: 0.4295
2024-05-24 11:24:36 [INFO]: Epoch 005 - training loss: 0.7864, validation loss: 0.4116
2024-05-24 11:24:48 [INFO]: Epoch 006 - training loss: 0.7616, validation loss: 0.3958
2024-05-24 11:25:00 [INFO]: Epoch 007 - training loss: 0.7405, validation loss: 0.3824
2024-05-24 11:25:12 [INFO]: Epoch 008 - training loss: 0.7234, validation loss: 0.3728
2024-05-24 11:25:24 [INFO]: Epoch 009 - training loss: 0.7081, validation loss: 0.3670
2024-05-24 11:25:35 [INFO]: Epoch 010 - training loss: 0.6954, validation loss: 0.3626
2024-05-24 11:25:47 [INFO]: Epoch 011 - training loss: 0.6845, validation loss: 0.3566
2024-05-24 11:25:59 [INFO]: Epoch 012 - training loss: 0.6753, validation loss: 0.3559
2024-05-24 11:26:11 [INFO]: Epoch 013 - training loss: 0.6679, validation loss: 0.3532
2024-05-24 11:26:23 [INFO]: Epoch 014 - training loss: 0.6614, validation loss: 0.3523
2024-05-24 11:26:35 [INFO]: Epoch 015 - training loss: 0.6541, validation loss: 0.3516
2024-05-24 11:26:47 [INFO]: Epoch 016 - training loss: 0.6487, validation loss: 0.3489
2024-05-24 11:26:58 [INFO]: Epoch 017 - training loss: 0.6432, validation loss: 0.3489
2024-05-24 11:27:10 [INFO]: Epoch 018 - training loss: 0.6394, validation loss: 0.3486
2024-05-24 11:27:22 [INFO]: Epoch 019 - training loss: 0.6347, validation loss: 0.3447
2024-05-24 11:27:34 [INFO]: Epoch 020 - training loss: 0.6307, validation loss: 0.3445
2024-05-24 11:27:46 [INFO]: Epoch 021 - training loss: 0.6280, validation loss: 0.3441
2024-05-24 11:27:58 [INFO]: Epoch 022 - training loss: 0.6242, validation loss: 0.3439
2024-05-24 11:28:09 [INFO]: Epoch 023 - training loss: 0.6208, validation loss: 0.3432
2024-05-24 11:28:21 [INFO]: Epoch 024 - training loss: 0.6173, validation loss: 0.3429
2024-05-24 11:28:33 [INFO]: Epoch 025 - training loss: 0.6148, validation loss: 0.3422
2024-05-24 11:28:45 [INFO]: Epoch 026 - training loss: 0.6112, validation loss: 0.3413
2024-05-24 11:28:57 [INFO]: Epoch 027 - training loss: 0.6091, validation loss: 0.3393
2024-05-24 11:29:09 [INFO]: Epoch 028 - training loss: 0.6060, validation loss: 0.3400
2024-05-24 11:29:20 [INFO]: Epoch 029 - training loss: 0.6019, validation loss: 0.3415
2024-05-24 11:29:32 [INFO]: Epoch 030 - training loss: 0.5996, validation loss: 0.3414
2024-05-24 11:29:44 [INFO]: Epoch 031 - training loss: 0.5966, validation loss: 0.3394
2024-05-24 11:29:56 [INFO]: Epoch 032 - training loss: 0.5943, validation loss: 0.3424
2024-05-24 11:30:08 [INFO]: Epoch 033 - training loss: 0.5940, validation loss: 0.3414
2024-05-24 11:30:20 [INFO]: Epoch 034 - training loss: 0.5883, validation loss: 0.3410
2024-05-24 11:30:31 [INFO]: Epoch 035 - training loss: 0.5849, validation loss: 0.3396
2024-05-24 11:30:43 [INFO]: Epoch 036 - training loss: 0.5823, validation loss: 0.3424
2024-05-24 11:30:55 [INFO]: Epoch 037 - training loss: 0.5795, validation loss: 0.3415
2024-05-24 11:30:55 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-05-24 11:30:55 [INFO]: Finished training. The best model is from epoch#27.
2024-05-24 11:30:55 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_4/BRITS_physionet_2012_seta/20240524_T112334/BRITS.pypots
2024-05-24 11:30:58 [INFO]: BRITS on PhysioNet-2012-seta: MAE=0.2536, MSE=0.2643
2024-05-24 11:31:07 [INFO]: Successfully saved to overlay_minibatchmasking_saved_results/round_4/BRITS_physionet_2012_seta/imputation.pkl
2024-05-24 11:31:07 [INFO]: Using the given device: cuda:0
2024-05-24 11:31:07 [INFO]: Model files will be saved to overlay_minibatchmasking_saved_results/round_4/MRNN_physionet_2012_seta/20240524_T113107
2024-05-24 11:31:07 [INFO]: Tensorboard file will be saved to overlay_minibatchmasking_saved_results/round_4/MRNN_physionet_2012_seta/20240524_T113107/tensorboard
2024-05-24 11:31:07 [INFO]: MRNN initialized with the given hyperparameters, the number of trainable parameters: 7,599
2024-05-24 11:31:13 [INFO]: Epoch 001 - training loss: 1.3607, validation loss: 0.9984
2024-05-24 11:31:13 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_4/MRNN_physionet_2012_seta/20240524_T113107/MRNN_epoch1_loss0.9983770906925201.pypots
2024-05-24 11:31:16 [INFO]: Epoch 002 - training loss: 0.8699, validation loss: 0.9623
2024-05-24 11:31:16 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_4/MRNN_physionet_2012_seta/20240524_T113107/MRNN_epoch2_loss0.9622642904520035.pypots
2024-05-24 11:31:18 [INFO]: Epoch 003 - training loss: 0.6620, validation loss: 0.9396
2024-05-24 11:31:18 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_4/MRNN_physionet_2012_seta/20240524_T113107/MRNN_epoch3_loss0.9396450966596603.pypots
2024-05-24 11:31:21 [INFO]: Epoch 004 - training loss: 0.6015, validation loss: 0.9285
2024-05-24 11:31:21 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_4/MRNN_physionet_2012_seta/20240524_T113107/MRNN_epoch4_loss0.9285027384757996.pypots
2024-05-24 11:31:24 [INFO]: Epoch 005 - training loss: 0.5749, validation loss: 0.9223
2024-05-24 11:31:24 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_4/MRNN_physionet_2012_seta/20240524_T113107/MRNN_epoch5_loss0.9223271101713181.pypots
2024-05-24 11:31:27 [INFO]: Epoch 006 - training loss: 0.5574, validation loss: 0.9196
2024-05-24 11:31:27 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_4/MRNN_physionet_2012_seta/20240524_T113107/MRNN_epoch6_loss0.9196393430233002.pypots
2024-05-24 11:31:29 [INFO]: Epoch 007 - training loss: 0.5396, validation loss: 0.9177
2024-05-24 11:31:29 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_4/MRNN_physionet_2012_seta/20240524_T113107/MRNN_epoch7_loss0.9177250385284423.pypots
2024-05-24 11:31:32 [INFO]: Epoch 008 - training loss: 0.5179, validation loss: 0.9161
2024-05-24 11:31:32 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_4/MRNN_physionet_2012_seta/20240524_T113107/MRNN_epoch8_loss0.9161371558904647.pypots
2024-05-24 11:31:35 [INFO]: Epoch 009 - training loss: 0.5078, validation loss: 0.9155
2024-05-24 11:31:35 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_4/MRNN_physionet_2012_seta/20240524_T113107/MRNN_epoch9_loss0.9155122101306915.pypots
2024-05-24 11:31:38 [INFO]: Epoch 010 - training loss: 0.5049, validation loss: 0.9144
2024-05-24 11:31:38 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_4/MRNN_physionet_2012_seta/20240524_T113107/MRNN_epoch10_loss0.9144293993711472.pypots
2024-05-24 11:31:40 [INFO]: Epoch 011 - training loss: 0.4959, validation loss: 0.9140
2024-05-24 11:31:40 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_4/MRNN_physionet_2012_seta/20240524_T113107/MRNN_epoch11_loss0.9140135943889618.pypots
2024-05-24 11:31:43 [INFO]: Epoch 012 - training loss: 0.4931, validation loss: 0.9154
2024-05-24 11:31:43 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_4/MRNN_physionet_2012_seta/20240524_T113107/MRNN_epoch12_loss0.9154074370861054.pypots
2024-05-24 11:31:46 [INFO]: Epoch 013 - training loss: 0.4808, validation loss: 0.9153
2024-05-24 11:31:46 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_4/MRNN_physionet_2012_seta/20240524_T113107/MRNN_epoch13_loss0.9152986347675324.pypots
2024-05-24 11:31:49 [INFO]: Epoch 014 - training loss: 0.4776, validation loss: 0.9165
2024-05-24 11:31:49 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_4/MRNN_physionet_2012_seta/20240524_T113107/MRNN_epoch14_loss0.9164985388517379.pypots
2024-05-24 11:31:51 [INFO]: Epoch 015 - training loss: 0.4706, validation loss: 0.9175
2024-05-24 11:31:51 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_4/MRNN_physionet_2012_seta/20240524_T113107/MRNN_epoch15_loss0.9175295025110245.pypots
2024-05-24 11:31:54 [INFO]: Epoch 016 - training loss: 0.4763, validation loss: 0.9194
2024-05-24 11:31:54 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_4/MRNN_physionet_2012_seta/20240524_T113107/MRNN_epoch16_loss0.919402664899826.pypots
2024-05-24 11:31:57 [INFO]: Epoch 017 - training loss: 0.4685, validation loss: 0.9214
2024-05-24 11:31:57 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_4/MRNN_physionet_2012_seta/20240524_T113107/MRNN_epoch17_loss0.9214135497808457.pypots
2024-05-24 11:32:00 [INFO]: Epoch 018 - training loss: 0.4583, validation loss: 0.9203
2024-05-24 11:32:00 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_4/MRNN_physionet_2012_seta/20240524_T113107/MRNN_epoch18_loss0.9203349351882935.pypots
2024-05-24 11:32:02 [INFO]: Epoch 019 - training loss: 0.4579, validation loss: 0.9214
2024-05-24 11:32:02 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_4/MRNN_physionet_2012_seta/20240524_T113107/MRNN_epoch19_loss0.9213759869337081.pypots
2024-05-24 11:32:05 [INFO]: Epoch 020 - training loss: 0.4566, validation loss: 0.9223
2024-05-24 11:32:05 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_4/MRNN_physionet_2012_seta/20240524_T113107/MRNN_epoch20_loss0.922274187207222.pypots
2024-05-24 11:32:08 [INFO]: Epoch 021 - training loss: 0.4482, validation loss: 0.9225
2024-05-24 11:32:08 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_4/MRNN_physionet_2012_seta/20240524_T113107/MRNN_epoch21_loss0.9225037395954132.pypots
2024-05-24 11:32:08 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-05-24 11:32:08 [INFO]: Finished training. The best model is from epoch#11.
2024-05-24 11:32:08 [INFO]: Saved the model to overlay_minibatchmasking_saved_results/round_4/MRNN_physionet_2012_seta/20240524_T113107/MRNN.pypots
2024-05-24 11:32:09 [INFO]: MRNN on PhysioNet-2012-seta: MAE=0.6820, MSE=0.9044
2024-05-24 11:32:13 [INFO]: Successfully saved to overlay_minibatchmasking_saved_results/round_4/MRNN_physionet_2012_seta/imputation.pkl
2024-05-24 11:32:13 [INFO]: Using the given device: cpu
2024-05-24 11:32:13 [INFO]: LOCF on PhysioNet-2012-seta: MAE=0.4109, MSE=0.5324
2024-05-24 11:32:13 [INFO]: Successfully created the given path "saved_results/round_4/LOCF_physionet_2012_seta".
2024-05-24 11:32:13 [INFO]: Successfully saved to overlay_minibatchmasking_saved_results/round_4/LOCF_physionet_2012_seta/imputation.pkl
2024-05-24 11:32:13 [INFO]: Median on PhysioNet-2012-seta: MAE=0.6868, MSE=1.0189
2024-05-24 11:32:13 [INFO]: Successfully created the given path "saved_results/round_4/Median_physionet_2012_seta".
2024-05-24 11:32:13 [INFO]: Successfully saved to overlay_minibatchmasking_saved_results/round_4/Median_physionet_2012_seta/imputation.pkl
2024-05-24 11:32:13 [INFO]: Mean on PhysioNet-2012-seta: MAE=0.7045, MSE=0.9898
2024-05-24 11:32:13 [INFO]: Successfully created the given path "saved_results/round_4/Mean_physionet_2012_seta".
2024-05-24 11:32:13 [INFO]: Successfully saved to overlay_minibatchmasking_saved_results/round_4/Mean_physionet_2012_seta/imputation.pkl
2024-05-24 11:32:13 [INFO]: 
SAITS on data/physionet_2012_seta: MAE=0.206±0.001809716042788206, MSE=0.227±0.00513624612487657
Transformer on data/physionet_2012_seta: MAE=0.219±0.003916998530521094, MSE=0.222±0.007457725665920664
TimesNet on data/physionet_2012_seta: MAE=0.273±0.010729223555613877, MSE=0.242±0.01803544963960059
CSDI on data/physionet_2012_seta: MAE=0.226±0.010016863998060305, MSE=0.279±0.051210486567591376
GPVAE on data/physionet_2012_seta: MAE=0.427±0.006272790205368261, MSE=0.453±0.007953025746931925
USGAN on data/physionet_2012_seta: MAE=0.295±0.0018741627637134685, MSE=0.261±0.007282356280488275
BRITS on data/physionet_2012_seta: MAE=0.254±0.0011896162018239181, MSE=0.265±0.0012707656050688374
MRNN on data/physionet_2012_seta: MAE=0.682±0.00041743453682351387, MSE=0.905±0.0010551924970197244
LOCF on data/physionet_2012_seta: MAE=0.411±0.0, MSE=0.532±0.0
Median on data/physionet_2012_seta: MAE=0.687±0.0, MSE=1.019±0.0
Mean on data/physionet_2012_seta: MAE=0.705±0.0, MSE=0.990±1.1102230246251565e-16

