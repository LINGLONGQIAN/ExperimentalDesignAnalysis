2024-05-23 17:31:39 [INFO]: Have set the random seed as 2023 for numpy and pytorch.
2024-05-23 17:31:40 [INFO]: Using the given device: cuda:0
2024-05-23 17:31:40 [INFO]: Model files will be saved to overlay_premask_post_norm_saved_results/round_0/SAITS_physionet_2012_seta/20240523_T173140
2024-05-23 17:31:40 [INFO]: Tensorboard file will be saved to overlay_premask_post_norm_saved_results/round_0/SAITS_physionet_2012_seta/20240523_T173140/tensorboard
2024-05-23 17:31:41 [INFO]: SAITS initialized with the given hyperparameters, the number of trainable parameters: 5,322,806
2024-05-23 17:31:42 [INFO]: Epoch 001 - training loss: 1.0356, validation loss: 0.4687
2024-05-23 17:31:44 [INFO]: Epoch 002 - training loss: 0.6650, validation loss: 0.4090
2024-05-23 17:31:45 [INFO]: Epoch 003 - training loss: 0.5559, validation loss: 0.3794
2024-05-23 17:31:47 [INFO]: Epoch 004 - training loss: 0.5046, validation loss: 0.3701
2024-05-23 17:31:48 [INFO]: Epoch 005 - training loss: 0.4582, validation loss: 0.3592
2024-05-23 17:31:50 [INFO]: Epoch 006 - training loss: 0.4357, validation loss: 0.3493
2024-05-23 17:31:51 [INFO]: Epoch 007 - training loss: 0.4124, validation loss: 0.3347
2024-05-23 17:31:52 [INFO]: Epoch 008 - training loss: 0.3831, validation loss: 0.3319
2024-05-23 17:31:53 [INFO]: Epoch 009 - training loss: 0.3676, validation loss: 0.3309
2024-05-23 17:31:54 [INFO]: Epoch 010 - training loss: 0.3597, validation loss: 0.3218
2024-05-23 17:31:55 [INFO]: Epoch 011 - training loss: 0.3327, validation loss: 0.3127
2024-05-23 17:31:56 [INFO]: Epoch 012 - training loss: 0.3241, validation loss: 0.3038
2024-05-23 17:31:57 [INFO]: Epoch 013 - training loss: 0.3059, validation loss: 0.3092
2024-05-23 17:31:59 [INFO]: Epoch 014 - training loss: 0.2932, validation loss: 0.3010
2024-05-23 17:32:00 [INFO]: Epoch 015 - training loss: 0.2899, validation loss: 0.3064
2024-05-23 17:32:01 [INFO]: Epoch 016 - training loss: 0.2772, validation loss: 0.3053
2024-05-23 17:32:02 [INFO]: Epoch 017 - training loss: 0.2630, validation loss: 0.3039
2024-05-23 17:32:03 [INFO]: Epoch 018 - training loss: 0.2543, validation loss: 0.2980
2024-05-23 17:32:04 [INFO]: Epoch 019 - training loss: 0.2485, validation loss: 0.3012
2024-05-23 17:32:05 [INFO]: Epoch 020 - training loss: 0.2507, validation loss: 0.2984
2024-05-23 17:32:06 [INFO]: Epoch 021 - training loss: 0.2406, validation loss: 0.2943
2024-05-23 17:32:08 [INFO]: Epoch 022 - training loss: 0.2273, validation loss: 0.2948
2024-05-23 17:32:09 [INFO]: Epoch 023 - training loss: 0.2262, validation loss: 0.2926
2024-05-23 17:32:10 [INFO]: Epoch 024 - training loss: 0.2180, validation loss: 0.2927
2024-05-23 17:32:11 [INFO]: Epoch 025 - training loss: 0.2097, validation loss: 0.2944
2024-05-23 17:32:12 [INFO]: Epoch 026 - training loss: 0.2068, validation loss: 0.2939
2024-05-23 17:32:13 [INFO]: Epoch 027 - training loss: 0.2030, validation loss: 0.2927
2024-05-23 17:32:14 [INFO]: Epoch 028 - training loss: 0.1991, validation loss: 0.2911
2024-05-23 17:32:16 [INFO]: Epoch 029 - training loss: 0.1949, validation loss: 0.2921
2024-05-23 17:32:17 [INFO]: Epoch 030 - training loss: 0.1937, validation loss: 0.2950
2024-05-23 17:32:18 [INFO]: Epoch 031 - training loss: 0.1885, validation loss: 0.2899
2024-05-23 17:32:19 [INFO]: Epoch 032 - training loss: 0.1848, validation loss: 0.2909
2024-05-23 17:32:20 [INFO]: Epoch 033 - training loss: 0.1825, validation loss: 0.2933
2024-05-23 17:32:21 [INFO]: Epoch 034 - training loss: 0.1786, validation loss: 0.2909
2024-05-23 17:32:22 [INFO]: Epoch 035 - training loss: 0.1776, validation loss: 0.2901
2024-05-23 17:32:23 [INFO]: Epoch 036 - training loss: 0.1728, validation loss: 0.2873
2024-05-23 17:32:25 [INFO]: Epoch 037 - training loss: 0.1717, validation loss: 0.2914
2024-05-23 17:32:26 [INFO]: Epoch 038 - training loss: 0.1664, validation loss: 0.2903
2024-05-23 17:32:27 [INFO]: Epoch 039 - training loss: 0.1630, validation loss: 0.2905
2024-05-23 17:32:28 [INFO]: Epoch 040 - training loss: 0.1618, validation loss: 0.2847
2024-05-23 17:32:29 [INFO]: Epoch 041 - training loss: 0.1597, validation loss: 0.2878
2024-05-23 17:32:30 [INFO]: Epoch 042 - training loss: 0.1618, validation loss: 0.2851
2024-05-23 17:32:31 [INFO]: Epoch 043 - training loss: 0.1590, validation loss: 0.2874
2024-05-23 17:32:32 [INFO]: Epoch 044 - training loss: 0.1583, validation loss: 0.2838
2024-05-23 17:32:34 [INFO]: Epoch 045 - training loss: 0.1553, validation loss: 0.2878
2024-05-23 17:32:35 [INFO]: Epoch 046 - training loss: 0.1534, validation loss: 0.2879
2024-05-23 17:32:36 [INFO]: Epoch 047 - training loss: 0.1507, validation loss: 0.2849
2024-05-23 17:32:37 [INFO]: Epoch 048 - training loss: 0.1486, validation loss: 0.2870
2024-05-23 17:32:38 [INFO]: Epoch 049 - training loss: 0.1444, validation loss: 0.2871
2024-05-23 17:32:39 [INFO]: Epoch 050 - training loss: 0.1466, validation loss: 0.2917
2024-05-23 17:32:41 [INFO]: Epoch 051 - training loss: 0.1456, validation loss: 0.2866
2024-05-23 17:32:42 [INFO]: Epoch 052 - training loss: 0.1427, validation loss: 0.2837
2024-05-23 17:32:43 [INFO]: Epoch 053 - training loss: 0.1389, validation loss: 0.2903
2024-05-23 17:32:44 [INFO]: Epoch 054 - training loss: 0.1371, validation loss: 0.2871
2024-05-23 17:32:45 [INFO]: Epoch 055 - training loss: 0.1387, validation loss: 0.2854
2024-05-23 17:32:46 [INFO]: Epoch 056 - training loss: 0.1364, validation loss: 0.2835
2024-05-23 17:32:48 [INFO]: Epoch 057 - training loss: 0.1363, validation loss: 0.2878
2024-05-23 17:32:49 [INFO]: Epoch 058 - training loss: 0.1349, validation loss: 0.2862
2024-05-23 17:32:50 [INFO]: Epoch 059 - training loss: 0.1339, validation loss: 0.2820
2024-05-23 17:32:51 [INFO]: Epoch 060 - training loss: 0.1317, validation loss: 0.2839
2024-05-23 17:32:52 [INFO]: Epoch 061 - training loss: 0.1301, validation loss: 0.2863
2024-05-23 17:32:53 [INFO]: Epoch 062 - training loss: 0.1299, validation loss: 0.2897
2024-05-23 17:32:54 [INFO]: Epoch 063 - training loss: 0.1296, validation loss: 0.2869
2024-05-23 17:32:55 [INFO]: Epoch 064 - training loss: 0.1302, validation loss: 0.2867
2024-05-23 17:32:57 [INFO]: Epoch 065 - training loss: 0.1292, validation loss: 0.2844
2024-05-23 17:32:58 [INFO]: Epoch 066 - training loss: 0.1285, validation loss: 0.2854
2024-05-23 17:32:59 [INFO]: Epoch 067 - training loss: 0.1262, validation loss: 0.2820
2024-05-23 17:33:00 [INFO]: Epoch 068 - training loss: 0.1239, validation loss: 0.2835
2024-05-23 17:33:01 [INFO]: Epoch 069 - training loss: 0.1248, validation loss: 0.2857
2024-05-23 17:33:02 [INFO]: Epoch 070 - training loss: 0.1225, validation loss: 0.2850
2024-05-23 17:33:03 [INFO]: Epoch 071 - training loss: 0.1238, validation loss: 0.2821
2024-05-23 17:33:05 [INFO]: Epoch 072 - training loss: 0.1229, validation loss: 0.2817
2024-05-23 17:33:06 [INFO]: Epoch 073 - training loss: 0.1227, validation loss: 0.2881
2024-05-23 17:33:07 [INFO]: Epoch 074 - training loss: 0.1211, validation loss: 0.2852
2024-05-23 17:33:08 [INFO]: Epoch 075 - training loss: 0.1172, validation loss: 0.2806
2024-05-23 17:33:10 [INFO]: Epoch 076 - training loss: 0.1177, validation loss: 0.2858
2024-05-23 17:33:11 [INFO]: Epoch 077 - training loss: 0.1199, validation loss: 0.2876
2024-05-23 17:33:12 [INFO]: Epoch 078 - training loss: 0.1172, validation loss: 0.2828
2024-05-23 17:33:13 [INFO]: Epoch 079 - training loss: 0.1152, validation loss: 0.2870
2024-05-23 17:33:14 [INFO]: Epoch 080 - training loss: 0.1180, validation loss: 0.2836
2024-05-23 17:33:15 [INFO]: Epoch 081 - training loss: 0.1163, validation loss: 0.2853
2024-05-23 17:33:17 [INFO]: Epoch 082 - training loss: 0.1119, validation loss: 0.2817
2024-05-23 17:33:18 [INFO]: Epoch 083 - training loss: 0.1116, validation loss: 0.2824
2024-05-23 17:33:19 [INFO]: Epoch 084 - training loss: 0.1130, validation loss: 0.2836
2024-05-23 17:33:21 [INFO]: Epoch 085 - training loss: 0.1133, validation loss: 0.2821
2024-05-23 17:33:21 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-05-23 17:33:21 [INFO]: Finished training. The best model is from epoch#75.
2024-05-23 17:33:21 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_0/SAITS_physionet_2012_seta/20240523_T173140/SAITS.pypots
2024-05-23 17:33:21 [INFO]: SAITS on PhysioNet-2012-seta: MAE=0.2623, MSE=0.3191
2024-05-23 17:33:21 [INFO]: Successfully saved to overlay_premask_post_norm_saved_results/round_0/SAITS_physionet_2012_seta/imputation.pkl
2024-05-23 17:33:21 [INFO]: Using the given device: cuda:0
2024-05-23 17:33:21 [INFO]: Model files will be saved to overlay_premask_post_norm_saved_results/round_0/Transformer_physionet_2012_seta/20240523_T173321
2024-05-23 17:33:21 [INFO]: Tensorboard file will be saved to overlay_premask_post_norm_saved_results/round_0/Transformer_physionet_2012_seta/20240523_T173321/tensorboard
2024-05-23 17:33:21 [INFO]: Transformer initialized with the given hyperparameters, the number of trainable parameters: 2,152,613
2024-05-23 17:33:22 [INFO]: Epoch 001 - training loss: 1.1701, validation loss: 0.5437
2024-05-23 17:33:23 [INFO]: Epoch 002 - training loss: 0.7427, validation loss: 0.4612
2024-05-23 17:33:23 [INFO]: Epoch 003 - training loss: 0.6375, validation loss: 0.4448
2024-05-23 17:33:24 [INFO]: Epoch 004 - training loss: 0.5821, validation loss: 0.4205
2024-05-23 17:33:24 [INFO]: Epoch 005 - training loss: 0.5440, validation loss: 0.4245
2024-05-23 17:33:25 [INFO]: Epoch 006 - training loss: 0.5209, validation loss: 0.3970
2024-05-23 17:33:25 [INFO]: Epoch 007 - training loss: 0.4815, validation loss: 0.3809
2024-05-23 17:33:26 [INFO]: Epoch 008 - training loss: 0.4589, validation loss: 0.3694
2024-05-23 17:33:27 [INFO]: Epoch 009 - training loss: 0.4456, validation loss: 0.3649
2024-05-23 17:33:27 [INFO]: Epoch 010 - training loss: 0.4276, validation loss: 0.3610
2024-05-23 17:33:28 [INFO]: Epoch 011 - training loss: 0.4095, validation loss: 0.3537
2024-05-23 17:33:28 [INFO]: Epoch 012 - training loss: 0.3946, validation loss: 0.3453
2024-05-23 17:33:29 [INFO]: Epoch 013 - training loss: 0.3829, validation loss: 0.3497
2024-05-23 17:33:30 [INFO]: Epoch 014 - training loss: 0.3763, validation loss: 0.3370
2024-05-23 17:33:30 [INFO]: Epoch 015 - training loss: 0.3666, validation loss: 0.3430
2024-05-23 17:33:31 [INFO]: Epoch 016 - training loss: 0.3548, validation loss: 0.3304
2024-05-23 17:33:31 [INFO]: Epoch 017 - training loss: 0.3532, validation loss: 0.3358
2024-05-23 17:33:32 [INFO]: Epoch 018 - training loss: 0.3414, validation loss: 0.3252
2024-05-23 17:33:33 [INFO]: Epoch 019 - training loss: 0.3308, validation loss: 0.3214
2024-05-23 17:33:33 [INFO]: Epoch 020 - training loss: 0.3200, validation loss: 0.3189
2024-05-23 17:33:34 [INFO]: Epoch 021 - training loss: 0.3136, validation loss: 0.3171
2024-05-23 17:33:35 [INFO]: Epoch 022 - training loss: 0.3083, validation loss: 0.3150
2024-05-23 17:33:36 [INFO]: Epoch 023 - training loss: 0.3053, validation loss: 0.3141
2024-05-23 17:33:36 [INFO]: Epoch 024 - training loss: 0.2958, validation loss: 0.3128
2024-05-23 17:33:37 [INFO]: Epoch 025 - training loss: 0.2924, validation loss: 0.3144
2024-05-23 17:33:37 [INFO]: Epoch 026 - training loss: 0.2902, validation loss: 0.3127
2024-05-23 17:33:38 [INFO]: Epoch 027 - training loss: 0.2878, validation loss: 0.3164
2024-05-23 17:33:39 [INFO]: Epoch 028 - training loss: 0.2766, validation loss: 0.3103
2024-05-23 17:33:39 [INFO]: Epoch 029 - training loss: 0.2743, validation loss: 0.3130
2024-05-23 17:33:40 [INFO]: Epoch 030 - training loss: 0.2750, validation loss: 0.3106
2024-05-23 17:33:41 [INFO]: Epoch 031 - training loss: 0.2619, validation loss: 0.3104
2024-05-23 17:33:42 [INFO]: Epoch 032 - training loss: 0.2609, validation loss: 0.3081
2024-05-23 17:33:42 [INFO]: Epoch 033 - training loss: 0.2608, validation loss: 0.3138
2024-05-23 17:33:43 [INFO]: Epoch 034 - training loss: 0.2534, validation loss: 0.3097
2024-05-23 17:33:43 [INFO]: Epoch 035 - training loss: 0.2502, validation loss: 0.3119
2024-05-23 17:33:44 [INFO]: Epoch 036 - training loss: 0.2463, validation loss: 0.3071
2024-05-23 17:33:45 [INFO]: Epoch 037 - training loss: 0.2412, validation loss: 0.3089
2024-05-23 17:33:45 [INFO]: Epoch 038 - training loss: 0.2420, validation loss: 0.3110
2024-05-23 17:33:46 [INFO]: Epoch 039 - training loss: 0.2335, validation loss: 0.3092
2024-05-23 17:33:46 [INFO]: Epoch 040 - training loss: 0.2313, validation loss: 0.3106
2024-05-23 17:33:47 [INFO]: Epoch 041 - training loss: 0.2309, validation loss: 0.3166
2024-05-23 17:33:47 [INFO]: Epoch 042 - training loss: 0.2304, validation loss: 0.3082
2024-05-23 17:33:48 [INFO]: Epoch 043 - training loss: 0.2250, validation loss: 0.3101
2024-05-23 17:33:49 [INFO]: Epoch 044 - training loss: 0.2217, validation loss: 0.3135
2024-05-23 17:33:49 [INFO]: Epoch 045 - training loss: 0.2183, validation loss: 0.3087
2024-05-23 17:33:50 [INFO]: Epoch 046 - training loss: 0.2157, validation loss: 0.3139
2024-05-23 17:33:50 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-05-23 17:33:50 [INFO]: Finished training. The best model is from epoch#36.
2024-05-23 17:33:50 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_0/Transformer_physionet_2012_seta/20240523_T173321/Transformer.pypots
2024-05-23 17:33:50 [INFO]: Transformer on PhysioNet-2012-seta: MAE=0.2886, MSE=0.3333
2024-05-23 17:33:50 [INFO]: Successfully saved to overlay_premask_post_norm_saved_results/round_0/Transformer_physionet_2012_seta/imputation.pkl
2024-05-23 17:33:50 [INFO]: Using the given device: cuda:0
2024-05-23 17:33:50 [INFO]: Model files will be saved to overlay_premask_post_norm_saved_results/round_0/TimesNet_physionet_2012_seta/20240523_T173350
2024-05-23 17:33:50 [INFO]: Tensorboard file will be saved to overlay_premask_post_norm_saved_results/round_0/TimesNet_physionet_2012_seta/20240523_T173350/tensorboard
2024-05-23 17:33:51 [INFO]: TimesNet initialized with the given hyperparameters, the number of trainable parameters: 21,649,573
2024-05-23 17:33:52 [INFO]: Epoch 001 - training loss: 0.4184, validation loss: 0.3380
2024-05-23 17:33:52 [INFO]: Epoch 002 - training loss: 0.6444, validation loss: 0.3650
2024-05-23 17:33:53 [INFO]: Epoch 003 - training loss: 0.3622, validation loss: 0.3746
2024-05-23 17:33:54 [INFO]: Epoch 004 - training loss: 0.4518, validation loss: 0.3055
2024-05-23 17:33:55 [INFO]: Epoch 005 - training loss: 0.3229, validation loss: 0.2960
2024-05-23 17:33:55 [INFO]: Epoch 006 - training loss: 0.2979, validation loss: 0.2935
2024-05-23 17:33:56 [INFO]: Epoch 007 - training loss: 0.2897, validation loss: 0.2929
2024-05-23 17:33:57 [INFO]: Epoch 008 - training loss: 0.2820, validation loss: 0.2862
2024-05-23 17:33:57 [INFO]: Epoch 009 - training loss: 0.2707, validation loss: 0.2979
2024-05-23 17:33:58 [INFO]: Epoch 010 - training loss: 0.2642, validation loss: 0.2947
2024-05-23 17:33:59 [INFO]: Epoch 011 - training loss: 0.2572, validation loss: 0.2938
2024-05-23 17:34:00 [INFO]: Epoch 012 - training loss: 0.2578, validation loss: 0.2932
2024-05-23 17:34:00 [INFO]: Epoch 013 - training loss: 0.2480, validation loss: 0.2907
2024-05-23 17:34:01 [INFO]: Epoch 014 - training loss: 0.2558, validation loss: 0.2927
2024-05-23 17:34:02 [INFO]: Epoch 015 - training loss: 0.2404, validation loss: 0.2932
2024-05-23 17:34:02 [INFO]: Epoch 016 - training loss: 0.2403, validation loss: 0.2948
2024-05-23 17:34:03 [INFO]: Epoch 017 - training loss: 0.2331, validation loss: 0.2910
2024-05-23 17:34:04 [INFO]: Epoch 018 - training loss: 0.2308, validation loss: 0.2970
2024-05-23 17:34:04 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-05-23 17:34:04 [INFO]: Finished training. The best model is from epoch#8.
2024-05-23 17:34:04 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_0/TimesNet_physionet_2012_seta/20240523_T173350/TimesNet.pypots
2024-05-23 17:34:04 [INFO]: TimesNet on PhysioNet-2012-seta: MAE=0.2901, MSE=0.2814
2024-05-23 17:34:04 [INFO]: Successfully saved to overlay_premask_post_norm_saved_results/round_0/TimesNet_physionet_2012_seta/imputation.pkl
2024-05-23 17:34:04 [INFO]: Using the given device: cuda:0
2024-05-23 17:34:04 [INFO]: Model files will be saved to overlay_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173404
2024-05-23 17:34:04 [INFO]: Tensorboard file will be saved to overlay_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173404/tensorboard
2024-05-23 17:34:04 [INFO]: CSDI initialized with the given hyperparameters, the number of trainable parameters: 1,694,753
2024-05-23 17:34:48 [INFO]: Epoch 001 - training loss: 0.4300, validation loss: 0.3395
2024-05-23 17:34:48 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173404/CSDI_epoch1_loss0.3395084604620934.pypots
2024-05-23 17:35:31 [INFO]: Epoch 002 - training loss: 0.3163, validation loss: 0.3048
2024-05-23 17:35:31 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173404/CSDI_epoch2_loss0.3047512426972389.pypots
2024-05-23 17:36:15 [INFO]: Epoch 003 - training loss: 0.3018, validation loss: 0.2775
2024-05-23 17:36:15 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173404/CSDI_epoch3_loss0.27746802717447283.pypots
2024-05-23 17:36:58 [INFO]: Epoch 004 - training loss: 0.2717, validation loss: 0.2710
2024-05-23 17:36:58 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173404/CSDI_epoch4_loss0.2710274413228035.pypots
2024-05-23 17:37:42 [INFO]: Epoch 005 - training loss: 0.2783, validation loss: 0.2506
2024-05-23 17:37:42 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173404/CSDI_epoch5_loss0.25063023045659066.pypots
2024-05-23 17:38:26 [INFO]: Epoch 006 - training loss: 0.2634, validation loss: 0.2428
2024-05-23 17:38:26 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173404/CSDI_epoch6_loss0.24277870431542398.pypots
2024-05-23 17:39:09 [INFO]: Epoch 007 - training loss: 0.2409, validation loss: 0.2305
2024-05-23 17:39:09 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173404/CSDI_epoch7_loss0.23053623959422112.pypots
2024-05-23 17:39:53 [INFO]: Epoch 008 - training loss: 0.2450, validation loss: 0.2303
2024-05-23 17:39:53 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173404/CSDI_epoch8_loss0.230337655544281.pypots
2024-05-23 17:40:36 [INFO]: Epoch 009 - training loss: 0.2382, validation loss: 0.2220
2024-05-23 17:40:36 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173404/CSDI_epoch9_loss0.22200497910380362.pypots
2024-05-23 17:41:20 [INFO]: Epoch 010 - training loss: 0.2241, validation loss: 0.2239
2024-05-23 17:41:20 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173404/CSDI_epoch10_loss0.22386833280324936.pypots
2024-05-23 17:42:04 [INFO]: Epoch 011 - training loss: 0.2304, validation loss: 0.2159
2024-05-23 17:42:04 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173404/CSDI_epoch11_loss0.21587566360831262.pypots
2024-05-23 17:42:47 [INFO]: Epoch 012 - training loss: 0.2241, validation loss: 0.2130
2024-05-23 17:42:47 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173404/CSDI_epoch12_loss0.2130481295287609.pypots
2024-05-23 17:43:31 [INFO]: Epoch 013 - training loss: 0.2161, validation loss: 0.2133
2024-05-23 17:43:31 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173404/CSDI_epoch13_loss0.21328290551900864.pypots
2024-05-23 17:44:14 [INFO]: Epoch 014 - training loss: 0.2308, validation loss: 0.2073
2024-05-23 17:44:14 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173404/CSDI_epoch14_loss0.20734104961156846.pypots
2024-05-23 17:44:58 [INFO]: Epoch 015 - training loss: 0.2160, validation loss: 0.2072
2024-05-23 17:44:58 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173404/CSDI_epoch15_loss0.20719544291496278.pypots
2024-05-23 17:45:42 [INFO]: Epoch 016 - training loss: 0.2133, validation loss: 0.2093
2024-05-23 17:45:42 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173404/CSDI_epoch16_loss0.20927442461252213.pypots
2024-05-23 17:46:25 [INFO]: Epoch 017 - training loss: 0.1989, validation loss: 0.2018
2024-05-23 17:46:25 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173404/CSDI_epoch17_loss0.20180017054080962.pypots
2024-05-23 17:47:09 [INFO]: Epoch 018 - training loss: 0.2121, validation loss: 0.2045
2024-05-23 17:47:09 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173404/CSDI_epoch18_loss0.20450561717152596.pypots
2024-05-23 17:47:53 [INFO]: Epoch 019 - training loss: 0.2066, validation loss: 0.2036
2024-05-23 17:47:53 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173404/CSDI_epoch19_loss0.20363868698477744.pypots
2024-05-23 17:48:36 [INFO]: Epoch 020 - training loss: 0.2152, validation loss: 0.1980
2024-05-23 17:48:36 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173404/CSDI_epoch20_loss0.19797231033444404.pypots
2024-05-23 17:49:20 [INFO]: Epoch 021 - training loss: 0.2148, validation loss: 0.1983
2024-05-23 17:49:20 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173404/CSDI_epoch21_loss0.19830882102251052.pypots
2024-05-23 17:50:03 [INFO]: Epoch 022 - training loss: 0.2000, validation loss: 0.1993
2024-05-23 17:50:03 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173404/CSDI_epoch22_loss0.19931376054883004.pypots
2024-05-23 17:50:47 [INFO]: Epoch 023 - training loss: 0.2090, validation loss: 0.2003
2024-05-23 17:50:47 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173404/CSDI_epoch23_loss0.20027294382452965.pypots
2024-05-23 17:51:31 [INFO]: Epoch 024 - training loss: 0.2053, validation loss: 0.2011
2024-05-23 17:51:31 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173404/CSDI_epoch24_loss0.20113002136349678.pypots
2024-05-23 17:52:14 [INFO]: Epoch 025 - training loss: 0.2159, validation loss: 0.1978
2024-05-23 17:52:14 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173404/CSDI_epoch25_loss0.19779670462012292.pypots
2024-05-23 17:52:58 [INFO]: Epoch 026 - training loss: 0.2113, validation loss: 0.2033
2024-05-23 17:52:58 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173404/CSDI_epoch26_loss0.20332837998867034.pypots
2024-05-23 17:53:42 [INFO]: Epoch 027 - training loss: 0.2029, validation loss: 0.1987
2024-05-23 17:53:42 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173404/CSDI_epoch27_loss0.19867042154073716.pypots
2024-05-23 17:54:25 [INFO]: Epoch 028 - training loss: 0.2063, validation loss: 0.1988
2024-05-23 17:54:25 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173404/CSDI_epoch28_loss0.19883178621530534.pypots
2024-05-23 17:55:09 [INFO]: Epoch 029 - training loss: 0.2210, validation loss: 0.1970
2024-05-23 17:55:09 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173404/CSDI_epoch29_loss0.19702113494277002.pypots
2024-05-23 17:55:53 [INFO]: Epoch 030 - training loss: 0.2091, validation loss: 0.1971
2024-05-23 17:55:53 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173404/CSDI_epoch30_loss0.1970949277281761.pypots
2024-05-23 17:56:36 [INFO]: Epoch 031 - training loss: 0.2041, validation loss: 0.1941
2024-05-23 17:56:36 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173404/CSDI_epoch31_loss0.1941005803644657.pypots
2024-05-23 17:57:20 [INFO]: Epoch 032 - training loss: 0.2072, validation loss: 0.1947
2024-05-23 17:57:20 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173404/CSDI_epoch32_loss0.19465693831443787.pypots
2024-05-23 17:58:04 [INFO]: Epoch 033 - training loss: 0.1975, validation loss: 0.1945
2024-05-23 17:58:04 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173404/CSDI_epoch33_loss0.1944561094045639.pypots
2024-05-23 17:58:47 [INFO]: Epoch 034 - training loss: 0.2040, validation loss: 0.1932
2024-05-23 17:58:47 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173404/CSDI_epoch34_loss0.1931771792471409.pypots
2024-05-23 17:59:31 [INFO]: Epoch 035 - training loss: 0.2111, validation loss: 0.1948
2024-05-23 17:59:31 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173404/CSDI_epoch35_loss0.1948312394320965.pypots
2024-05-23 18:00:15 [INFO]: Epoch 036 - training loss: 0.1963, validation loss: 0.1912
2024-05-23 18:00:15 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173404/CSDI_epoch36_loss0.19124839305877686.pypots
2024-05-23 18:00:58 [INFO]: Epoch 037 - training loss: 0.2003, validation loss: 0.1897
2024-05-23 18:00:58 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173404/CSDI_epoch37_loss0.18966824188828468.pypots
2024-05-23 18:01:42 [INFO]: Epoch 038 - training loss: 0.2035, validation loss: 0.1938
2024-05-23 18:01:42 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173404/CSDI_epoch38_loss0.19376282766461372.pypots
2024-05-23 18:02:26 [INFO]: Epoch 039 - training loss: 0.2014, validation loss: 0.1908
2024-05-23 18:02:26 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173404/CSDI_epoch39_loss0.1908281348645687.pypots
2024-05-23 18:03:09 [INFO]: Epoch 040 - training loss: 0.1901, validation loss: 0.1886
2024-05-23 18:03:09 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173404/CSDI_epoch40_loss0.1886402927339077.pypots
2024-05-23 18:03:53 [INFO]: Epoch 041 - training loss: 0.1982, validation loss: 0.1917
2024-05-23 18:03:53 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173404/CSDI_epoch41_loss0.19169582650065423.pypots
2024-05-23 18:04:37 [INFO]: Epoch 042 - training loss: 0.1917, validation loss: 0.1891
2024-05-23 18:04:37 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173404/CSDI_epoch42_loss0.18906639963388444.pypots
2024-05-23 18:05:20 [INFO]: Epoch 043 - training loss: 0.2027, validation loss: 0.1937
2024-05-23 18:05:20 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173404/CSDI_epoch43_loss0.19372003376483918.pypots
2024-05-23 18:06:04 [INFO]: Epoch 044 - training loss: 0.1956, validation loss: 0.1915
2024-05-23 18:06:04 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173404/CSDI_epoch44_loss0.1914525657892227.pypots
2024-05-23 18:06:48 [INFO]: Epoch 045 - training loss: 0.1965, validation loss: 0.1882
2024-05-23 18:06:48 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173404/CSDI_epoch45_loss0.18820852637290955.pypots
2024-05-23 18:07:32 [INFO]: Epoch 046 - training loss: 0.1994, validation loss: 0.1911
2024-05-23 18:07:32 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173404/CSDI_epoch46_loss0.19105010703206063.pypots
2024-05-23 18:08:15 [INFO]: Epoch 047 - training loss: 0.1942, validation loss: 0.1899
2024-05-23 18:08:16 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173404/CSDI_epoch47_loss0.18987088575959205.pypots
2024-05-23 18:08:59 [INFO]: Epoch 048 - training loss: 0.1991, validation loss: 0.1888
2024-05-23 18:08:59 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173404/CSDI_epoch48_loss0.1887548789381981.pypots
2024-05-23 18:09:43 [INFO]: Epoch 049 - training loss: 0.1909, validation loss: 0.1903
2024-05-23 18:09:43 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173404/CSDI_epoch49_loss0.19027357101440429.pypots
2024-05-23 18:10:26 [INFO]: Epoch 050 - training loss: 0.2037, validation loss: 0.1933
2024-05-23 18:10:26 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173404/CSDI_epoch50_loss0.19331317469477655.pypots
2024-05-23 18:11:10 [INFO]: Epoch 051 - training loss: 0.1923, validation loss: 0.1925
2024-05-23 18:11:10 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173404/CSDI_epoch51_loss0.19249491170048713.pypots
2024-05-23 18:11:54 [INFO]: Epoch 052 - training loss: 0.1938, validation loss: 0.1888
2024-05-23 18:11:54 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173404/CSDI_epoch52_loss0.1887923449277878.pypots
2024-05-23 18:12:37 [INFO]: Epoch 053 - training loss: 0.1967, validation loss: 0.1879
2024-05-23 18:12:37 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173404/CSDI_epoch53_loss0.1878874532878399.pypots
2024-05-23 18:13:21 [INFO]: Epoch 054 - training loss: 0.1915, validation loss: 0.1898
2024-05-23 18:13:21 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173404/CSDI_epoch54_loss0.18984383419156076.pypots
2024-05-23 18:14:05 [INFO]: Epoch 055 - training loss: 0.1843, validation loss: 0.1866
2024-05-23 18:14:05 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173404/CSDI_epoch55_loss0.18660317212343216.pypots
2024-05-23 18:14:48 [INFO]: Epoch 056 - training loss: 0.1891, validation loss: 0.1875
2024-05-23 18:14:49 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173404/CSDI_epoch56_loss0.18750276789069176.pypots
2024-05-23 18:15:32 [INFO]: Epoch 057 - training loss: 0.1979, validation loss: 0.1894
2024-05-23 18:15:32 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173404/CSDI_epoch57_loss0.18940931782126427.pypots
2024-05-23 18:16:16 [INFO]: Epoch 058 - training loss: 0.1962, validation loss: 0.1871
2024-05-23 18:16:16 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173404/CSDI_epoch58_loss0.18705014735460282.pypots
2024-05-23 18:17:00 [INFO]: Epoch 059 - training loss: 0.1978, validation loss: 0.1859
2024-05-23 18:17:00 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173404/CSDI_epoch59_loss0.18593529835343361.pypots
2024-05-23 18:17:44 [INFO]: Epoch 060 - training loss: 0.1951, validation loss: 0.1853
2024-05-23 18:17:44 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173404/CSDI_epoch60_loss0.18527972921729088.pypots
2024-05-23 18:18:27 [INFO]: Epoch 061 - training loss: 0.1882, validation loss: 0.1849
2024-05-23 18:18:27 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173404/CSDI_epoch61_loss0.18487414792180062.pypots
2024-05-23 18:19:11 [INFO]: Epoch 062 - training loss: 0.1971, validation loss: 0.1868
2024-05-23 18:19:11 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173404/CSDI_epoch62_loss0.1867963746190071.pypots
2024-05-23 18:19:55 [INFO]: Epoch 063 - training loss: 0.1939, validation loss: 0.1842
2024-05-23 18:19:55 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173404/CSDI_epoch63_loss0.18419497609138488.pypots
2024-05-23 18:20:39 [INFO]: Epoch 064 - training loss: 0.2001, validation loss: 0.1871
2024-05-23 18:20:39 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173404/CSDI_epoch64_loss0.18713171258568764.pypots
2024-05-23 18:21:23 [INFO]: Epoch 065 - training loss: 0.1908, validation loss: 0.1857
2024-05-23 18:21:23 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173404/CSDI_epoch65_loss0.1857258714735508.pypots
2024-05-23 18:22:06 [INFO]: Epoch 066 - training loss: 0.1959, validation loss: 0.1848
2024-05-23 18:22:06 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173404/CSDI_epoch66_loss0.18483155965805054.pypots
2024-05-23 18:22:50 [INFO]: Epoch 067 - training loss: 0.1925, validation loss: 0.1846
2024-05-23 18:22:50 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173404/CSDI_epoch67_loss0.18461766764521598.pypots
2024-05-23 18:23:34 [INFO]: Epoch 068 - training loss: 0.1946, validation loss: 0.1849
2024-05-23 18:23:34 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173404/CSDI_epoch68_loss0.1849285051226616.pypots
2024-05-23 18:24:17 [INFO]: Epoch 069 - training loss: 0.1911, validation loss: 0.1883
2024-05-23 18:24:17 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173404/CSDI_epoch69_loss0.18831201940774916.pypots
2024-05-23 18:25:01 [INFO]: Epoch 070 - training loss: 0.1993, validation loss: 0.1852
2024-05-23 18:25:01 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173404/CSDI_epoch70_loss0.1851536899805069.pypots
2024-05-23 18:25:45 [INFO]: Epoch 071 - training loss: 0.1883, validation loss: 0.1892
2024-05-23 18:25:45 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173404/CSDI_epoch71_loss0.18916777819395064.pypots
2024-05-23 18:26:29 [INFO]: Epoch 072 - training loss: 0.1849, validation loss: 0.1832
2024-05-23 18:26:29 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173404/CSDI_epoch72_loss0.18322419375181198.pypots
2024-05-23 18:27:12 [INFO]: Epoch 073 - training loss: 0.1910, validation loss: 0.1837
2024-05-23 18:27:12 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173404/CSDI_epoch73_loss0.1836797721683979.pypots
2024-05-23 18:27:56 [INFO]: Epoch 074 - training loss: 0.2002, validation loss: 0.1875
2024-05-23 18:27:56 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173404/CSDI_epoch74_loss0.18753549382090567.pypots
2024-05-23 18:28:40 [INFO]: Epoch 075 - training loss: 0.1811, validation loss: 0.1813
2024-05-23 18:28:40 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173404/CSDI_epoch75_loss0.18127046823501586.pypots
2024-05-23 18:29:24 [INFO]: Epoch 076 - training loss: 0.1895, validation loss: 0.1873
2024-05-23 18:29:24 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173404/CSDI_epoch76_loss0.18728291764855384.pypots
2024-05-23 18:30:07 [INFO]: Epoch 077 - training loss: 0.1958, validation loss: 0.1871
2024-05-23 18:30:07 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173404/CSDI_epoch77_loss0.1870519109070301.pypots
2024-05-23 18:30:51 [INFO]: Epoch 078 - training loss: 0.1929, validation loss: 0.1859
2024-05-23 18:30:51 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173404/CSDI_epoch78_loss0.18590158075094224.pypots
2024-05-23 18:31:35 [INFO]: Epoch 079 - training loss: 0.1857, validation loss: 0.1845
2024-05-23 18:31:35 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173404/CSDI_epoch79_loss0.18453225046396254.pypots
2024-05-23 18:32:19 [INFO]: Epoch 080 - training loss: 0.1901, validation loss: 0.1810
2024-05-23 18:32:19 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173404/CSDI_epoch80_loss0.18101112693548202.pypots
2024-05-23 18:33:02 [INFO]: Epoch 081 - training loss: 0.1913, validation loss: 0.1834
2024-05-23 18:33:02 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173404/CSDI_epoch81_loss0.18344718888401984.pypots
2024-05-23 18:33:46 [INFO]: Epoch 082 - training loss: 0.1948, validation loss: 0.1840
2024-05-23 18:33:46 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173404/CSDI_epoch82_loss0.18404597863554956.pypots
2024-05-23 18:34:30 [INFO]: Epoch 083 - training loss: 0.2027, validation loss: 0.1872
2024-05-23 18:34:30 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173404/CSDI_epoch83_loss0.18722473308444024.pypots
2024-05-23 18:35:14 [INFO]: Epoch 084 - training loss: 0.1839, validation loss: 0.1839
2024-05-23 18:35:14 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173404/CSDI_epoch84_loss0.18390441909432412.pypots
2024-05-23 18:35:57 [INFO]: Epoch 085 - training loss: 0.1903, validation loss: 0.1826
2024-05-23 18:35:57 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173404/CSDI_epoch85_loss0.18264547660946845.pypots
2024-05-23 18:36:41 [INFO]: Epoch 086 - training loss: 0.1918, validation loss: 0.1806
2024-05-23 18:36:41 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173404/CSDI_epoch86_loss0.18059032857418061.pypots
2024-05-23 18:37:25 [INFO]: Epoch 087 - training loss: 0.1944, validation loss: 0.1819
2024-05-23 18:37:25 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173404/CSDI_epoch87_loss0.18188737630844115.pypots
2024-05-23 18:38:09 [INFO]: Epoch 088 - training loss: 0.1940, validation loss: 0.1839
2024-05-23 18:38:09 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173404/CSDI_epoch88_loss0.18391920402646064.pypots
2024-05-23 18:38:53 [INFO]: Epoch 089 - training loss: 0.1809, validation loss: 0.1835
2024-05-23 18:38:53 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173404/CSDI_epoch89_loss0.1835134856402874.pypots
2024-05-23 18:39:36 [INFO]: Epoch 090 - training loss: 0.1900, validation loss: 0.1802
2024-05-23 18:39:37 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173404/CSDI_epoch90_loss0.18021469414234162.pypots
2024-05-23 18:40:20 [INFO]: Epoch 091 - training loss: 0.1823, validation loss: 0.1828
2024-05-23 18:40:20 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173404/CSDI_epoch91_loss0.18280513361096382.pypots
2024-05-23 18:41:04 [INFO]: Epoch 092 - training loss: 0.1851, validation loss: 0.1822
2024-05-23 18:41:04 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173404/CSDI_epoch92_loss0.18217229917645456.pypots
2024-05-23 18:41:48 [INFO]: Epoch 093 - training loss: 0.1858, validation loss: 0.1807
2024-05-23 18:41:48 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173404/CSDI_epoch93_loss0.1806598760187626.pypots
2024-05-23 18:42:31 [INFO]: Epoch 094 - training loss: 0.1896, validation loss: 0.1808
2024-05-23 18:42:31 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173404/CSDI_epoch94_loss0.18076590448617935.pypots
2024-05-23 18:43:15 [INFO]: Epoch 095 - training loss: 0.1819, validation loss: 0.1823
2024-05-23 18:43:15 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173404/CSDI_epoch95_loss0.18226394429802895.pypots
2024-05-23 18:43:59 [INFO]: Epoch 096 - training loss: 0.1891, validation loss: 0.1833
2024-05-23 18:43:59 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173404/CSDI_epoch96_loss0.18327682539820672.pypots
2024-05-23 18:44:42 [INFO]: Epoch 097 - training loss: 0.1830, validation loss: 0.1822
2024-05-23 18:44:42 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173404/CSDI_epoch97_loss0.18216097429394723.pypots
2024-05-23 18:45:26 [INFO]: Epoch 098 - training loss: 0.1929, validation loss: 0.1827
2024-05-23 18:45:26 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173404/CSDI_epoch98_loss0.1826587699353695.pypots
2024-05-23 18:46:10 [INFO]: Epoch 099 - training loss: 0.1779, validation loss: 0.1822
2024-05-23 18:46:10 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173404/CSDI_epoch99_loss0.18218621090054513.pypots
2024-05-23 18:46:54 [INFO]: Epoch 100 - training loss: 0.1880, validation loss: 0.1816
2024-05-23 18:46:54 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173404/CSDI_epoch100_loss0.18160659372806548.pypots
2024-05-23 18:46:54 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-05-23 18:46:54 [INFO]: Finished training. The best model is from epoch#90.
2024-05-23 18:46:54 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T173404/CSDI.pypots
2024-05-23 18:54:17 [INFO]: CSDI on PhysioNet-2012-seta: MAE=0.2291, MSE=0.2547
2024-05-23 19:23:38 [INFO]: Successfully saved to overlay_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/imputation.pkl
2024-05-23 19:23:38 [INFO]: Using the given device: cuda:0
2024-05-23 19:23:38 [INFO]: Model files will be saved to overlay_premask_post_norm_saved_results/round_0/GPVAE_physionet_2012_seta/20240523_T192338
2024-05-23 19:23:38 [INFO]: Tensorboard file will be saved to overlay_premask_post_norm_saved_results/round_0/GPVAE_physionet_2012_seta/20240523_T192338/tensorboard
2024-05-23 19:23:38 [INFO]: GPVAE initialized with the given hyperparameters, the number of trainable parameters: 1,028,244
2024-05-23 19:23:39 [INFO]: Epoch 001 - training loss: 43375.7775, validation loss: 0.9341
2024-05-23 19:23:39 [INFO]: Epoch 002 - training loss: 24455.3585, validation loss: 0.7326
2024-05-23 19:23:40 [INFO]: Epoch 003 - training loss: 23489.4174, validation loss: 0.6970
2024-05-23 19:23:40 [INFO]: Epoch 004 - training loss: 23187.2802, validation loss: 0.6576
2024-05-23 19:23:41 [INFO]: Epoch 005 - training loss: 23040.0216, validation loss: 0.6537
2024-05-23 19:23:42 [INFO]: Epoch 006 - training loss: 22957.1736, validation loss: 0.6539
2024-05-23 19:23:42 [INFO]: Epoch 007 - training loss: 22907.6857, validation loss: 0.6485
2024-05-23 19:23:43 [INFO]: Epoch 008 - training loss: 22874.8844, validation loss: 0.6524
2024-05-23 19:23:43 [INFO]: Epoch 009 - training loss: 22853.2171, validation loss: 0.6384
2024-05-23 19:23:44 [INFO]: Epoch 010 - training loss: 22837.7736, validation loss: 0.6416
2024-05-23 19:23:45 [INFO]: Epoch 011 - training loss: 22826.8345, validation loss: 0.6403
2024-05-23 19:23:45 [INFO]: Epoch 012 - training loss: 22818.6701, validation loss: 0.6324
2024-05-23 19:23:46 [INFO]: Epoch 013 - training loss: 22813.1862, validation loss: 0.6327
2024-05-23 19:23:46 [INFO]: Epoch 014 - training loss: 22807.4801, validation loss: 0.6291
2024-05-23 19:23:47 [INFO]: Epoch 015 - training loss: 22804.2944, validation loss: 0.6258
2024-05-23 19:23:48 [INFO]: Epoch 016 - training loss: 22800.3807, validation loss: 0.6240
2024-05-23 19:23:48 [INFO]: Epoch 017 - training loss: 22797.8065, validation loss: 0.6228
2024-05-23 19:23:49 [INFO]: Epoch 018 - training loss: 22795.3156, validation loss: 0.6192
2024-05-23 19:23:49 [INFO]: Epoch 019 - training loss: 22793.1486, validation loss: 0.6212
2024-05-23 19:23:50 [INFO]: Epoch 020 - training loss: 22791.1331, validation loss: 0.6102
2024-05-23 19:23:50 [INFO]: Epoch 021 - training loss: 22790.0409, validation loss: 0.6155
2024-05-23 19:23:51 [INFO]: Epoch 022 - training loss: 22788.6360, validation loss: 0.6031
2024-05-23 19:23:52 [INFO]: Epoch 023 - training loss: 22787.9287, validation loss: 0.6049
2024-05-23 19:23:52 [INFO]: Epoch 024 - training loss: 22787.0747, validation loss: 0.6116
2024-05-23 19:23:53 [INFO]: Epoch 025 - training loss: 22786.0344, validation loss: 0.6025
2024-05-23 19:23:53 [INFO]: Epoch 026 - training loss: 22784.9594, validation loss: 0.5981
2024-05-23 19:23:54 [INFO]: Epoch 027 - training loss: 22783.8477, validation loss: 0.6067
2024-05-23 19:23:55 [INFO]: Epoch 028 - training loss: 22783.6601, validation loss: 0.5997
2024-05-23 19:23:55 [INFO]: Epoch 029 - training loss: 22783.0729, validation loss: 0.5949
2024-05-23 19:23:56 [INFO]: Epoch 030 - training loss: 22782.3478, validation loss: 0.5952
2024-05-23 19:23:56 [INFO]: Epoch 031 - training loss: 22782.0692, validation loss: 0.6062
2024-05-23 19:23:57 [INFO]: Epoch 032 - training loss: 22782.1474, validation loss: 0.5905
2024-05-23 19:23:57 [INFO]: Epoch 033 - training loss: 22781.1571, validation loss: 0.5920
2024-05-23 19:23:58 [INFO]: Epoch 034 - training loss: 22780.8121, validation loss: 0.5933
2024-05-23 19:23:59 [INFO]: Epoch 035 - training loss: 22780.1204, validation loss: 0.5893
2024-05-23 19:23:59 [INFO]: Epoch 036 - training loss: 22779.5432, validation loss: 0.5881
2024-05-23 19:24:00 [INFO]: Epoch 037 - training loss: 22778.8820, validation loss: 0.5891
2024-05-23 19:24:00 [INFO]: Epoch 038 - training loss: 22778.1727, validation loss: 0.5843
2024-05-23 19:24:01 [INFO]: Epoch 039 - training loss: 22777.5281, validation loss: 0.5839
2024-05-23 19:24:02 [INFO]: Epoch 040 - training loss: 22776.4727, validation loss: 0.5781
2024-05-23 19:24:02 [INFO]: Epoch 041 - training loss: 22775.6781, validation loss: 0.5773
2024-05-23 19:24:03 [INFO]: Epoch 042 - training loss: 22774.8073, validation loss: 0.5715
2024-05-23 19:24:03 [INFO]: Epoch 043 - training loss: 22773.6487, validation loss: 0.5699
2024-05-23 19:24:04 [INFO]: Epoch 044 - training loss: 22772.7312, validation loss: 0.5645
2024-05-23 19:24:05 [INFO]: Epoch 045 - training loss: 22773.2965, validation loss: 0.5648
2024-05-23 19:24:05 [INFO]: Epoch 046 - training loss: 22771.8574, validation loss: 0.5608
2024-05-23 19:24:06 [INFO]: Epoch 047 - training loss: 22771.7210, validation loss: 0.5631
2024-05-23 19:24:06 [INFO]: Epoch 048 - training loss: 22770.2549, validation loss: 0.5587
2024-05-23 19:24:07 [INFO]: Epoch 049 - training loss: 22769.3084, validation loss: 0.5525
2024-05-23 19:24:07 [INFO]: Epoch 050 - training loss: 22769.4396, validation loss: 0.5534
2024-05-23 19:24:08 [INFO]: Epoch 051 - training loss: 22768.1296, validation loss: 0.5501
2024-05-23 19:24:09 [INFO]: Epoch 052 - training loss: 22767.7361, validation loss: 0.5458
2024-05-23 19:24:09 [INFO]: Epoch 053 - training loss: 22766.6864, validation loss: 0.5466
2024-05-23 19:24:10 [INFO]: Epoch 054 - training loss: 22766.3405, validation loss: 0.5394
2024-05-23 19:24:10 [INFO]: Epoch 055 - training loss: 22765.0574, validation loss: 0.5328
2024-05-23 19:24:11 [INFO]: Epoch 056 - training loss: 22765.0327, validation loss: 0.5351
2024-05-23 19:24:12 [INFO]: Epoch 057 - training loss: 22763.9845, validation loss: 0.5274
2024-05-23 19:24:12 [INFO]: Epoch 058 - training loss: 22763.1628, validation loss: 0.5287
2024-05-23 19:24:13 [INFO]: Epoch 059 - training loss: 22762.4165, validation loss: 0.5252
2024-05-23 19:24:13 [INFO]: Epoch 060 - training loss: 22761.9607, validation loss: 0.5236
2024-05-23 19:24:14 [INFO]: Epoch 061 - training loss: 22761.1750, validation loss: 0.5160
2024-05-23 19:24:15 [INFO]: Epoch 062 - training loss: 22760.5428, validation loss: 0.5450
2024-05-23 19:24:15 [INFO]: Epoch 063 - training loss: 22760.2410, validation loss: 0.5118
2024-05-23 19:24:16 [INFO]: Epoch 064 - training loss: 22759.2330, validation loss: 0.5084
2024-05-23 19:24:16 [INFO]: Epoch 065 - training loss: 22758.6653, validation loss: 0.5050
2024-05-23 19:24:17 [INFO]: Epoch 066 - training loss: 22759.5588, validation loss: 0.5018
2024-05-23 19:24:17 [INFO]: Epoch 067 - training loss: 22757.3800, validation loss: 0.5033
2024-05-23 19:24:18 [INFO]: Epoch 068 - training loss: 22757.0912, validation loss: 0.5024
2024-05-23 19:24:19 [INFO]: Epoch 069 - training loss: 22757.2627, validation loss: 0.5091
2024-05-23 19:24:19 [INFO]: Epoch 070 - training loss: 22757.6021, validation loss: 0.4966
2024-05-23 19:24:20 [INFO]: Epoch 071 - training loss: 22755.8942, validation loss: 0.4991
2024-05-23 19:24:20 [INFO]: Epoch 072 - training loss: 22755.4397, validation loss: 0.4965
2024-05-23 19:24:21 [INFO]: Epoch 073 - training loss: 22754.8761, validation loss: 0.4987
2024-05-23 19:24:22 [INFO]: Epoch 074 - training loss: 22754.7769, validation loss: 0.4949
2024-05-23 19:24:22 [INFO]: Epoch 075 - training loss: 22754.3958, validation loss: 0.4997
2024-05-23 19:24:23 [INFO]: Epoch 076 - training loss: 22754.3993, validation loss: 0.4908
2024-05-23 19:24:23 [INFO]: Epoch 077 - training loss: 22753.7914, validation loss: 0.4899
2024-05-23 19:24:24 [INFO]: Epoch 078 - training loss: 22753.3898, validation loss: 0.4947
2024-05-23 19:24:25 [INFO]: Epoch 079 - training loss: 22753.4294, validation loss: 0.4873
2024-05-23 19:24:25 [INFO]: Epoch 080 - training loss: 22753.7007, validation loss: 0.4889
2024-05-23 19:24:26 [INFO]: Epoch 081 - training loss: 22752.9639, validation loss: 0.4890
2024-05-23 19:24:26 [INFO]: Epoch 082 - training loss: 22752.6969, validation loss: 0.4914
2024-05-23 19:24:27 [INFO]: Epoch 083 - training loss: 22752.6035, validation loss: 0.4899
2024-05-23 19:24:27 [INFO]: Epoch 084 - training loss: 22753.7328, validation loss: 0.4888
2024-05-23 19:24:28 [INFO]: Epoch 085 - training loss: 22752.2440, validation loss: 0.4855
2024-05-23 19:24:29 [INFO]: Epoch 086 - training loss: 22752.2067, validation loss: 0.4969
2024-05-23 19:24:29 [INFO]: Epoch 087 - training loss: 22751.7945, validation loss: 0.4845
2024-05-23 19:24:30 [INFO]: Epoch 088 - training loss: 22751.5986, validation loss: 0.4885
2024-05-23 19:24:30 [INFO]: Epoch 089 - training loss: 22751.4034, validation loss: 0.4896
2024-05-23 19:24:31 [INFO]: Epoch 090 - training loss: 22751.1727, validation loss: 0.4892
2024-05-23 19:24:32 [INFO]: Epoch 091 - training loss: 22751.8857, validation loss: 0.4897
2024-05-23 19:24:32 [INFO]: Epoch 092 - training loss: 22750.4665, validation loss: 0.4853
2024-05-23 19:24:33 [INFO]: Epoch 093 - training loss: 22750.3059, validation loss: 0.4810
2024-05-23 19:24:33 [INFO]: Epoch 094 - training loss: 22750.5285, validation loss: 0.4868
2024-05-23 19:24:34 [INFO]: Epoch 095 - training loss: 22750.4713, validation loss: 0.4830
2024-05-23 19:24:35 [INFO]: Epoch 096 - training loss: 22750.0141, validation loss: 0.4843
2024-05-23 19:24:35 [INFO]: Epoch 097 - training loss: 22751.1712, validation loss: 0.4860
2024-05-23 19:24:36 [INFO]: Epoch 098 - training loss: 22750.0704, validation loss: 0.4877
2024-05-23 19:24:36 [INFO]: Epoch 099 - training loss: 22749.4907, validation loss: 0.4852
2024-05-23 19:24:37 [INFO]: Epoch 100 - training loss: 22749.8993, validation loss: 0.4808
2024-05-23 19:24:37 [INFO]: Epoch 101 - training loss: 22749.7987, validation loss: 0.4834
2024-05-23 19:24:38 [INFO]: Epoch 102 - training loss: 22749.1012, validation loss: 0.4776
2024-05-23 19:24:39 [INFO]: Epoch 103 - training loss: 22749.2101, validation loss: 0.4816
2024-05-23 19:24:39 [INFO]: Epoch 104 - training loss: 22750.1610, validation loss: 0.4905
2024-05-23 19:24:40 [INFO]: Epoch 105 - training loss: 22752.0196, validation loss: 0.4812
2024-05-23 19:24:40 [INFO]: Epoch 106 - training loss: 22751.3456, validation loss: 0.4845
2024-05-23 19:24:41 [INFO]: Epoch 107 - training loss: 22749.6635, validation loss: 0.4801
2024-05-23 19:24:42 [INFO]: Epoch 108 - training loss: 22749.4758, validation loss: 0.4762
2024-05-23 19:24:42 [INFO]: Epoch 109 - training loss: 22748.4289, validation loss: 0.4881
2024-05-23 19:24:43 [INFO]: Epoch 110 - training loss: 22748.3128, validation loss: 0.4804
2024-05-23 19:24:43 [INFO]: Epoch 111 - training loss: 22747.9831, validation loss: 0.4783
2024-05-23 19:24:44 [INFO]: Epoch 112 - training loss: 22747.9733, validation loss: 0.4800
2024-05-23 19:24:45 [INFO]: Epoch 113 - training loss: 22748.1783, validation loss: 0.4809
2024-05-23 19:24:45 [INFO]: Epoch 114 - training loss: 22748.0093, validation loss: 0.4790
2024-05-23 19:24:46 [INFO]: Epoch 115 - training loss: 22748.3926, validation loss: 0.4774
2024-05-23 19:24:46 [INFO]: Epoch 116 - training loss: 22749.3507, validation loss: 0.4777
2024-05-23 19:24:47 [INFO]: Epoch 117 - training loss: 22748.2773, validation loss: 0.4763
2024-05-23 19:24:47 [INFO]: Epoch 118 - training loss: 22749.2783, validation loss: 0.4816
2024-05-23 19:24:47 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-05-23 19:24:47 [INFO]: Finished training. The best model is from epoch#108.
2024-05-23 19:24:47 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_0/GPVAE_physionet_2012_seta/20240523_T192338/GPVAE.pypots
2024-05-23 19:24:48 [INFO]: GP-VAE on PhysioNet-2012-seta: MAE=0.4221, MSE=0.4906
2024-05-23 19:24:48 [INFO]: Successfully saved to overlay_premask_post_norm_saved_results/round_0/GPVAE_physionet_2012_seta/imputation.pkl
2024-05-23 19:24:48 [INFO]: Using the given device: cuda:0
2024-05-23 19:24:48 [INFO]: Model files will be saved to overlay_premask_post_norm_saved_results/round_0/USGAN_physionet_2012_seta/20240523_T192448
2024-05-23 19:24:48 [INFO]: Tensorboard file will be saved to overlay_premask_post_norm_saved_results/round_0/USGAN_physionet_2012_seta/20240523_T192448/tensorboard
2024-05-23 19:24:48 [INFO]: USGAN initialized with the given hyperparameters, the number of trainable parameters: 16,010,261
2024-05-23 19:25:10 [INFO]: Epoch 001 - generator training loss: 0.6024, discriminator training loss: 0.3786, validation loss: 0.6203
2024-05-23 19:25:29 [INFO]: Epoch 002 - generator training loss: 0.4813, discriminator training loss: 0.2719, validation loss: 0.5253
2024-05-23 19:25:48 [INFO]: Epoch 003 - generator training loss: 0.4374, discriminator training loss: 0.2376, validation loss: 0.5070
2024-05-23 19:26:06 [INFO]: Epoch 004 - generator training loss: 0.4467, discriminator training loss: 0.1926, validation loss: 0.4980
2024-05-23 19:26:25 [INFO]: Epoch 005 - generator training loss: 0.4507, discriminator training loss: 0.1604, validation loss: 0.4884
2024-05-23 19:26:44 [INFO]: Epoch 006 - generator training loss: 0.4439, discriminator training loss: 0.1405, validation loss: 0.4643
2024-05-23 19:27:03 [INFO]: Epoch 007 - generator training loss: 0.4301, discriminator training loss: 0.1264, validation loss: 0.4618
2024-05-23 19:27:22 [INFO]: Epoch 008 - generator training loss: 0.4205, discriminator training loss: 0.1159, validation loss: 0.4441
2024-05-23 19:27:40 [INFO]: Epoch 009 - generator training loss: 0.4052, discriminator training loss: 0.1073, validation loss: 0.4410
2024-05-23 19:27:59 [INFO]: Epoch 010 - generator training loss: 0.3953, discriminator training loss: 0.1001, validation loss: 0.4321
2024-05-23 19:28:18 [INFO]: Epoch 011 - generator training loss: 0.3916, discriminator training loss: 0.0943, validation loss: 0.4218
2024-05-23 19:28:37 [INFO]: Epoch 012 - generator training loss: 0.3847, discriminator training loss: 0.0888, validation loss: 0.4169
2024-05-23 19:28:55 [INFO]: Epoch 013 - generator training loss: 0.3793, discriminator training loss: 0.0842, validation loss: 0.4112
2024-05-23 19:29:14 [INFO]: Epoch 014 - generator training loss: 0.3734, discriminator training loss: 0.0804, validation loss: 0.4053
2024-05-23 19:29:33 [INFO]: Epoch 015 - generator training loss: 0.3698, discriminator training loss: 0.0771, validation loss: 0.4029
2024-05-23 19:29:52 [INFO]: Epoch 016 - generator training loss: 0.3696, discriminator training loss: 0.0738, validation loss: 0.3990
2024-05-23 19:30:11 [INFO]: Epoch 017 - generator training loss: 0.3619, discriminator training loss: 0.0709, validation loss: 0.3926
2024-05-23 19:30:29 [INFO]: Epoch 018 - generator training loss: 0.3575, discriminator training loss: 0.0679, validation loss: 0.3864
2024-05-23 19:30:48 [INFO]: Epoch 019 - generator training loss: 0.3532, discriminator training loss: 0.0657, validation loss: 0.3817
2024-05-23 19:31:07 [INFO]: Epoch 020 - generator training loss: 0.3457, discriminator training loss: 0.0635, validation loss: 0.3769
2024-05-23 19:31:26 [INFO]: Epoch 021 - generator training loss: 0.3419, discriminator training loss: 0.0619, validation loss: 0.3732
2024-05-23 19:31:45 [INFO]: Epoch 022 - generator training loss: 0.3367, discriminator training loss: 0.0599, validation loss: 0.3667
2024-05-23 19:32:04 [INFO]: Epoch 023 - generator training loss: 0.3344, discriminator training loss: 0.0581, validation loss: 0.3635
2024-05-23 19:32:23 [INFO]: Epoch 024 - generator training loss: 0.3295, discriminator training loss: 0.0569, validation loss: 0.3631
2024-05-23 19:32:42 [INFO]: Epoch 025 - generator training loss: 0.3257, discriminator training loss: 0.0553, validation loss: 0.3658
2024-05-23 19:33:01 [INFO]: Epoch 026 - generator training loss: 0.3238, discriminator training loss: 0.0544, validation loss: 0.3569
2024-05-23 19:33:20 [INFO]: Epoch 027 - generator training loss: 0.3165, discriminator training loss: 0.0532, validation loss: 0.3536
2024-05-23 19:33:39 [INFO]: Epoch 028 - generator training loss: 0.3142, discriminator training loss: 0.0524, validation loss: 0.3518
2024-05-23 19:33:58 [INFO]: Epoch 029 - generator training loss: 0.3123, discriminator training loss: 0.0518, validation loss: 0.3456
2024-05-23 19:34:16 [INFO]: Epoch 030 - generator training loss: 0.3115, discriminator training loss: 0.0509, validation loss: 0.3444
2024-05-23 19:34:35 [INFO]: Epoch 031 - generator training loss: 0.3036, discriminator training loss: 0.0503, validation loss: 0.3412
2024-05-23 19:34:54 [INFO]: Epoch 032 - generator training loss: 0.3085, discriminator training loss: 0.0497, validation loss: 0.3406
2024-05-23 19:35:13 [INFO]: Epoch 033 - generator training loss: 0.2990, discriminator training loss: 0.0490, validation loss: 0.3340
2024-05-23 19:35:33 [INFO]: Epoch 034 - generator training loss: 0.2919, discriminator training loss: 0.0485, validation loss: 0.3337
2024-05-23 19:35:52 [INFO]: Epoch 035 - generator training loss: 0.2890, discriminator training loss: 0.0480, validation loss: 0.3303
2024-05-23 19:36:11 [INFO]: Epoch 036 - generator training loss: 0.2859, discriminator training loss: 0.0476, validation loss: 0.3252
2024-05-23 19:36:30 [INFO]: Epoch 037 - generator training loss: 0.2818, discriminator training loss: 0.0470, validation loss: 0.3187
2024-05-23 19:36:49 [INFO]: Epoch 038 - generator training loss: 0.2771, discriminator training loss: 0.0468, validation loss: 0.3277
2024-05-23 19:37:08 [INFO]: Epoch 039 - generator training loss: 0.2735, discriminator training loss: 0.0465, validation loss: 0.3223
2024-05-23 19:37:27 [INFO]: Epoch 040 - generator training loss: 0.2719, discriminator training loss: 0.0462, validation loss: 0.3153
2024-05-23 19:37:46 [INFO]: Epoch 041 - generator training loss: 0.2679, discriminator training loss: 0.0459, validation loss: 0.3137
2024-05-23 19:38:05 [INFO]: Epoch 042 - generator training loss: 0.2666, discriminator training loss: 0.0454, validation loss: 0.3208
2024-05-23 19:38:24 [INFO]: Epoch 043 - generator training loss: 0.2738, discriminator training loss: 0.0452, validation loss: 0.3144
2024-05-23 19:38:43 [INFO]: Epoch 044 - generator training loss: 0.2725, discriminator training loss: 0.0450, validation loss: 0.3143
2024-05-23 19:39:02 [INFO]: Epoch 045 - generator training loss: 0.2635, discriminator training loss: 0.0444, validation loss: 0.3121
2024-05-23 19:39:21 [INFO]: Epoch 046 - generator training loss: 0.2561, discriminator training loss: 0.0445, validation loss: 0.3081
2024-05-23 19:39:41 [INFO]: Epoch 047 - generator training loss: 0.2528, discriminator training loss: 0.0441, validation loss: 0.3069
2024-05-23 19:40:01 [INFO]: Epoch 048 - generator training loss: 0.2499, discriminator training loss: 0.0440, validation loss: 0.3106
2024-05-23 19:40:21 [INFO]: Epoch 049 - generator training loss: 0.2507, discriminator training loss: 0.0438, validation loss: 0.3099
2024-05-23 19:40:41 [INFO]: Epoch 050 - generator training loss: 0.2508, discriminator training loss: 0.0436, validation loss: 0.3122
2024-05-23 19:41:01 [INFO]: Epoch 051 - generator training loss: 0.2490, discriminator training loss: 0.0433, validation loss: 0.3077
2024-05-23 19:41:21 [INFO]: Epoch 052 - generator training loss: 0.2431, discriminator training loss: 0.0432, validation loss: 0.3076
2024-05-23 19:41:41 [INFO]: Epoch 053 - generator training loss: 0.2431, discriminator training loss: 0.0431, validation loss: 0.3050
2024-05-23 19:42:00 [INFO]: Epoch 054 - generator training loss: 0.2384, discriminator training loss: 0.0426, validation loss: 0.3074
2024-05-23 19:42:19 [INFO]: Epoch 055 - generator training loss: 0.2369, discriminator training loss: 0.0425, validation loss: 0.3026
2024-05-23 19:42:38 [INFO]: Epoch 056 - generator training loss: 0.2338, discriminator training loss: 0.0423, validation loss: 0.3025
2024-05-23 19:42:57 [INFO]: Epoch 057 - generator training loss: 0.2323, discriminator training loss: 0.0422, validation loss: 0.3027
2024-05-23 19:43:16 [INFO]: Epoch 058 - generator training loss: 0.2314, discriminator training loss: 0.0421, validation loss: 0.3099
2024-05-23 19:43:35 [INFO]: Epoch 059 - generator training loss: 0.2343, discriminator training loss: 0.0422, validation loss: 0.3039
2024-05-23 19:43:54 [INFO]: Epoch 060 - generator training loss: 0.2326, discriminator training loss: 0.0418, validation loss: 0.3050
2024-05-23 19:44:13 [INFO]: Epoch 061 - generator training loss: 0.2291, discriminator training loss: 0.0418, validation loss: 0.3039
2024-05-23 19:44:32 [INFO]: Epoch 062 - generator training loss: 0.2263, discriminator training loss: 0.0416, validation loss: 0.3024
2024-05-23 19:44:51 [INFO]: Epoch 063 - generator training loss: 0.2198, discriminator training loss: 0.0418, validation loss: 0.3058
2024-05-23 19:45:10 [INFO]: Epoch 064 - generator training loss: 0.2152, discriminator training loss: 0.0413, validation loss: 0.3001
2024-05-23 19:45:29 [INFO]: Epoch 065 - generator training loss: 0.2322, discriminator training loss: 0.0416, validation loss: 0.3059
2024-05-23 19:45:48 [INFO]: Epoch 066 - generator training loss: 0.2227, discriminator training loss: 0.0412, validation loss: 0.3027
2024-05-23 19:46:07 [INFO]: Epoch 067 - generator training loss: 0.2155, discriminator training loss: 0.0412, validation loss: 0.3014
2024-05-23 19:46:26 [INFO]: Epoch 068 - generator training loss: 0.2092, discriminator training loss: 0.0410, validation loss: 0.3016
2024-05-23 19:46:45 [INFO]: Epoch 069 - generator training loss: 0.2097, discriminator training loss: 0.0409, validation loss: 0.3063
2024-05-23 19:47:04 [INFO]: Epoch 070 - generator training loss: 0.2179, discriminator training loss: 0.0409, validation loss: 0.3090
2024-05-23 19:47:23 [INFO]: Epoch 071 - generator training loss: 0.2142, discriminator training loss: 0.0407, validation loss: 0.3020
2024-05-23 19:47:42 [INFO]: Epoch 072 - generator training loss: 0.2109, discriminator training loss: 0.0405, validation loss: 0.3064
2024-05-23 19:48:01 [INFO]: Epoch 073 - generator training loss: 0.2072, discriminator training loss: 0.0402, validation loss: 0.3063
2024-05-23 19:48:20 [INFO]: Epoch 074 - generator training loss: 0.2027, discriminator training loss: 0.0401, validation loss: 0.3033
2024-05-23 19:48:20 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-05-23 19:48:20 [INFO]: Finished training. The best model is from epoch#64.
2024-05-23 19:48:20 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_0/USGAN_physionet_2012_seta/20240523_T192448/USGAN.pypots
2024-05-23 19:48:23 [INFO]: US-GAN on PhysioNet-2012-seta: MAE=0.2979, MSE=0.2872
2024-05-23 19:48:33 [INFO]: Successfully saved to overlay_premask_post_norm_saved_results/round_0/USGAN_physionet_2012_seta/imputation.pkl
2024-05-23 19:48:33 [INFO]: Using the given device: cuda:0
2024-05-23 19:48:33 [INFO]: Model files will be saved to overlay_premask_post_norm_saved_results/round_0/BRITS_physionet_2012_seta/20240523_T194833
2024-05-23 19:48:33 [INFO]: Tensorboard file will be saved to overlay_premask_post_norm_saved_results/round_0/BRITS_physionet_2012_seta/20240523_T194833/tensorboard
2024-05-23 19:48:33 [INFO]: BRITS initialized with the given hyperparameters, the number of trainable parameters: 9,176,048
2024-05-23 19:48:48 [INFO]: Epoch 001 - training loss: 1.1259, validation loss: 0.5152
2024-05-23 19:49:00 [INFO]: Epoch 002 - training loss: 0.9195, validation loss: 0.4576
2024-05-23 19:49:13 [INFO]: Epoch 003 - training loss: 0.8578, validation loss: 0.4273
2024-05-23 19:49:25 [INFO]: Epoch 004 - training loss: 0.8191, validation loss: 0.4048
2024-05-23 19:49:37 [INFO]: Epoch 005 - training loss: 0.7908, validation loss: 0.3889
2024-05-23 19:49:50 [INFO]: Epoch 006 - training loss: 0.7693, validation loss: 0.3743
2024-05-23 19:50:03 [INFO]: Epoch 007 - training loss: 0.7519, validation loss: 0.3664
2024-05-23 19:50:16 [INFO]: Epoch 008 - training loss: 0.7371, validation loss: 0.3577
2024-05-23 19:50:28 [INFO]: Epoch 009 - training loss: 0.7251, validation loss: 0.3527
2024-05-23 19:50:41 [INFO]: Epoch 010 - training loss: 0.7141, validation loss: 0.3479
2024-05-23 19:50:53 [INFO]: Epoch 011 - training loss: 0.7057, validation loss: 0.3442
2024-05-23 19:51:05 [INFO]: Epoch 012 - training loss: 0.6966, validation loss: 0.3410
2024-05-23 19:51:18 [INFO]: Epoch 013 - training loss: 0.6899, validation loss: 0.3402
2024-05-23 19:51:30 [INFO]: Epoch 014 - training loss: 0.6841, validation loss: 0.3382
2024-05-23 19:51:43 [INFO]: Epoch 015 - training loss: 0.6786, validation loss: 0.3357
2024-05-23 19:51:55 [INFO]: Epoch 016 - training loss: 0.6736, validation loss: 0.3347
2024-05-23 19:52:07 [INFO]: Epoch 017 - training loss: 0.6685, validation loss: 0.3334
2024-05-23 19:52:20 [INFO]: Epoch 018 - training loss: 0.6642, validation loss: 0.3327
2024-05-23 19:52:32 [INFO]: Epoch 019 - training loss: 0.6609, validation loss: 0.3310
2024-05-23 19:52:44 [INFO]: Epoch 020 - training loss: 0.6574, validation loss: 0.3307
2024-05-23 19:52:57 [INFO]: Epoch 021 - training loss: 0.6550, validation loss: 0.3307
2024-05-23 19:53:09 [INFO]: Epoch 022 - training loss: 0.6506, validation loss: 0.3289
2024-05-23 19:53:21 [INFO]: Epoch 023 - training loss: 0.6468, validation loss: 0.3295
2024-05-23 19:53:34 [INFO]: Epoch 024 - training loss: 0.6435, validation loss: 0.3285
2024-05-23 19:53:46 [INFO]: Epoch 025 - training loss: 0.6418, validation loss: 0.3302
2024-05-23 19:53:59 [INFO]: Epoch 026 - training loss: 0.6389, validation loss: 0.3285
2024-05-23 19:54:11 [INFO]: Epoch 027 - training loss: 0.6358, validation loss: 0.3294
2024-05-23 19:54:23 [INFO]: Epoch 028 - training loss: 0.6333, validation loss: 0.3280
2024-05-23 19:54:36 [INFO]: Epoch 029 - training loss: 0.6296, validation loss: 0.3275
2024-05-23 19:54:48 [INFO]: Epoch 030 - training loss: 0.6269, validation loss: 0.3279
2024-05-23 19:55:00 [INFO]: Epoch 031 - training loss: 0.6277, validation loss: 0.3283
2024-05-23 19:55:13 [INFO]: Epoch 032 - training loss: 0.6231, validation loss: 0.3285
2024-05-23 19:55:25 [INFO]: Epoch 033 - training loss: 0.6204, validation loss: 0.3287
2024-05-23 19:55:38 [INFO]: Epoch 034 - training loss: 0.6182, validation loss: 0.3285
2024-05-23 19:55:50 [INFO]: Epoch 035 - training loss: 0.6148, validation loss: 0.3288
2024-05-23 19:56:03 [INFO]: Epoch 036 - training loss: 0.6112, validation loss: 0.3292
2024-05-23 19:56:15 [INFO]: Epoch 037 - training loss: 0.6080, validation loss: 0.3299
2024-05-23 19:56:27 [INFO]: Epoch 038 - training loss: 0.6065, validation loss: 0.3284
2024-05-23 19:56:40 [INFO]: Epoch 039 - training loss: 0.6030, validation loss: 0.3292
2024-05-23 19:56:40 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-05-23 19:56:40 [INFO]: Finished training. The best model is from epoch#29.
2024-05-23 19:56:40 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_0/BRITS_physionet_2012_seta/20240523_T194833/BRITS.pypots
2024-05-23 19:56:42 [INFO]: BRITS on PhysioNet-2012-seta: MAE=0.2615, MSE=0.2968
2024-05-23 19:56:52 [INFO]: Successfully saved to overlay_premask_post_norm_saved_results/round_0/BRITS_physionet_2012_seta/imputation.pkl
2024-05-23 19:56:52 [INFO]: Using the given device: cuda:0
2024-05-23 19:56:52 [INFO]: Model files will be saved to overlay_premask_post_norm_saved_results/round_0/MRNN_physionet_2012_seta/20240523_T195652
2024-05-23 19:56:52 [INFO]: Tensorboard file will be saved to overlay_premask_post_norm_saved_results/round_0/MRNN_physionet_2012_seta/20240523_T195652/tensorboard
2024-05-23 19:56:52 [INFO]: MRNN initialized with the given hyperparameters, the number of trainable parameters: 7,599
2024-05-23 19:56:58 [INFO]: Epoch 001 - training loss: 1.2419, validation loss: 1.0003
2024-05-23 19:56:58 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_0/MRNN_physionet_2012_seta/20240523_T195652/MRNN_epoch1_loss1.0003443598747253.pypots
2024-05-23 19:57:01 [INFO]: Epoch 002 - training loss: 0.7701, validation loss: 0.9716
2024-05-23 19:57:01 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_0/MRNN_physionet_2012_seta/20240523_T195652/MRNN_epoch2_loss0.9715986579656601.pypots
2024-05-23 19:57:04 [INFO]: Epoch 003 - training loss: 0.6110, validation loss: 0.9439
2024-05-23 19:57:04 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_0/MRNN_physionet_2012_seta/20240523_T195652/MRNN_epoch3_loss0.9439138144254684.pypots
2024-05-23 19:57:07 [INFO]: Epoch 004 - training loss: 0.5674, validation loss: 0.9299
2024-05-23 19:57:07 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_0/MRNN_physionet_2012_seta/20240523_T195652/MRNN_epoch4_loss0.9299045681953431.pypots
2024-05-23 19:57:09 [INFO]: Epoch 005 - training loss: 0.5394, validation loss: 0.9231
2024-05-23 19:57:09 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_0/MRNN_physionet_2012_seta/20240523_T195652/MRNN_epoch5_loss0.9231128931045532.pypots
2024-05-23 19:57:12 [INFO]: Epoch 006 - training loss: 0.5200, validation loss: 0.9178
2024-05-23 19:57:12 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_0/MRNN_physionet_2012_seta/20240523_T195652/MRNN_epoch6_loss0.917750209569931.pypots
2024-05-23 19:57:15 [INFO]: Epoch 007 - training loss: 0.5013, validation loss: 0.9137
2024-05-23 19:57:15 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_0/MRNN_physionet_2012_seta/20240523_T195652/MRNN_epoch7_loss0.913685542345047.pypots
2024-05-23 19:57:18 [INFO]: Epoch 008 - training loss: 0.4920, validation loss: 0.9118
2024-05-23 19:57:18 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_0/MRNN_physionet_2012_seta/20240523_T195652/MRNN_epoch8_loss0.9118448168039321.pypots
2024-05-23 19:57:21 [INFO]: Epoch 009 - training loss: 0.4836, validation loss: 0.9107
2024-05-23 19:57:21 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_0/MRNN_physionet_2012_seta/20240523_T195652/MRNN_epoch9_loss0.9106736898422241.pypots
2024-05-23 19:57:24 [INFO]: Epoch 010 - training loss: 0.4780, validation loss: 0.9105
2024-05-23 19:57:24 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_0/MRNN_physionet_2012_seta/20240523_T195652/MRNN_epoch10_loss0.9104952335357666.pypots
2024-05-23 19:57:26 [INFO]: Epoch 011 - training loss: 0.4776, validation loss: 0.9109
2024-05-23 19:57:26 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_0/MRNN_physionet_2012_seta/20240523_T195652/MRNN_epoch11_loss0.9109375894069671.pypots
2024-05-23 19:57:29 [INFO]: Epoch 012 - training loss: 0.4680, validation loss: 0.9125
2024-05-23 19:57:29 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_0/MRNN_physionet_2012_seta/20240523_T195652/MRNN_epoch12_loss0.9124950259923935.pypots
2024-05-23 19:57:32 [INFO]: Epoch 013 - training loss: 0.4648, validation loss: 0.9151
2024-05-23 19:57:32 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_0/MRNN_physionet_2012_seta/20240523_T195652/MRNN_epoch13_loss0.9150516211986541.pypots
2024-05-23 19:57:35 [INFO]: Epoch 014 - training loss: 0.4499, validation loss: 0.9170
2024-05-23 19:57:35 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_0/MRNN_physionet_2012_seta/20240523_T195652/MRNN_epoch14_loss0.9169920027256012.pypots
2024-05-23 19:57:38 [INFO]: Epoch 015 - training loss: 0.4493, validation loss: 0.9192
2024-05-23 19:57:38 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_0/MRNN_physionet_2012_seta/20240523_T195652/MRNN_epoch15_loss0.9192283838987351.pypots
2024-05-23 19:57:41 [INFO]: Epoch 016 - training loss: 0.4444, validation loss: 0.9201
2024-05-23 19:57:41 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_0/MRNN_physionet_2012_seta/20240523_T195652/MRNN_epoch16_loss0.9200500249862671.pypots
2024-05-23 19:57:43 [INFO]: Epoch 017 - training loss: 0.4428, validation loss: 0.9229
2024-05-23 19:57:43 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_0/MRNN_physionet_2012_seta/20240523_T195652/MRNN_epoch17_loss0.9228569120168686.pypots
2024-05-23 19:57:46 [INFO]: Epoch 018 - training loss: 0.4516, validation loss: 0.9248
2024-05-23 19:57:46 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_0/MRNN_physionet_2012_seta/20240523_T195652/MRNN_epoch18_loss0.9248102575540542.pypots
2024-05-23 19:57:49 [INFO]: Epoch 019 - training loss: 0.4326, validation loss: 0.9254
2024-05-23 19:57:49 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_0/MRNN_physionet_2012_seta/20240523_T195652/MRNN_epoch19_loss0.925428956747055.pypots
2024-05-23 19:57:52 [INFO]: Epoch 020 - training loss: 0.4386, validation loss: 0.9274
2024-05-23 19:57:52 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_0/MRNN_physionet_2012_seta/20240523_T195652/MRNN_epoch20_loss0.9274473518133164.pypots
2024-05-23 19:57:52 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-05-23 19:57:52 [INFO]: Finished training. The best model is from epoch#10.
2024-05-23 19:57:52 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_0/MRNN_physionet_2012_seta/20240523_T195652/MRNN.pypots
2024-05-23 19:57:53 [INFO]: MRNN on PhysioNet-2012-seta: MAE=0.6854, MSE=0.9236
2024-05-23 19:57:57 [INFO]: Successfully saved to overlay_premask_post_norm_saved_results/round_0/MRNN_physionet_2012_seta/imputation.pkl
2024-05-23 19:57:57 [INFO]: Using the given device: cpu
2024-05-23 19:57:57 [INFO]: LOCF on PhysioNet-2012-seta: MAE=0.4085, MSE=0.5400
2024-05-23 19:57:58 [INFO]: Successfully saved to overlay_premask_post_norm_saved_results/round_0/LOCF_physionet_2012_seta/imputation.pkl
2024-05-23 19:57:58 [INFO]: Median on PhysioNet-2012-seta: MAE=0.6857, MSE=1.0298
2024-05-23 19:57:58 [INFO]: Successfully saved to overlay_premask_post_norm_saved_results/round_0/Median_physionet_2012_seta/imputation.pkl
2024-05-23 19:57:58 [INFO]: Mean on PhysioNet-2012-seta: MAE=0.7024, MSE=1.0000
2024-05-23 19:57:58 [INFO]: Successfully saved to overlay_premask_post_norm_saved_results/round_0/Mean_physionet_2012_seta/imputation.pkl
2024-05-23 19:57:58 [INFO]: Have set the random seed as 2024 for numpy and pytorch.
2024-05-23 19:57:58 [INFO]: Using the given device: cuda:0
2024-05-23 19:57:58 [INFO]: Model files will be saved to overlay_premask_post_norm_saved_results/round_1/SAITS_physionet_2012_seta/20240523_T195758
2024-05-23 19:57:58 [INFO]: Tensorboard file will be saved to overlay_premask_post_norm_saved_results/round_1/SAITS_physionet_2012_seta/20240523_T195758/tensorboard
2024-05-23 19:57:58 [INFO]: SAITS initialized with the given hyperparameters, the number of trainable parameters: 5,322,806
2024-05-23 19:57:59 [INFO]: Epoch 001 - training loss: 1.1221, validation loss: 0.4780
2024-05-23 19:58:00 [INFO]: Epoch 002 - training loss: 0.8288, validation loss: 0.4186
2024-05-23 19:58:01 [INFO]: Epoch 003 - training loss: 0.6997, validation loss: 0.3763
2024-05-23 19:58:03 [INFO]: Epoch 004 - training loss: 0.5745, validation loss: 0.3697
2024-05-23 19:58:04 [INFO]: Epoch 005 - training loss: 0.5020, validation loss: 0.3561
2024-05-23 19:58:05 [INFO]: Epoch 006 - training loss: 0.4512, validation loss: 0.3427
2024-05-23 19:58:06 [INFO]: Epoch 007 - training loss: 0.4158, validation loss: 0.3329
2024-05-23 19:58:07 [INFO]: Epoch 008 - training loss: 0.3910, validation loss: 0.3295
2024-05-23 19:58:08 [INFO]: Epoch 009 - training loss: 0.3791, validation loss: 0.3174
2024-05-23 19:58:10 [INFO]: Epoch 010 - training loss: 0.3577, validation loss: 0.3129
2024-05-23 19:58:11 [INFO]: Epoch 011 - training loss: 0.3363, validation loss: 0.3121
2024-05-23 19:58:12 [INFO]: Epoch 012 - training loss: 0.3213, validation loss: 0.3092
2024-05-23 19:58:13 [INFO]: Epoch 013 - training loss: 0.3108, validation loss: 0.3036
2024-05-23 19:58:14 [INFO]: Epoch 014 - training loss: 0.2968, validation loss: 0.2971
2024-05-23 19:58:15 [INFO]: Epoch 015 - training loss: 0.2841, validation loss: 0.2971
2024-05-23 19:58:17 [INFO]: Epoch 016 - training loss: 0.2825, validation loss: 0.2975
2024-05-23 19:58:18 [INFO]: Epoch 017 - training loss: 0.2654, validation loss: 0.2928
2024-05-23 19:58:19 [INFO]: Epoch 018 - training loss: 0.2616, validation loss: 0.2949
2024-05-23 19:58:20 [INFO]: Epoch 019 - training loss: 0.2510, validation loss: 0.2917
2024-05-23 19:58:21 [INFO]: Epoch 020 - training loss: 0.2443, validation loss: 0.2942
2024-05-23 19:58:22 [INFO]: Epoch 021 - training loss: 0.2347, validation loss: 0.2922
2024-05-23 19:58:24 [INFO]: Epoch 022 - training loss: 0.2288, validation loss: 0.2935
2024-05-23 19:58:25 [INFO]: Epoch 023 - training loss: 0.2280, validation loss: 0.2908
2024-05-23 19:58:26 [INFO]: Epoch 024 - training loss: 0.2203, validation loss: 0.2894
2024-05-23 19:58:27 [INFO]: Epoch 025 - training loss: 0.2143, validation loss: 0.2963
2024-05-23 19:58:28 [INFO]: Epoch 026 - training loss: 0.2094, validation loss: 0.2912
2024-05-23 19:58:29 [INFO]: Epoch 027 - training loss: 0.2034, validation loss: 0.2961
2024-05-23 19:58:31 [INFO]: Epoch 028 - training loss: 0.2055, validation loss: 0.2937
2024-05-23 19:58:32 [INFO]: Epoch 029 - training loss: 0.1988, validation loss: 0.2928
2024-05-23 19:58:33 [INFO]: Epoch 030 - training loss: 0.1931, validation loss: 0.2974
2024-05-23 19:58:34 [INFO]: Epoch 031 - training loss: 0.1884, validation loss: 0.2919
2024-05-23 19:58:35 [INFO]: Epoch 032 - training loss: 0.1868, validation loss: 0.2983
2024-05-23 19:58:36 [INFO]: Epoch 033 - training loss: 0.1831, validation loss: 0.2928
2024-05-23 19:58:38 [INFO]: Epoch 034 - training loss: 0.1806, validation loss: 0.2918
2024-05-23 19:58:38 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-05-23 19:58:38 [INFO]: Finished training. The best model is from epoch#24.
2024-05-23 19:58:38 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_1/SAITS_physionet_2012_seta/20240523_T195758/SAITS.pypots
2024-05-23 19:58:38 [INFO]: SAITS on PhysioNet-2012-seta: MAE=0.2770, MSE=0.3251
2024-05-23 19:58:38 [INFO]: Successfully saved to overlay_premask_post_norm_saved_results/round_1/SAITS_physionet_2012_seta/imputation.pkl
2024-05-23 19:58:38 [INFO]: Using the given device: cuda:0
2024-05-23 19:58:38 [INFO]: Model files will be saved to overlay_premask_post_norm_saved_results/round_1/Transformer_physionet_2012_seta/20240523_T195838
2024-05-23 19:58:38 [INFO]: Tensorboard file will be saved to overlay_premask_post_norm_saved_results/round_1/Transformer_physionet_2012_seta/20240523_T195838/tensorboard
2024-05-23 19:58:38 [INFO]: Transformer initialized with the given hyperparameters, the number of trainable parameters: 2,152,613
2024-05-23 19:58:39 [INFO]: Epoch 001 - training loss: 1.1266, validation loss: 0.5455
2024-05-23 19:58:39 [INFO]: Epoch 002 - training loss: 0.7283, validation loss: 0.4787
2024-05-23 19:58:40 [INFO]: Epoch 003 - training loss: 0.6325, validation loss: 0.4274
2024-05-23 19:58:41 [INFO]: Epoch 004 - training loss: 0.5759, validation loss: 0.4122
2024-05-23 19:58:41 [INFO]: Epoch 005 - training loss: 0.5351, validation loss: 0.3970
2024-05-23 19:58:42 [INFO]: Epoch 006 - training loss: 0.5028, validation loss: 0.3858
2024-05-23 19:58:42 [INFO]: Epoch 007 - training loss: 0.4837, validation loss: 0.3837
2024-05-23 19:58:43 [INFO]: Epoch 008 - training loss: 0.4596, validation loss: 0.3715
2024-05-23 19:58:44 [INFO]: Epoch 009 - training loss: 0.4353, validation loss: 0.3650
2024-05-23 19:58:44 [INFO]: Epoch 010 - training loss: 0.4191, validation loss: 0.3526
2024-05-23 19:58:45 [INFO]: Epoch 011 - training loss: 0.4095, validation loss: 0.3521
2024-05-23 19:58:45 [INFO]: Epoch 012 - training loss: 0.3976, validation loss: 0.3471
2024-05-23 19:58:46 [INFO]: Epoch 013 - training loss: 0.3838, validation loss: 0.3379
2024-05-23 19:58:47 [INFO]: Epoch 014 - training loss: 0.3750, validation loss: 0.3365
2024-05-23 19:58:47 [INFO]: Epoch 015 - training loss: 0.3630, validation loss: 0.3400
2024-05-23 19:58:48 [INFO]: Epoch 016 - training loss: 0.3541, validation loss: 0.3305
2024-05-23 19:58:48 [INFO]: Epoch 017 - training loss: 0.3444, validation loss: 0.3245
2024-05-23 19:58:49 [INFO]: Epoch 018 - training loss: 0.3405, validation loss: 0.3253
2024-05-23 19:58:49 [INFO]: Epoch 019 - training loss: 0.3367, validation loss: 0.3225
2024-05-23 19:58:50 [INFO]: Epoch 020 - training loss: 0.3276, validation loss: 0.3218
2024-05-23 19:58:51 [INFO]: Epoch 021 - training loss: 0.3194, validation loss: 0.3270
2024-05-23 19:58:51 [INFO]: Epoch 022 - training loss: 0.3155, validation loss: 0.3187
2024-05-23 19:58:52 [INFO]: Epoch 023 - training loss: 0.3053, validation loss: 0.3179
2024-05-23 19:58:52 [INFO]: Epoch 024 - training loss: 0.2994, validation loss: 0.3210
2024-05-23 19:58:53 [INFO]: Epoch 025 - training loss: 0.2952, validation loss: 0.3202
2024-05-23 19:58:54 [INFO]: Epoch 026 - training loss: 0.2917, validation loss: 0.3162
2024-05-23 19:58:54 [INFO]: Epoch 027 - training loss: 0.2819, validation loss: 0.3176
2024-05-23 19:58:55 [INFO]: Epoch 028 - training loss: 0.2821, validation loss: 0.3142
2024-05-23 19:58:55 [INFO]: Epoch 029 - training loss: 0.2785, validation loss: 0.3178
2024-05-23 19:58:56 [INFO]: Epoch 030 - training loss: 0.2707, validation loss: 0.3158
2024-05-23 19:58:57 [INFO]: Epoch 031 - training loss: 0.2692, validation loss: 0.3133
2024-05-23 19:58:57 [INFO]: Epoch 032 - training loss: 0.2589, validation loss: 0.3124
2024-05-23 19:58:58 [INFO]: Epoch 033 - training loss: 0.2553, validation loss: 0.3113
2024-05-23 19:58:58 [INFO]: Epoch 034 - training loss: 0.2541, validation loss: 0.3155
2024-05-23 19:58:59 [INFO]: Epoch 035 - training loss: 0.2488, validation loss: 0.3104
2024-05-23 19:58:59 [INFO]: Epoch 036 - training loss: 0.2503, validation loss: 0.3149
2024-05-23 19:59:00 [INFO]: Epoch 037 - training loss: 0.2444, validation loss: 0.3136
2024-05-23 19:59:01 [INFO]: Epoch 038 - training loss: 0.2384, validation loss: 0.3155
2024-05-23 19:59:01 [INFO]: Epoch 039 - training loss: 0.2369, validation loss: 0.3128
2024-05-23 19:59:02 [INFO]: Epoch 040 - training loss: 0.2351, validation loss: 0.3158
2024-05-23 19:59:02 [INFO]: Epoch 041 - training loss: 0.2353, validation loss: 0.3133
2024-05-23 19:59:03 [INFO]: Epoch 042 - training loss: 0.2267, validation loss: 0.3133
2024-05-23 19:59:04 [INFO]: Epoch 043 - training loss: 0.2235, validation loss: 0.3137
2024-05-23 19:59:04 [INFO]: Epoch 044 - training loss: 0.2232, validation loss: 0.3149
2024-05-23 19:59:05 [INFO]: Epoch 045 - training loss: 0.2227, validation loss: 0.3154
2024-05-23 19:59:05 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-05-23 19:59:05 [INFO]: Finished training. The best model is from epoch#35.
2024-05-23 19:59:05 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_1/Transformer_physionet_2012_seta/20240523_T195838/Transformer.pypots
2024-05-23 19:59:05 [INFO]: Transformer on PhysioNet-2012-seta: MAE=0.2916, MSE=0.3399
2024-05-23 19:59:05 [INFO]: Successfully saved to overlay_premask_post_norm_saved_results/round_1/Transformer_physionet_2012_seta/imputation.pkl
2024-05-23 19:59:05 [INFO]: Using the given device: cuda:0
2024-05-23 19:59:05 [INFO]: Model files will be saved to overlay_premask_post_norm_saved_results/round_1/TimesNet_physionet_2012_seta/20240523_T195905
2024-05-23 19:59:05 [INFO]: Tensorboard file will be saved to overlay_premask_post_norm_saved_results/round_1/TimesNet_physionet_2012_seta/20240523_T195905/tensorboard
2024-05-23 19:59:05 [INFO]: TimesNet initialized with the given hyperparameters, the number of trainable parameters: 21,649,573
2024-05-23 19:59:06 [INFO]: Epoch 001 - training loss: 0.4517, validation loss: 0.4125
2024-05-23 19:59:07 [INFO]: Epoch 002 - training loss: 0.6694, validation loss: 0.3366
2024-05-23 19:59:07 [INFO]: Epoch 003 - training loss: 0.3771, validation loss: 0.3238
2024-05-23 19:59:08 [INFO]: Epoch 004 - training loss: 0.4029, validation loss: 0.3201
2024-05-23 19:59:09 [INFO]: Epoch 005 - training loss: 0.3051, validation loss: 0.3233
2024-05-23 19:59:10 [INFO]: Epoch 006 - training loss: 0.2916, validation loss: 0.2979
2024-05-23 19:59:10 [INFO]: Epoch 007 - training loss: 0.2771, validation loss: 0.3089
2024-05-23 19:59:11 [INFO]: Epoch 008 - training loss: 0.2707, validation loss: 0.3078
2024-05-23 19:59:12 [INFO]: Epoch 009 - training loss: 0.2715, validation loss: 0.2970
2024-05-23 19:59:12 [INFO]: Epoch 010 - training loss: 0.2673, validation loss: 0.2980
2024-05-23 19:59:13 [INFO]: Epoch 011 - training loss: 0.2556, validation loss: 0.2999
2024-05-23 19:59:14 [INFO]: Epoch 012 - training loss: 0.2535, validation loss: 0.2948
2024-05-23 19:59:15 [INFO]: Epoch 013 - training loss: 0.2397, validation loss: 0.2960
2024-05-23 19:59:15 [INFO]: Epoch 014 - training loss: 0.2353, validation loss: 0.2913
2024-05-23 19:59:16 [INFO]: Epoch 015 - training loss: 0.2327, validation loss: 0.2908
2024-05-23 19:59:17 [INFO]: Epoch 016 - training loss: 0.2293, validation loss: 0.2993
2024-05-23 19:59:17 [INFO]: Epoch 017 - training loss: 0.2483, validation loss: 0.3084
2024-05-23 19:59:18 [INFO]: Epoch 018 - training loss: 0.2307, validation loss: 0.2989
2024-05-23 19:59:19 [INFO]: Epoch 019 - training loss: 0.2176, validation loss: 0.2975
2024-05-23 19:59:20 [INFO]: Epoch 020 - training loss: 0.2104, validation loss: 0.2999
2024-05-23 19:59:20 [INFO]: Epoch 021 - training loss: 0.2068, validation loss: 0.2999
2024-05-23 19:59:21 [INFO]: Epoch 022 - training loss: 0.2024, validation loss: 0.2951
2024-05-23 19:59:22 [INFO]: Epoch 023 - training loss: 0.2024, validation loss: 0.2981
2024-05-23 19:59:22 [INFO]: Epoch 024 - training loss: 0.2028, validation loss: 0.2959
2024-05-23 19:59:23 [INFO]: Epoch 025 - training loss: 0.1947, validation loss: 0.2956
2024-05-23 19:59:23 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-05-23 19:59:23 [INFO]: Finished training. The best model is from epoch#15.
2024-05-23 19:59:23 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_1/TimesNet_physionet_2012_seta/20240523_T195905/TimesNet.pypots
2024-05-23 19:59:23 [INFO]: TimesNet on PhysioNet-2012-seta: MAE=0.2914, MSE=0.2833
2024-05-23 19:59:23 [INFO]: Successfully saved to overlay_premask_post_norm_saved_results/round_1/TimesNet_physionet_2012_seta/imputation.pkl
2024-05-23 19:59:23 [INFO]: Using the given device: cuda:0
2024-05-23 19:59:23 [INFO]: Model files will be saved to overlay_premask_post_norm_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T195923
2024-05-23 19:59:23 [INFO]: Tensorboard file will be saved to overlay_premask_post_norm_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T195923/tensorboard
2024-05-23 19:59:23 [INFO]: CSDI initialized with the given hyperparameters, the number of trainable parameters: 1,694,753
2024-05-23 20:00:07 [INFO]: Epoch 001 - training loss: 0.4129, validation loss: 0.3315
2024-05-23 20:00:07 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T195923/CSDI_epoch1_loss0.33150248378515246.pypots
2024-05-23 20:00:50 [INFO]: Epoch 002 - training loss: 0.3151, validation loss: 0.2975
2024-05-23 20:00:50 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T195923/CSDI_epoch2_loss0.2974609375.pypots
2024-05-23 20:01:34 [INFO]: Epoch 003 - training loss: 0.2898, validation loss: 0.2740
2024-05-23 20:01:34 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T195923/CSDI_epoch3_loss0.27401086688041687.pypots
2024-05-23 20:02:17 [INFO]: Epoch 004 - training loss: 0.2736, validation loss: 0.2584
2024-05-23 20:02:17 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T195923/CSDI_epoch4_loss0.2584323488175869.pypots
2024-05-23 20:03:01 [INFO]: Epoch 005 - training loss: 0.2580, validation loss: 0.2440
2024-05-23 20:03:01 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T195923/CSDI_epoch5_loss0.24402216225862502.pypots
2024-05-23 20:03:45 [INFO]: Epoch 006 - training loss: 0.2475, validation loss: 0.2358
2024-05-23 20:03:45 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T195923/CSDI_epoch6_loss0.2358467735350132.pypots
2024-05-23 20:04:28 [INFO]: Epoch 007 - training loss: 0.2515, validation loss: 0.2351
2024-05-23 20:04:28 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T195923/CSDI_epoch7_loss0.23507606387138366.pypots
2024-05-23 20:05:12 [INFO]: Epoch 008 - training loss: 0.2394, validation loss: 0.2232
2024-05-23 20:05:12 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T195923/CSDI_epoch8_loss0.22319992929697036.pypots
2024-05-23 20:05:56 [INFO]: Epoch 009 - training loss: 0.2290, validation loss: 0.2234
2024-05-23 20:05:56 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T195923/CSDI_epoch9_loss0.2233748935163021.pypots
2024-05-23 20:06:39 [INFO]: Epoch 010 - training loss: 0.2297, validation loss: 0.2178
2024-05-23 20:06:40 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T195923/CSDI_epoch10_loss0.21777549460530282.pypots
2024-05-23 20:07:23 [INFO]: Epoch 011 - training loss: 0.2210, validation loss: 0.2176
2024-05-23 20:07:23 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T195923/CSDI_epoch11_loss0.2176022171974182.pypots
2024-05-23 20:08:07 [INFO]: Epoch 012 - training loss: 0.2228, validation loss: 0.2175
2024-05-23 20:08:07 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T195923/CSDI_epoch12_loss0.21751099824905396.pypots
2024-05-23 20:08:51 [INFO]: Epoch 013 - training loss: 0.2174, validation loss: 0.2141
2024-05-23 20:08:51 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T195923/CSDI_epoch13_loss0.21414131075143814.pypots
2024-05-23 20:09:35 [INFO]: Epoch 014 - training loss: 0.2182, validation loss: 0.2104
2024-05-23 20:09:35 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T195923/CSDI_epoch14_loss0.21037398055195808.pypots
2024-05-23 20:10:19 [INFO]: Epoch 015 - training loss: 0.2146, validation loss: 0.2065
2024-05-23 20:10:19 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T195923/CSDI_epoch15_loss0.2064696244895458.pypots
2024-05-23 20:11:03 [INFO]: Epoch 016 - training loss: 0.2130, validation loss: 0.2057
2024-05-23 20:11:03 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T195923/CSDI_epoch16_loss0.20572960004210472.pypots
2024-05-23 20:11:47 [INFO]: Epoch 017 - training loss: 0.2116, validation loss: 0.2043
2024-05-23 20:11:47 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T195923/CSDI_epoch17_loss0.20432205572724343.pypots
2024-05-23 20:12:30 [INFO]: Epoch 018 - training loss: 0.2171, validation loss: 0.2084
2024-05-23 20:12:30 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T195923/CSDI_epoch18_loss0.20841663852334022.pypots
2024-05-23 20:13:14 [INFO]: Epoch 019 - training loss: 0.2133, validation loss: 0.2047
2024-05-23 20:13:14 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T195923/CSDI_epoch19_loss0.2047003075480461.pypots
2024-05-23 20:13:58 [INFO]: Epoch 020 - training loss: 0.2035, validation loss: 0.2047
2024-05-23 20:13:58 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T195923/CSDI_epoch20_loss0.20471544861793517.pypots
2024-05-23 20:14:42 [INFO]: Epoch 021 - training loss: 0.2195, validation loss: 0.1982
2024-05-23 20:14:42 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T195923/CSDI_epoch21_loss0.1981846161186695.pypots
2024-05-23 20:15:26 [INFO]: Epoch 022 - training loss: 0.2100, validation loss: 0.1971
2024-05-23 20:15:26 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T195923/CSDI_epoch22_loss0.19705064818263054.pypots
2024-05-23 20:16:10 [INFO]: Epoch 023 - training loss: 0.2072, validation loss: 0.2008
2024-05-23 20:16:10 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T195923/CSDI_epoch23_loss0.20077284872531892.pypots
2024-05-23 20:16:53 [INFO]: Epoch 024 - training loss: 0.2045, validation loss: 0.1962
2024-05-23 20:16:54 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T195923/CSDI_epoch24_loss0.19620058983564376.pypots
2024-05-23 20:17:37 [INFO]: Epoch 025 - training loss: 0.2106, validation loss: 0.1977
2024-05-23 20:17:37 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T195923/CSDI_epoch25_loss0.19774695634841918.pypots
2024-05-23 20:18:21 [INFO]: Epoch 026 - training loss: 0.2100, validation loss: 0.1958
2024-05-23 20:18:21 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T195923/CSDI_epoch26_loss0.19578128680586815.pypots
2024-05-23 20:19:05 [INFO]: Epoch 027 - training loss: 0.2104, validation loss: 0.1979
2024-05-23 20:19:05 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T195923/CSDI_epoch27_loss0.19786906763911247.pypots
2024-05-23 20:19:49 [INFO]: Epoch 028 - training loss: 0.1993, validation loss: 0.1970
2024-05-23 20:19:49 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T195923/CSDI_epoch28_loss0.19695902317762376.pypots
2024-05-23 20:20:33 [INFO]: Epoch 029 - training loss: 0.2055, validation loss: 0.1946
2024-05-23 20:20:33 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T195923/CSDI_epoch29_loss0.1946206085383892.pypots
2024-05-23 20:21:16 [INFO]: Epoch 030 - training loss: 0.1974, validation loss: 0.1961
2024-05-23 20:21:16 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T195923/CSDI_epoch30_loss0.19609179794788362.pypots
2024-05-23 20:22:00 [INFO]: Epoch 031 - training loss: 0.2096, validation loss: 0.1940
2024-05-23 20:22:00 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T195923/CSDI_epoch31_loss0.1939658023416996.pypots
2024-05-23 20:22:44 [INFO]: Epoch 032 - training loss: 0.2116, validation loss: 0.1951
2024-05-23 20:22:44 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T195923/CSDI_epoch32_loss0.19507728070020675.pypots
2024-05-23 20:23:28 [INFO]: Epoch 033 - training loss: 0.2065, validation loss: 0.1935
2024-05-23 20:23:28 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T195923/CSDI_epoch33_loss0.19349486082792283.pypots
2024-05-23 20:24:12 [INFO]: Epoch 034 - training loss: 0.1877, validation loss: 0.1931
2024-05-23 20:24:12 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T195923/CSDI_epoch34_loss0.19311586320400237.pypots
2024-05-23 20:24:55 [INFO]: Epoch 035 - training loss: 0.1941, validation loss: 0.1927
2024-05-23 20:24:55 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T195923/CSDI_epoch35_loss0.19266885071992873.pypots
2024-05-23 20:25:39 [INFO]: Epoch 036 - training loss: 0.1875, validation loss: 0.1944
2024-05-23 20:25:39 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T195923/CSDI_epoch36_loss0.19442360177636148.pypots
2024-05-23 20:26:23 [INFO]: Epoch 037 - training loss: 0.2068, validation loss: 0.1928
2024-05-23 20:26:23 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T195923/CSDI_epoch37_loss0.19276112839579582.pypots
2024-05-23 20:27:06 [INFO]: Epoch 038 - training loss: 0.1946, validation loss: 0.1973
2024-05-23 20:27:06 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T195923/CSDI_epoch38_loss0.19728718772530557.pypots
2024-05-23 20:27:50 [INFO]: Epoch 039 - training loss: 0.1947, validation loss: 0.1919
2024-05-23 20:27:50 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T195923/CSDI_epoch39_loss0.19194526672363282.pypots
2024-05-23 20:28:34 [INFO]: Epoch 040 - training loss: 0.2030, validation loss: 0.1922
2024-05-23 20:28:34 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T195923/CSDI_epoch40_loss0.19222562983632088.pypots
2024-05-23 20:29:18 [INFO]: Epoch 041 - training loss: 0.2017, validation loss: 0.1903
2024-05-23 20:29:18 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T195923/CSDI_epoch41_loss0.1903268501162529.pypots
2024-05-23 20:30:01 [INFO]: Epoch 042 - training loss: 0.1950, validation loss: 0.1945
2024-05-23 20:30:01 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T195923/CSDI_epoch42_loss0.19449841380119323.pypots
2024-05-23 20:30:45 [INFO]: Epoch 043 - training loss: 0.1966, validation loss: 0.1885
2024-05-23 20:30:45 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T195923/CSDI_epoch43_loss0.18849576711654664.pypots
2024-05-23 20:31:29 [INFO]: Epoch 044 - training loss: 0.1989, validation loss: 0.1941
2024-05-23 20:31:29 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T195923/CSDI_epoch44_loss0.1941247306764126.pypots
2024-05-23 20:32:12 [INFO]: Epoch 045 - training loss: 0.1978, validation loss: 0.1898
2024-05-23 20:32:12 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T195923/CSDI_epoch45_loss0.18983240723609923.pypots
2024-05-23 20:32:56 [INFO]: Epoch 046 - training loss: 0.1850, validation loss: 0.1883
2024-05-23 20:32:56 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T195923/CSDI_epoch46_loss0.18829638063907622.pypots
2024-05-23 20:33:40 [INFO]: Epoch 047 - training loss: 0.1912, validation loss: 0.1895
2024-05-23 20:33:40 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T195923/CSDI_epoch47_loss0.18945866376161574.pypots
2024-05-23 20:34:24 [INFO]: Epoch 048 - training loss: 0.1988, validation loss: 0.1911
2024-05-23 20:34:24 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T195923/CSDI_epoch48_loss0.19109826534986496.pypots
2024-05-23 20:35:07 [INFO]: Epoch 049 - training loss: 0.1956, validation loss: 0.1879
2024-05-23 20:35:07 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T195923/CSDI_epoch49_loss0.18793482482433319.pypots
2024-05-23 20:35:51 [INFO]: Epoch 050 - training loss: 0.1981, validation loss: 0.1913
2024-05-23 20:35:51 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T195923/CSDI_epoch50_loss0.19128895401954651.pypots
2024-05-23 20:36:35 [INFO]: Epoch 051 - training loss: 0.1999, validation loss: 0.1890
2024-05-23 20:36:35 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T195923/CSDI_epoch51_loss0.1889624647796154.pypots
2024-05-23 20:37:19 [INFO]: Epoch 052 - training loss: 0.1880, validation loss: 0.1886
2024-05-23 20:37:19 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T195923/CSDI_epoch52_loss0.18856547251343728.pypots
2024-05-23 20:38:02 [INFO]: Epoch 053 - training loss: 0.1919, validation loss: 0.1868
2024-05-23 20:38:02 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T195923/CSDI_epoch53_loss0.18680304363369943.pypots
2024-05-23 20:38:46 [INFO]: Epoch 054 - training loss: 0.1949, validation loss: 0.1883
2024-05-23 20:38:46 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T195923/CSDI_epoch54_loss0.1883240148425102.pypots
2024-05-23 20:39:30 [INFO]: Epoch 055 - training loss: 0.1915, validation loss: 0.1871
2024-05-23 20:39:30 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T195923/CSDI_epoch55_loss0.18712258040905.pypots
2024-05-23 20:40:13 [INFO]: Epoch 056 - training loss: 0.1943, validation loss: 0.1887
2024-05-23 20:40:13 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T195923/CSDI_epoch56_loss0.18872494772076606.pypots
2024-05-23 20:40:57 [INFO]: Epoch 057 - training loss: 0.2087, validation loss: 0.1903
2024-05-23 20:40:57 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T195923/CSDI_epoch57_loss0.19028549045324325.pypots
2024-05-23 20:41:41 [INFO]: Epoch 058 - training loss: 0.1838, validation loss: 0.1879
2024-05-23 20:41:41 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T195923/CSDI_epoch58_loss0.1878955826163292.pypots
2024-05-23 20:42:24 [INFO]: Epoch 059 - training loss: 0.1973, validation loss: 0.1913
2024-05-23 20:42:25 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T195923/CSDI_epoch59_loss0.1912925899028778.pypots
2024-05-23 20:43:08 [INFO]: Epoch 060 - training loss: 0.1972, validation loss: 0.1868
2024-05-23 20:43:08 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T195923/CSDI_epoch60_loss0.18680757209658622.pypots
2024-05-23 20:43:52 [INFO]: Epoch 061 - training loss: 0.1832, validation loss: 0.1883
2024-05-23 20:43:52 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T195923/CSDI_epoch61_loss0.18832604363560676.pypots
2024-05-23 20:44:36 [INFO]: Epoch 062 - training loss: 0.1993, validation loss: 0.1864
2024-05-23 20:44:36 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T195923/CSDI_epoch62_loss0.18638161718845367.pypots
2024-05-23 20:45:19 [INFO]: Epoch 063 - training loss: 0.1849, validation loss: 0.1870
2024-05-23 20:45:19 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T195923/CSDI_epoch63_loss0.18702252954244614.pypots
2024-05-23 20:46:03 [INFO]: Epoch 064 - training loss: 0.2030, validation loss: 0.1882
2024-05-23 20:46:03 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T195923/CSDI_epoch64_loss0.18816352710127832.pypots
2024-05-23 20:46:47 [INFO]: Epoch 065 - training loss: 0.1851, validation loss: 0.1864
2024-05-23 20:46:47 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T195923/CSDI_epoch65_loss0.1863879829645157.pypots
2024-05-23 20:47:31 [INFO]: Epoch 066 - training loss: 0.1939, validation loss: 0.1861
2024-05-23 20:47:31 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T195923/CSDI_epoch66_loss0.18608960807323455.pypots
2024-05-23 20:48:14 [INFO]: Epoch 067 - training loss: 0.1896, validation loss: 0.1833
2024-05-23 20:48:14 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T195923/CSDI_epoch67_loss0.18334840089082718.pypots
2024-05-23 20:48:58 [INFO]: Epoch 068 - training loss: 0.1939, validation loss: 0.1873
2024-05-23 20:48:58 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T195923/CSDI_epoch68_loss0.18727679178118706.pypots
2024-05-23 20:49:42 [INFO]: Epoch 069 - training loss: 0.1987, validation loss: 0.1835
2024-05-23 20:49:42 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T195923/CSDI_epoch69_loss0.18350099325180053.pypots
2024-05-23 20:50:26 [INFO]: Epoch 070 - training loss: 0.1951, validation loss: 0.1876
2024-05-23 20:50:26 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T195923/CSDI_epoch70_loss0.18764769583940505.pypots
2024-05-23 20:51:10 [INFO]: Epoch 071 - training loss: 0.1867, validation loss: 0.1841
2024-05-23 20:51:10 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T195923/CSDI_epoch71_loss0.18405505865812302.pypots
2024-05-23 20:51:54 [INFO]: Epoch 072 - training loss: 0.2047, validation loss: 0.1875
2024-05-23 20:51:54 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T195923/CSDI_epoch72_loss0.18749155923724176.pypots
2024-05-23 20:52:38 [INFO]: Epoch 073 - training loss: 0.2081, validation loss: 0.1839
2024-05-23 20:52:38 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T195923/CSDI_epoch73_loss0.18388744071125984.pypots
2024-05-23 20:53:21 [INFO]: Epoch 074 - training loss: 0.1920, validation loss: 0.1877
2024-05-23 20:53:21 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T195923/CSDI_epoch74_loss0.1877368062734604.pypots
2024-05-23 20:54:05 [INFO]: Epoch 075 - training loss: 0.1798, validation loss: 0.1859
2024-05-23 20:54:05 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T195923/CSDI_epoch75_loss0.18586532324552535.pypots
2024-05-23 20:54:49 [INFO]: Epoch 076 - training loss: 0.1895, validation loss: 0.1839
2024-05-23 20:54:49 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T195923/CSDI_epoch76_loss0.1839266911149025.pypots
2024-05-23 20:55:33 [INFO]: Epoch 077 - training loss: 0.1883, validation loss: 0.1848
2024-05-23 20:55:33 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T195923/CSDI_epoch77_loss0.18476345986127854.pypots
2024-05-23 20:55:33 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-05-23 20:55:33 [INFO]: Finished training. The best model is from epoch#67.
2024-05-23 20:55:33 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T195923/CSDI.pypots
2024-05-23 21:02:55 [INFO]: CSDI on PhysioNet-2012-seta: MAE=0.2482, MSE=0.5595
2024-05-23 21:32:17 [INFO]: Successfully saved to overlay_premask_post_norm_saved_results/round_1/CSDI_physionet_2012_seta/imputation.pkl
2024-05-23 21:32:17 [INFO]: Using the given device: cuda:0
2024-05-23 21:32:17 [INFO]: Model files will be saved to overlay_premask_post_norm_saved_results/round_1/GPVAE_physionet_2012_seta/20240523_T213217
2024-05-23 21:32:17 [INFO]: Tensorboard file will be saved to overlay_premask_post_norm_saved_results/round_1/GPVAE_physionet_2012_seta/20240523_T213217/tensorboard
2024-05-23 21:32:17 [INFO]: GPVAE initialized with the given hyperparameters, the number of trainable parameters: 1,028,244
2024-05-23 21:32:18 [INFO]: Epoch 001 - training loss: 42457.4927, validation loss: 0.9097
2024-05-23 21:32:18 [INFO]: Epoch 002 - training loss: 24424.2755, validation loss: 0.7326
2024-05-23 21:32:19 [INFO]: Epoch 003 - training loss: 23481.5011, validation loss: 0.7125
2024-05-23 21:32:19 [INFO]: Epoch 004 - training loss: 23181.3705, validation loss: 0.6801
2024-05-23 21:32:20 [INFO]: Epoch 005 - training loss: 23034.3502, validation loss: 0.6484
2024-05-23 21:32:21 [INFO]: Epoch 006 - training loss: 22953.4263, validation loss: 0.6453
2024-05-23 21:32:21 [INFO]: Epoch 007 - training loss: 22903.4381, validation loss: 0.6459
2024-05-23 21:32:22 [INFO]: Epoch 008 - training loss: 22871.9409, validation loss: 0.6400
2024-05-23 21:32:23 [INFO]: Epoch 009 - training loss: 22851.5450, validation loss: 0.6372
2024-05-23 21:32:23 [INFO]: Epoch 010 - training loss: 22836.2551, validation loss: 0.6337
2024-05-23 21:32:24 [INFO]: Epoch 011 - training loss: 22825.5158, validation loss: 0.6400
2024-05-23 21:32:24 [INFO]: Epoch 012 - training loss: 22817.6479, validation loss: 0.6350
2024-05-23 21:32:25 [INFO]: Epoch 013 - training loss: 22812.2062, validation loss: 0.6289
2024-05-23 21:32:26 [INFO]: Epoch 014 - training loss: 22808.1624, validation loss: 0.6269
2024-05-23 21:32:26 [INFO]: Epoch 015 - training loss: 22803.9274, validation loss: 0.6510
2024-05-23 21:32:27 [INFO]: Epoch 016 - training loss: 22801.0790, validation loss: 0.6250
2024-05-23 21:32:28 [INFO]: Epoch 017 - training loss: 22797.8801, validation loss: 0.6267
2024-05-23 21:32:28 [INFO]: Epoch 018 - training loss: 22795.4788, validation loss: 0.6228
2024-05-23 21:32:29 [INFO]: Epoch 019 - training loss: 22793.1906, validation loss: 0.6117
2024-05-23 21:32:29 [INFO]: Epoch 020 - training loss: 22791.1402, validation loss: 0.6094
2024-05-23 21:32:30 [INFO]: Epoch 021 - training loss: 22790.2170, validation loss: 0.6090
2024-05-23 21:32:31 [INFO]: Epoch 022 - training loss: 22789.4676, validation loss: 0.6050
2024-05-23 21:32:31 [INFO]: Epoch 023 - training loss: 22788.8551, validation loss: 0.6158
2024-05-23 21:32:32 [INFO]: Epoch 024 - training loss: 22787.7289, validation loss: 0.6052
2024-05-23 21:32:32 [INFO]: Epoch 025 - training loss: 22785.6828, validation loss: 0.6024
2024-05-23 21:32:33 [INFO]: Epoch 026 - training loss: 22785.6492, validation loss: 0.6021
2024-05-23 21:32:34 [INFO]: Epoch 027 - training loss: 22784.8924, validation loss: 0.6008
2024-05-23 21:32:34 [INFO]: Epoch 028 - training loss: 22783.8257, validation loss: 0.6107
2024-05-23 21:32:35 [INFO]: Epoch 029 - training loss: 22783.4658, validation loss: 0.5998
2024-05-23 21:32:36 [INFO]: Epoch 030 - training loss: 22783.3974, validation loss: 0.5956
2024-05-23 21:32:36 [INFO]: Epoch 031 - training loss: 22783.4590, validation loss: 0.5955
2024-05-23 21:32:37 [INFO]: Epoch 032 - training loss: 22782.4023, validation loss: 0.5947
2024-05-23 21:32:37 [INFO]: Epoch 033 - training loss: 22782.4976, validation loss: 0.6021
2024-05-23 21:32:38 [INFO]: Epoch 034 - training loss: 22781.7107, validation loss: 0.5929
2024-05-23 21:32:39 [INFO]: Epoch 035 - training loss: 22781.1497, validation loss: 0.5927
2024-05-23 21:32:39 [INFO]: Epoch 036 - training loss: 22780.5795, validation loss: 0.5936
2024-05-23 21:32:40 [INFO]: Epoch 037 - training loss: 22780.6620, validation loss: 0.5908
2024-05-23 21:32:41 [INFO]: Epoch 038 - training loss: 22780.9289, validation loss: 0.5906
2024-05-23 21:32:41 [INFO]: Epoch 039 - training loss: 22780.2753, validation loss: 0.5897
2024-05-23 21:32:42 [INFO]: Epoch 040 - training loss: 22778.4354, validation loss: 0.6036
2024-05-23 21:32:42 [INFO]: Epoch 041 - training loss: 22778.6946, validation loss: 0.5862
2024-05-23 21:32:43 [INFO]: Epoch 042 - training loss: 22777.5612, validation loss: 0.6014
2024-05-23 21:32:44 [INFO]: Epoch 043 - training loss: 22777.4368, validation loss: 0.5881
2024-05-23 21:32:44 [INFO]: Epoch 044 - training loss: 22776.1374, validation loss: 0.5806
2024-05-23 21:32:45 [INFO]: Epoch 045 - training loss: 22775.6545, validation loss: 0.5771
2024-05-23 21:32:45 [INFO]: Epoch 046 - training loss: 22774.3650, validation loss: 0.5707
2024-05-23 21:32:46 [INFO]: Epoch 047 - training loss: 22772.9397, validation loss: 0.5695
2024-05-23 21:32:47 [INFO]: Epoch 048 - training loss: 22772.0528, validation loss: 0.5636
2024-05-23 21:32:47 [INFO]: Epoch 049 - training loss: 22770.3792, validation loss: 0.5930
2024-05-23 21:32:48 [INFO]: Epoch 050 - training loss: 22770.4302, validation loss: 0.5511
2024-05-23 21:32:48 [INFO]: Epoch 051 - training loss: 22769.7678, validation loss: 0.5463
2024-05-23 21:32:49 [INFO]: Epoch 052 - training loss: 22767.0547, validation loss: 0.5405
2024-05-23 21:32:50 [INFO]: Epoch 053 - training loss: 22766.0577, validation loss: 0.5316
2024-05-23 21:32:50 [INFO]: Epoch 054 - training loss: 22765.6867, validation loss: 0.5332
2024-05-23 21:32:51 [INFO]: Epoch 055 - training loss: 22764.2059, validation loss: 0.5410
2024-05-23 21:32:52 [INFO]: Epoch 056 - training loss: 22763.3126, validation loss: 0.5249
2024-05-23 21:32:52 [INFO]: Epoch 057 - training loss: 22762.7291, validation loss: 0.5261
2024-05-23 21:32:53 [INFO]: Epoch 058 - training loss: 22762.6154, validation loss: 0.5194
2024-05-23 21:32:53 [INFO]: Epoch 059 - training loss: 22761.4876, validation loss: 0.5603
2024-05-23 21:32:54 [INFO]: Epoch 060 - training loss: 22762.4697, validation loss: 0.5246
2024-05-23 21:32:55 [INFO]: Epoch 061 - training loss: 22760.0497, validation loss: 0.5426
2024-05-23 21:32:55 [INFO]: Epoch 062 - training loss: 22761.6943, validation loss: 0.5109
2024-05-23 21:32:56 [INFO]: Epoch 063 - training loss: 22760.3827, validation loss: 0.5160
2024-05-23 21:32:57 [INFO]: Epoch 064 - training loss: 22758.6331, validation loss: 0.5044
2024-05-23 21:32:57 [INFO]: Epoch 065 - training loss: 22758.1747, validation loss: 0.5208
2024-05-23 21:32:58 [INFO]: Epoch 066 - training loss: 22758.1147, validation loss: 0.5029
2024-05-23 21:32:58 [INFO]: Epoch 067 - training loss: 22757.6218, validation loss: 0.5078
2024-05-23 21:32:59 [INFO]: Epoch 068 - training loss: 22756.5750, validation loss: 0.5039
2024-05-23 21:33:00 [INFO]: Epoch 069 - training loss: 22756.5913, validation loss: 0.5146
2024-05-23 21:33:00 [INFO]: Epoch 070 - training loss: 22757.0189, validation loss: 0.5090
2024-05-23 21:33:01 [INFO]: Epoch 071 - training loss: 22756.9865, validation loss: 0.4968
2024-05-23 21:33:01 [INFO]: Epoch 072 - training loss: 22755.0121, validation loss: 0.4957
2024-05-23 21:33:02 [INFO]: Epoch 073 - training loss: 22756.1220, validation loss: 0.4964
2024-05-23 21:33:03 [INFO]: Epoch 074 - training loss: 22754.7216, validation loss: 0.5040
2024-05-23 21:33:03 [INFO]: Epoch 075 - training loss: 22754.4998, validation loss: 0.4966
2024-05-23 21:33:04 [INFO]: Epoch 076 - training loss: 22753.5859, validation loss: 0.4927
2024-05-23 21:33:05 [INFO]: Epoch 077 - training loss: 22753.2100, validation loss: 0.4912
2024-05-23 21:33:05 [INFO]: Epoch 078 - training loss: 22753.2134, validation loss: 0.4920
2024-05-23 21:33:06 [INFO]: Epoch 079 - training loss: 22752.8446, validation loss: 0.4901
2024-05-23 21:33:06 [INFO]: Epoch 080 - training loss: 22752.6869, validation loss: 0.4920
2024-05-23 21:33:07 [INFO]: Epoch 081 - training loss: 22752.2104, validation loss: 0.5004
2024-05-23 21:33:08 [INFO]: Epoch 082 - training loss: 22752.6427, validation loss: 0.4919
2024-05-23 21:33:08 [INFO]: Epoch 083 - training loss: 22752.2682, validation loss: 0.4858
2024-05-23 21:33:09 [INFO]: Epoch 084 - training loss: 22751.9612, validation loss: 0.4859
2024-05-23 21:33:09 [INFO]: Epoch 085 - training loss: 22752.1599, validation loss: 0.4883
2024-05-23 21:33:10 [INFO]: Epoch 086 - training loss: 22752.3424, validation loss: 0.4888
2024-05-23 21:33:11 [INFO]: Epoch 087 - training loss: 22751.6990, validation loss: 0.4884
2024-05-23 21:33:11 [INFO]: Epoch 088 - training loss: 22751.1981, validation loss: 0.4841
2024-05-23 21:33:12 [INFO]: Epoch 089 - training loss: 22750.9405, validation loss: 0.4818
2024-05-23 21:33:13 [INFO]: Epoch 090 - training loss: 22751.6578, validation loss: 0.4837
2024-05-23 21:33:13 [INFO]: Epoch 091 - training loss: 22750.8078, validation loss: 0.4847
2024-05-23 21:33:14 [INFO]: Epoch 092 - training loss: 22751.4566, validation loss: 0.4865
2024-05-23 21:33:14 [INFO]: Epoch 093 - training loss: 22751.7385, validation loss: 0.4879
2024-05-23 21:33:15 [INFO]: Epoch 094 - training loss: 22751.5431, validation loss: 0.4860
2024-05-23 21:33:16 [INFO]: Epoch 095 - training loss: 22751.5473, validation loss: 0.4823
2024-05-23 21:33:16 [INFO]: Epoch 096 - training loss: 22750.4541, validation loss: 0.4831
2024-05-23 21:33:17 [INFO]: Epoch 097 - training loss: 22750.6258, validation loss: 0.4875
2024-05-23 21:33:17 [INFO]: Epoch 098 - training loss: 22750.9109, validation loss: 0.4831
2024-05-23 21:33:18 [INFO]: Epoch 099 - training loss: 22749.7275, validation loss: 0.4879
2024-05-23 21:33:18 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-05-23 21:33:18 [INFO]: Finished training. The best model is from epoch#89.
2024-05-23 21:33:18 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_1/GPVAE_physionet_2012_seta/20240523_T213217/GPVAE.pypots
2024-05-23 21:33:18 [INFO]: GP-VAE on PhysioNet-2012-seta: MAE=0.4283, MSE=0.4951
2024-05-23 21:33:19 [INFO]: Successfully saved to overlay_premask_post_norm_saved_results/round_1/GPVAE_physionet_2012_seta/imputation.pkl
2024-05-23 21:33:19 [INFO]: Using the given device: cuda:0
2024-05-23 21:33:19 [INFO]: Model files will be saved to overlay_premask_post_norm_saved_results/round_1/USGAN_physionet_2012_seta/20240523_T213319
2024-05-23 21:33:19 [INFO]: Tensorboard file will be saved to overlay_premask_post_norm_saved_results/round_1/USGAN_physionet_2012_seta/20240523_T213319/tensorboard
2024-05-23 21:33:19 [INFO]: USGAN initialized with the given hyperparameters, the number of trainable parameters: 16,010,261
2024-05-23 21:33:41 [INFO]: Epoch 001 - generator training loss: 0.6135, discriminator training loss: 0.3836, validation loss: 0.6264
2024-05-23 21:34:00 [INFO]: Epoch 002 - generator training loss: 0.4866, discriminator training loss: 0.2721, validation loss: 0.5362
2024-05-23 21:34:18 [INFO]: Epoch 003 - generator training loss: 0.4410, discriminator training loss: 0.2353, validation loss: 0.5147
2024-05-23 21:34:37 [INFO]: Epoch 004 - generator training loss: 0.4510, discriminator training loss: 0.1895, validation loss: 0.5074
2024-05-23 21:34:56 [INFO]: Epoch 005 - generator training loss: 0.4592, discriminator training loss: 0.1585, validation loss: 0.5002
2024-05-23 21:35:15 [INFO]: Epoch 006 - generator training loss: 0.4521, discriminator training loss: 0.1393, validation loss: 0.4844
2024-05-23 21:35:34 [INFO]: Epoch 007 - generator training loss: 0.4355, discriminator training loss: 0.1249, validation loss: 0.4760
2024-05-23 21:35:53 [INFO]: Epoch 008 - generator training loss: 0.4213, discriminator training loss: 0.1147, validation loss: 0.4600
2024-05-23 21:36:12 [INFO]: Epoch 009 - generator training loss: 0.4106, discriminator training loss: 0.1064, validation loss: 0.4489
2024-05-23 21:36:31 [INFO]: Epoch 010 - generator training loss: 0.4010, discriminator training loss: 0.0996, validation loss: 0.4395
2024-05-23 21:36:50 [INFO]: Epoch 011 - generator training loss: 0.3956, discriminator training loss: 0.0934, validation loss: 0.4381
2024-05-23 21:37:09 [INFO]: Epoch 012 - generator training loss: 0.3900, discriminator training loss: 0.0883, validation loss: 0.4299
2024-05-23 21:37:28 [INFO]: Epoch 013 - generator training loss: 0.3847, discriminator training loss: 0.0837, validation loss: 0.4214
2024-05-23 21:37:47 [INFO]: Epoch 014 - generator training loss: 0.3803, discriminator training loss: 0.0799, validation loss: 0.4188
2024-05-23 21:38:06 [INFO]: Epoch 015 - generator training loss: 0.3759, discriminator training loss: 0.0764, validation loss: 0.4120
2024-05-23 21:38:25 [INFO]: Epoch 016 - generator training loss: 0.3702, discriminator training loss: 0.0732, validation loss: 0.4083
2024-05-23 21:38:45 [INFO]: Epoch 017 - generator training loss: 0.3667, discriminator training loss: 0.0705, validation loss: 0.4034
2024-05-23 21:39:06 [INFO]: Epoch 018 - generator training loss: 0.3620, discriminator training loss: 0.0680, validation loss: 0.4018
2024-05-23 21:39:26 [INFO]: Epoch 019 - generator training loss: 0.3574, discriminator training loss: 0.0658, validation loss: 0.3959
2024-05-23 21:39:46 [INFO]: Epoch 020 - generator training loss: 0.3547, discriminator training loss: 0.0638, validation loss: 0.3933
2024-05-23 21:40:06 [INFO]: Epoch 021 - generator training loss: 0.3501, discriminator training loss: 0.0619, validation loss: 0.3890
2024-05-23 21:40:27 [INFO]: Epoch 022 - generator training loss: 0.3456, discriminator training loss: 0.0601, validation loss: 0.3848
2024-05-23 21:40:47 [INFO]: Epoch 023 - generator training loss: 0.3399, discriminator training loss: 0.0587, validation loss: 0.3793
2024-05-23 21:41:06 [INFO]: Epoch 024 - generator training loss: 0.3359, discriminator training loss: 0.0571, validation loss: 0.3772
2024-05-23 21:41:25 [INFO]: Epoch 025 - generator training loss: 0.3317, discriminator training loss: 0.0561, validation loss: 0.3734
2024-05-23 21:41:44 [INFO]: Epoch 026 - generator training loss: 0.3269, discriminator training loss: 0.0549, validation loss: 0.3757
2024-05-23 21:42:03 [INFO]: Epoch 027 - generator training loss: 0.3257, discriminator training loss: 0.0540, validation loss: 0.3668
2024-05-23 21:42:22 [INFO]: Epoch 028 - generator training loss: 0.3208, discriminator training loss: 0.0528, validation loss: 0.3669
2024-05-23 21:42:41 [INFO]: Epoch 029 - generator training loss: 0.3142, discriminator training loss: 0.0521, validation loss: 0.3595
2024-05-23 21:43:00 [INFO]: Epoch 030 - generator training loss: 0.3089, discriminator training loss: 0.0515, validation loss: 0.3548
2024-05-23 21:43:19 [INFO]: Epoch 031 - generator training loss: 0.3063, discriminator training loss: 0.0505, validation loss: 0.3519
2024-05-23 21:43:38 [INFO]: Epoch 032 - generator training loss: 0.3040, discriminator training loss: 0.0501, validation loss: 0.3571
2024-05-23 21:43:57 [INFO]: Epoch 033 - generator training loss: 0.3048, discriminator training loss: 0.0495, validation loss: 0.3480
2024-05-23 21:44:16 [INFO]: Epoch 034 - generator training loss: 0.2972, discriminator training loss: 0.0489, validation loss: 0.3433
2024-05-23 21:44:35 [INFO]: Epoch 035 - generator training loss: 0.2929, discriminator training loss: 0.0485, validation loss: 0.3364
2024-05-23 21:44:54 [INFO]: Epoch 036 - generator training loss: 0.2860, discriminator training loss: 0.0480, validation loss: 0.3386
2024-05-23 21:45:13 [INFO]: Epoch 037 - generator training loss: 0.2835, discriminator training loss: 0.0474, validation loss: 0.3335
2024-05-23 21:45:32 [INFO]: Epoch 038 - generator training loss: 0.2785, discriminator training loss: 0.0472, validation loss: 0.3306
2024-05-23 21:45:51 [INFO]: Epoch 039 - generator training loss: 0.2751, discriminator training loss: 0.0468, validation loss: 0.3276
2024-05-23 21:46:10 [INFO]: Epoch 040 - generator training loss: 0.2719, discriminator training loss: 0.0464, validation loss: 0.3256
2024-05-23 21:46:29 [INFO]: Epoch 041 - generator training loss: 0.2710, discriminator training loss: 0.0460, validation loss: 0.3297
2024-05-23 21:46:48 [INFO]: Epoch 042 - generator training loss: 0.2760, discriminator training loss: 0.0460, validation loss: 0.3248
2024-05-23 21:47:07 [INFO]: Epoch 043 - generator training loss: 0.2680, discriminator training loss: 0.0457, validation loss: 0.3209
2024-05-23 21:47:26 [INFO]: Epoch 044 - generator training loss: 0.2615, discriminator training loss: 0.0453, validation loss: 0.3197
2024-05-23 21:47:45 [INFO]: Epoch 045 - generator training loss: 0.2573, discriminator training loss: 0.0452, validation loss: 0.3193
2024-05-23 21:48:04 [INFO]: Epoch 046 - generator training loss: 0.2570, discriminator training loss: 0.0449, validation loss: 0.3182
2024-05-23 21:48:23 [INFO]: Epoch 047 - generator training loss: 0.2579, discriminator training loss: 0.0446, validation loss: 0.3134
2024-05-23 21:48:42 [INFO]: Epoch 048 - generator training loss: 0.2532, discriminator training loss: 0.0445, validation loss: 0.3118
2024-05-23 21:49:01 [INFO]: Epoch 049 - generator training loss: 0.2539, discriminator training loss: 0.0443, validation loss: 0.3166
2024-05-23 21:49:20 [INFO]: Epoch 050 - generator training loss: 0.2494, discriminator training loss: 0.0442, validation loss: 0.3102
2024-05-23 21:49:39 [INFO]: Epoch 051 - generator training loss: 0.2444, discriminator training loss: 0.0440, validation loss: 0.3091
2024-05-23 21:49:57 [INFO]: Epoch 052 - generator training loss: 0.2424, discriminator training loss: 0.0436, validation loss: 0.3113
2024-05-23 21:50:16 [INFO]: Epoch 053 - generator training loss: 0.2400, discriminator training loss: 0.0437, validation loss: 0.3055
2024-05-23 21:50:35 [INFO]: Epoch 054 - generator training loss: 0.2390, discriminator training loss: 0.0435, validation loss: 0.3120
2024-05-23 21:50:54 [INFO]: Epoch 055 - generator training loss: 0.2327, discriminator training loss: 0.0433, validation loss: 0.3122
2024-05-23 21:51:13 [INFO]: Epoch 056 - generator training loss: 0.2312, discriminator training loss: 0.0433, validation loss: 0.3106
2024-05-23 21:51:32 [INFO]: Epoch 057 - generator training loss: 0.2290, discriminator training loss: 0.0430, validation loss: 0.3067
2024-05-23 21:51:51 [INFO]: Epoch 058 - generator training loss: 0.2264, discriminator training loss: 0.0428, validation loss: 0.3103
2024-05-23 21:52:10 [INFO]: Epoch 059 - generator training loss: 0.2236, discriminator training loss: 0.0426, validation loss: 0.3071
2024-05-23 21:52:28 [INFO]: Epoch 060 - generator training loss: 0.2289, discriminator training loss: 0.0425, validation loss: 0.3159
2024-05-23 21:52:47 [INFO]: Epoch 061 - generator training loss: 0.2379, discriminator training loss: 0.0422, validation loss: 0.3176
2024-05-23 21:53:06 [INFO]: Epoch 062 - generator training loss: 0.2341, discriminator training loss: 0.0422, validation loss: 0.3094
2024-05-23 21:53:25 [INFO]: Epoch 063 - generator training loss: 0.2296, discriminator training loss: 0.0421, validation loss: 0.3059
2024-05-23 21:53:25 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-05-23 21:53:25 [INFO]: Finished training. The best model is from epoch#53.
2024-05-23 21:53:25 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_1/USGAN_physionet_2012_seta/20240523_T213319/USGAN.pypots
2024-05-23 21:53:27 [INFO]: US-GAN on PhysioNet-2012-seta: MAE=0.3051, MSE=0.2803
2024-05-23 21:53:37 [INFO]: Successfully saved to overlay_premask_post_norm_saved_results/round_1/USGAN_physionet_2012_seta/imputation.pkl
2024-05-23 21:53:37 [INFO]: Using the given device: cuda:0
2024-05-23 21:53:37 [INFO]: Model files will be saved to overlay_premask_post_norm_saved_results/round_1/BRITS_physionet_2012_seta/20240523_T215337
2024-05-23 21:53:37 [INFO]: Tensorboard file will be saved to overlay_premask_post_norm_saved_results/round_1/BRITS_physionet_2012_seta/20240523_T215337/tensorboard
2024-05-23 21:53:37 [INFO]: BRITS initialized with the given hyperparameters, the number of trainable parameters: 9,176,048
2024-05-23 21:53:52 [INFO]: Epoch 001 - training loss: 1.1272, validation loss: 0.5247
2024-05-23 21:54:04 [INFO]: Epoch 002 - training loss: 0.9167, validation loss: 0.4555
2024-05-23 21:54:17 [INFO]: Epoch 003 - training loss: 0.8555, validation loss: 0.4302
2024-05-23 21:54:29 [INFO]: Epoch 004 - training loss: 0.8199, validation loss: 0.4088
2024-05-23 21:54:41 [INFO]: Epoch 005 - training loss: 0.7924, validation loss: 0.3910
2024-05-23 21:54:53 [INFO]: Epoch 006 - training loss: 0.7692, validation loss: 0.3781
2024-05-23 21:55:05 [INFO]: Epoch 007 - training loss: 0.7513, validation loss: 0.3690
2024-05-23 21:55:18 [INFO]: Epoch 008 - training loss: 0.7367, validation loss: 0.3604
2024-05-23 21:55:30 [INFO]: Epoch 009 - training loss: 0.7242, validation loss: 0.3567
2024-05-23 21:55:42 [INFO]: Epoch 010 - training loss: 0.7132, validation loss: 0.3523
2024-05-23 21:55:54 [INFO]: Epoch 011 - training loss: 0.7038, validation loss: 0.3481
2024-05-23 21:56:06 [INFO]: Epoch 012 - training loss: 0.6967, validation loss: 0.3461
2024-05-23 21:56:18 [INFO]: Epoch 013 - training loss: 0.6884, validation loss: 0.3429
2024-05-23 21:56:31 [INFO]: Epoch 014 - training loss: 0.6828, validation loss: 0.3402
2024-05-23 21:56:43 [INFO]: Epoch 015 - training loss: 0.6784, validation loss: 0.3402
2024-05-23 21:56:55 [INFO]: Epoch 016 - training loss: 0.6725, validation loss: 0.3391
2024-05-23 21:57:07 [INFO]: Epoch 017 - training loss: 0.6680, validation loss: 0.3359
2024-05-23 21:57:19 [INFO]: Epoch 018 - training loss: 0.6633, validation loss: 0.3374
2024-05-23 21:57:32 [INFO]: Epoch 019 - training loss: 0.6617, validation loss: 0.3361
2024-05-23 21:57:44 [INFO]: Epoch 020 - training loss: 0.6574, validation loss: 0.3334
2024-05-23 21:57:56 [INFO]: Epoch 021 - training loss: 0.6543, validation loss: 0.3349
2024-05-23 21:58:08 [INFO]: Epoch 022 - training loss: 0.6500, validation loss: 0.3326
2024-05-23 21:58:20 [INFO]: Epoch 023 - training loss: 0.6465, validation loss: 0.3336
2024-05-23 21:58:32 [INFO]: Epoch 024 - training loss: 0.6436, validation loss: 0.3329
2024-05-23 21:58:44 [INFO]: Epoch 025 - training loss: 0.6434, validation loss: 0.3355
2024-05-23 21:58:57 [INFO]: Epoch 026 - training loss: 0.6462, validation loss: 0.3352
2024-05-23 21:59:09 [INFO]: Epoch 027 - training loss: 0.6421, validation loss: 0.3320
2024-05-23 21:59:21 [INFO]: Epoch 028 - training loss: 0.6382, validation loss: 0.3321
2024-05-23 21:59:33 [INFO]: Epoch 029 - training loss: 0.6341, validation loss: 0.3322
2024-05-23 21:59:45 [INFO]: Epoch 030 - training loss: 0.6303, validation loss: 0.3310
2024-05-23 21:59:57 [INFO]: Epoch 031 - training loss: 0.6269, validation loss: 0.3302
2024-05-23 22:00:09 [INFO]: Epoch 032 - training loss: 0.6240, validation loss: 0.3303
2024-05-23 22:00:22 [INFO]: Epoch 033 - training loss: 0.6214, validation loss: 0.3297
2024-05-23 22:00:34 [INFO]: Epoch 034 - training loss: 0.6187, validation loss: 0.3305
2024-05-23 22:00:46 [INFO]: Epoch 035 - training loss: 0.6186, validation loss: 0.3307
2024-05-23 22:00:58 [INFO]: Epoch 036 - training loss: 0.6162, validation loss: 0.3302
2024-05-23 22:01:10 [INFO]: Epoch 037 - training loss: 0.6120, validation loss: 0.3293
2024-05-23 22:01:23 [INFO]: Epoch 038 - training loss: 0.6100, validation loss: 0.3310
2024-05-23 22:01:35 [INFO]: Epoch 039 - training loss: 0.6057, validation loss: 0.3288
2024-05-23 22:01:47 [INFO]: Epoch 040 - training loss: 0.6028, validation loss: 0.3302
2024-05-23 22:01:59 [INFO]: Epoch 041 - training loss: 0.5997, validation loss: 0.3303
2024-05-23 22:02:11 [INFO]: Epoch 042 - training loss: 0.5969, validation loss: 0.3301
2024-05-23 22:02:23 [INFO]: Epoch 043 - training loss: 0.5943, validation loss: 0.3316
2024-05-23 22:02:35 [INFO]: Epoch 044 - training loss: 0.5918, validation loss: 0.3299
2024-05-23 22:02:48 [INFO]: Epoch 045 - training loss: 0.5898, validation loss: 0.3309
2024-05-23 22:03:00 [INFO]: Epoch 046 - training loss: 0.5886, validation loss: 0.3303
2024-05-23 22:03:12 [INFO]: Epoch 047 - training loss: 0.5878, validation loss: 0.3328
2024-05-23 22:03:24 [INFO]: Epoch 048 - training loss: 0.5860, validation loss: 0.3308
2024-05-23 22:03:36 [INFO]: Epoch 049 - training loss: 0.5785, validation loss: 0.3317
2024-05-23 22:03:36 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-05-23 22:03:36 [INFO]: Finished training. The best model is from epoch#39.
2024-05-23 22:03:36 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_1/BRITS_physionet_2012_seta/20240523_T215337/BRITS.pypots
2024-05-23 22:03:39 [INFO]: BRITS on PhysioNet-2012-seta: MAE=0.2622, MSE=0.2928
2024-05-23 22:03:49 [INFO]: Successfully saved to overlay_premask_post_norm_saved_results/round_1/BRITS_physionet_2012_seta/imputation.pkl
2024-05-23 22:03:49 [INFO]: Using the given device: cuda:0
2024-05-23 22:03:49 [INFO]: Model files will be saved to overlay_premask_post_norm_saved_results/round_1/MRNN_physionet_2012_seta/20240523_T220349
2024-05-23 22:03:49 [INFO]: Tensorboard file will be saved to overlay_premask_post_norm_saved_results/round_1/MRNN_physionet_2012_seta/20240523_T220349/tensorboard
2024-05-23 22:03:49 [INFO]: MRNN initialized with the given hyperparameters, the number of trainable parameters: 7,599
2024-05-23 22:03:55 [INFO]: Epoch 001 - training loss: 1.3172, validation loss: 1.0060
2024-05-23 22:03:55 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_1/MRNN_physionet_2012_seta/20240523_T220349/MRNN_epoch1_loss1.0059580266475678.pypots
2024-05-23 22:03:57 [INFO]: Epoch 002 - training loss: 0.8537, validation loss: 0.9732
2024-05-23 22:03:57 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_1/MRNN_physionet_2012_seta/20240523_T220349/MRNN_epoch2_loss0.9731983125209809.pypots
2024-05-23 22:04:00 [INFO]: Epoch 003 - training loss: 0.6362, validation loss: 0.9505
2024-05-23 22:04:00 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_1/MRNN_physionet_2012_seta/20240523_T220349/MRNN_epoch3_loss0.9504877775907516.pypots
2024-05-23 22:04:03 [INFO]: Epoch 004 - training loss: 0.5890, validation loss: 0.9340
2024-05-23 22:04:03 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_1/MRNN_physionet_2012_seta/20240523_T220349/MRNN_epoch4_loss0.9340006649494171.pypots
2024-05-23 22:04:06 [INFO]: Epoch 005 - training loss: 0.5616, validation loss: 0.9245
2024-05-23 22:04:06 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_1/MRNN_physionet_2012_seta/20240523_T220349/MRNN_epoch5_loss0.9244679868221283.pypots
2024-05-23 22:04:09 [INFO]: Epoch 006 - training loss: 0.5394, validation loss: 0.9200
2024-05-23 22:04:09 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_1/MRNN_physionet_2012_seta/20240523_T220349/MRNN_epoch6_loss0.9199974447488785.pypots
2024-05-23 22:04:11 [INFO]: Epoch 007 - training loss: 0.5200, validation loss: 0.9162
2024-05-23 22:04:11 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_1/MRNN_physionet_2012_seta/20240523_T220349/MRNN_epoch7_loss0.916185948252678.pypots
2024-05-23 22:04:14 [INFO]: Epoch 008 - training loss: 0.5046, validation loss: 0.9146
2024-05-23 22:04:14 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_1/MRNN_physionet_2012_seta/20240523_T220349/MRNN_epoch8_loss0.9145858526229859.pypots
2024-05-23 22:04:17 [INFO]: Epoch 009 - training loss: 0.4947, validation loss: 0.9143
2024-05-23 22:04:17 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_1/MRNN_physionet_2012_seta/20240523_T220349/MRNN_epoch9_loss0.9142607003450394.pypots
2024-05-23 22:04:20 [INFO]: Epoch 010 - training loss: 0.4862, validation loss: 0.9142
2024-05-23 22:04:20 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_1/MRNN_physionet_2012_seta/20240523_T220349/MRNN_epoch10_loss0.9142189919948578.pypots
2024-05-23 22:04:23 [INFO]: Epoch 011 - training loss: 0.4758, validation loss: 0.9135
2024-05-23 22:04:23 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_1/MRNN_physionet_2012_seta/20240523_T220349/MRNN_epoch11_loss0.9135308474302292.pypots
2024-05-23 22:04:25 [INFO]: Epoch 012 - training loss: 0.4709, validation loss: 0.9148
2024-05-23 22:04:25 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_1/MRNN_physionet_2012_seta/20240523_T220349/MRNN_epoch12_loss0.9148355633020401.pypots
2024-05-23 22:04:28 [INFO]: Epoch 013 - training loss: 0.4685, validation loss: 0.9176
2024-05-23 22:04:28 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_1/MRNN_physionet_2012_seta/20240523_T220349/MRNN_epoch13_loss0.9175713926553726.pypots
2024-05-23 22:04:31 [INFO]: Epoch 014 - training loss: 0.4575, validation loss: 0.9187
2024-05-23 22:04:31 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_1/MRNN_physionet_2012_seta/20240523_T220349/MRNN_epoch14_loss0.918749937415123.pypots
2024-05-23 22:04:34 [INFO]: Epoch 015 - training loss: 0.4591, validation loss: 0.9206
2024-05-23 22:04:34 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_1/MRNN_physionet_2012_seta/20240523_T220349/MRNN_epoch15_loss0.9206011414527893.pypots
2024-05-23 22:04:37 [INFO]: Epoch 016 - training loss: 0.4588, validation loss: 0.9219
2024-05-23 22:04:37 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_1/MRNN_physionet_2012_seta/20240523_T220349/MRNN_epoch16_loss0.9218698114156723.pypots
2024-05-23 22:04:39 [INFO]: Epoch 017 - training loss: 0.4448, validation loss: 0.9236
2024-05-23 22:04:39 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_1/MRNN_physionet_2012_seta/20240523_T220349/MRNN_epoch17_loss0.9235572546720505.pypots
2024-05-23 22:04:42 [INFO]: Epoch 018 - training loss: 0.4478, validation loss: 0.9258
2024-05-23 22:04:42 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_1/MRNN_physionet_2012_seta/20240523_T220349/MRNN_epoch18_loss0.9257657021284104.pypots
2024-05-23 22:04:45 [INFO]: Epoch 019 - training loss: 0.4512, validation loss: 0.9263
2024-05-23 22:04:45 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_1/MRNN_physionet_2012_seta/20240523_T220349/MRNN_epoch19_loss0.9263002395629882.pypots
2024-05-23 22:04:48 [INFO]: Epoch 020 - training loss: 0.4452, validation loss: 0.9289
2024-05-23 22:04:48 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_1/MRNN_physionet_2012_seta/20240523_T220349/MRNN_epoch20_loss0.9288761377334595.pypots
2024-05-23 22:04:51 [INFO]: Epoch 021 - training loss: 0.4381, validation loss: 0.9284
2024-05-23 22:04:51 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_1/MRNN_physionet_2012_seta/20240523_T220349/MRNN_epoch21_loss0.9283625096082687.pypots
2024-05-23 22:04:51 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-05-23 22:04:51 [INFO]: Finished training. The best model is from epoch#11.
2024-05-23 22:04:51 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_1/MRNN_physionet_2012_seta/20240523_T220349/MRNN.pypots
2024-05-23 22:04:52 [INFO]: MRNN on PhysioNet-2012-seta: MAE=0.6863, MSE=0.9262
2024-05-23 22:04:56 [INFO]: Successfully saved to overlay_premask_post_norm_saved_results/round_1/MRNN_physionet_2012_seta/imputation.pkl
2024-05-23 22:04:56 [INFO]: Using the given device: cpu
2024-05-23 22:04:56 [INFO]: LOCF on PhysioNet-2012-seta: MAE=0.4085, MSE=0.5400
2024-05-23 22:04:56 [INFO]: Successfully created the given path "saved_results/round_1/LOCF_physionet_2012_seta".
2024-05-23 22:04:56 [INFO]: Successfully saved to overlay_premask_post_norm_saved_results/round_1/LOCF_physionet_2012_seta/imputation.pkl
2024-05-23 22:04:56 [INFO]: Median on PhysioNet-2012-seta: MAE=0.6857, MSE=1.0298
2024-05-23 22:04:56 [INFO]: Successfully created the given path "saved_results/round_1/Median_physionet_2012_seta".
2024-05-23 22:04:56 [INFO]: Successfully saved to overlay_premask_post_norm_saved_results/round_1/Median_physionet_2012_seta/imputation.pkl
2024-05-23 22:04:56 [INFO]: Mean on PhysioNet-2012-seta: MAE=0.7024, MSE=1.0000
2024-05-23 22:04:56 [INFO]: Successfully created the given path "saved_results/round_1/Mean_physionet_2012_seta".
2024-05-23 22:04:56 [INFO]: Successfully saved to overlay_premask_post_norm_saved_results/round_1/Mean_physionet_2012_seta/imputation.pkl
2024-05-23 22:04:56 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
2024-05-23 22:04:56 [INFO]: Using the given device: cuda:0
2024-05-23 22:04:56 [INFO]: Model files will be saved to overlay_premask_post_norm_saved_results/round_2/SAITS_physionet_2012_seta/20240523_T220456
2024-05-23 22:04:56 [INFO]: Tensorboard file will be saved to overlay_premask_post_norm_saved_results/round_2/SAITS_physionet_2012_seta/20240523_T220456/tensorboard
2024-05-23 22:04:56 [INFO]: SAITS initialized with the given hyperparameters, the number of trainable parameters: 5,322,806
2024-05-23 22:04:58 [INFO]: Epoch 001 - training loss: 1.0624, validation loss: 0.4760
2024-05-23 22:04:59 [INFO]: Epoch 002 - training loss: 0.6819, validation loss: 0.4214
2024-05-23 22:05:00 [INFO]: Epoch 003 - training loss: 0.5651, validation loss: 0.3912
2024-05-23 22:05:01 [INFO]: Epoch 004 - training loss: 0.5139, validation loss: 0.3718
2024-05-23 22:05:02 [INFO]: Epoch 005 - training loss: 0.4660, validation loss: 0.3572
2024-05-23 22:05:03 [INFO]: Epoch 006 - training loss: 0.4376, validation loss: 0.3436
2024-05-23 22:05:05 [INFO]: Epoch 007 - training loss: 0.4120, validation loss: 0.3381
2024-05-23 22:05:06 [INFO]: Epoch 008 - training loss: 0.3905, validation loss: 0.3296
2024-05-23 22:05:07 [INFO]: Epoch 009 - training loss: 0.3699, validation loss: 0.3253
2024-05-23 22:05:08 [INFO]: Epoch 010 - training loss: 0.3531, validation loss: 0.3183
2024-05-23 22:05:09 [INFO]: Epoch 011 - training loss: 0.3350, validation loss: 0.3070
2024-05-23 22:05:10 [INFO]: Epoch 012 - training loss: 0.3196, validation loss: 0.3053
2024-05-23 22:05:12 [INFO]: Epoch 013 - training loss: 0.3087, validation loss: 0.3013
2024-05-23 22:05:13 [INFO]: Epoch 014 - training loss: 0.2964, validation loss: 0.3007
2024-05-23 22:05:14 [INFO]: Epoch 015 - training loss: 0.2911, validation loss: 0.2970
2024-05-23 22:05:15 [INFO]: Epoch 016 - training loss: 0.2761, validation loss: 0.2909
2024-05-23 22:05:16 [INFO]: Epoch 017 - training loss: 0.2660, validation loss: 0.2996
2024-05-23 22:05:17 [INFO]: Epoch 018 - training loss: 0.2563, validation loss: 0.2969
2024-05-23 22:05:19 [INFO]: Epoch 019 - training loss: 0.2565, validation loss: 0.2926
2024-05-23 22:05:20 [INFO]: Epoch 020 - training loss: 0.2455, validation loss: 0.2892
2024-05-23 22:05:21 [INFO]: Epoch 021 - training loss: 0.2397, validation loss: 0.2922
2024-05-23 22:05:22 [INFO]: Epoch 022 - training loss: 0.2306, validation loss: 0.2891
2024-05-23 22:05:23 [INFO]: Epoch 023 - training loss: 0.2300, validation loss: 0.2910
2024-05-23 22:05:24 [INFO]: Epoch 024 - training loss: 0.2216, validation loss: 0.2871
2024-05-23 22:05:25 [INFO]: Epoch 025 - training loss: 0.2166, validation loss: 0.2908
2024-05-23 22:05:27 [INFO]: Epoch 026 - training loss: 0.2112, validation loss: 0.2890
2024-05-23 22:05:28 [INFO]: Epoch 027 - training loss: 0.2030, validation loss: 0.2917
2024-05-23 22:05:29 [INFO]: Epoch 028 - training loss: 0.2021, validation loss: 0.2895
2024-05-23 22:05:30 [INFO]: Epoch 029 - training loss: 0.1976, validation loss: 0.2928
2024-05-23 22:05:31 [INFO]: Epoch 030 - training loss: 0.1925, validation loss: 0.2864
2024-05-23 22:05:32 [INFO]: Epoch 031 - training loss: 0.1900, validation loss: 0.2843
2024-05-23 22:05:34 [INFO]: Epoch 032 - training loss: 0.1870, validation loss: 0.2908
2024-05-23 22:05:35 [INFO]: Epoch 033 - training loss: 0.1828, validation loss: 0.2827
2024-05-23 22:05:36 [INFO]: Epoch 034 - training loss: 0.1790, validation loss: 0.2874
2024-05-23 22:05:37 [INFO]: Epoch 035 - training loss: 0.1770, validation loss: 0.2867
2024-05-23 22:05:38 [INFO]: Epoch 036 - training loss: 0.1728, validation loss: 0.2865
2024-05-23 22:05:39 [INFO]: Epoch 037 - training loss: 0.1735, validation loss: 0.2847
2024-05-23 22:05:41 [INFO]: Epoch 038 - training loss: 0.1689, validation loss: 0.2899
2024-05-23 22:05:42 [INFO]: Epoch 039 - training loss: 0.1662, validation loss: 0.2851
2024-05-23 22:05:43 [INFO]: Epoch 040 - training loss: 0.1647, validation loss: 0.2912
2024-05-23 22:05:44 [INFO]: Epoch 041 - training loss: 0.1590, validation loss: 0.2812
2024-05-23 22:05:45 [INFO]: Epoch 042 - training loss: 0.1572, validation loss: 0.2848
2024-05-23 22:05:46 [INFO]: Epoch 043 - training loss: 0.1585, validation loss: 0.2856
2024-05-23 22:05:48 [INFO]: Epoch 044 - training loss: 0.1549, validation loss: 0.2880
2024-05-23 22:05:49 [INFO]: Epoch 045 - training loss: 0.1562, validation loss: 0.2854
2024-05-23 22:05:50 [INFO]: Epoch 046 - training loss: 0.1519, validation loss: 0.2885
2024-05-23 22:05:51 [INFO]: Epoch 047 - training loss: 0.1497, validation loss: 0.2822
2024-05-23 22:05:52 [INFO]: Epoch 048 - training loss: 0.1492, validation loss: 0.2838
2024-05-23 22:05:53 [INFO]: Epoch 049 - training loss: 0.1497, validation loss: 0.2838
2024-05-23 22:05:55 [INFO]: Epoch 050 - training loss: 0.1451, validation loss: 0.2845
2024-05-23 22:05:56 [INFO]: Epoch 051 - training loss: 0.1435, validation loss: 0.2858
2024-05-23 22:05:56 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-05-23 22:05:56 [INFO]: Finished training. The best model is from epoch#41.
2024-05-23 22:05:56 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_2/SAITS_physionet_2012_seta/20240523_T220456/SAITS.pypots
2024-05-23 22:05:56 [INFO]: SAITS on PhysioNet-2012-seta: MAE=0.2654, MSE=0.3214
2024-05-23 22:05:56 [INFO]: Successfully saved to overlay_premask_post_norm_saved_results/round_2/SAITS_physionet_2012_seta/imputation.pkl
2024-05-23 22:05:56 [INFO]: Using the given device: cuda:0
2024-05-23 22:05:56 [INFO]: Model files will be saved to overlay_premask_post_norm_saved_results/round_2/Transformer_physionet_2012_seta/20240523_T220556
2024-05-23 22:05:56 [INFO]: Tensorboard file will be saved to overlay_premask_post_norm_saved_results/round_2/Transformer_physionet_2012_seta/20240523_T220556/tensorboard
2024-05-23 22:05:56 [INFO]: Transformer initialized with the given hyperparameters, the number of trainable parameters: 2,152,613
2024-05-23 22:05:57 [INFO]: Epoch 001 - training loss: 1.1782, validation loss: 0.5359
2024-05-23 22:05:58 [INFO]: Epoch 002 - training loss: 0.7351, validation loss: 0.4649
2024-05-23 22:05:58 [INFO]: Epoch 003 - training loss: 0.6388, validation loss: 0.4499
2024-05-23 22:05:59 [INFO]: Epoch 004 - training loss: 0.5807, validation loss: 0.4158
2024-05-23 22:05:59 [INFO]: Epoch 005 - training loss: 0.5394, validation loss: 0.3950
2024-05-23 22:06:00 [INFO]: Epoch 006 - training loss: 0.5016, validation loss: 0.3835
2024-05-23 22:06:01 [INFO]: Epoch 007 - training loss: 0.4742, validation loss: 0.3728
2024-05-23 22:06:01 [INFO]: Epoch 008 - training loss: 0.4541, validation loss: 0.3696
2024-05-23 22:06:02 [INFO]: Epoch 009 - training loss: 0.4311, validation loss: 0.3571
2024-05-23 22:06:02 [INFO]: Epoch 010 - training loss: 0.4174, validation loss: 0.3529
2024-05-23 22:06:03 [INFO]: Epoch 011 - training loss: 0.4081, validation loss: 0.3480
2024-05-23 22:06:03 [INFO]: Epoch 012 - training loss: 0.3910, validation loss: 0.3480
2024-05-23 22:06:04 [INFO]: Epoch 013 - training loss: 0.3825, validation loss: 0.3470
2024-05-23 22:06:05 [INFO]: Epoch 014 - training loss: 0.3690, validation loss: 0.3360
2024-05-23 22:06:05 [INFO]: Epoch 015 - training loss: 0.3684, validation loss: 0.3413
2024-05-23 22:06:06 [INFO]: Epoch 016 - training loss: 0.3509, validation loss: 0.3342
2024-05-23 22:06:06 [INFO]: Epoch 017 - training loss: 0.3420, validation loss: 0.3303
2024-05-23 22:06:07 [INFO]: Epoch 018 - training loss: 0.3358, validation loss: 0.3312
2024-05-23 22:06:08 [INFO]: Epoch 019 - training loss: 0.3247, validation loss: 0.3246
2024-05-23 22:06:08 [INFO]: Epoch 020 - training loss: 0.3243, validation loss: 0.3200
2024-05-23 22:06:09 [INFO]: Epoch 021 - training loss: 0.3188, validation loss: 0.3262
2024-05-23 22:06:09 [INFO]: Epoch 022 - training loss: 0.3110, validation loss: 0.3218
2024-05-23 22:06:10 [INFO]: Epoch 023 - training loss: 0.3081, validation loss: 0.3197
2024-05-23 22:06:10 [INFO]: Epoch 024 - training loss: 0.2991, validation loss: 0.3200
2024-05-23 22:06:11 [INFO]: Epoch 025 - training loss: 0.2912, validation loss: 0.3227
2024-05-23 22:06:12 [INFO]: Epoch 026 - training loss: 0.2852, validation loss: 0.3117
2024-05-23 22:06:13 [INFO]: Epoch 027 - training loss: 0.2858, validation loss: 0.3155
2024-05-23 22:06:13 [INFO]: Epoch 028 - training loss: 0.2759, validation loss: 0.3147
2024-05-23 22:06:14 [INFO]: Epoch 029 - training loss: 0.2715, validation loss: 0.3118
2024-05-23 22:06:14 [INFO]: Epoch 030 - training loss: 0.2668, validation loss: 0.3173
2024-05-23 22:06:15 [INFO]: Epoch 031 - training loss: 0.2658, validation loss: 0.3152
2024-05-23 22:06:16 [INFO]: Epoch 032 - training loss: 0.2592, validation loss: 0.3127
2024-05-23 22:06:16 [INFO]: Epoch 033 - training loss: 0.2553, validation loss: 0.3069
2024-05-23 22:06:17 [INFO]: Epoch 034 - training loss: 0.2490, validation loss: 0.3107
2024-05-23 22:06:17 [INFO]: Epoch 035 - training loss: 0.2483, validation loss: 0.3134
2024-05-23 22:06:18 [INFO]: Epoch 036 - training loss: 0.2478, validation loss: 0.3104
2024-05-23 22:06:18 [INFO]: Epoch 037 - training loss: 0.2396, validation loss: 0.3111
2024-05-23 22:06:19 [INFO]: Epoch 038 - training loss: 0.2377, validation loss: 0.3095
2024-05-23 22:06:20 [INFO]: Epoch 039 - training loss: 0.2362, validation loss: 0.3140
2024-05-23 22:06:20 [INFO]: Epoch 040 - training loss: 0.2354, validation loss: 0.3085
2024-05-23 22:06:21 [INFO]: Epoch 041 - training loss: 0.2285, validation loss: 0.3113
2024-05-23 22:06:21 [INFO]: Epoch 042 - training loss: 0.2241, validation loss: 0.3111
2024-05-23 22:06:22 [INFO]: Epoch 043 - training loss: 0.2241, validation loss: 0.3122
2024-05-23 22:06:22 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-05-23 22:06:22 [INFO]: Finished training. The best model is from epoch#33.
2024-05-23 22:06:22 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_2/Transformer_physionet_2012_seta/20240523_T220556/Transformer.pypots
2024-05-23 22:06:22 [INFO]: Transformer on PhysioNet-2012-seta: MAE=0.2881, MSE=0.3316
2024-05-23 22:06:22 [INFO]: Successfully saved to overlay_premask_post_norm_saved_results/round_2/Transformer_physionet_2012_seta/imputation.pkl
2024-05-23 22:06:22 [INFO]: Using the given device: cuda:0
2024-05-23 22:06:22 [INFO]: Model files will be saved to overlay_premask_post_norm_saved_results/round_2/TimesNet_physionet_2012_seta/20240523_T220622
2024-05-23 22:06:22 [INFO]: Tensorboard file will be saved to overlay_premask_post_norm_saved_results/round_2/TimesNet_physionet_2012_seta/20240523_T220622/tensorboard
2024-05-23 22:06:22 [INFO]: TimesNet initialized with the given hyperparameters, the number of trainable parameters: 21,649,573
2024-05-23 22:06:23 [INFO]: Epoch 001 - training loss: 0.4555, validation loss: 0.3747
2024-05-23 22:06:24 [INFO]: Epoch 002 - training loss: 0.5399, validation loss: 0.3756
2024-05-23 22:06:25 [INFO]: Epoch 003 - training loss: 0.4580, validation loss: 0.3995
2024-05-23 22:06:25 [INFO]: Epoch 004 - training loss: 0.4153, validation loss: 0.3190
2024-05-23 22:06:26 [INFO]: Epoch 005 - training loss: 0.3476, validation loss: 0.3187
2024-05-23 22:06:27 [INFO]: Epoch 006 - training loss: 0.3089, validation loss: 0.3102
2024-05-23 22:06:27 [INFO]: Epoch 007 - training loss: 0.2886, validation loss: 0.3021
2024-05-23 22:06:28 [INFO]: Epoch 008 - training loss: 0.2888, validation loss: 0.3015
2024-05-23 22:06:29 [INFO]: Epoch 009 - training loss: 0.2819, validation loss: 0.2976
2024-05-23 22:06:30 [INFO]: Epoch 010 - training loss: 0.2706, validation loss: 0.3021
2024-05-23 22:06:30 [INFO]: Epoch 011 - training loss: 0.2686, validation loss: 0.2963
2024-05-23 22:06:31 [INFO]: Epoch 012 - training loss: 0.2637, validation loss: 0.3022
2024-05-23 22:06:32 [INFO]: Epoch 013 - training loss: 0.2556, validation loss: 0.2926
2024-05-23 22:06:32 [INFO]: Epoch 014 - training loss: 0.2551, validation loss: 0.2952
2024-05-23 22:06:33 [INFO]: Epoch 015 - training loss: 0.2538, validation loss: 0.2911
2024-05-23 22:06:34 [INFO]: Epoch 016 - training loss: 0.2557, validation loss: 0.2931
2024-05-23 22:06:34 [INFO]: Epoch 017 - training loss: 0.2630, validation loss: 0.2862
2024-05-23 22:06:35 [INFO]: Epoch 018 - training loss: 0.2533, validation loss: 0.2883
2024-05-23 22:06:36 [INFO]: Epoch 019 - training loss: 0.2429, validation loss: 0.2880
2024-05-23 22:06:37 [INFO]: Epoch 020 - training loss: 0.2301, validation loss: 0.2974
2024-05-23 22:06:37 [INFO]: Epoch 021 - training loss: 0.2185, validation loss: 0.2949
2024-05-23 22:06:38 [INFO]: Epoch 022 - training loss: 0.2145, validation loss: 0.2991
2024-05-23 22:06:39 [INFO]: Epoch 023 - training loss: 0.2215, validation loss: 0.2953
2024-05-23 22:06:39 [INFO]: Epoch 024 - training loss: 0.2097, validation loss: 0.2964
2024-05-23 22:06:40 [INFO]: Epoch 025 - training loss: 0.2097, validation loss: 0.2951
2024-05-23 22:06:41 [INFO]: Epoch 026 - training loss: 0.2162, validation loss: 0.2977
2024-05-23 22:06:41 [INFO]: Epoch 027 - training loss: 0.2031, validation loss: 0.2954
2024-05-23 22:06:41 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-05-23 22:06:41 [INFO]: Finished training. The best model is from epoch#17.
2024-05-23 22:06:42 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_2/TimesNet_physionet_2012_seta/20240523_T220622/TimesNet.pypots
2024-05-23 22:06:42 [INFO]: TimesNet on PhysioNet-2012-seta: MAE=0.2899, MSE=0.2929
2024-05-23 22:06:42 [INFO]: Successfully saved to overlay_premask_post_norm_saved_results/round_2/TimesNet_physionet_2012_seta/imputation.pkl
2024-05-23 22:06:42 [INFO]: Using the given device: cuda:0
2024-05-23 22:06:42 [INFO]: Model files will be saved to overlay_premask_post_norm_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T220642
2024-05-23 22:06:42 [INFO]: Tensorboard file will be saved to overlay_premask_post_norm_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T220642/tensorboard
2024-05-23 22:06:42 [INFO]: CSDI initialized with the given hyperparameters, the number of trainable parameters: 1,694,753
2024-05-23 22:07:25 [INFO]: Epoch 001 - training loss: 0.4080, validation loss: 0.3363
2024-05-23 22:07:25 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T220642/CSDI_epoch1_loss0.33626403361558915.pypots
2024-05-23 22:08:08 [INFO]: Epoch 002 - training loss: 0.3293, validation loss: 0.3009
2024-05-23 22:08:08 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T220642/CSDI_epoch2_loss0.30089186877012253.pypots
2024-05-23 22:08:52 [INFO]: Epoch 003 - training loss: 0.2944, validation loss: 0.2598
2024-05-23 22:08:52 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T220642/CSDI_epoch3_loss0.25975646525621415.pypots
2024-05-23 22:09:35 [INFO]: Epoch 004 - training loss: 0.2587, validation loss: 0.2472
2024-05-23 22:09:35 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T220642/CSDI_epoch4_loss0.2472011588513851.pypots
2024-05-23 22:10:19 [INFO]: Epoch 005 - training loss: 0.2627, validation loss: 0.2406
2024-05-23 22:10:19 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T220642/CSDI_epoch5_loss0.2406152866780758.pypots
2024-05-23 22:11:03 [INFO]: Epoch 006 - training loss: 0.2396, validation loss: 0.2280
2024-05-23 22:11:03 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T220642/CSDI_epoch6_loss0.22797142416238786.pypots
2024-05-23 22:11:46 [INFO]: Epoch 007 - training loss: 0.2388, validation loss: 0.2239
2024-05-23 22:11:46 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T220642/CSDI_epoch7_loss0.22385216429829596.pypots
2024-05-23 22:12:30 [INFO]: Epoch 008 - training loss: 0.2302, validation loss: 0.2184
2024-05-23 22:12:30 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T220642/CSDI_epoch8_loss0.21840160787105561.pypots
2024-05-23 22:13:14 [INFO]: Epoch 009 - training loss: 0.2190, validation loss: 0.2182
2024-05-23 22:13:14 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T220642/CSDI_epoch9_loss0.21821329072117807.pypots
2024-05-23 22:13:57 [INFO]: Epoch 010 - training loss: 0.2263, validation loss: 0.2144
2024-05-23 22:13:57 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T220642/CSDI_epoch10_loss0.21436959877610207.pypots
2024-05-23 22:14:41 [INFO]: Epoch 011 - training loss: 0.2198, validation loss: 0.2148
2024-05-23 22:14:41 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T220642/CSDI_epoch11_loss0.21481718122959137.pypots
2024-05-23 22:15:25 [INFO]: Epoch 012 - training loss: 0.2139, validation loss: 0.2099
2024-05-23 22:15:25 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T220642/CSDI_epoch12_loss0.20986393764615058.pypots
2024-05-23 22:16:09 [INFO]: Epoch 013 - training loss: 0.2167, validation loss: 0.2091
2024-05-23 22:16:09 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T220642/CSDI_epoch13_loss0.20909524857997894.pypots
2024-05-23 22:16:52 [INFO]: Epoch 014 - training loss: 0.2157, validation loss: 0.2018
2024-05-23 22:16:52 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T220642/CSDI_epoch14_loss0.20176725536584855.pypots
2024-05-23 22:17:36 [INFO]: Epoch 015 - training loss: 0.2204, validation loss: 0.2058
2024-05-23 22:17:36 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T220642/CSDI_epoch15_loss0.20583680644631386.pypots
2024-05-23 22:18:20 [INFO]: Epoch 016 - training loss: 0.2187, validation loss: 0.2035
2024-05-23 22:18:20 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T220642/CSDI_epoch16_loss0.20346224009990693.pypots
2024-05-23 22:19:03 [INFO]: Epoch 017 - training loss: 0.2161, validation loss: 0.2094
2024-05-23 22:19:03 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T220642/CSDI_epoch17_loss0.20944781824946404.pypots
2024-05-23 22:19:47 [INFO]: Epoch 018 - training loss: 0.2123, validation loss: 0.2060
2024-05-23 22:19:47 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T220642/CSDI_epoch18_loss0.20595732256770133.pypots
2024-05-23 22:20:31 [INFO]: Epoch 019 - training loss: 0.2187, validation loss: 0.2013
2024-05-23 22:20:31 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T220642/CSDI_epoch19_loss0.20132711827754973.pypots
2024-05-23 22:21:14 [INFO]: Epoch 020 - training loss: 0.2175, validation loss: 0.2033
2024-05-23 22:21:14 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T220642/CSDI_epoch20_loss0.20331798121333122.pypots
2024-05-23 22:21:58 [INFO]: Epoch 021 - training loss: 0.2131, validation loss: 0.1976
2024-05-23 22:21:58 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T220642/CSDI_epoch21_loss0.19762403890490532.pypots
2024-05-23 22:22:42 [INFO]: Epoch 022 - training loss: 0.2075, validation loss: 0.2002
2024-05-23 22:22:42 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T220642/CSDI_epoch22_loss0.20017174929380416.pypots
2024-05-23 22:23:25 [INFO]: Epoch 023 - training loss: 0.2034, validation loss: 0.2014
2024-05-23 22:23:25 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T220642/CSDI_epoch23_loss0.2013596087694168.pypots
2024-05-23 22:24:09 [INFO]: Epoch 024 - training loss: 0.2075, validation loss: 0.1953
2024-05-23 22:24:09 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T220642/CSDI_epoch24_loss0.19525422379374505.pypots
2024-05-23 22:24:53 [INFO]: Epoch 025 - training loss: 0.2099, validation loss: 0.1984
2024-05-23 22:24:53 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T220642/CSDI_epoch25_loss0.19838200882077217.pypots
2024-05-23 22:25:36 [INFO]: Epoch 026 - training loss: 0.2048, validation loss: 0.1943
2024-05-23 22:25:36 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T220642/CSDI_epoch26_loss0.19428798034787179.pypots
2024-05-23 22:26:20 [INFO]: Epoch 027 - training loss: 0.1995, validation loss: 0.2009
2024-05-23 22:26:20 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T220642/CSDI_epoch27_loss0.2009315550327301.pypots
2024-05-23 22:27:04 [INFO]: Epoch 028 - training loss: 0.2017, validation loss: 0.1955
2024-05-23 22:27:04 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T220642/CSDI_epoch28_loss0.1955393597483635.pypots
2024-05-23 22:27:48 [INFO]: Epoch 029 - training loss: 0.2036, validation loss: 0.1953
2024-05-23 22:27:48 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T220642/CSDI_epoch29_loss0.1953425593674183.pypots
2024-05-23 22:28:31 [INFO]: Epoch 030 - training loss: 0.2022, validation loss: 0.1943
2024-05-23 22:28:31 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T220642/CSDI_epoch30_loss0.19428651556372642.pypots
2024-05-23 22:29:15 [INFO]: Epoch 031 - training loss: 0.2043, validation loss: 0.1953
2024-05-23 22:29:15 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T220642/CSDI_epoch31_loss0.19529236033558844.pypots
2024-05-23 22:29:59 [INFO]: Epoch 032 - training loss: 0.1986, validation loss: 0.1963
2024-05-23 22:29:59 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T220642/CSDI_epoch32_loss0.19630298912525176.pypots
2024-05-23 22:30:42 [INFO]: Epoch 033 - training loss: 0.2042, validation loss: 0.1936
2024-05-23 22:30:42 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T220642/CSDI_epoch33_loss0.193626868724823.pypots
2024-05-23 22:31:26 [INFO]: Epoch 034 - training loss: 0.1924, validation loss: 0.1937
2024-05-23 22:31:26 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T220642/CSDI_epoch34_loss0.19372157007455826.pypots
2024-05-23 22:32:10 [INFO]: Epoch 035 - training loss: 0.1947, validation loss: 0.1941
2024-05-23 22:32:10 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T220642/CSDI_epoch35_loss0.19405351281166078.pypots
2024-05-23 22:32:53 [INFO]: Epoch 036 - training loss: 0.2062, validation loss: 0.1933
2024-05-23 22:32:53 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T220642/CSDI_epoch36_loss0.193307763338089.pypots
2024-05-23 22:33:37 [INFO]: Epoch 037 - training loss: 0.1980, validation loss: 0.1932
2024-05-23 22:33:37 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T220642/CSDI_epoch37_loss0.19319143071770667.pypots
2024-05-23 22:34:21 [INFO]: Epoch 038 - training loss: 0.2021, validation loss: 0.1930
2024-05-23 22:34:21 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T220642/CSDI_epoch38_loss0.19295238554477692.pypots
2024-05-23 22:35:04 [INFO]: Epoch 039 - training loss: 0.1951, validation loss: 0.1903
2024-05-23 22:35:04 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T220642/CSDI_epoch39_loss0.19030732437968254.pypots
2024-05-23 22:35:48 [INFO]: Epoch 040 - training loss: 0.2021, validation loss: 0.1889
2024-05-23 22:35:48 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T220642/CSDI_epoch40_loss0.18892457261681556.pypots
2024-05-23 22:36:32 [INFO]: Epoch 041 - training loss: 0.2000, validation loss: 0.1957
2024-05-23 22:36:32 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T220642/CSDI_epoch41_loss0.19573641195893288.pypots
2024-05-23 22:37:15 [INFO]: Epoch 042 - training loss: 0.2078, validation loss: 0.1947
2024-05-23 22:37:15 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T220642/CSDI_epoch42_loss0.19472787231206895.pypots
2024-05-23 22:37:59 [INFO]: Epoch 043 - training loss: 0.2052, validation loss: 0.1878
2024-05-23 22:37:59 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T220642/CSDI_epoch43_loss0.18778997138142586.pypots
2024-05-23 22:38:43 [INFO]: Epoch 044 - training loss: 0.2023, validation loss: 0.1903
2024-05-23 22:38:43 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T220642/CSDI_epoch44_loss0.19034237116575242.pypots
2024-05-23 22:39:26 [INFO]: Epoch 045 - training loss: 0.2040, validation loss: 0.1871
2024-05-23 22:39:26 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T220642/CSDI_epoch45_loss0.18713393062353134.pypots
2024-05-23 22:40:10 [INFO]: Epoch 046 - training loss: 0.1996, validation loss: 0.1925
2024-05-23 22:40:10 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T220642/CSDI_epoch46_loss0.19250548332929612.pypots
2024-05-23 22:40:53 [INFO]: Epoch 047 - training loss: 0.1818, validation loss: 0.1861
2024-05-23 22:40:53 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T220642/CSDI_epoch47_loss0.18612086847424508.pypots
2024-05-23 22:41:37 [INFO]: Epoch 048 - training loss: 0.1912, validation loss: 0.1865
2024-05-23 22:41:37 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T220642/CSDI_epoch48_loss0.1865305244922638.pypots
2024-05-23 22:42:21 [INFO]: Epoch 049 - training loss: 0.2001, validation loss: 0.1879
2024-05-23 22:42:21 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T220642/CSDI_epoch49_loss0.1878971166908741.pypots
2024-05-23 22:43:04 [INFO]: Epoch 050 - training loss: 0.1901, validation loss: 0.1877
2024-05-23 22:43:04 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T220642/CSDI_epoch50_loss0.18774569034576416.pypots
2024-05-23 22:43:48 [INFO]: Epoch 051 - training loss: 0.2033, validation loss: 0.1920
2024-05-23 22:43:48 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T220642/CSDI_epoch51_loss0.1920289397239685.pypots
2024-05-23 22:44:32 [INFO]: Epoch 052 - training loss: 0.1979, validation loss: 0.1877
2024-05-23 22:44:32 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T220642/CSDI_epoch52_loss0.18772401735186578.pypots
2024-05-23 22:45:15 [INFO]: Epoch 053 - training loss: 0.1989, validation loss: 0.1873
2024-05-23 22:45:15 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T220642/CSDI_epoch53_loss0.18732862845063208.pypots
2024-05-23 22:45:59 [INFO]: Epoch 054 - training loss: 0.1933, validation loss: 0.1863
2024-05-23 22:45:59 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T220642/CSDI_epoch54_loss0.18628355711698533.pypots
2024-05-23 22:46:43 [INFO]: Epoch 055 - training loss: 0.1864, validation loss: 0.1915
2024-05-23 22:46:43 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T220642/CSDI_epoch55_loss0.19153198152780532.pypots
2024-05-23 22:47:26 [INFO]: Epoch 056 - training loss: 0.1893, validation loss: 0.1917
2024-05-23 22:47:26 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T220642/CSDI_epoch56_loss0.1916935086250305.pypots
2024-05-23 22:48:10 [INFO]: Epoch 057 - training loss: 0.2054, validation loss: 0.1854
2024-05-23 22:48:10 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T220642/CSDI_epoch57_loss0.18538727909326552.pypots
2024-05-23 22:48:54 [INFO]: Epoch 058 - training loss: 0.1805, validation loss: 0.1865
2024-05-23 22:48:54 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T220642/CSDI_epoch58_loss0.18651751354336737.pypots
2024-05-23 22:49:37 [INFO]: Epoch 059 - training loss: 0.1833, validation loss: 0.1852
2024-05-23 22:49:37 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T220642/CSDI_epoch59_loss0.18523699045181274.pypots
2024-05-23 22:50:21 [INFO]: Epoch 060 - training loss: 0.1966, validation loss: 0.1886
2024-05-23 22:50:21 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T220642/CSDI_epoch60_loss0.1886143647134304.pypots
2024-05-23 22:51:05 [INFO]: Epoch 061 - training loss: 0.1890, validation loss: 0.1841
2024-05-23 22:51:05 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T220642/CSDI_epoch61_loss0.18412848487496375.pypots
2024-05-23 22:51:48 [INFO]: Epoch 062 - training loss: 0.1962, validation loss: 0.1877
2024-05-23 22:51:48 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T220642/CSDI_epoch62_loss0.1876710057258606.pypots
2024-05-23 22:52:32 [INFO]: Epoch 063 - training loss: 0.1926, validation loss: 0.1844
2024-05-23 22:52:32 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T220642/CSDI_epoch63_loss0.18436028435826302.pypots
2024-05-23 22:53:16 [INFO]: Epoch 064 - training loss: 0.1955, validation loss: 0.1882
2024-05-23 22:53:16 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T220642/CSDI_epoch64_loss0.188161201775074.pypots
2024-05-23 22:53:59 [INFO]: Epoch 065 - training loss: 0.1988, validation loss: 0.1874
2024-05-23 22:53:59 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T220642/CSDI_epoch65_loss0.1873668909072876.pypots
2024-05-23 22:54:43 [INFO]: Epoch 066 - training loss: 0.1841, validation loss: 0.1861
2024-05-23 22:54:43 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T220642/CSDI_epoch66_loss0.18608858659863473.pypots
2024-05-23 22:55:27 [INFO]: Epoch 067 - training loss: 0.1858, validation loss: 0.1821
2024-05-23 22:55:27 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T220642/CSDI_epoch67_loss0.18211797326803209.pypots
2024-05-23 22:56:10 [INFO]: Epoch 068 - training loss: 0.1804, validation loss: 0.1835
2024-05-23 22:56:10 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T220642/CSDI_epoch68_loss0.18346193432807922.pypots
2024-05-23 22:56:54 [INFO]: Epoch 069 - training loss: 0.1895, validation loss: 0.1909
2024-05-23 22:56:54 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T220642/CSDI_epoch69_loss0.1908682756125927.pypots
2024-05-23 22:57:38 [INFO]: Epoch 070 - training loss: 0.1868, validation loss: 0.1878
2024-05-23 22:57:38 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T220642/CSDI_epoch70_loss0.18782359436154367.pypots
2024-05-23 22:58:22 [INFO]: Epoch 071 - training loss: 0.1842, validation loss: 0.1842
2024-05-23 22:58:22 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T220642/CSDI_epoch71_loss0.18423180729150773.pypots
2024-05-23 22:59:05 [INFO]: Epoch 072 - training loss: 0.1966, validation loss: 0.1882
2024-05-23 22:59:05 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T220642/CSDI_epoch72_loss0.1881656639277935.pypots
2024-05-23 22:59:49 [INFO]: Epoch 073 - training loss: 0.1842, validation loss: 0.1843
2024-05-23 22:59:49 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T220642/CSDI_epoch73_loss0.1842507965862751.pypots
2024-05-23 23:00:33 [INFO]: Epoch 074 - training loss: 0.1879, validation loss: 0.1828
2024-05-23 23:00:33 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T220642/CSDI_epoch74_loss0.1828228421509266.pypots
2024-05-23 23:01:16 [INFO]: Epoch 075 - training loss: 0.1897, validation loss: 0.1816
2024-05-23 23:01:16 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T220642/CSDI_epoch75_loss0.18158819302916526.pypots
2024-05-23 23:02:00 [INFO]: Epoch 076 - training loss: 0.1963, validation loss: 0.1836
2024-05-23 23:02:00 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T220642/CSDI_epoch76_loss0.18355369046330453.pypots
2024-05-23 23:02:44 [INFO]: Epoch 077 - training loss: 0.1908, validation loss: 0.1850
2024-05-23 23:02:44 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T220642/CSDI_epoch77_loss0.18501253202557563.pypots
2024-05-23 23:03:27 [INFO]: Epoch 078 - training loss: 0.1902, validation loss: 0.1834
2024-05-23 23:03:27 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T220642/CSDI_epoch78_loss0.18339226469397546.pypots
2024-05-23 23:04:11 [INFO]: Epoch 079 - training loss: 0.1877, validation loss: 0.1835
2024-05-23 23:04:11 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T220642/CSDI_epoch79_loss0.18350359201431274.pypots
2024-05-23 23:04:55 [INFO]: Epoch 080 - training loss: 0.1945, validation loss: 0.1843
2024-05-23 23:04:55 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T220642/CSDI_epoch80_loss0.1843247301876545.pypots
2024-05-23 23:05:38 [INFO]: Epoch 081 - training loss: 0.1886, validation loss: 0.1842
2024-05-23 23:05:38 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T220642/CSDI_epoch81_loss0.18416862189769745.pypots
2024-05-23 23:06:22 [INFO]: Epoch 082 - training loss: 0.1839, validation loss: 0.1818
2024-05-23 23:06:22 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T220642/CSDI_epoch82_loss0.18179647997021675.pypots
2024-05-23 23:07:06 [INFO]: Epoch 083 - training loss: 0.1982, validation loss: 0.1835
2024-05-23 23:07:06 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T220642/CSDI_epoch83_loss0.18345920667052268.pypots
2024-05-23 23:07:49 [INFO]: Epoch 084 - training loss: 0.1818, validation loss: 0.1853
2024-05-23 23:07:49 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T220642/CSDI_epoch84_loss0.18531937077641486.pypots
2024-05-23 23:08:33 [INFO]: Epoch 085 - training loss: 0.1982, validation loss: 0.1867
2024-05-23 23:08:33 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T220642/CSDI_epoch85_loss0.18674197793006897.pypots
2024-05-23 23:08:33 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-05-23 23:08:33 [INFO]: Finished training. The best model is from epoch#75.
2024-05-23 23:08:33 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T220642/CSDI.pypots
2024-05-23 23:15:53 [INFO]: CSDI on PhysioNet-2012-seta: MAE=0.2399, MSE=0.3019
2024-05-23 23:45:13 [INFO]: Successfully saved to overlay_premask_post_norm_saved_results/round_2/CSDI_physionet_2012_seta/imputation.pkl
2024-05-23 23:45:13 [INFO]: Using the given device: cuda:0
2024-05-23 23:45:13 [INFO]: Model files will be saved to overlay_premask_post_norm_saved_results/round_2/GPVAE_physionet_2012_seta/20240523_T234513
2024-05-23 23:45:13 [INFO]: Tensorboard file will be saved to overlay_premask_post_norm_saved_results/round_2/GPVAE_physionet_2012_seta/20240523_T234513/tensorboard
2024-05-23 23:45:13 [INFO]: GPVAE initialized with the given hyperparameters, the number of trainable parameters: 1,028,244
2024-05-23 23:45:14 [INFO]: Epoch 001 - training loss: 42814.6677, validation loss: 0.9116
2024-05-23 23:45:14 [INFO]: Epoch 002 - training loss: 24423.2891, validation loss: 0.7388
2024-05-23 23:45:15 [INFO]: Epoch 003 - training loss: 23477.1990, validation loss: 0.7195
2024-05-23 23:45:16 [INFO]: Epoch 004 - training loss: 23180.5278, validation loss: 0.6834
2024-05-23 23:45:16 [INFO]: Epoch 005 - training loss: 23032.9898, validation loss: 0.6690
2024-05-23 23:45:17 [INFO]: Epoch 006 - training loss: 22950.9800, validation loss: 0.6475
2024-05-23 23:45:17 [INFO]: Epoch 007 - training loss: 22902.1128, validation loss: 0.6420
2024-05-23 23:45:18 [INFO]: Epoch 008 - training loss: 22870.8207, validation loss: 0.6478
2024-05-23 23:45:19 [INFO]: Epoch 009 - training loss: 22850.2847, validation loss: 0.6412
2024-05-23 23:45:19 [INFO]: Epoch 010 - training loss: 22835.8931, validation loss: 0.6394
2024-05-23 23:45:20 [INFO]: Epoch 011 - training loss: 22825.0752, validation loss: 0.6396
2024-05-23 23:45:20 [INFO]: Epoch 012 - training loss: 22817.4084, validation loss: 0.6358
2024-05-23 23:45:21 [INFO]: Epoch 013 - training loss: 22811.8938, validation loss: 0.6570
2024-05-23 23:45:22 [INFO]: Epoch 014 - training loss: 22807.3260, validation loss: 0.6321
2024-05-23 23:45:22 [INFO]: Epoch 015 - training loss: 22803.5372, validation loss: 0.6333
2024-05-23 23:45:23 [INFO]: Epoch 016 - training loss: 22800.8194, validation loss: 0.6329
2024-05-23 23:45:23 [INFO]: Epoch 017 - training loss: 22798.1576, validation loss: 0.6306
2024-05-23 23:45:24 [INFO]: Epoch 018 - training loss: 22795.6643, validation loss: 0.6227
2024-05-23 23:45:25 [INFO]: Epoch 019 - training loss: 22793.6146, validation loss: 0.6240
2024-05-23 23:45:25 [INFO]: Epoch 020 - training loss: 22792.4108, validation loss: 0.6210
2024-05-23 23:45:26 [INFO]: Epoch 021 - training loss: 22790.0955, validation loss: 0.6130
2024-05-23 23:45:26 [INFO]: Epoch 022 - training loss: 22789.7621, validation loss: 0.6119
2024-05-23 23:45:27 [INFO]: Epoch 023 - training loss: 22788.5539, validation loss: 0.6182
2024-05-23 23:45:28 [INFO]: Epoch 024 - training loss: 22786.7735, validation loss: 0.6042
2024-05-23 23:45:28 [INFO]: Epoch 025 - training loss: 22785.9779, validation loss: 0.6089
2024-05-23 23:45:29 [INFO]: Epoch 026 - training loss: 22784.8782, validation loss: 0.6019
2024-05-23 23:45:29 [INFO]: Epoch 027 - training loss: 22784.6169, validation loss: 0.6014
2024-05-23 23:45:30 [INFO]: Epoch 028 - training loss: 22783.4769, validation loss: 0.5969
2024-05-23 23:45:31 [INFO]: Epoch 029 - training loss: 22782.5265, validation loss: 0.6047
2024-05-23 23:45:31 [INFO]: Epoch 030 - training loss: 22782.8952, validation loss: 0.5976
2024-05-23 23:45:32 [INFO]: Epoch 031 - training loss: 22781.5452, validation loss: 0.5989
2024-05-23 23:45:32 [INFO]: Epoch 032 - training loss: 22782.2546, validation loss: 0.5991
2024-05-23 23:45:33 [INFO]: Epoch 033 - training loss: 22780.1909, validation loss: 0.5859
2024-05-23 23:45:34 [INFO]: Epoch 034 - training loss: 22778.7138, validation loss: 0.5836
2024-05-23 23:45:34 [INFO]: Epoch 035 - training loss: 22777.7487, validation loss: 0.5803
2024-05-23 23:45:35 [INFO]: Epoch 036 - training loss: 22776.2147, validation loss: 0.5763
2024-05-23 23:45:35 [INFO]: Epoch 037 - training loss: 22775.3805, validation loss: 0.5735
2024-05-23 23:45:36 [INFO]: Epoch 038 - training loss: 22774.1664, validation loss: 0.5725
2024-05-23 23:45:37 [INFO]: Epoch 039 - training loss: 22773.4697, validation loss: 0.5748
2024-05-23 23:45:37 [INFO]: Epoch 040 - training loss: 22772.4966, validation loss: 0.5665
2024-05-23 23:45:38 [INFO]: Epoch 041 - training loss: 22771.8532, validation loss: 0.5704
2024-05-23 23:45:38 [INFO]: Epoch 042 - training loss: 22771.5743, validation loss: 0.5729
2024-05-23 23:45:39 [INFO]: Epoch 043 - training loss: 22770.2760, validation loss: 0.5647
2024-05-23 23:45:40 [INFO]: Epoch 044 - training loss: 22769.7253, validation loss: 0.5646
2024-05-23 23:45:40 [INFO]: Epoch 045 - training loss: 22769.4207, validation loss: 0.5649
2024-05-23 23:45:41 [INFO]: Epoch 046 - training loss: 22769.0309, validation loss: 0.5657
2024-05-23 23:45:41 [INFO]: Epoch 047 - training loss: 22768.2171, validation loss: 0.5645
2024-05-23 23:45:42 [INFO]: Epoch 048 - training loss: 22768.5326, validation loss: 0.5659
2024-05-23 23:45:43 [INFO]: Epoch 049 - training loss: 22768.4364, validation loss: 0.5659
2024-05-23 23:45:43 [INFO]: Epoch 050 - training loss: 22767.5577, validation loss: 0.5637
2024-05-23 23:45:44 [INFO]: Epoch 051 - training loss: 22767.0348, validation loss: 0.5659
2024-05-23 23:45:44 [INFO]: Epoch 052 - training loss: 22766.8487, validation loss: 0.5651
2024-05-23 23:45:45 [INFO]: Epoch 053 - training loss: 22766.6962, validation loss: 0.5634
2024-05-23 23:45:46 [INFO]: Epoch 054 - training loss: 22766.6693, validation loss: 0.5660
2024-05-23 23:45:46 [INFO]: Epoch 055 - training loss: 22765.9948, validation loss: 0.5582
2024-05-23 23:45:47 [INFO]: Epoch 056 - training loss: 22765.4559, validation loss: 0.5607
2024-05-23 23:45:47 [INFO]: Epoch 057 - training loss: 22764.9583, validation loss: 0.5548
2024-05-23 23:45:48 [INFO]: Epoch 058 - training loss: 22764.5945, validation loss: 0.5550
2024-05-23 23:45:49 [INFO]: Epoch 059 - training loss: 22765.0982, validation loss: 0.5470
2024-05-23 23:45:49 [INFO]: Epoch 060 - training loss: 22762.8051, validation loss: 0.5547
2024-05-23 23:45:50 [INFO]: Epoch 061 - training loss: 22762.6321, validation loss: 0.5373
2024-05-23 23:45:50 [INFO]: Epoch 062 - training loss: 22761.4514, validation loss: 0.5345
2024-05-23 23:45:51 [INFO]: Epoch 063 - training loss: 22760.8853, validation loss: 0.5337
2024-05-23 23:45:52 [INFO]: Epoch 064 - training loss: 22760.1226, validation loss: 0.5304
2024-05-23 23:45:52 [INFO]: Epoch 065 - training loss: 22761.1359, validation loss: 0.5292
2024-05-23 23:45:53 [INFO]: Epoch 066 - training loss: 22759.6409, validation loss: 0.5228
2024-05-23 23:45:53 [INFO]: Epoch 067 - training loss: 22759.5540, validation loss: 0.5222
2024-05-23 23:45:54 [INFO]: Epoch 068 - training loss: 22758.8126, validation loss: 0.5214
2024-05-23 23:45:55 [INFO]: Epoch 069 - training loss: 22758.1419, validation loss: 0.5216
2024-05-23 23:45:55 [INFO]: Epoch 070 - training loss: 22757.9179, validation loss: 0.5158
2024-05-23 23:45:56 [INFO]: Epoch 071 - training loss: 22759.0289, validation loss: 0.5197
2024-05-23 23:45:56 [INFO]: Epoch 072 - training loss: 22758.3835, validation loss: 0.5117
2024-05-23 23:45:57 [INFO]: Epoch 073 - training loss: 22759.3535, validation loss: 0.5138
2024-05-23 23:45:58 [INFO]: Epoch 074 - training loss: 22756.8598, validation loss: 0.5116
2024-05-23 23:45:58 [INFO]: Epoch 075 - training loss: 22756.0923, validation loss: 0.5044
2024-05-23 23:45:59 [INFO]: Epoch 076 - training loss: 22755.5380, validation loss: 0.5025
2024-05-23 23:45:59 [INFO]: Epoch 077 - training loss: 22754.9409, validation loss: 0.5097
2024-05-23 23:46:00 [INFO]: Epoch 078 - training loss: 22755.3570, validation loss: 0.5036
2024-05-23 23:46:01 [INFO]: Epoch 079 - training loss: 22755.4918, validation loss: 0.5138
2024-05-23 23:46:01 [INFO]: Epoch 080 - training loss: 22754.7772, validation loss: 0.5011
2024-05-23 23:46:02 [INFO]: Epoch 081 - training loss: 22754.8375, validation loss: 0.4990
2024-05-23 23:46:02 [INFO]: Epoch 082 - training loss: 22753.5602, validation loss: 0.4962
2024-05-23 23:46:03 [INFO]: Epoch 083 - training loss: 22753.1949, validation loss: 0.4986
2024-05-23 23:46:04 [INFO]: Epoch 084 - training loss: 22752.9932, validation loss: 0.4982
2024-05-23 23:46:04 [INFO]: Epoch 085 - training loss: 22752.7540, validation loss: 0.4938
2024-05-23 23:46:05 [INFO]: Epoch 086 - training loss: 22752.2521, validation loss: 0.5004
2024-05-23 23:46:05 [INFO]: Epoch 087 - training loss: 22752.7433, validation loss: 0.5046
2024-05-23 23:46:06 [INFO]: Epoch 088 - training loss: 22752.5367, validation loss: 0.4918
2024-05-23 23:46:07 [INFO]: Epoch 089 - training loss: 22752.1389, validation loss: 0.4916
2024-05-23 23:46:07 [INFO]: Epoch 090 - training loss: 22751.8361, validation loss: 0.4954
2024-05-23 23:46:08 [INFO]: Epoch 091 - training loss: 22751.9149, validation loss: 0.4947
2024-05-23 23:46:08 [INFO]: Epoch 092 - training loss: 22751.5780, validation loss: 0.4900
2024-05-23 23:46:09 [INFO]: Epoch 093 - training loss: 22751.6913, validation loss: 0.4937
2024-05-23 23:46:10 [INFO]: Epoch 094 - training loss: 22752.6197, validation loss: 0.4901
2024-05-23 23:46:10 [INFO]: Epoch 095 - training loss: 22752.5686, validation loss: 0.4913
2024-05-23 23:46:11 [INFO]: Epoch 096 - training loss: 22750.9259, validation loss: 0.4932
2024-05-23 23:46:11 [INFO]: Epoch 097 - training loss: 22751.3894, validation loss: 0.4846
2024-05-23 23:46:12 [INFO]: Epoch 098 - training loss: 22750.5217, validation loss: 0.4866
2024-05-23 23:46:13 [INFO]: Epoch 099 - training loss: 22750.6197, validation loss: 0.4857
2024-05-23 23:46:13 [INFO]: Epoch 100 - training loss: 22749.9603, validation loss: 0.4862
2024-05-23 23:46:14 [INFO]: Epoch 101 - training loss: 22749.8149, validation loss: 0.4870
2024-05-23 23:46:14 [INFO]: Epoch 102 - training loss: 22749.9207, validation loss: 0.4864
2024-05-23 23:46:15 [INFO]: Epoch 103 - training loss: 22749.7147, validation loss: 0.4864
2024-05-23 23:46:16 [INFO]: Epoch 104 - training loss: 22749.5173, validation loss: 0.4844
2024-05-23 23:46:16 [INFO]: Epoch 105 - training loss: 22749.2631, validation loss: 0.4806
2024-05-23 23:46:17 [INFO]: Epoch 106 - training loss: 22749.7119, validation loss: 0.4846
2024-05-23 23:46:17 [INFO]: Epoch 107 - training loss: 22749.7547, validation loss: 0.4857
2024-05-23 23:46:18 [INFO]: Epoch 108 - training loss: 22750.2169, validation loss: 0.4863
2024-05-23 23:46:19 [INFO]: Epoch 109 - training loss: 22750.7730, validation loss: 0.4838
2024-05-23 23:46:19 [INFO]: Epoch 110 - training loss: 22752.4906, validation loss: 0.4835
2024-05-23 23:46:20 [INFO]: Epoch 111 - training loss: 22749.3320, validation loss: 0.5053
2024-05-23 23:46:20 [INFO]: Epoch 112 - training loss: 22749.6508, validation loss: 0.4864
2024-05-23 23:46:21 [INFO]: Epoch 113 - training loss: 22748.4923, validation loss: 0.4794
2024-05-23 23:46:22 [INFO]: Epoch 114 - training loss: 22748.2210, validation loss: 0.4760
2024-05-23 23:46:22 [INFO]: Epoch 115 - training loss: 22747.8095, validation loss: 0.4805
2024-05-23 23:46:23 [INFO]: Epoch 116 - training loss: 22747.9768, validation loss: 0.4797
2024-05-23 23:46:23 [INFO]: Epoch 117 - training loss: 22747.9653, validation loss: 0.4778
2024-05-23 23:46:24 [INFO]: Epoch 118 - training loss: 22748.2978, validation loss: 0.4714
2024-05-23 23:46:25 [INFO]: Epoch 119 - training loss: 22748.2077, validation loss: 0.4750
2024-05-23 23:46:25 [INFO]: Epoch 120 - training loss: 22747.8184, validation loss: 0.4743
2024-05-23 23:46:26 [INFO]: Epoch 121 - training loss: 22747.1935, validation loss: 0.4765
2024-05-23 23:46:26 [INFO]: Epoch 122 - training loss: 22747.0903, validation loss: 0.4748
2024-05-23 23:46:27 [INFO]: Epoch 123 - training loss: 22746.6355, validation loss: 0.4767
2024-05-23 23:46:28 [INFO]: Epoch 124 - training loss: 22746.8746, validation loss: 0.4781
2024-05-23 23:46:28 [INFO]: Epoch 125 - training loss: 22747.8948, validation loss: 0.4735
2024-05-23 23:46:29 [INFO]: Epoch 126 - training loss: 22746.9657, validation loss: 0.4804
2024-05-23 23:46:29 [INFO]: Epoch 127 - training loss: 22746.5464, validation loss: 0.4822
2024-05-23 23:46:30 [INFO]: Epoch 128 - training loss: 22747.3252, validation loss: 0.4740
2024-05-23 23:46:30 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-05-23 23:46:30 [INFO]: Finished training. The best model is from epoch#118.
2024-05-23 23:46:30 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_2/GPVAE_physionet_2012_seta/20240523_T234513/GPVAE.pypots
2024-05-23 23:46:30 [INFO]: GP-VAE on PhysioNet-2012-seta: MAE=0.4186, MSE=0.4857
2024-05-23 23:46:31 [INFO]: Successfully saved to overlay_premask_post_norm_saved_results/round_2/GPVAE_physionet_2012_seta/imputation.pkl
2024-05-23 23:46:31 [INFO]: Using the given device: cuda:0
2024-05-23 23:46:31 [INFO]: Model files will be saved to overlay_premask_post_norm_saved_results/round_2/USGAN_physionet_2012_seta/20240523_T234631
2024-05-23 23:46:31 [INFO]: Tensorboard file will be saved to overlay_premask_post_norm_saved_results/round_2/USGAN_physionet_2012_seta/20240523_T234631/tensorboard
2024-05-23 23:46:31 [INFO]: USGAN initialized with the given hyperparameters, the number of trainable parameters: 16,010,261
2024-05-23 23:46:52 [INFO]: Epoch 001 - generator training loss: 0.5833, discriminator training loss: 0.3838, validation loss: 0.6083
2024-05-23 23:47:11 [INFO]: Epoch 002 - generator training loss: 0.4720, discriminator training loss: 0.2722, validation loss: 0.5234
2024-05-23 23:47:30 [INFO]: Epoch 003 - generator training loss: 0.4277, discriminator training loss: 0.2386, validation loss: 0.4941
2024-05-23 23:47:49 [INFO]: Epoch 004 - generator training loss: 0.4433, discriminator training loss: 0.1915, validation loss: 0.4921
2024-05-23 23:48:08 [INFO]: Epoch 005 - generator training loss: 0.4449, discriminator training loss: 0.1602, validation loss: 0.4782
2024-05-23 23:48:27 [INFO]: Epoch 006 - generator training loss: 0.4330, discriminator training loss: 0.1405, validation loss: 0.4615
2024-05-23 23:48:45 [INFO]: Epoch 007 - generator training loss: 0.4212, discriminator training loss: 0.1258, validation loss: 0.4523
2024-05-23 23:49:04 [INFO]: Epoch 008 - generator training loss: 0.4060, discriminator training loss: 0.1155, validation loss: 0.4393
2024-05-23 23:49:23 [INFO]: Epoch 009 - generator training loss: 0.3943, discriminator training loss: 0.1072, validation loss: 0.4276
2024-05-23 23:49:42 [INFO]: Epoch 010 - generator training loss: 0.3888, discriminator training loss: 0.1000, validation loss: 0.4197
2024-05-23 23:50:01 [INFO]: Epoch 011 - generator training loss: 0.3784, discriminator training loss: 0.0940, validation loss: 0.4127
2024-05-23 23:50:20 [INFO]: Epoch 012 - generator training loss: 0.3737, discriminator training loss: 0.0888, validation loss: 0.4034
2024-05-23 23:50:38 [INFO]: Epoch 013 - generator training loss: 0.3698, discriminator training loss: 0.0839, validation loss: 0.3996
2024-05-23 23:50:57 [INFO]: Epoch 014 - generator training loss: 0.3624, discriminator training loss: 0.0800, validation loss: 0.3949
2024-05-23 23:51:16 [INFO]: Epoch 015 - generator training loss: 0.3595, discriminator training loss: 0.0766, validation loss: 0.3901
2024-05-23 23:51:35 [INFO]: Epoch 016 - generator training loss: 0.3519, discriminator training loss: 0.0732, validation loss: 0.3867
2024-05-23 23:51:54 [INFO]: Epoch 017 - generator training loss: 0.3489, discriminator training loss: 0.0702, validation loss: 0.3780
2024-05-23 23:52:13 [INFO]: Epoch 018 - generator training loss: 0.3453, discriminator training loss: 0.0675, validation loss: 0.3767
2024-05-23 23:52:31 [INFO]: Epoch 019 - generator training loss: 0.3395, discriminator training loss: 0.0653, validation loss: 0.3721
2024-05-23 23:52:50 [INFO]: Epoch 020 - generator training loss: 0.3359, discriminator training loss: 0.0634, validation loss: 0.3662
2024-05-23 23:53:09 [INFO]: Epoch 021 - generator training loss: 0.3336, discriminator training loss: 0.0612, validation loss: 0.3647
2024-05-23 23:53:28 [INFO]: Epoch 022 - generator training loss: 0.3288, discriminator training loss: 0.0597, validation loss: 0.3593
2024-05-23 23:53:47 [INFO]: Epoch 023 - generator training loss: 0.3250, discriminator training loss: 0.0579, validation loss: 0.3555
2024-05-23 23:54:06 [INFO]: Epoch 024 - generator training loss: 0.3210, discriminator training loss: 0.0567, validation loss: 0.3522
2024-05-23 23:54:24 [INFO]: Epoch 025 - generator training loss: 0.3162, discriminator training loss: 0.0552, validation loss: 0.3552
2024-05-23 23:54:43 [INFO]: Epoch 026 - generator training loss: 0.3142, discriminator training loss: 0.0543, validation loss: 0.3498
2024-05-23 23:55:02 [INFO]: Epoch 027 - generator training loss: 0.3083, discriminator training loss: 0.0533, validation loss: 0.3466
2024-05-23 23:55:21 [INFO]: Epoch 028 - generator training loss: 0.3057, discriminator training loss: 0.0525, validation loss: 0.3404
2024-05-23 23:55:40 [INFO]: Epoch 029 - generator training loss: 0.2999, discriminator training loss: 0.0514, validation loss: 0.3400
2024-05-23 23:55:59 [INFO]: Epoch 030 - generator training loss: 0.2968, discriminator training loss: 0.0508, validation loss: 0.3334
2024-05-23 23:56:17 [INFO]: Epoch 031 - generator training loss: 0.2911, discriminator training loss: 0.0502, validation loss: 0.3308
2024-05-23 23:56:36 [INFO]: Epoch 032 - generator training loss: 0.2928, discriminator training loss: 0.0496, validation loss: 0.3317
2024-05-23 23:56:55 [INFO]: Epoch 033 - generator training loss: 0.2898, discriminator training loss: 0.0492, validation loss: 0.3286
2024-05-23 23:57:14 [INFO]: Epoch 034 - generator training loss: 0.2849, discriminator training loss: 0.0485, validation loss: 0.3246
2024-05-23 23:57:33 [INFO]: Epoch 035 - generator training loss: 0.2836, discriminator training loss: 0.0481, validation loss: 0.3249
2024-05-23 23:57:51 [INFO]: Epoch 036 - generator training loss: 0.2805, discriminator training loss: 0.0476, validation loss: 0.3265
2024-05-23 23:58:10 [INFO]: Epoch 037 - generator training loss: 0.2767, discriminator training loss: 0.0472, validation loss: 0.3233
2024-05-23 23:58:29 [INFO]: Epoch 038 - generator training loss: 0.2704, discriminator training loss: 0.0469, validation loss: 0.3229
2024-05-23 23:58:48 [INFO]: Epoch 039 - generator training loss: 0.2673, discriminator training loss: 0.0465, validation loss: 0.3161
2024-05-23 23:59:06 [INFO]: Epoch 040 - generator training loss: 0.2643, discriminator training loss: 0.0461, validation loss: 0.3163
2024-05-23 23:59:25 [INFO]: Epoch 041 - generator training loss: 0.2670, discriminator training loss: 0.0460, validation loss: 0.3133
2024-05-23 23:59:44 [INFO]: Epoch 042 - generator training loss: 0.2625, discriminator training loss: 0.0455, validation loss: 0.3170
2024-05-24 00:00:03 [INFO]: Epoch 043 - generator training loss: 0.2586, discriminator training loss: 0.0452, validation loss: 0.3148
2024-05-24 00:00:22 [INFO]: Epoch 044 - generator training loss: 0.2556, discriminator training loss: 0.0451, validation loss: 0.3131
2024-05-24 00:00:41 [INFO]: Epoch 045 - generator training loss: 0.2548, discriminator training loss: 0.0448, validation loss: 0.3115
2024-05-24 00:00:59 [INFO]: Epoch 046 - generator training loss: 0.2486, discriminator training loss: 0.0442, validation loss: 0.3075
2024-05-24 00:01:18 [INFO]: Epoch 047 - generator training loss: 0.2487, discriminator training loss: 0.0445, validation loss: 0.3118
2024-05-24 00:01:37 [INFO]: Epoch 048 - generator training loss: 0.2473, discriminator training loss: 0.0443, validation loss: 0.3086
2024-05-24 00:01:56 [INFO]: Epoch 049 - generator training loss: 0.2427, discriminator training loss: 0.0440, validation loss: 0.3087
2024-05-24 00:02:15 [INFO]: Epoch 050 - generator training loss: 0.2409, discriminator training loss: 0.0439, validation loss: 0.3064
2024-05-24 00:02:33 [INFO]: Epoch 051 - generator training loss: 0.2377, discriminator training loss: 0.0436, validation loss: 0.3122
2024-05-24 00:02:52 [INFO]: Epoch 052 - generator training loss: 0.2388, discriminator training loss: 0.0436, validation loss: 0.3115
2024-05-24 00:03:11 [INFO]: Epoch 053 - generator training loss: 0.2442, discriminator training loss: 0.0436, validation loss: 0.3118
2024-05-24 00:03:29 [INFO]: Epoch 054 - generator training loss: 0.2345, discriminator training loss: 0.0431, validation loss: 0.3070
2024-05-24 00:03:48 [INFO]: Epoch 055 - generator training loss: 0.2304, discriminator training loss: 0.0431, validation loss: 0.3067
2024-05-24 00:04:07 [INFO]: Epoch 056 - generator training loss: 0.2325, discriminator training loss: 0.0431, validation loss: 0.3085
2024-05-24 00:04:26 [INFO]: Epoch 057 - generator training loss: 0.2460, discriminator training loss: 0.0430, validation loss: 0.3115
2024-05-24 00:04:44 [INFO]: Epoch 058 - generator training loss: 0.2391, discriminator training loss: 0.0427, validation loss: 0.3082
2024-05-24 00:05:03 [INFO]: Epoch 059 - generator training loss: 0.2327, discriminator training loss: 0.0430, validation loss: 0.3038
2024-05-24 00:05:22 [INFO]: Epoch 060 - generator training loss: 0.2260, discriminator training loss: 0.0423, validation loss: 0.3082
2024-05-24 00:05:41 [INFO]: Epoch 061 - generator training loss: 0.2328, discriminator training loss: 0.0425, validation loss: 0.3028
2024-05-24 00:06:00 [INFO]: Epoch 062 - generator training loss: 0.2212, discriminator training loss: 0.0421, validation loss: 0.3046
2024-05-24 00:06:18 [INFO]: Epoch 063 - generator training loss: 0.2185, discriminator training loss: 0.0420, validation loss: 0.3041
2024-05-24 00:06:37 [INFO]: Epoch 064 - generator training loss: 0.2143, discriminator training loss: 0.0420, validation loss: 0.3038
2024-05-24 00:06:56 [INFO]: Epoch 065 - generator training loss: 0.2140, discriminator training loss: 0.0419, validation loss: 0.3102
2024-05-24 00:07:15 [INFO]: Epoch 066 - generator training loss: 0.2143, discriminator training loss: 0.0416, validation loss: 0.3123
2024-05-24 00:07:34 [INFO]: Epoch 067 - generator training loss: 0.2128, discriminator training loss: 0.0415, validation loss: 0.3059
2024-05-24 00:07:53 [INFO]: Epoch 068 - generator training loss: 0.2068, discriminator training loss: 0.0412, validation loss: 0.3079
2024-05-24 00:08:11 [INFO]: Epoch 069 - generator training loss: 0.2089, discriminator training loss: 0.0411, validation loss: 0.3076
2024-05-24 00:08:30 [INFO]: Epoch 070 - generator training loss: 0.2028, discriminator training loss: 0.0411, validation loss: 0.3067
2024-05-24 00:08:49 [INFO]: Epoch 071 - generator training loss: 0.2019, discriminator training loss: 0.0411, validation loss: 0.3054
2024-05-24 00:08:49 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-05-24 00:08:49 [INFO]: Finished training. The best model is from epoch#61.
2024-05-24 00:08:49 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_2/USGAN_physionet_2012_seta/20240523_T234631/USGAN.pypots
2024-05-24 00:08:51 [INFO]: US-GAN on PhysioNet-2012-seta: MAE=0.2973, MSE=0.2886
2024-05-24 00:09:01 [INFO]: Successfully saved to overlay_premask_post_norm_saved_results/round_2/USGAN_physionet_2012_seta/imputation.pkl
2024-05-24 00:09:01 [INFO]: Using the given device: cuda:0
2024-05-24 00:09:01 [INFO]: Model files will be saved to overlay_premask_post_norm_saved_results/round_2/BRITS_physionet_2012_seta/20240524_T000901
2024-05-24 00:09:01 [INFO]: Tensorboard file will be saved to overlay_premask_post_norm_saved_results/round_2/BRITS_physionet_2012_seta/20240524_T000901/tensorboard
2024-05-24 00:09:01 [INFO]: BRITS initialized with the given hyperparameters, the number of trainable parameters: 9,176,048
2024-05-24 00:09:16 [INFO]: Epoch 001 - training loss: 1.1215, validation loss: 0.5288
2024-05-24 00:09:29 [INFO]: Epoch 002 - training loss: 0.9164, validation loss: 0.4641
2024-05-24 00:09:41 [INFO]: Epoch 003 - training loss: 0.8564, validation loss: 0.4324
2024-05-24 00:09:53 [INFO]: Epoch 004 - training loss: 0.8210, validation loss: 0.4134
2024-05-24 00:10:05 [INFO]: Epoch 005 - training loss: 0.7925, validation loss: 0.3978
2024-05-24 00:10:17 [INFO]: Epoch 006 - training loss: 0.7705, validation loss: 0.3833
2024-05-24 00:10:29 [INFO]: Epoch 007 - training loss: 0.7536, validation loss: 0.3744
2024-05-24 00:10:41 [INFO]: Epoch 008 - training loss: 0.7389, validation loss: 0.3659
2024-05-24 00:10:54 [INFO]: Epoch 009 - training loss: 0.7254, validation loss: 0.3577
2024-05-24 00:11:06 [INFO]: Epoch 010 - training loss: 0.7137, validation loss: 0.3551
2024-05-24 00:11:18 [INFO]: Epoch 011 - training loss: 0.7045, validation loss: 0.3509
2024-05-24 00:11:30 [INFO]: Epoch 012 - training loss: 0.6960, validation loss: 0.3484
2024-05-24 00:11:42 [INFO]: Epoch 013 - training loss: 0.6890, validation loss: 0.3444
2024-05-24 00:11:55 [INFO]: Epoch 014 - training loss: 0.6829, validation loss: 0.3409
2024-05-24 00:12:07 [INFO]: Epoch 015 - training loss: 0.6770, validation loss: 0.3425
2024-05-24 00:12:19 [INFO]: Epoch 016 - training loss: 0.6725, validation loss: 0.3399
2024-05-24 00:12:31 [INFO]: Epoch 017 - training loss: 0.6686, validation loss: 0.3376
2024-05-24 00:12:43 [INFO]: Epoch 018 - training loss: 0.6647, validation loss: 0.3368
2024-05-24 00:12:56 [INFO]: Epoch 019 - training loss: 0.6605, validation loss: 0.3373
2024-05-24 00:13:08 [INFO]: Epoch 020 - training loss: 0.6574, validation loss: 0.3342
2024-05-24 00:13:20 [INFO]: Epoch 021 - training loss: 0.6531, validation loss: 0.3351
2024-05-24 00:13:32 [INFO]: Epoch 022 - training loss: 0.6514, validation loss: 0.3355
2024-05-24 00:13:44 [INFO]: Epoch 023 - training loss: 0.6494, validation loss: 0.3336
2024-05-24 00:13:56 [INFO]: Epoch 024 - training loss: 0.6449, validation loss: 0.3342
2024-05-24 00:14:09 [INFO]: Epoch 025 - training loss: 0.6412, validation loss: 0.3331
2024-05-24 00:14:21 [INFO]: Epoch 026 - training loss: 0.6387, validation loss: 0.3331
2024-05-24 00:14:33 [INFO]: Epoch 027 - training loss: 0.6371, validation loss: 0.3318
2024-05-24 00:14:45 [INFO]: Epoch 028 - training loss: 0.6327, validation loss: 0.3318
2024-05-24 00:14:57 [INFO]: Epoch 029 - training loss: 0.6295, validation loss: 0.3295
2024-05-24 00:15:09 [INFO]: Epoch 030 - training loss: 0.6312, validation loss: 0.3302
2024-05-24 00:15:21 [INFO]: Epoch 031 - training loss: 0.6260, validation loss: 0.3319
2024-05-24 00:15:33 [INFO]: Epoch 032 - training loss: 0.6277, validation loss: 0.3314
2024-05-24 00:15:46 [INFO]: Epoch 033 - training loss: 0.6217, validation loss: 0.3314
2024-05-24 00:15:58 [INFO]: Epoch 034 - training loss: 0.6181, validation loss: 0.3319
2024-05-24 00:16:10 [INFO]: Epoch 035 - training loss: 0.6157, validation loss: 0.3297
2024-05-24 00:16:22 [INFO]: Epoch 036 - training loss: 0.6125, validation loss: 0.3296
2024-05-24 00:16:34 [INFO]: Epoch 037 - training loss: 0.6099, validation loss: 0.3302
2024-05-24 00:16:46 [INFO]: Epoch 038 - training loss: 0.6073, validation loss: 0.3298
2024-05-24 00:16:58 [INFO]: Epoch 039 - training loss: 0.6044, validation loss: 0.3301
2024-05-24 00:16:58 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-05-24 00:16:58 [INFO]: Finished training. The best model is from epoch#29.
2024-05-24 00:16:58 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_2/BRITS_physionet_2012_seta/20240524_T000901/BRITS.pypots
2024-05-24 00:17:01 [INFO]: BRITS on PhysioNet-2012-seta: MAE=0.2619, MSE=0.2883
2024-05-24 00:17:11 [INFO]: Successfully saved to overlay_premask_post_norm_saved_results/round_2/BRITS_physionet_2012_seta/imputation.pkl
2024-05-24 00:17:11 [INFO]: Using the given device: cuda:0
2024-05-24 00:17:11 [INFO]: Model files will be saved to overlay_premask_post_norm_saved_results/round_2/MRNN_physionet_2012_seta/20240524_T001711
2024-05-24 00:17:11 [INFO]: Tensorboard file will be saved to overlay_premask_post_norm_saved_results/round_2/MRNN_physionet_2012_seta/20240524_T001711/tensorboard
2024-05-24 00:17:11 [INFO]: MRNN initialized with the given hyperparameters, the number of trainable parameters: 7,599
2024-05-24 00:17:16 [INFO]: Epoch 001 - training loss: 1.1548, validation loss: 0.9964
2024-05-24 00:17:16 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_2/MRNN_physionet_2012_seta/20240524_T001711/MRNN_epoch1_loss0.9964262068271637.pypots
2024-05-24 00:17:19 [INFO]: Epoch 002 - training loss: 0.6910, validation loss: 0.9657
2024-05-24 00:17:19 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_2/MRNN_physionet_2012_seta/20240524_T001711/MRNN_epoch2_loss0.9656528770923615.pypots
2024-05-24 00:17:22 [INFO]: Epoch 003 - training loss: 0.5955, validation loss: 0.9393
2024-05-24 00:17:22 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_2/MRNN_physionet_2012_seta/20240524_T001711/MRNN_epoch3_loss0.9393302232027054.pypots
2024-05-24 00:17:25 [INFO]: Epoch 004 - training loss: 0.5569, validation loss: 0.9278
2024-05-24 00:17:25 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_2/MRNN_physionet_2012_seta/20240524_T001711/MRNN_epoch4_loss0.9278167188167572.pypots
2024-05-24 00:17:28 [INFO]: Epoch 005 - training loss: 0.5347, validation loss: 0.9219
2024-05-24 00:17:28 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_2/MRNN_physionet_2012_seta/20240524_T001711/MRNN_epoch5_loss0.9218680709600449.pypots
2024-05-24 00:17:30 [INFO]: Epoch 006 - training loss: 0.5174, validation loss: 0.9170
2024-05-24 00:17:30 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_2/MRNN_physionet_2012_seta/20240524_T001711/MRNN_epoch6_loss0.9169971644878387.pypots
2024-05-24 00:17:33 [INFO]: Epoch 007 - training loss: 0.5057, validation loss: 0.9138
2024-05-24 00:17:33 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_2/MRNN_physionet_2012_seta/20240524_T001711/MRNN_epoch7_loss0.9138033598661423.pypots
2024-05-24 00:17:36 [INFO]: Epoch 008 - training loss: 0.4989, validation loss: 0.9116
2024-05-24 00:17:36 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_2/MRNN_physionet_2012_seta/20240524_T001711/MRNN_epoch8_loss0.911626935005188.pypots
2024-05-24 00:17:39 [INFO]: Epoch 009 - training loss: 0.4892, validation loss: 0.9097
2024-05-24 00:17:39 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_2/MRNN_physionet_2012_seta/20240524_T001711/MRNN_epoch9_loss0.9096871703863144.pypots
2024-05-24 00:17:42 [INFO]: Epoch 010 - training loss: 0.4740, validation loss: 0.9096
2024-05-24 00:17:42 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_2/MRNN_physionet_2012_seta/20240524_T001711/MRNN_epoch10_loss0.9095684885978699.pypots
2024-05-24 00:17:44 [INFO]: Epoch 011 - training loss: 0.4740, validation loss: 0.9113
2024-05-24 00:17:44 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_2/MRNN_physionet_2012_seta/20240524_T001711/MRNN_epoch11_loss0.9113023549318313.pypots
2024-05-24 00:17:47 [INFO]: Epoch 012 - training loss: 0.4701, validation loss: 0.9108
2024-05-24 00:17:47 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_2/MRNN_physionet_2012_seta/20240524_T001711/MRNN_epoch12_loss0.910791763663292.pypots
2024-05-24 00:17:50 [INFO]: Epoch 013 - training loss: 0.4553, validation loss: 0.9127
2024-05-24 00:17:50 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_2/MRNN_physionet_2012_seta/20240524_T001711/MRNN_epoch13_loss0.9127191454172134.pypots
2024-05-24 00:17:53 [INFO]: Epoch 014 - training loss: 0.4620, validation loss: 0.9148
2024-05-24 00:17:53 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_2/MRNN_physionet_2012_seta/20240524_T001711/MRNN_epoch14_loss0.9148353576660156.pypots
2024-05-24 00:17:56 [INFO]: Epoch 015 - training loss: 0.4549, validation loss: 0.9168
2024-05-24 00:17:56 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_2/MRNN_physionet_2012_seta/20240524_T001711/MRNN_epoch15_loss0.9168286055326462.pypots
2024-05-24 00:17:58 [INFO]: Epoch 016 - training loss: 0.4520, validation loss: 0.9186
2024-05-24 00:17:58 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_2/MRNN_physionet_2012_seta/20240524_T001711/MRNN_epoch16_loss0.9185650736093521.pypots
2024-05-24 00:18:01 [INFO]: Epoch 017 - training loss: 0.4516, validation loss: 0.9210
2024-05-24 00:18:01 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_2/MRNN_physionet_2012_seta/20240524_T001711/MRNN_epoch17_loss0.9209794849157333.pypots
2024-05-24 00:18:04 [INFO]: Epoch 018 - training loss: 0.4385, validation loss: 0.9223
2024-05-24 00:18:04 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_2/MRNN_physionet_2012_seta/20240524_T001711/MRNN_epoch18_loss0.9222540587186814.pypots
2024-05-24 00:18:07 [INFO]: Epoch 019 - training loss: 0.4611, validation loss: 0.9311
2024-05-24 00:18:07 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_2/MRNN_physionet_2012_seta/20240524_T001711/MRNN_epoch19_loss0.9310918658971786.pypots
2024-05-24 00:18:10 [INFO]: Epoch 020 - training loss: 0.4497, validation loss: 0.9252
2024-05-24 00:18:10 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_2/MRNN_physionet_2012_seta/20240524_T001711/MRNN_epoch20_loss0.9252367645502091.pypots
2024-05-24 00:18:10 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-05-24 00:18:10 [INFO]: Finished training. The best model is from epoch#10.
2024-05-24 00:18:10 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_2/MRNN_physionet_2012_seta/20240524_T001711/MRNN.pypots
2024-05-24 00:18:11 [INFO]: MRNN on PhysioNet-2012-seta: MAE=0.6845, MSE=0.9225
2024-05-24 00:18:15 [INFO]: Successfully saved to overlay_premask_post_norm_saved_results/round_2/MRNN_physionet_2012_seta/imputation.pkl
2024-05-24 00:18:15 [INFO]: Using the given device: cpu
2024-05-24 00:18:15 [INFO]: LOCF on PhysioNet-2012-seta: MAE=0.4085, MSE=0.5400
2024-05-24 00:18:15 [INFO]: Successfully created the given path "saved_results/round_2/LOCF_physionet_2012_seta".
2024-05-24 00:18:15 [INFO]: Successfully saved to overlay_premask_post_norm_saved_results/round_2/LOCF_physionet_2012_seta/imputation.pkl
2024-05-24 00:18:15 [INFO]: Median on PhysioNet-2012-seta: MAE=0.6857, MSE=1.0298
2024-05-24 00:18:15 [INFO]: Successfully created the given path "saved_results/round_2/Median_physionet_2012_seta".
2024-05-24 00:18:15 [INFO]: Successfully saved to overlay_premask_post_norm_saved_results/round_2/Median_physionet_2012_seta/imputation.pkl
2024-05-24 00:18:15 [INFO]: Mean on PhysioNet-2012-seta: MAE=0.7024, MSE=1.0000
2024-05-24 00:18:15 [INFO]: Successfully created the given path "saved_results/round_2/Mean_physionet_2012_seta".
2024-05-24 00:18:15 [INFO]: Successfully saved to overlay_premask_post_norm_saved_results/round_2/Mean_physionet_2012_seta/imputation.pkl
2024-05-24 00:18:15 [INFO]: Have set the random seed as 2026 for numpy and pytorch.
2024-05-24 00:18:15 [INFO]: Using the given device: cuda:0
2024-05-24 00:18:15 [INFO]: Model files will be saved to overlay_premask_post_norm_saved_results/round_3/SAITS_physionet_2012_seta/20240524_T001815
2024-05-24 00:18:15 [INFO]: Tensorboard file will be saved to overlay_premask_post_norm_saved_results/round_3/SAITS_physionet_2012_seta/20240524_T001815/tensorboard
2024-05-24 00:18:15 [INFO]: SAITS initialized with the given hyperparameters, the number of trainable parameters: 5,322,806
2024-05-24 00:18:17 [INFO]: Epoch 001 - training loss: 1.1012, validation loss: 0.4872
2024-05-24 00:18:18 [INFO]: Epoch 002 - training loss: 0.7462, validation loss: 0.4170
2024-05-24 00:18:19 [INFO]: Epoch 003 - training loss: 0.5960, validation loss: 0.4002
2024-05-24 00:18:20 [INFO]: Epoch 004 - training loss: 0.5268, validation loss: 0.3661
2024-05-24 00:18:21 [INFO]: Epoch 005 - training loss: 0.4756, validation loss: 0.3569
2024-05-24 00:18:22 [INFO]: Epoch 006 - training loss: 0.4428, validation loss: 0.3440
2024-05-24 00:18:24 [INFO]: Epoch 007 - training loss: 0.4106, validation loss: 0.3446
2024-05-24 00:18:25 [INFO]: Epoch 008 - training loss: 0.3917, validation loss: 0.3245
2024-05-24 00:18:26 [INFO]: Epoch 009 - training loss: 0.3706, validation loss: 0.3224
2024-05-24 00:18:27 [INFO]: Epoch 010 - training loss: 0.3523, validation loss: 0.3130
2024-05-24 00:18:28 [INFO]: Epoch 011 - training loss: 0.3318, validation loss: 0.3099
2024-05-24 00:18:29 [INFO]: Epoch 012 - training loss: 0.3194, validation loss: 0.3152
2024-05-24 00:18:30 [INFO]: Epoch 013 - training loss: 0.3051, validation loss: 0.3084
2024-05-24 00:18:32 [INFO]: Epoch 014 - training loss: 0.2919, validation loss: 0.3006
2024-05-24 00:18:33 [INFO]: Epoch 015 - training loss: 0.2851, validation loss: 0.2974
2024-05-24 00:18:34 [INFO]: Epoch 016 - training loss: 0.2728, validation loss: 0.2990
2024-05-24 00:18:35 [INFO]: Epoch 017 - training loss: 0.2645, validation loss: 0.2965
2024-05-24 00:18:36 [INFO]: Epoch 018 - training loss: 0.2564, validation loss: 0.2967
2024-05-24 00:18:37 [INFO]: Epoch 019 - training loss: 0.2473, validation loss: 0.3035
2024-05-24 00:18:39 [INFO]: Epoch 020 - training loss: 0.2407, validation loss: 0.2930
2024-05-24 00:18:40 [INFO]: Epoch 021 - training loss: 0.2373, validation loss: 0.2939
2024-05-24 00:18:41 [INFO]: Epoch 022 - training loss: 0.2315, validation loss: 0.2968
2024-05-24 00:18:42 [INFO]: Epoch 023 - training loss: 0.2283, validation loss: 0.2943
2024-05-24 00:18:43 [INFO]: Epoch 024 - training loss: 0.2210, validation loss: 0.2927
2024-05-24 00:18:44 [INFO]: Epoch 025 - training loss: 0.2139, validation loss: 0.3013
2024-05-24 00:18:46 [INFO]: Epoch 026 - training loss: 0.2054, validation loss: 0.2952
2024-05-24 00:18:47 [INFO]: Epoch 027 - training loss: 0.2022, validation loss: 0.3012
2024-05-24 00:18:48 [INFO]: Epoch 028 - training loss: 0.2040, validation loss: 0.2954
2024-05-24 00:18:49 [INFO]: Epoch 029 - training loss: 0.1980, validation loss: 0.2959
2024-05-24 00:18:50 [INFO]: Epoch 030 - training loss: 0.1900, validation loss: 0.2907
2024-05-24 00:18:51 [INFO]: Epoch 031 - training loss: 0.1855, validation loss: 0.2949
2024-05-24 00:18:53 [INFO]: Epoch 032 - training loss: 0.1883, validation loss: 0.2989
2024-05-24 00:18:54 [INFO]: Epoch 033 - training loss: 0.1806, validation loss: 0.2942
2024-05-24 00:18:55 [INFO]: Epoch 034 - training loss: 0.1772, validation loss: 0.2938
2024-05-24 00:18:56 [INFO]: Epoch 035 - training loss: 0.1735, validation loss: 0.2934
2024-05-24 00:18:57 [INFO]: Epoch 036 - training loss: 0.1758, validation loss: 0.2893
2024-05-24 00:18:58 [INFO]: Epoch 037 - training loss: 0.1699, validation loss: 0.2952
2024-05-24 00:19:00 [INFO]: Epoch 038 - training loss: 0.1705, validation loss: 0.2961
2024-05-24 00:19:01 [INFO]: Epoch 039 - training loss: 0.1714, validation loss: 0.2946
2024-05-24 00:19:02 [INFO]: Epoch 040 - training loss: 0.1657, validation loss: 0.2916
2024-05-24 00:19:03 [INFO]: Epoch 041 - training loss: 0.1634, validation loss: 0.2917
2024-05-24 00:19:04 [INFO]: Epoch 042 - training loss: 0.1593, validation loss: 0.2936
2024-05-24 00:19:05 [INFO]: Epoch 043 - training loss: 0.1602, validation loss: 0.2928
2024-05-24 00:19:07 [INFO]: Epoch 044 - training loss: 0.1553, validation loss: 0.2929
2024-05-24 00:19:08 [INFO]: Epoch 045 - training loss: 0.1581, validation loss: 0.2931
2024-05-24 00:19:09 [INFO]: Epoch 046 - training loss: 0.1558, validation loss: 0.2913
2024-05-24 00:19:09 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-05-24 00:19:09 [INFO]: Finished training. The best model is from epoch#36.
2024-05-24 00:19:09 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_3/SAITS_physionet_2012_seta/20240524_T001815/SAITS.pypots
2024-05-24 00:19:09 [INFO]: SAITS on PhysioNet-2012-seta: MAE=0.2733, MSE=0.3300
2024-05-24 00:19:09 [INFO]: Successfully saved to overlay_premask_post_norm_saved_results/round_3/SAITS_physionet_2012_seta/imputation.pkl
2024-05-24 00:19:09 [INFO]: Using the given device: cuda:0
2024-05-24 00:19:09 [INFO]: Model files will be saved to overlay_premask_post_norm_saved_results/round_3/Transformer_physionet_2012_seta/20240524_T001909
2024-05-24 00:19:09 [INFO]: Tensorboard file will be saved to overlay_premask_post_norm_saved_results/round_3/Transformer_physionet_2012_seta/20240524_T001909/tensorboard
2024-05-24 00:19:09 [INFO]: Transformer initialized with the given hyperparameters, the number of trainable parameters: 2,152,613
2024-05-24 00:19:10 [INFO]: Epoch 001 - training loss: 1.1339, validation loss: 0.5535
2024-05-24 00:19:11 [INFO]: Epoch 002 - training loss: 0.7335, validation loss: 0.4731
2024-05-24 00:19:11 [INFO]: Epoch 003 - training loss: 0.6304, validation loss: 0.4274
2024-05-24 00:19:12 [INFO]: Epoch 004 - training loss: 0.5762, validation loss: 0.4136
2024-05-24 00:19:12 [INFO]: Epoch 005 - training loss: 0.5367, validation loss: 0.4006
2024-05-24 00:19:13 [INFO]: Epoch 006 - training loss: 0.5090, validation loss: 0.3954
2024-05-24 00:19:14 [INFO]: Epoch 007 - training loss: 0.4796, validation loss: 0.3737
2024-05-24 00:19:14 [INFO]: Epoch 008 - training loss: 0.4551, validation loss: 0.3648
2024-05-24 00:19:15 [INFO]: Epoch 009 - training loss: 0.4355, validation loss: 0.3665
2024-05-24 00:19:15 [INFO]: Epoch 010 - training loss: 0.4210, validation loss: 0.3562
2024-05-24 00:19:16 [INFO]: Epoch 011 - training loss: 0.4099, validation loss: 0.3519
2024-05-24 00:19:17 [INFO]: Epoch 012 - training loss: 0.3923, validation loss: 0.3516
2024-05-24 00:19:17 [INFO]: Epoch 013 - training loss: 0.3815, validation loss: 0.3442
2024-05-24 00:19:18 [INFO]: Epoch 014 - training loss: 0.3715, validation loss: 0.3370
2024-05-24 00:19:18 [INFO]: Epoch 015 - training loss: 0.3629, validation loss: 0.3335
2024-05-24 00:19:19 [INFO]: Epoch 016 - training loss: 0.3513, validation loss: 0.3315
2024-05-24 00:19:20 [INFO]: Epoch 017 - training loss: 0.3454, validation loss: 0.3345
2024-05-24 00:19:20 [INFO]: Epoch 018 - training loss: 0.3375, validation loss: 0.3317
2024-05-24 00:19:21 [INFO]: Epoch 019 - training loss: 0.3310, validation loss: 0.3296
2024-05-24 00:19:21 [INFO]: Epoch 020 - training loss: 0.3264, validation loss: 0.3286
2024-05-24 00:19:22 [INFO]: Epoch 021 - training loss: 0.3158, validation loss: 0.3243
2024-05-24 00:19:23 [INFO]: Epoch 022 - training loss: 0.3067, validation loss: 0.3242
2024-05-24 00:19:23 [INFO]: Epoch 023 - training loss: 0.3053, validation loss: 0.3229
2024-05-24 00:19:24 [INFO]: Epoch 024 - training loss: 0.2983, validation loss: 0.3174
2024-05-24 00:19:24 [INFO]: Epoch 025 - training loss: 0.2899, validation loss: 0.3169
2024-05-24 00:19:25 [INFO]: Epoch 026 - training loss: 0.2880, validation loss: 0.3118
2024-05-24 00:19:25 [INFO]: Epoch 027 - training loss: 0.2818, validation loss: 0.3138
2024-05-24 00:19:26 [INFO]: Epoch 028 - training loss: 0.2785, validation loss: 0.3145
2024-05-24 00:19:27 [INFO]: Epoch 029 - training loss: 0.2723, validation loss: 0.3151
2024-05-24 00:19:27 [INFO]: Epoch 030 - training loss: 0.2713, validation loss: 0.3133
2024-05-24 00:19:28 [INFO]: Epoch 031 - training loss: 0.2653, validation loss: 0.3144
2024-05-24 00:19:28 [INFO]: Epoch 032 - training loss: 0.2633, validation loss: 0.3183
2024-05-24 00:19:29 [INFO]: Epoch 033 - training loss: 0.2590, validation loss: 0.3164
2024-05-24 00:19:30 [INFO]: Epoch 034 - training loss: 0.2499, validation loss: 0.3118
2024-05-24 00:19:30 [INFO]: Epoch 035 - training loss: 0.2446, validation loss: 0.3125
2024-05-24 00:19:31 [INFO]: Epoch 036 - training loss: 0.2427, validation loss: 0.3145
2024-05-24 00:19:31 [INFO]: Epoch 037 - training loss: 0.2423, validation loss: 0.3136
2024-05-24 00:19:32 [INFO]: Epoch 038 - training loss: 0.2397, validation loss: 0.3166
2024-05-24 00:19:33 [INFO]: Epoch 039 - training loss: 0.2398, validation loss: 0.3187
2024-05-24 00:19:33 [INFO]: Epoch 040 - training loss: 0.2360, validation loss: 0.3146
2024-05-24 00:19:34 [INFO]: Epoch 041 - training loss: 0.2269, validation loss: 0.3160
2024-05-24 00:19:34 [INFO]: Epoch 042 - training loss: 0.2266, validation loss: 0.3164
2024-05-24 00:19:35 [INFO]: Epoch 043 - training loss: 0.2248, validation loss: 0.3148
2024-05-24 00:19:36 [INFO]: Epoch 044 - training loss: 0.2215, validation loss: 0.3113
2024-05-24 00:19:36 [INFO]: Epoch 045 - training loss: 0.2194, validation loss: 0.3126
2024-05-24 00:19:37 [INFO]: Epoch 046 - training loss: 0.2178, validation loss: 0.3135
2024-05-24 00:19:37 [INFO]: Epoch 047 - training loss: 0.2149, validation loss: 0.3157
2024-05-24 00:19:38 [INFO]: Epoch 048 - training loss: 0.2136, validation loss: 0.3147
2024-05-24 00:19:39 [INFO]: Epoch 049 - training loss: 0.2113, validation loss: 0.3154
2024-05-24 00:19:39 [INFO]: Epoch 050 - training loss: 0.2077, validation loss: 0.3173
2024-05-24 00:19:40 [INFO]: Epoch 051 - training loss: 0.2040, validation loss: 0.3175
2024-05-24 00:19:40 [INFO]: Epoch 052 - training loss: 0.2013, validation loss: 0.3182
2024-05-24 00:19:41 [INFO]: Epoch 053 - training loss: 0.2024, validation loss: 0.3154
2024-05-24 00:19:42 [INFO]: Epoch 054 - training loss: 0.2010, validation loss: 0.3141
2024-05-24 00:19:42 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-05-24 00:19:42 [INFO]: Finished training. The best model is from epoch#44.
2024-05-24 00:19:42 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_3/Transformer_physionet_2012_seta/20240524_T001909/Transformer.pypots
2024-05-24 00:19:42 [INFO]: Transformer on PhysioNet-2012-seta: MAE=0.2920, MSE=0.3395
2024-05-24 00:19:42 [INFO]: Successfully saved to overlay_premask_post_norm_saved_results/round_3/Transformer_physionet_2012_seta/imputation.pkl
2024-05-24 00:19:42 [INFO]: Using the given device: cuda:0
2024-05-24 00:19:42 [INFO]: Model files will be saved to overlay_premask_post_norm_saved_results/round_3/TimesNet_physionet_2012_seta/20240524_T001942
2024-05-24 00:19:42 [INFO]: Tensorboard file will be saved to overlay_premask_post_norm_saved_results/round_3/TimesNet_physionet_2012_seta/20240524_T001942/tensorboard
2024-05-24 00:19:42 [INFO]: TimesNet initialized with the given hyperparameters, the number of trainable parameters: 21,649,573
2024-05-24 00:19:43 [INFO]: Epoch 001 - training loss: 0.4943, validation loss: 0.3516
2024-05-24 00:19:43 [INFO]: Epoch 002 - training loss: 0.4118, validation loss: 0.3619
2024-05-24 00:19:44 [INFO]: Epoch 003 - training loss: 0.6873, validation loss: 0.3863
2024-05-24 00:19:45 [INFO]: Epoch 004 - training loss: 0.4349, validation loss: 0.3361
2024-05-24 00:19:46 [INFO]: Epoch 005 - training loss: 0.3949, validation loss: 0.3369
2024-05-24 00:19:46 [INFO]: Epoch 006 - training loss: 0.3364, validation loss: 0.3228
2024-05-24 00:19:47 [INFO]: Epoch 007 - training loss: 0.3188, validation loss: 0.3116
2024-05-24 00:19:48 [INFO]: Epoch 008 - training loss: 0.3080, validation loss: 0.3145
2024-05-24 00:19:48 [INFO]: Epoch 009 - training loss: 0.3020, validation loss: 0.3091
2024-05-24 00:19:49 [INFO]: Epoch 010 - training loss: 0.2963, validation loss: 0.3038
2024-05-24 00:19:50 [INFO]: Epoch 011 - training loss: 0.2878, validation loss: 0.3017
2024-05-24 00:19:50 [INFO]: Epoch 012 - training loss: 0.2889, validation loss: 0.2984
2024-05-24 00:19:51 [INFO]: Epoch 013 - training loss: 0.2809, validation loss: 0.3023
2024-05-24 00:19:52 [INFO]: Epoch 014 - training loss: 0.2685, validation loss: 0.2972
2024-05-24 00:19:53 [INFO]: Epoch 015 - training loss: 0.2669, validation loss: 0.2975
2024-05-24 00:19:53 [INFO]: Epoch 016 - training loss: 0.2640, validation loss: 0.2978
2024-05-24 00:19:54 [INFO]: Epoch 017 - training loss: 0.2606, validation loss: 0.2945
2024-05-24 00:19:55 [INFO]: Epoch 018 - training loss: 0.2545, validation loss: 0.2931
2024-05-24 00:19:55 [INFO]: Epoch 019 - training loss: 0.2591, validation loss: 0.2977
2024-05-24 00:19:56 [INFO]: Epoch 020 - training loss: 0.2629, validation loss: 0.2907
2024-05-24 00:19:57 [INFO]: Epoch 021 - training loss: 0.2894, validation loss: 0.2907
2024-05-24 00:19:57 [INFO]: Epoch 022 - training loss: 0.3048, validation loss: 0.3058
2024-05-24 00:19:58 [INFO]: Epoch 023 - training loss: 0.2687, validation loss: 0.2899
2024-05-24 00:19:59 [INFO]: Epoch 024 - training loss: 0.2377, validation loss: 0.2888
2024-05-24 00:20:00 [INFO]: Epoch 025 - training loss: 0.2266, validation loss: 0.2935
2024-05-24 00:20:00 [INFO]: Epoch 026 - training loss: 0.2227, validation loss: 0.2961
2024-05-24 00:20:01 [INFO]: Epoch 027 - training loss: 0.2189, validation loss: 0.2986
2024-05-24 00:20:02 [INFO]: Epoch 028 - training loss: 0.2224, validation loss: 0.2899
2024-05-24 00:20:02 [INFO]: Epoch 029 - training loss: 0.2150, validation loss: 0.3046
2024-05-24 00:20:03 [INFO]: Epoch 030 - training loss: 0.2080, validation loss: 0.3070
2024-05-24 00:20:04 [INFO]: Epoch 031 - training loss: 0.2068, validation loss: 0.3034
2024-05-24 00:20:05 [INFO]: Epoch 032 - training loss: 0.2010, validation loss: 0.3062
2024-05-24 00:20:05 [INFO]: Epoch 033 - training loss: 0.2110, validation loss: 0.2964
2024-05-24 00:20:06 [INFO]: Epoch 034 - training loss: 0.1995, validation loss: 0.3103
2024-05-24 00:20:06 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-05-24 00:20:06 [INFO]: Finished training. The best model is from epoch#24.
2024-05-24 00:20:06 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_3/TimesNet_physionet_2012_seta/20240524_T001942/TimesNet.pypots
2024-05-24 00:20:06 [INFO]: TimesNet on PhysioNet-2012-seta: MAE=0.2954, MSE=0.2992
2024-05-24 00:20:06 [INFO]: Successfully saved to overlay_premask_post_norm_saved_results/round_3/TimesNet_physionet_2012_seta/imputation.pkl
2024-05-24 00:20:06 [INFO]: Using the given device: cuda:0
2024-05-24 00:20:06 [INFO]: Model files will be saved to overlay_premask_post_norm_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T002006
2024-05-24 00:20:06 [INFO]: Tensorboard file will be saved to overlay_premask_post_norm_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T002006/tensorboard
2024-05-24 00:20:06 [INFO]: CSDI initialized with the given hyperparameters, the number of trainable parameters: 1,694,753
2024-05-24 00:20:49 [INFO]: Epoch 001 - training loss: 0.4265, validation loss: 0.3315
2024-05-24 00:20:50 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T002006/CSDI_epoch1_loss0.33147290349006653.pypots
2024-05-24 00:21:33 [INFO]: Epoch 002 - training loss: 0.3115, validation loss: 0.3000
2024-05-24 00:21:33 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T002006/CSDI_epoch2_loss0.30003399699926375.pypots
2024-05-24 00:22:16 [INFO]: Epoch 003 - training loss: 0.2858, validation loss: 0.2775
2024-05-24 00:22:16 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T002006/CSDI_epoch3_loss0.2774800628423691.pypots
2024-05-24 00:23:00 [INFO]: Epoch 004 - training loss: 0.2823, validation loss: 0.2584
2024-05-24 00:23:00 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T002006/CSDI_epoch4_loss0.2583827681839466.pypots
2024-05-24 00:23:43 [INFO]: Epoch 005 - training loss: 0.2612, validation loss: 0.2558
2024-05-24 00:23:43 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T002006/CSDI_epoch5_loss0.25576376393437383.pypots
2024-05-24 00:24:27 [INFO]: Epoch 006 - training loss: 0.2606, validation loss: 0.2431
2024-05-24 00:24:27 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T002006/CSDI_epoch6_loss0.24310229420661927.pypots
2024-05-24 00:25:11 [INFO]: Epoch 007 - training loss: 0.2411, validation loss: 0.2358
2024-05-24 00:25:11 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T002006/CSDI_epoch7_loss0.2357561968266964.pypots
2024-05-24 00:25:54 [INFO]: Epoch 008 - training loss: 0.2575, validation loss: 0.2383
2024-05-24 00:25:54 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T002006/CSDI_epoch8_loss0.2383063904941082.pypots
2024-05-24 00:26:38 [INFO]: Epoch 009 - training loss: 0.2343, validation loss: 0.2206
2024-05-24 00:26:38 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T002006/CSDI_epoch9_loss0.22060477882623672.pypots
2024-05-24 00:27:22 [INFO]: Epoch 010 - training loss: 0.2224, validation loss: 0.2217
2024-05-24 00:27:22 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T002006/CSDI_epoch10_loss0.2216775044798851.pypots
2024-05-24 00:28:05 [INFO]: Epoch 011 - training loss: 0.2256, validation loss: 0.2186
2024-05-24 00:28:05 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T002006/CSDI_epoch11_loss0.2185891941189766.pypots
2024-05-24 00:28:49 [INFO]: Epoch 012 - training loss: 0.2244, validation loss: 0.2130
2024-05-24 00:28:49 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T002006/CSDI_epoch12_loss0.21301354393362998.pypots
2024-05-24 00:29:32 [INFO]: Epoch 013 - training loss: 0.2335, validation loss: 0.2166
2024-05-24 00:29:33 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T002006/CSDI_epoch13_loss0.21655036360025406.pypots
2024-05-24 00:30:16 [INFO]: Epoch 014 - training loss: 0.2131, validation loss: 0.2048
2024-05-24 00:30:16 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T002006/CSDI_epoch14_loss0.20476714745163918.pypots
2024-05-24 00:31:00 [INFO]: Epoch 015 - training loss: 0.2166, validation loss: 0.2082
2024-05-24 00:31:00 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T002006/CSDI_epoch15_loss0.20821312814950943.pypots
2024-05-24 00:31:43 [INFO]: Epoch 016 - training loss: 0.2194, validation loss: 0.2043
2024-05-24 00:31:43 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T002006/CSDI_epoch16_loss0.20434847474098206.pypots
2024-05-24 00:32:27 [INFO]: Epoch 017 - training loss: 0.2201, validation loss: 0.2033
2024-05-24 00:32:27 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T002006/CSDI_epoch17_loss0.20329350754618644.pypots
2024-05-24 00:33:11 [INFO]: Epoch 018 - training loss: 0.2153, validation loss: 0.2032
2024-05-24 00:33:11 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T002006/CSDI_epoch18_loss0.20315681397914886.pypots
2024-05-24 00:33:54 [INFO]: Epoch 019 - training loss: 0.2098, validation loss: 0.2028
2024-05-24 00:33:54 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T002006/CSDI_epoch19_loss0.20278398171067238.pypots
2024-05-24 00:34:38 [INFO]: Epoch 020 - training loss: 0.2084, validation loss: 0.1991
2024-05-24 00:34:38 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T002006/CSDI_epoch20_loss0.19909792616963387.pypots
2024-05-24 00:35:22 [INFO]: Epoch 021 - training loss: 0.2084, validation loss: 0.2010
2024-05-24 00:35:22 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T002006/CSDI_epoch21_loss0.2010326899588108.pypots
2024-05-24 00:36:05 [INFO]: Epoch 022 - training loss: 0.2060, validation loss: 0.1956
2024-05-24 00:36:05 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T002006/CSDI_epoch22_loss0.19557926207780837.pypots
2024-05-24 00:36:49 [INFO]: Epoch 023 - training loss: 0.2037, validation loss: 0.1955
2024-05-24 00:36:49 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T002006/CSDI_epoch23_loss0.1954530231654644.pypots
2024-05-24 00:37:32 [INFO]: Epoch 024 - training loss: 0.2104, validation loss: 0.2045
2024-05-24 00:37:32 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T002006/CSDI_epoch24_loss0.20448944419622422.pypots
2024-05-24 00:38:16 [INFO]: Epoch 025 - training loss: 0.1991, validation loss: 0.1963
2024-05-24 00:38:16 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T002006/CSDI_epoch25_loss0.19626477360725403.pypots
2024-05-24 00:39:00 [INFO]: Epoch 026 - training loss: 0.2043, validation loss: 0.1941
2024-05-24 00:39:00 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T002006/CSDI_epoch26_loss0.1940595805644989.pypots
2024-05-24 00:39:43 [INFO]: Epoch 027 - training loss: 0.2148, validation loss: 0.1947
2024-05-24 00:39:43 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T002006/CSDI_epoch27_loss0.1947110615670681.pypots
2024-05-24 00:40:27 [INFO]: Epoch 028 - training loss: 0.1972, validation loss: 0.1950
2024-05-24 00:40:27 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T002006/CSDI_epoch28_loss0.19499033242464064.pypots
2024-05-24 00:41:11 [INFO]: Epoch 029 - training loss: 0.2030, validation loss: 0.1947
2024-05-24 00:41:11 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T002006/CSDI_epoch29_loss0.1946989357471466.pypots
2024-05-24 00:41:54 [INFO]: Epoch 030 - training loss: 0.2071, validation loss: 0.1960
2024-05-24 00:41:54 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T002006/CSDI_epoch30_loss0.19598100259900092.pypots
2024-05-24 00:42:38 [INFO]: Epoch 031 - training loss: 0.2133, validation loss: 0.1947
2024-05-24 00:42:38 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T002006/CSDI_epoch31_loss0.19469243735074998.pypots
2024-05-24 00:43:22 [INFO]: Epoch 032 - training loss: 0.1984, validation loss: 0.1918
2024-05-24 00:43:22 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T002006/CSDI_epoch32_loss0.1918190136551857.pypots
2024-05-24 00:44:05 [INFO]: Epoch 033 - training loss: 0.2064, validation loss: 0.1953
2024-05-24 00:44:05 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T002006/CSDI_epoch33_loss0.1953311748802662.pypots
2024-05-24 00:44:49 [INFO]: Epoch 034 - training loss: 0.2122, validation loss: 0.1942
2024-05-24 00:44:49 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T002006/CSDI_epoch34_loss0.19420818239450455.pypots
2024-05-24 00:45:33 [INFO]: Epoch 035 - training loss: 0.1951, validation loss: 0.1943
2024-05-24 00:45:33 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T002006/CSDI_epoch35_loss0.194294211268425.pypots
2024-05-24 00:46:16 [INFO]: Epoch 036 - training loss: 0.2102, validation loss: 0.1925
2024-05-24 00:46:16 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T002006/CSDI_epoch36_loss0.19245412051677704.pypots
2024-05-24 00:47:00 [INFO]: Epoch 037 - training loss: 0.2020, validation loss: 0.1923
2024-05-24 00:47:00 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T002006/CSDI_epoch37_loss0.19233611077070237.pypots
2024-05-24 00:47:44 [INFO]: Epoch 038 - training loss: 0.1901, validation loss: 0.1937
2024-05-24 00:47:44 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T002006/CSDI_epoch38_loss0.19370757415890694.pypots
2024-05-24 00:48:27 [INFO]: Epoch 039 - training loss: 0.1906, validation loss: 0.1944
2024-05-24 00:48:27 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T002006/CSDI_epoch39_loss0.19438679069280623.pypots
2024-05-24 00:49:11 [INFO]: Epoch 040 - training loss: 0.2033, validation loss: 0.1927
2024-05-24 00:49:11 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T002006/CSDI_epoch40_loss0.19274889454245567.pypots
2024-05-24 00:49:55 [INFO]: Epoch 041 - training loss: 0.1989, validation loss: 0.1921
2024-05-24 00:49:55 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T002006/CSDI_epoch41_loss0.19208822771906853.pypots
2024-05-24 00:50:38 [INFO]: Epoch 042 - training loss: 0.2089, validation loss: 0.1925
2024-05-24 00:50:38 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T002006/CSDI_epoch42_loss0.19248967468738556.pypots
2024-05-24 00:50:38 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-05-24 00:50:38 [INFO]: Finished training. The best model is from epoch#32.
2024-05-24 00:50:38 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T002006/CSDI.pypots
2024-05-24 00:57:59 [INFO]: CSDI on PhysioNet-2012-seta: MAE=0.2366, MSE=0.3116
2024-05-24 07:30:31 [INFO]: Successfully saved to overlay_premask_post_norm_saved_results/round_3/CSDI_physionet_2012_seta/imputation.pkl
2024-05-24 07:34:00 [INFO]: Using the given device: cuda:0
2024-05-24 07:34:01 [INFO]: Model files will be saved to overlay_premask_post_norm_saved_results/round_3/GPVAE_physionet_2012_seta/20240524_T073400
2024-05-24 07:34:01 [INFO]: Tensorboard file will be saved to overlay_premask_post_norm_saved_results/round_3/GPVAE_physionet_2012_seta/20240524_T073400/tensorboard
2024-05-24 07:34:01 [INFO]: GPVAE initialized with the given hyperparameters, the number of trainable parameters: 1,028,244
2024-05-24 07:34:03 [INFO]: Epoch 001 - training loss: 43142.6593, validation loss: 0.9593
2024-05-24 07:34:03 [INFO]: Epoch 002 - training loss: 24611.5158, validation loss: 0.7454
2024-05-24 07:34:04 [INFO]: Epoch 003 - training loss: 23532.1458, validation loss: 0.7166
2024-05-24 07:34:05 [INFO]: Epoch 004 - training loss: 23209.5801, validation loss: 0.6819
2024-05-24 07:34:05 [INFO]: Epoch 005 - training loss: 23049.5432, validation loss: 0.6553
2024-05-24 07:34:06 [INFO]: Epoch 006 - training loss: 22962.6705, validation loss: 0.6459
2024-05-24 07:34:07 [INFO]: Epoch 007 - training loss: 22910.5258, validation loss: 0.6476
2024-05-24 07:34:07 [INFO]: Epoch 008 - training loss: 22877.6293, validation loss: 0.6450
2024-05-24 07:34:08 [INFO]: Epoch 009 - training loss: 22855.2333, validation loss: 0.6406
2024-05-24 07:34:09 [INFO]: Epoch 010 - training loss: 22840.0631, validation loss: 0.6379
2024-05-24 07:34:09 [INFO]: Epoch 011 - training loss: 22828.5675, validation loss: 0.6392
2024-05-24 07:34:10 [INFO]: Epoch 012 - training loss: 22820.4308, validation loss: 0.6408
2024-05-24 07:34:11 [INFO]: Epoch 013 - training loss: 22813.9851, validation loss: 0.6357
2024-05-24 07:34:11 [INFO]: Epoch 014 - training loss: 22808.9336, validation loss: 0.6317
2024-05-24 07:34:12 [INFO]: Epoch 015 - training loss: 22805.4339, validation loss: 0.6306
2024-05-24 07:34:13 [INFO]: Epoch 016 - training loss: 22802.7146, validation loss: 0.6285
2024-05-24 07:34:14 [INFO]: Epoch 017 - training loss: 22799.5469, validation loss: 0.6291
2024-05-24 07:34:14 [INFO]: Epoch 018 - training loss: 22797.2615, validation loss: 0.6275
2024-05-24 07:34:15 [INFO]: Epoch 019 - training loss: 22795.3222, validation loss: 0.6189
2024-05-24 07:34:16 [INFO]: Epoch 020 - training loss: 22792.5689, validation loss: 0.6155
2024-05-24 07:34:16 [INFO]: Epoch 021 - training loss: 22791.3672, validation loss: 0.6173
2024-05-24 07:34:17 [INFO]: Epoch 022 - training loss: 22789.9239, validation loss: 0.6077
2024-05-24 07:34:18 [INFO]: Epoch 023 - training loss: 22789.3671, validation loss: 0.6045
2024-05-24 07:34:18 [INFO]: Epoch 024 - training loss: 22788.5706, validation loss: 0.6040
2024-05-24 07:34:19 [INFO]: Epoch 025 - training loss: 22787.4993, validation loss: 0.6069
2024-05-24 07:34:20 [INFO]: Epoch 026 - training loss: 22786.7129, validation loss: 0.6005
2024-05-24 07:34:20 [INFO]: Epoch 027 - training loss: 22784.6344, validation loss: 0.6013
2024-05-24 07:34:21 [INFO]: Epoch 028 - training loss: 22783.9054, validation loss: 0.5964
2024-05-24 07:34:22 [INFO]: Epoch 029 - training loss: 22783.0668, validation loss: 0.5948
2024-05-24 07:34:22 [INFO]: Epoch 030 - training loss: 22783.1477, validation loss: 0.6167
2024-05-24 07:34:23 [INFO]: Epoch 031 - training loss: 22782.1456, validation loss: 0.5918
2024-05-24 07:34:24 [INFO]: Epoch 032 - training loss: 22781.4083, validation loss: 0.5879
2024-05-24 07:34:25 [INFO]: Epoch 033 - training loss: 22780.6694, validation loss: 0.5899
2024-05-24 07:34:25 [INFO]: Epoch 034 - training loss: 22779.2102, validation loss: 0.5887
2024-05-24 07:34:26 [INFO]: Epoch 035 - training loss: 22778.4890, validation loss: 0.5793
2024-05-24 07:34:27 [INFO]: Epoch 036 - training loss: 22777.2607, validation loss: 0.5750
2024-05-24 07:34:27 [INFO]: Epoch 037 - training loss: 22775.8105, validation loss: 0.5689
2024-05-24 07:34:28 [INFO]: Epoch 038 - training loss: 22774.5204, validation loss: 0.5690
2024-05-24 07:34:29 [INFO]: Epoch 039 - training loss: 22774.7299, validation loss: 0.5701
2024-05-24 07:34:29 [INFO]: Epoch 040 - training loss: 22774.0379, validation loss: 0.5604
2024-05-24 07:34:30 [INFO]: Epoch 041 - training loss: 22773.1820, validation loss: 0.5629
2024-05-24 07:34:31 [INFO]: Epoch 042 - training loss: 22771.9643, validation loss: 0.5604
2024-05-24 07:34:31 [INFO]: Epoch 043 - training loss: 22770.6313, validation loss: 0.5542
2024-05-24 07:34:32 [INFO]: Epoch 044 - training loss: 22769.5963, validation loss: 0.5540
2024-05-24 07:34:33 [INFO]: Epoch 045 - training loss: 22769.3531, validation loss: 0.5766
2024-05-24 07:34:33 [INFO]: Epoch 046 - training loss: 22768.7095, validation loss: 0.5543
2024-05-24 07:34:34 [INFO]: Epoch 047 - training loss: 22767.3124, validation loss: 0.5469
2024-05-24 07:34:35 [INFO]: Epoch 048 - training loss: 22766.6716, validation loss: 0.5464
2024-05-24 07:34:35 [INFO]: Epoch 049 - training loss: 22766.1762, validation loss: 0.5470
2024-05-24 07:34:36 [INFO]: Epoch 050 - training loss: 22766.3688, validation loss: 0.5470
2024-05-24 07:34:37 [INFO]: Epoch 051 - training loss: 22765.7307, validation loss: 0.5416
2024-05-24 07:34:37 [INFO]: Epoch 052 - training loss: 22764.7237, validation loss: 0.5492
2024-05-24 07:34:38 [INFO]: Epoch 053 - training loss: 22765.1358, validation loss: 0.5468
2024-05-24 07:34:39 [INFO]: Epoch 054 - training loss: 22763.5125, validation loss: 0.5338
2024-05-24 07:34:39 [INFO]: Epoch 055 - training loss: 22762.8132, validation loss: 0.5427
2024-05-24 07:34:40 [INFO]: Epoch 056 - training loss: 22762.7306, validation loss: 0.5336
2024-05-24 07:34:41 [INFO]: Epoch 057 - training loss: 22762.4903, validation loss: 0.5313
2024-05-24 07:34:41 [INFO]: Epoch 058 - training loss: 22761.7803, validation loss: 0.5267
2024-05-24 07:34:42 [INFO]: Epoch 059 - training loss: 22761.2081, validation loss: 0.5252
2024-05-24 07:34:43 [INFO]: Epoch 060 - training loss: 22760.6875, validation loss: 0.5252
2024-05-24 07:34:43 [INFO]: Epoch 061 - training loss: 22760.2858, validation loss: 0.5214
2024-05-24 07:34:44 [INFO]: Epoch 062 - training loss: 22759.9410, validation loss: 0.5247
2024-05-24 07:34:45 [INFO]: Epoch 063 - training loss: 22760.4235, validation loss: 0.5214
2024-05-24 07:34:45 [INFO]: Epoch 064 - training loss: 22759.8013, validation loss: 0.5251
2024-05-24 07:34:46 [INFO]: Epoch 065 - training loss: 22760.2302, validation loss: 0.5186
2024-05-24 07:34:47 [INFO]: Epoch 066 - training loss: 22758.9449, validation loss: 0.5310
2024-05-24 07:34:48 [INFO]: Epoch 067 - training loss: 22758.7006, validation loss: 0.5213
2024-05-24 07:34:48 [INFO]: Epoch 068 - training loss: 22757.7200, validation loss: 0.5089
2024-05-24 07:34:49 [INFO]: Epoch 069 - training loss: 22756.9996, validation loss: 0.5085
2024-05-24 07:34:50 [INFO]: Epoch 070 - training loss: 22757.1155, validation loss: 0.5073
2024-05-24 07:34:50 [INFO]: Epoch 071 - training loss: 22756.3694, validation loss: 0.5044
2024-05-24 07:34:51 [INFO]: Epoch 072 - training loss: 22756.2918, validation loss: 0.5038
2024-05-24 07:34:52 [INFO]: Epoch 073 - training loss: 22757.2152, validation loss: 0.5053
2024-05-24 07:34:52 [INFO]: Epoch 074 - training loss: 22756.3724, validation loss: 0.5109
2024-05-24 07:34:53 [INFO]: Epoch 075 - training loss: 22755.9065, validation loss: 0.5010
2024-05-24 07:34:54 [INFO]: Epoch 076 - training loss: 22755.9281, validation loss: 0.5000
2024-05-24 07:34:54 [INFO]: Epoch 077 - training loss: 22755.0961, validation loss: 0.5012
2024-05-24 07:34:55 [INFO]: Epoch 078 - training loss: 22755.9768, validation loss: 0.5022
2024-05-24 07:34:56 [INFO]: Epoch 079 - training loss: 22754.6011, validation loss: 0.5016
2024-05-24 07:34:56 [INFO]: Epoch 080 - training loss: 22754.7590, validation loss: 0.5042
2024-05-24 07:34:57 [INFO]: Epoch 081 - training loss: 22754.4493, validation loss: 0.5034
2024-05-24 07:34:58 [INFO]: Epoch 082 - training loss: 22754.2482, validation loss: 0.4967
2024-05-24 07:34:58 [INFO]: Epoch 083 - training loss: 22753.3982, validation loss: 0.4964
2024-05-24 07:34:59 [INFO]: Epoch 084 - training loss: 22753.1554, validation loss: 0.4987
2024-05-24 07:35:00 [INFO]: Epoch 085 - training loss: 22753.3784, validation loss: 0.4979
2024-05-24 07:35:00 [INFO]: Epoch 086 - training loss: 22752.5972, validation loss: 0.4969
2024-05-24 07:35:01 [INFO]: Epoch 087 - training loss: 22752.2497, validation loss: 0.4943
2024-05-24 07:35:02 [INFO]: Epoch 088 - training loss: 22752.3725, validation loss: 0.4986
2024-05-24 07:35:02 [INFO]: Epoch 089 - training loss: 22752.1808, validation loss: 0.4945
2024-05-24 07:35:03 [INFO]: Epoch 090 - training loss: 22752.6760, validation loss: 0.4954
2024-05-24 07:35:04 [INFO]: Epoch 091 - training loss: 22751.9668, validation loss: 0.4956
2024-05-24 07:35:04 [INFO]: Epoch 092 - training loss: 22752.0792, validation loss: 0.4920
2024-05-24 07:35:05 [INFO]: Epoch 093 - training loss: 22751.5755, validation loss: 0.4950
2024-05-24 07:35:06 [INFO]: Epoch 094 - training loss: 22751.3702, validation loss: 0.4964
2024-05-24 07:35:06 [INFO]: Epoch 095 - training loss: 22751.3260, validation loss: 0.5046
2024-05-24 07:35:07 [INFO]: Epoch 096 - training loss: 22754.7270, validation loss: 0.4986
2024-05-24 07:35:08 [INFO]: Epoch 097 - training loss: 22753.5017, validation loss: 0.4947
2024-05-24 07:35:08 [INFO]: Epoch 098 - training loss: 22752.2822, validation loss: 0.5017
2024-05-24 07:35:09 [INFO]: Epoch 099 - training loss: 22751.7507, validation loss: 0.4992
2024-05-24 07:35:10 [INFO]: Epoch 100 - training loss: 22751.5507, validation loss: 0.4913
2024-05-24 07:35:10 [INFO]: Epoch 101 - training loss: 22751.1149, validation loss: 0.4883
2024-05-24 07:35:11 [INFO]: Epoch 102 - training loss: 22750.1551, validation loss: 0.4863
2024-05-24 07:35:12 [INFO]: Epoch 103 - training loss: 22750.2396, validation loss: 0.4840
2024-05-24 07:35:13 [INFO]: Epoch 104 - training loss: 22749.8242, validation loss: 0.4857
2024-05-24 07:35:13 [INFO]: Epoch 105 - training loss: 22749.9471, validation loss: 0.4834
2024-05-24 07:35:14 [INFO]: Epoch 106 - training loss: 22749.9894, validation loss: 0.4836
2024-05-24 07:35:15 [INFO]: Epoch 107 - training loss: 22749.3207, validation loss: 0.4825
2024-05-24 07:35:15 [INFO]: Epoch 108 - training loss: 22748.9640, validation loss: 0.4803
2024-05-24 07:35:16 [INFO]: Epoch 109 - training loss: 22749.2020, validation loss: 0.4843
2024-05-24 07:35:17 [INFO]: Epoch 110 - training loss: 22749.0177, validation loss: 0.4789
2024-05-24 07:35:17 [INFO]: Epoch 111 - training loss: 22748.8283, validation loss: 0.4856
2024-05-24 07:35:18 [INFO]: Epoch 112 - training loss: 22748.7077, validation loss: 0.4802
2024-05-24 07:35:19 [INFO]: Epoch 113 - training loss: 22748.9010, validation loss: 0.4793
2024-05-24 07:35:19 [INFO]: Epoch 114 - training loss: 22749.1435, validation loss: 0.4800
2024-05-24 07:35:20 [INFO]: Epoch 115 - training loss: 22748.9861, validation loss: 0.4850
2024-05-24 07:35:21 [INFO]: Epoch 116 - training loss: 22750.6499, validation loss: 0.4887
2024-05-24 07:35:21 [INFO]: Epoch 117 - training loss: 22751.2810, validation loss: 0.4817
2024-05-24 07:35:22 [INFO]: Epoch 118 - training loss: 22749.0395, validation loss: 0.4807
2024-05-24 07:35:23 [INFO]: Epoch 119 - training loss: 22748.6731, validation loss: 0.4813
2024-05-24 07:35:24 [INFO]: Epoch 120 - training loss: 22748.4412, validation loss: 0.4787
2024-05-24 07:35:24 [INFO]: Epoch 121 - training loss: 22747.8537, validation loss: 0.4803
2024-05-24 07:35:25 [INFO]: Epoch 122 - training loss: 22747.9517, validation loss: 0.4787
2024-05-24 07:35:26 [INFO]: Epoch 123 - training loss: 22748.1448, validation loss: 0.4767
2024-05-24 07:35:26 [INFO]: Epoch 124 - training loss: 22747.7647, validation loss: 0.4788
2024-05-24 07:35:27 [INFO]: Epoch 125 - training loss: 22747.7110, validation loss: 0.4794
2024-05-24 07:35:28 [INFO]: Epoch 126 - training loss: 22747.4906, validation loss: 0.4783
2024-05-24 07:35:28 [INFO]: Epoch 127 - training loss: 22747.9175, validation loss: 0.4740
2024-05-24 07:35:29 [INFO]: Epoch 128 - training loss: 22747.8919, validation loss: 0.4772
2024-05-24 07:35:30 [INFO]: Epoch 129 - training loss: 22747.4553, validation loss: 0.4739
2024-05-24 07:35:30 [INFO]: Epoch 130 - training loss: 22747.1069, validation loss: 0.4764
2024-05-24 07:35:31 [INFO]: Epoch 131 - training loss: 22746.6459, validation loss: 0.4751
2024-05-24 07:35:32 [INFO]: Epoch 132 - training loss: 22746.5864, validation loss: 0.4726
2024-05-24 07:35:32 [INFO]: Epoch 133 - training loss: 22747.1090, validation loss: 0.4747
2024-05-24 07:35:33 [INFO]: Epoch 134 - training loss: 22746.7209, validation loss: 0.4820
2024-05-24 07:35:34 [INFO]: Epoch 135 - training loss: 22748.7339, validation loss: 0.4743
2024-05-24 07:35:34 [INFO]: Epoch 136 - training loss: 22747.9595, validation loss: 0.4750
2024-05-24 07:35:35 [INFO]: Epoch 137 - training loss: 22746.2870, validation loss: 0.4729
2024-05-24 07:35:36 [INFO]: Epoch 138 - training loss: 22745.7427, validation loss: 0.4779
2024-05-24 07:35:36 [INFO]: Epoch 139 - training loss: 22746.3910, validation loss: 0.4716
2024-05-24 07:35:37 [INFO]: Epoch 140 - training loss: 22745.9435, validation loss: 0.4718
2024-05-24 07:35:38 [INFO]: Epoch 141 - training loss: 22745.4052, validation loss: 0.4703
2024-05-24 07:35:38 [INFO]: Epoch 142 - training loss: 22745.2418, validation loss: 0.4727
2024-05-24 07:35:39 [INFO]: Epoch 143 - training loss: 22745.2291, validation loss: 0.4714
2024-05-24 07:35:40 [INFO]: Epoch 144 - training loss: 22745.3779, validation loss: 0.4659
2024-05-24 07:35:40 [INFO]: Epoch 145 - training loss: 22745.7849, validation loss: 0.4690
2024-05-24 07:35:41 [INFO]: Epoch 146 - training loss: 22745.4177, validation loss: 0.4758
2024-05-24 07:35:42 [INFO]: Epoch 147 - training loss: 22745.5042, validation loss: 0.4729
2024-05-24 07:35:42 [INFO]: Epoch 148 - training loss: 22745.0611, validation loss: 0.4681
2024-05-24 07:35:43 [INFO]: Epoch 149 - training loss: 22746.6082, validation loss: 0.4694
2024-05-24 07:35:44 [INFO]: Epoch 150 - training loss: 22746.5855, validation loss: 0.4690
2024-05-24 07:35:44 [INFO]: Epoch 151 - training loss: 22744.5210, validation loss: 0.4654
2024-05-24 07:35:45 [INFO]: Epoch 152 - training loss: 22744.4308, validation loss: 0.4663
2024-05-24 07:35:46 [INFO]: Epoch 153 - training loss: 22744.3069, validation loss: 0.4615
2024-05-24 07:35:46 [INFO]: Epoch 154 - training loss: 22744.2924, validation loss: 0.4703
2024-05-24 07:35:47 [INFO]: Epoch 155 - training loss: 22744.4377, validation loss: 0.4642
2024-05-24 07:35:47 [INFO]: Epoch 156 - training loss: 22744.0541, validation loss: 0.4698
2024-05-24 07:35:48 [INFO]: Epoch 157 - training loss: 22743.9697, validation loss: 0.4645
2024-05-24 07:35:49 [INFO]: Epoch 158 - training loss: 22744.6343, validation loss: 0.4614
2024-05-24 07:35:49 [INFO]: Epoch 159 - training loss: 22745.3040, validation loss: 0.4658
2024-05-24 07:35:50 [INFO]: Epoch 160 - training loss: 22744.2287, validation loss: 0.4722
2024-05-24 07:35:50 [INFO]: Epoch 161 - training loss: 22744.7442, validation loss: 0.4640
2024-05-24 07:35:51 [INFO]: Epoch 162 - training loss: 22744.8175, validation loss: 0.4669
2024-05-24 07:35:52 [INFO]: Epoch 163 - training loss: 22743.6266, validation loss: 0.4639
2024-05-24 07:35:52 [INFO]: Epoch 164 - training loss: 22743.3074, validation loss: 0.4612
2024-05-24 07:35:53 [INFO]: Epoch 165 - training loss: 22743.4001, validation loss: 0.4629
2024-05-24 07:35:53 [INFO]: Epoch 166 - training loss: 22743.0220, validation loss: 0.4606
2024-05-24 07:35:54 [INFO]: Epoch 167 - training loss: 22743.1951, validation loss: 0.4606
2024-05-24 07:35:54 [INFO]: Epoch 168 - training loss: 22742.9318, validation loss: 0.4603
2024-05-24 07:35:55 [INFO]: Epoch 169 - training loss: 22743.1585, validation loss: 0.4629
2024-05-24 07:35:56 [INFO]: Epoch 170 - training loss: 22742.9479, validation loss: 0.4632
2024-05-24 07:35:56 [INFO]: Epoch 171 - training loss: 22742.9648, validation loss: 0.4601
2024-05-24 07:35:57 [INFO]: Epoch 172 - training loss: 22742.7908, validation loss: 0.4577
2024-05-24 07:35:57 [INFO]: Epoch 173 - training loss: 22742.6889, validation loss: 0.4623
2024-05-24 07:35:58 [INFO]: Epoch 174 - training loss: 22743.1860, validation loss: 0.4633
2024-05-24 07:35:59 [INFO]: Epoch 175 - training loss: 22743.2883, validation loss: 0.4637
2024-05-24 07:35:59 [INFO]: Epoch 176 - training loss: 22743.3386, validation loss: 0.4600
2024-05-24 07:36:00 [INFO]: Epoch 177 - training loss: 22742.9150, validation loss: 0.4600
2024-05-24 07:36:00 [INFO]: Epoch 178 - training loss: 22742.6073, validation loss: 0.4582
2024-05-24 07:36:01 [INFO]: Epoch 179 - training loss: 22742.6353, validation loss: 0.4596
2024-05-24 07:36:02 [INFO]: Epoch 180 - training loss: 22742.4886, validation loss: 0.4603
2024-05-24 07:36:02 [INFO]: Epoch 181 - training loss: 22742.4631, validation loss: 0.4608
2024-05-24 07:36:03 [INFO]: Epoch 182 - training loss: 22742.5335, validation loss: 0.4548
2024-05-24 07:36:03 [INFO]: Epoch 183 - training loss: 22742.7087, validation loss: 0.4546
2024-05-24 07:36:04 [INFO]: Epoch 184 - training loss: 22742.6557, validation loss: 0.4585
2024-05-24 07:36:05 [INFO]: Epoch 185 - training loss: 22742.3699, validation loss: 0.4582
2024-05-24 07:36:05 [INFO]: Epoch 186 - training loss: 22742.0814, validation loss: 0.4573
2024-05-24 07:36:06 [INFO]: Epoch 187 - training loss: 22742.4473, validation loss: 0.4599
2024-05-24 07:36:06 [INFO]: Epoch 188 - training loss: 22742.3563, validation loss: 0.4579
2024-05-24 07:36:07 [INFO]: Epoch 189 - training loss: 22742.4782, validation loss: 0.4642
2024-05-24 07:36:07 [INFO]: Epoch 190 - training loss: 22742.4162, validation loss: 0.4587
2024-05-24 07:36:08 [INFO]: Epoch 191 - training loss: 22742.4487, validation loss: 0.4577
2024-05-24 07:36:09 [INFO]: Epoch 192 - training loss: 22742.0914, validation loss: 0.4571
2024-05-24 07:36:09 [INFO]: Epoch 193 - training loss: 22741.7163, validation loss: 0.4543
2024-05-24 07:36:10 [INFO]: Epoch 194 - training loss: 22742.0840, validation loss: 0.4563
2024-05-24 07:36:10 [INFO]: Epoch 195 - training loss: 22741.4916, validation loss: 0.4572
2024-05-24 07:36:11 [INFO]: Epoch 196 - training loss: 22741.1932, validation loss: 0.4593
2024-05-24 07:36:12 [INFO]: Epoch 197 - training loss: 22741.3343, validation loss: 0.4565
2024-05-24 07:36:12 [INFO]: Epoch 198 - training loss: 22741.5495, validation loss: 0.4587
2024-05-24 07:36:13 [INFO]: Epoch 199 - training loss: 22741.5883, validation loss: 0.4550
2024-05-24 07:36:13 [INFO]: Epoch 200 - training loss: 22741.5715, validation loss: 0.4531
2024-05-24 07:36:14 [INFO]: Epoch 201 - training loss: 22741.2331, validation loss: 0.4597
2024-05-24 07:36:15 [INFO]: Epoch 202 - training loss: 22740.9242, validation loss: 0.4608
2024-05-24 07:36:15 [INFO]: Epoch 203 - training loss: 22740.9516, validation loss: 0.4565
2024-05-24 07:36:16 [INFO]: Epoch 204 - training loss: 22741.3406, validation loss: 0.4536
2024-05-24 07:36:16 [INFO]: Epoch 205 - training loss: 22741.5119, validation loss: 0.4542
2024-05-24 07:36:17 [INFO]: Epoch 206 - training loss: 22741.3464, validation loss: 0.4549
2024-05-24 07:36:18 [INFO]: Epoch 207 - training loss: 22741.4039, validation loss: 0.4564
2024-05-24 07:36:18 [INFO]: Epoch 208 - training loss: 22741.0900, validation loss: 0.4513
2024-05-24 07:36:19 [INFO]: Epoch 209 - training loss: 22741.1869, validation loss: 0.4548
2024-05-24 07:36:19 [INFO]: Epoch 210 - training loss: 22740.3761, validation loss: 0.4526
2024-05-24 07:36:20 [INFO]: Epoch 211 - training loss: 22740.5533, validation loss: 0.4558
2024-05-24 07:36:21 [INFO]: Epoch 212 - training loss: 22741.1618, validation loss: 0.4526
2024-05-24 07:36:21 [INFO]: Epoch 213 - training loss: 22740.6740, validation loss: 0.4491
2024-05-24 07:36:22 [INFO]: Epoch 214 - training loss: 22740.7520, validation loss: 0.4524
2024-05-24 07:36:22 [INFO]: Epoch 215 - training loss: 22740.1937, validation loss: 0.4516
2024-05-24 07:36:23 [INFO]: Epoch 216 - training loss: 22740.3994, validation loss: 0.4483
2024-05-24 07:36:23 [INFO]: Epoch 217 - training loss: 22739.8685, validation loss: 0.4507
2024-05-24 07:36:24 [INFO]: Epoch 218 - training loss: 22739.7805, validation loss: 0.4487
2024-05-24 07:36:25 [INFO]: Epoch 219 - training loss: 22739.5519, validation loss: 0.4494
2024-05-24 07:36:25 [INFO]: Epoch 220 - training loss: 22739.6942, validation loss: 0.4481
2024-05-24 07:36:26 [INFO]: Epoch 221 - training loss: 22739.7268, validation loss: 0.4514
2024-05-24 07:36:26 [INFO]: Epoch 222 - training loss: 22739.6623, validation loss: 0.4450
2024-05-24 07:36:27 [INFO]: Epoch 223 - training loss: 22739.6603, validation loss: 0.4450
2024-05-24 07:36:28 [INFO]: Epoch 224 - training loss: 22740.0541, validation loss: 0.4492
2024-05-24 07:36:28 [INFO]: Epoch 225 - training loss: 22740.3972, validation loss: 0.4471
2024-05-24 07:36:29 [INFO]: Epoch 226 - training loss: 22739.8764, validation loss: 0.4418
2024-05-24 07:36:30 [INFO]: Epoch 227 - training loss: 22739.1778, validation loss: 0.4435
2024-05-24 07:36:30 [INFO]: Epoch 228 - training loss: 22739.0655, validation loss: 0.4443
2024-05-24 07:36:31 [INFO]: Epoch 229 - training loss: 22739.1455, validation loss: 0.4469
2024-05-24 07:36:31 [INFO]: Epoch 230 - training loss: 22738.8653, validation loss: 0.4461
2024-05-24 07:36:32 [INFO]: Epoch 231 - training loss: 22739.0384, validation loss: 0.4462
2024-05-24 07:36:33 [INFO]: Epoch 232 - training loss: 22738.6625, validation loss: 0.4445
2024-05-24 07:36:33 [INFO]: Epoch 233 - training loss: 22738.4007, validation loss: 0.4445
2024-05-24 07:36:34 [INFO]: Epoch 234 - training loss: 22738.5864, validation loss: 0.4463
2024-05-24 07:36:34 [INFO]: Epoch 235 - training loss: 22738.7545, validation loss: 0.4476
2024-05-24 07:36:35 [INFO]: Epoch 236 - training loss: 22739.0180, validation loss: 0.4429
2024-05-24 07:36:35 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-05-24 07:36:35 [INFO]: Finished training. The best model is from epoch#226.
2024-05-24 07:36:35 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_3/GPVAE_physionet_2012_seta/20240524_T073400/GPVAE.pypots
2024-05-24 07:36:35 [INFO]: GP-VAE on PhysioNet-2012-seta: MAE=0.4031, MSE=0.4765
2024-05-24 07:36:36 [INFO]: Successfully saved to overlay_premask_post_norm_saved_results/round_3/GPVAE_physionet_2012_seta/imputation.pkl
2024-05-24 07:36:36 [INFO]: Using the given device: cuda:0
2024-05-24 07:36:36 [INFO]: Model files will be saved to overlay_premask_post_norm_saved_results/round_3/USGAN_physionet_2012_seta/20240524_T073636
2024-05-24 07:36:36 [INFO]: Tensorboard file will be saved to overlay_premask_post_norm_saved_results/round_3/USGAN_physionet_2012_seta/20240524_T073636/tensorboard
2024-05-24 07:36:36 [INFO]: USGAN initialized with the given hyperparameters, the number of trainable parameters: 16,010,261
2024-05-24 07:36:57 [INFO]: Epoch 001 - generator training loss: 0.6063, discriminator training loss: 0.3827, validation loss: 0.6176
2024-05-24 07:37:16 [INFO]: Epoch 002 - generator training loss: 0.4738, discriminator training loss: 0.2724, validation loss: 0.5250
2024-05-24 07:37:35 [INFO]: Epoch 003 - generator training loss: 0.4350, discriminator training loss: 0.2373, validation loss: 0.5084
2024-05-24 07:37:53 [INFO]: Epoch 004 - generator training loss: 0.4566, discriminator training loss: 0.1900, validation loss: 0.4982
2024-05-24 07:38:12 [INFO]: Epoch 005 - generator training loss: 0.4462, discriminator training loss: 0.1604, validation loss: 0.4913
2024-05-24 07:38:31 [INFO]: Epoch 006 - generator training loss: 0.4367, discriminator training loss: 0.1410, validation loss: 0.4674
2024-05-24 07:38:50 [INFO]: Epoch 007 - generator training loss: 0.4197, discriminator training loss: 0.1271, validation loss: 0.4560
2024-05-24 07:39:08 [INFO]: Epoch 008 - generator training loss: 0.4095, discriminator training loss: 0.1162, validation loss: 0.4478
2024-05-24 07:39:27 [INFO]: Epoch 009 - generator training loss: 0.4002, discriminator training loss: 0.1077, validation loss: 0.4343
2024-05-24 07:39:45 [INFO]: Epoch 010 - generator training loss: 0.3921, discriminator training loss: 0.1005, validation loss: 0.4291
2024-05-24 07:40:04 [INFO]: Epoch 011 - generator training loss: 0.3872, discriminator training loss: 0.0943, validation loss: 0.4212
2024-05-24 07:40:23 [INFO]: Epoch 012 - generator training loss: 0.3799, discriminator training loss: 0.0892, validation loss: 0.4172
2024-05-24 07:40:42 [INFO]: Epoch 013 - generator training loss: 0.3739, discriminator training loss: 0.0844, validation loss: 0.4083
2024-05-24 07:41:00 [INFO]: Epoch 014 - generator training loss: 0.3697, discriminator training loss: 0.0806, validation loss: 0.4060
2024-05-24 07:41:19 [INFO]: Epoch 015 - generator training loss: 0.3666, discriminator training loss: 0.0770, validation loss: 0.4010
2024-05-24 07:41:38 [INFO]: Epoch 016 - generator training loss: 0.3603, discriminator training loss: 0.0738, validation loss: 0.3943
2024-05-24 07:41:56 [INFO]: Epoch 017 - generator training loss: 0.3558, discriminator training loss: 0.0711, validation loss: 0.3938
2024-05-24 07:42:15 [INFO]: Epoch 018 - generator training loss: 0.3540, discriminator training loss: 0.0686, validation loss: 0.3902
2024-05-24 07:42:34 [INFO]: Epoch 019 - generator training loss: 0.3481, discriminator training loss: 0.0662, validation loss: 0.3837
2024-05-24 07:42:52 [INFO]: Epoch 020 - generator training loss: 0.3436, discriminator training loss: 0.0642, validation loss: 0.3815
2024-05-24 07:43:11 [INFO]: Epoch 021 - generator training loss: 0.3402, discriminator training loss: 0.0620, validation loss: 0.3758
2024-05-24 07:43:30 [INFO]: Epoch 022 - generator training loss: 0.3357, discriminator training loss: 0.0603, validation loss: 0.3729
2024-05-24 07:43:48 [INFO]: Epoch 023 - generator training loss: 0.3310, discriminator training loss: 0.0586, validation loss: 0.3699
2024-05-24 07:44:07 [INFO]: Epoch 024 - generator training loss: 0.3283, discriminator training loss: 0.0574, validation loss: 0.3705
2024-05-24 07:44:26 [INFO]: Epoch 025 - generator training loss: 0.3251, discriminator training loss: 0.0561, validation loss: 0.3614
2024-05-24 07:44:45 [INFO]: Epoch 026 - generator training loss: 0.3192, discriminator training loss: 0.0547, validation loss: 0.3612
2024-05-24 07:45:03 [INFO]: Epoch 027 - generator training loss: 0.3143, discriminator training loss: 0.0538, validation loss: 0.3546
2024-05-24 07:45:22 [INFO]: Epoch 028 - generator training loss: 0.3082, discriminator training loss: 0.0528, validation loss: 0.3605
2024-05-24 07:45:41 [INFO]: Epoch 029 - generator training loss: 0.3096, discriminator training loss: 0.0519, validation loss: 0.3529
2024-05-24 07:45:59 [INFO]: Epoch 030 - generator training loss: 0.3053, discriminator training loss: 0.0513, validation loss: 0.3732
2024-05-24 07:46:18 [INFO]: Epoch 031 - generator training loss: 0.3109, discriminator training loss: 0.0505, validation loss: 0.3463
2024-05-24 07:46:37 [INFO]: Epoch 032 - generator training loss: 0.3013, discriminator training loss: 0.0498, validation loss: 0.3432
2024-05-24 07:46:55 [INFO]: Epoch 033 - generator training loss: 0.2993, discriminator training loss: 0.0493, validation loss: 0.3438
2024-05-24 07:47:14 [INFO]: Epoch 034 - generator training loss: 0.2936, discriminator training loss: 0.0488, validation loss: 0.3400
2024-05-24 07:47:33 [INFO]: Epoch 035 - generator training loss: 0.2933, discriminator training loss: 0.0482, validation loss: 0.3348
2024-05-24 07:47:51 [INFO]: Epoch 036 - generator training loss: 0.2831, discriminator training loss: 0.0476, validation loss: 0.3350
2024-05-24 07:48:10 [INFO]: Epoch 037 - generator training loss: 0.2809, discriminator training loss: 0.0474, validation loss: 0.3316
2024-05-24 07:48:29 [INFO]: Epoch 038 - generator training loss: 0.2751, discriminator training loss: 0.0467, validation loss: 0.3302
2024-05-24 07:48:48 [INFO]: Epoch 039 - generator training loss: 0.2725, discriminator training loss: 0.0464, validation loss: 0.3292
2024-05-24 07:49:06 [INFO]: Epoch 040 - generator training loss: 0.2748, discriminator training loss: 0.0463, validation loss: 0.3285
2024-05-24 07:49:25 [INFO]: Epoch 041 - generator training loss: 0.2788, discriminator training loss: 0.0459, validation loss: 0.3263
2024-05-24 07:49:44 [INFO]: Epoch 042 - generator training loss: 0.2812, discriminator training loss: 0.0455, validation loss: 0.3269
2024-05-24 07:50:02 [INFO]: Epoch 043 - generator training loss: 0.2744, discriminator training loss: 0.0455, validation loss: 0.3354
2024-05-24 07:50:21 [INFO]: Epoch 044 - generator training loss: 0.2685, discriminator training loss: 0.0451, validation loss: 0.3194
2024-05-24 07:50:40 [INFO]: Epoch 045 - generator training loss: 0.2615, discriminator training loss: 0.0450, validation loss: 0.3178
2024-05-24 07:50:59 [INFO]: Epoch 046 - generator training loss: 0.2571, discriminator training loss: 0.0444, validation loss: 0.3193
2024-05-24 07:51:17 [INFO]: Epoch 047 - generator training loss: 0.2554, discriminator training loss: 0.0445, validation loss: 0.3153
2024-05-24 07:51:36 [INFO]: Epoch 048 - generator training loss: 0.2566, discriminator training loss: 0.0443, validation loss: 0.3147
2024-05-24 07:51:54 [INFO]: Epoch 049 - generator training loss: 0.2521, discriminator training loss: 0.0441, validation loss: 0.3140
2024-05-24 07:52:13 [INFO]: Epoch 050 - generator training loss: 0.2526, discriminator training loss: 0.0441, validation loss: 0.3137
2024-05-24 07:52:32 [INFO]: Epoch 051 - generator training loss: 0.2461, discriminator training loss: 0.0439, validation loss: 0.3120
2024-05-24 07:52:50 [INFO]: Epoch 052 - generator training loss: 0.2448, discriminator training loss: 0.0439, validation loss: 0.3101
2024-05-24 07:53:09 [INFO]: Epoch 053 - generator training loss: 0.2398, discriminator training loss: 0.0436, validation loss: 0.3155
2024-05-24 07:53:28 [INFO]: Epoch 054 - generator training loss: 0.2551, discriminator training loss: 0.0437, validation loss: 0.3119
2024-05-24 07:53:46 [INFO]: Epoch 055 - generator training loss: 0.2439, discriminator training loss: 0.0435, validation loss: 0.3094
2024-05-24 07:54:05 [INFO]: Epoch 056 - generator training loss: 0.2395, discriminator training loss: 0.0434, validation loss: 0.3142
2024-05-24 07:54:24 [INFO]: Epoch 057 - generator training loss: 0.2338, discriminator training loss: 0.0434, validation loss: 0.3075
2024-05-24 07:54:43 [INFO]: Epoch 058 - generator training loss: 0.2322, discriminator training loss: 0.0434, validation loss: 0.3085
2024-05-24 07:55:01 [INFO]: Epoch 059 - generator training loss: 0.2287, discriminator training loss: 0.0431, validation loss: 0.3069
2024-05-24 07:55:20 [INFO]: Epoch 060 - generator training loss: 0.2275, discriminator training loss: 0.0430, validation loss: 0.3090
2024-05-24 07:55:39 [INFO]: Epoch 061 - generator training loss: 0.2306, discriminator training loss: 0.0429, validation loss: 0.3150
2024-05-24 07:55:57 [INFO]: Epoch 062 - generator training loss: 0.2303, discriminator training loss: 0.0430, validation loss: 0.3152
2024-05-24 07:56:16 [INFO]: Epoch 063 - generator training loss: 0.2383, discriminator training loss: 0.0429, validation loss: 0.3174
2024-05-24 07:56:34 [INFO]: Epoch 064 - generator training loss: 0.2267, discriminator training loss: 0.0427, validation loss: 0.3064
2024-05-24 07:56:53 [INFO]: Epoch 065 - generator training loss: 0.2193, discriminator training loss: 0.0427, validation loss: 0.3051
2024-05-24 07:57:12 [INFO]: Epoch 066 - generator training loss: 0.2186, discriminator training loss: 0.0424, validation loss: 0.3079
2024-05-24 07:57:31 [INFO]: Epoch 067 - generator training loss: 0.2142, discriminator training loss: 0.0426, validation loss: 0.3054
2024-05-24 07:57:49 [INFO]: Epoch 068 - generator training loss: 0.2203, discriminator training loss: 0.0424, validation loss: 0.3056
2024-05-24 07:58:08 [INFO]: Epoch 069 - generator training loss: 0.2140, discriminator training loss: 0.0423, validation loss: 0.3079
2024-05-24 07:58:27 [INFO]: Epoch 070 - generator training loss: 0.2071, discriminator training loss: 0.0422, validation loss: 0.3071
2024-05-24 07:58:45 [INFO]: Epoch 071 - generator training loss: 0.2061, discriminator training loss: 0.0420, validation loss: 0.3060
2024-05-24 07:59:04 [INFO]: Epoch 072 - generator training loss: 0.2058, discriminator training loss: 0.0420, validation loss: 0.3061
2024-05-24 07:59:23 [INFO]: Epoch 073 - generator training loss: 0.2048, discriminator training loss: 0.0418, validation loss: 0.3077
2024-05-24 07:59:41 [INFO]: Epoch 074 - generator training loss: 0.2015, discriminator training loss: 0.0415, validation loss: 0.3070
2024-05-24 08:00:00 [INFO]: Epoch 075 - generator training loss: 0.1979, discriminator training loss: 0.0416, validation loss: 0.3050
2024-05-24 08:00:19 [INFO]: Epoch 076 - generator training loss: 0.1986, discriminator training loss: 0.0416, validation loss: 0.3092
2024-05-24 08:00:37 [INFO]: Epoch 077 - generator training loss: 0.1983, discriminator training loss: 0.0416, validation loss: 0.3088
2024-05-24 08:00:56 [INFO]: Epoch 078 - generator training loss: 0.1962, discriminator training loss: 0.0414, validation loss: 0.3041
2024-05-24 08:01:15 [INFO]: Epoch 079 - generator training loss: 0.1911, discriminator training loss: 0.0413, validation loss: 0.3075
2024-05-24 08:01:34 [INFO]: Epoch 080 - generator training loss: 0.1930, discriminator training loss: 0.0412, validation loss: 0.3065
2024-05-24 08:01:52 [INFO]: Epoch 081 - generator training loss: 0.1936, discriminator training loss: 0.0409, validation loss: 0.3065
2024-05-24 08:02:11 [INFO]: Epoch 082 - generator training loss: 0.2115, discriminator training loss: 0.0411, validation loss: 0.3220
2024-05-24 08:02:29 [INFO]: Epoch 083 - generator training loss: 0.2097, discriminator training loss: 0.0410, validation loss: 0.3072
2024-05-24 08:02:48 [INFO]: Epoch 084 - generator training loss: 0.1974, discriminator training loss: 0.0408, validation loss: 0.3051
2024-05-24 08:03:07 [INFO]: Epoch 085 - generator training loss: 0.1894, discriminator training loss: 0.0407, validation loss: 0.3047
2024-05-24 08:03:26 [INFO]: Epoch 086 - generator training loss: 0.1851, discriminator training loss: 0.0407, validation loss: 0.3037
2024-05-24 08:03:44 [INFO]: Epoch 087 - generator training loss: 0.1825, discriminator training loss: 0.0405, validation loss: 0.3045
2024-05-24 08:04:03 [INFO]: Epoch 088 - generator training loss: 0.1823, discriminator training loss: 0.0406, validation loss: 0.3053
2024-05-24 08:04:22 [INFO]: Epoch 089 - generator training loss: 0.1812, discriminator training loss: 0.0405, validation loss: 0.3089
2024-05-24 08:04:40 [INFO]: Epoch 090 - generator training loss: 0.1808, discriminator training loss: 0.0405, validation loss: 0.3055
2024-05-24 08:04:59 [INFO]: Epoch 091 - generator training loss: 0.1772, discriminator training loss: 0.0403, validation loss: 0.3045
2024-05-24 08:05:18 [INFO]: Epoch 092 - generator training loss: 0.1805, discriminator training loss: 0.0402, validation loss: 0.3070
2024-05-24 08:05:36 [INFO]: Epoch 093 - generator training loss: 0.1791, discriminator training loss: 0.0400, validation loss: 0.3086
2024-05-24 08:05:55 [INFO]: Epoch 094 - generator training loss: 0.1771, discriminator training loss: 0.0398, validation loss: 0.3067
2024-05-24 08:06:13 [INFO]: Epoch 095 - generator training loss: 0.1744, discriminator training loss: 0.0400, validation loss: 0.3037
2024-05-24 08:06:32 [INFO]: Epoch 096 - generator training loss: 0.1707, discriminator training loss: 0.0397, validation loss: 0.3082
2024-05-24 08:06:51 [INFO]: Epoch 097 - generator training loss: 0.1680, discriminator training loss: 0.0397, validation loss: 0.3077
2024-05-24 08:07:10 [INFO]: Epoch 098 - generator training loss: 0.1653, discriminator training loss: 0.0396, validation loss: 0.3074
2024-05-24 08:07:28 [INFO]: Epoch 099 - generator training loss: 0.1649, discriminator training loss: 0.0396, validation loss: 0.3082
2024-05-24 08:07:47 [INFO]: Epoch 100 - generator training loss: 0.1622, discriminator training loss: 0.0395, validation loss: 0.3090
2024-05-24 08:08:06 [INFO]: Epoch 101 - generator training loss: 0.1623, discriminator training loss: 0.0393, validation loss: 0.3100
2024-05-24 08:08:24 [INFO]: Epoch 102 - generator training loss: 0.1602, discriminator training loss: 0.0392, validation loss: 0.3097
2024-05-24 08:08:43 [INFO]: Epoch 103 - generator training loss: 0.1605, discriminator training loss: 0.0391, validation loss: 0.3092
2024-05-24 08:09:02 [INFO]: Epoch 104 - generator training loss: 0.1593, discriminator training loss: 0.0391, validation loss: 0.3110
2024-05-24 08:09:20 [INFO]: Epoch 105 - generator training loss: 0.1558, discriminator training loss: 0.0393, validation loss: 0.3095
2024-05-24 08:09:20 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-05-24 08:09:20 [INFO]: Finished training. The best model is from epoch#95.
2024-05-24 08:09:20 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_3/USGAN_physionet_2012_seta/20240524_T073636/USGAN.pypots
2024-05-24 08:09:23 [INFO]: US-GAN on PhysioNet-2012-seta: MAE=0.2992, MSE=0.2928
2024-05-24 08:09:33 [INFO]: Successfully saved to overlay_premask_post_norm_saved_results/round_3/USGAN_physionet_2012_seta/imputation.pkl
2024-05-24 08:09:33 [INFO]: Using the given device: cuda:0
2024-05-24 08:09:33 [INFO]: Model files will be saved to overlay_premask_post_norm_saved_results/round_3/BRITS_physionet_2012_seta/20240524_T080933
2024-05-24 08:09:33 [INFO]: Tensorboard file will be saved to overlay_premask_post_norm_saved_results/round_3/BRITS_physionet_2012_seta/20240524_T080933/tensorboard
2024-05-24 08:09:33 [INFO]: BRITS initialized with the given hyperparameters, the number of trainable parameters: 9,176,048
2024-05-24 08:09:48 [INFO]: Epoch 001 - training loss: 1.1288, validation loss: 0.5192
2024-05-24 08:10:00 [INFO]: Epoch 002 - training loss: 0.9219, validation loss: 0.4686
2024-05-24 08:10:12 [INFO]: Epoch 003 - training loss: 0.8633, validation loss: 0.4415
2024-05-24 08:10:24 [INFO]: Epoch 004 - training loss: 0.8263, validation loss: 0.4161
2024-05-24 08:10:36 [INFO]: Epoch 005 - training loss: 0.7994, validation loss: 0.4006
2024-05-24 08:10:49 [INFO]: Epoch 006 - training loss: 0.7786, validation loss: 0.3860
2024-05-24 08:11:01 [INFO]: Epoch 007 - training loss: 0.7604, validation loss: 0.3753
2024-05-24 08:11:13 [INFO]: Epoch 008 - training loss: 0.7447, validation loss: 0.3661
2024-05-24 08:11:25 [INFO]: Epoch 009 - training loss: 0.7313, validation loss: 0.3605
2024-05-24 08:11:37 [INFO]: Epoch 010 - training loss: 0.7204, validation loss: 0.3541
2024-05-24 08:11:49 [INFO]: Epoch 011 - training loss: 0.7094, validation loss: 0.3492
2024-05-24 08:12:01 [INFO]: Epoch 012 - training loss: 0.7002, validation loss: 0.3456
2024-05-24 08:12:14 [INFO]: Epoch 013 - training loss: 0.6937, validation loss: 0.3449
2024-05-24 08:12:26 [INFO]: Epoch 014 - training loss: 0.6858, validation loss: 0.3414
2024-05-24 08:12:38 [INFO]: Epoch 015 - training loss: 0.6805, validation loss: 0.3392
2024-05-24 08:12:50 [INFO]: Epoch 016 - training loss: 0.6751, validation loss: 0.3398
2024-05-24 08:13:02 [INFO]: Epoch 017 - training loss: 0.6701, validation loss: 0.3362
2024-05-24 08:13:14 [INFO]: Epoch 018 - training loss: 0.6663, validation loss: 0.3361
2024-05-24 08:13:27 [INFO]: Epoch 019 - training loss: 0.6615, validation loss: 0.3344
2024-05-24 08:13:39 [INFO]: Epoch 020 - training loss: 0.6590, validation loss: 0.3341
2024-05-24 08:13:51 [INFO]: Epoch 021 - training loss: 0.6544, validation loss: 0.3330
2024-05-24 08:14:03 [INFO]: Epoch 022 - training loss: 0.6522, validation loss: 0.3335
2024-05-24 08:14:15 [INFO]: Epoch 023 - training loss: 0.6484, validation loss: 0.3325
2024-05-24 08:14:28 [INFO]: Epoch 024 - training loss: 0.6449, validation loss: 0.3313
2024-05-24 08:14:40 [INFO]: Epoch 025 - training loss: 0.6431, validation loss: 0.3336
2024-05-24 08:14:52 [INFO]: Epoch 026 - training loss: 0.6445, validation loss: 0.3301
2024-05-24 08:15:04 [INFO]: Epoch 027 - training loss: 0.6382, validation loss: 0.3293
2024-05-24 08:15:17 [INFO]: Epoch 028 - training loss: 0.6339, validation loss: 0.3306
2024-05-24 08:15:29 [INFO]: Epoch 029 - training loss: 0.6309, validation loss: 0.3312
2024-05-24 08:15:41 [INFO]: Epoch 030 - training loss: 0.6292, validation loss: 0.3294
2024-05-24 08:15:53 [INFO]: Epoch 031 - training loss: 0.6263, validation loss: 0.3301
2024-05-24 08:16:05 [INFO]: Epoch 032 - training loss: 0.6266, validation loss: 0.3306
2024-05-24 08:16:18 [INFO]: Epoch 033 - training loss: 0.6248, validation loss: 0.3293
2024-05-24 08:16:30 [INFO]: Epoch 034 - training loss: 0.6200, validation loss: 0.3299
2024-05-24 08:16:42 [INFO]: Epoch 035 - training loss: 0.6151, validation loss: 0.3284
2024-05-24 08:16:54 [INFO]: Epoch 036 - training loss: 0.6131, validation loss: 0.3313
2024-05-24 08:17:06 [INFO]: Epoch 037 - training loss: 0.6131, validation loss: 0.3328
2024-05-24 08:17:18 [INFO]: Epoch 038 - training loss: 0.6124, validation loss: 0.3311
2024-05-24 08:17:30 [INFO]: Epoch 039 - training loss: 0.6058, validation loss: 0.3301
2024-05-24 08:17:43 [INFO]: Epoch 040 - training loss: 0.6032, validation loss: 0.3307
2024-05-24 08:17:55 [INFO]: Epoch 041 - training loss: 0.5996, validation loss: 0.3312
2024-05-24 08:18:07 [INFO]: Epoch 042 - training loss: 0.5976, validation loss: 0.3311
2024-05-24 08:18:20 [INFO]: Epoch 043 - training loss: 0.5949, validation loss: 0.3321
2024-05-24 08:18:33 [INFO]: Epoch 044 - training loss: 0.5923, validation loss: 0.3336
2024-05-24 08:18:46 [INFO]: Epoch 045 - training loss: 0.5943, validation loss: 0.3343
2024-05-24 08:18:46 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-05-24 08:18:46 [INFO]: Finished training. The best model is from epoch#35.
2024-05-24 08:18:46 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_3/BRITS_physionet_2012_seta/20240524_T080933/BRITS.pypots
2024-05-24 08:18:48 [INFO]: BRITS on PhysioNet-2012-seta: MAE=0.2646, MSE=0.2973
2024-05-24 08:18:58 [INFO]: Successfully saved to overlay_premask_post_norm_saved_results/round_3/BRITS_physionet_2012_seta/imputation.pkl
2024-05-24 08:18:58 [INFO]: Using the given device: cuda:0
2024-05-24 08:18:58 [INFO]: Model files will be saved to overlay_premask_post_norm_saved_results/round_3/MRNN_physionet_2012_seta/20240524_T081858
2024-05-24 08:18:58 [INFO]: Tensorboard file will be saved to overlay_premask_post_norm_saved_results/round_3/MRNN_physionet_2012_seta/20240524_T081858/tensorboard
2024-05-24 08:18:58 [INFO]: MRNN initialized with the given hyperparameters, the number of trainable parameters: 7,599
2024-05-24 08:19:04 [INFO]: Epoch 001 - training loss: 1.1775, validation loss: 0.9987
2024-05-24 08:19:04 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_3/MRNN_physionet_2012_seta/20240524_T081858/MRNN_epoch1_loss0.9987297087907792.pypots
2024-05-24 08:19:07 [INFO]: Epoch 002 - training loss: 0.7060, validation loss: 0.9749
2024-05-24 08:19:07 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_3/MRNN_physionet_2012_seta/20240524_T081858/MRNN_epoch2_loss0.9748700588941575.pypots
2024-05-24 08:19:10 [INFO]: Epoch 003 - training loss: 0.5981, validation loss: 0.9449
2024-05-24 08:19:10 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_3/MRNN_physionet_2012_seta/20240524_T081858/MRNN_epoch3_loss0.9448951005935669.pypots
2024-05-24 08:19:13 [INFO]: Epoch 004 - training loss: 0.5579, validation loss: 0.9307
2024-05-24 08:19:13 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_3/MRNN_physionet_2012_seta/20240524_T081858/MRNN_epoch4_loss0.9307424783706665.pypots
2024-05-24 08:19:16 [INFO]: Epoch 005 - training loss: 0.5305, validation loss: 0.9220
2024-05-24 08:19:16 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_3/MRNN_physionet_2012_seta/20240524_T081858/MRNN_epoch5_loss0.9220418751239776.pypots
2024-05-24 08:19:19 [INFO]: Epoch 006 - training loss: 0.5164, validation loss: 0.9171
2024-05-24 08:19:19 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_3/MRNN_physionet_2012_seta/20240524_T081858/MRNN_epoch6_loss0.9170753538608551.pypots
2024-05-24 08:19:22 [INFO]: Epoch 007 - training loss: 0.5010, validation loss: 0.9142
2024-05-24 08:19:22 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_3/MRNN_physionet_2012_seta/20240524_T081858/MRNN_epoch7_loss0.9142198622226715.pypots
2024-05-24 08:19:25 [INFO]: Epoch 008 - training loss: 0.4941, validation loss: 0.9115
2024-05-24 08:19:25 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_3/MRNN_physionet_2012_seta/20240524_T081858/MRNN_epoch8_loss0.9115079909563064.pypots
2024-05-24 08:19:28 [INFO]: Epoch 009 - training loss: 0.4841, validation loss: 0.9103
2024-05-24 08:19:28 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_3/MRNN_physionet_2012_seta/20240524_T081858/MRNN_epoch9_loss0.9102925449609757.pypots
2024-05-24 08:19:31 [INFO]: Epoch 010 - training loss: 0.4761, validation loss: 0.9106
2024-05-24 08:19:31 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_3/MRNN_physionet_2012_seta/20240524_T081858/MRNN_epoch10_loss0.9105919688940048.pypots
2024-05-24 08:19:34 [INFO]: Epoch 011 - training loss: 0.4753, validation loss: 0.9112
2024-05-24 08:19:34 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_3/MRNN_physionet_2012_seta/20240524_T081858/MRNN_epoch11_loss0.9112020343542099.pypots
2024-05-24 08:19:37 [INFO]: Epoch 012 - training loss: 0.4565, validation loss: 0.9126
2024-05-24 08:19:37 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_3/MRNN_physionet_2012_seta/20240524_T081858/MRNN_epoch12_loss0.9126072227954865.pypots
2024-05-24 08:19:40 [INFO]: Epoch 013 - training loss: 0.4641, validation loss: 0.9143
2024-05-24 08:19:40 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_3/MRNN_physionet_2012_seta/20240524_T081858/MRNN_epoch13_loss0.9143334746360778.pypots
2024-05-24 08:19:43 [INFO]: Epoch 014 - training loss: 0.4601, validation loss: 0.9164
2024-05-24 08:19:43 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_3/MRNN_physionet_2012_seta/20240524_T081858/MRNN_epoch14_loss0.9163858443498611.pypots
2024-05-24 08:19:46 [INFO]: Epoch 015 - training loss: 0.4505, validation loss: 0.9173
2024-05-24 08:19:46 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_3/MRNN_physionet_2012_seta/20240524_T081858/MRNN_epoch15_loss0.9172821938991547.pypots
2024-05-24 08:19:49 [INFO]: Epoch 016 - training loss: 0.4561, validation loss: 0.9203
2024-05-24 08:19:49 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_3/MRNN_physionet_2012_seta/20240524_T081858/MRNN_epoch16_loss0.9203395545482635.pypots
2024-05-24 08:19:52 [INFO]: Epoch 017 - training loss: 0.4629, validation loss: 0.9213
2024-05-24 08:19:52 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_3/MRNN_physionet_2012_seta/20240524_T081858/MRNN_epoch17_loss0.9212867677211761.pypots
2024-05-24 08:19:55 [INFO]: Epoch 018 - training loss: 0.4448, validation loss: 0.9215
2024-05-24 08:19:55 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_3/MRNN_physionet_2012_seta/20240524_T081858/MRNN_epoch18_loss0.9215366542339325.pypots
2024-05-24 08:19:58 [INFO]: Epoch 019 - training loss: 0.4471, validation loss: 0.9227
2024-05-24 08:19:58 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_3/MRNN_physionet_2012_seta/20240524_T081858/MRNN_epoch19_loss0.9226696819067002.pypots
2024-05-24 08:19:58 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-05-24 08:19:58 [INFO]: Finished training. The best model is from epoch#9.
2024-05-24 08:19:58 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_3/MRNN_physionet_2012_seta/20240524_T081858/MRNN.pypots
2024-05-24 08:19:59 [INFO]: MRNN on PhysioNet-2012-seta: MAE=0.6825, MSE=0.9205
2024-05-24 08:20:04 [INFO]: Successfully saved to overlay_premask_post_norm_saved_results/round_3/MRNN_physionet_2012_seta/imputation.pkl
2024-05-24 08:20:04 [INFO]: Using the given device: cpu
2024-05-24 08:20:04 [INFO]: LOCF on PhysioNet-2012-seta: MAE=0.4085, MSE=0.5400
2024-05-24 08:20:04 [INFO]: Successfully created the given path "saved_results/round_3/LOCF_physionet_2012_seta".
2024-05-24 08:20:04 [INFO]: Successfully saved to overlay_premask_post_norm_saved_results/round_3/LOCF_physionet_2012_seta/imputation.pkl
2024-05-24 08:20:04 [INFO]: Median on PhysioNet-2012-seta: MAE=0.6857, MSE=1.0298
2024-05-24 08:20:04 [INFO]: Successfully created the given path "saved_results/round_3/Median_physionet_2012_seta".
2024-05-24 08:20:04 [INFO]: Successfully saved to overlay_premask_post_norm_saved_results/round_3/Median_physionet_2012_seta/imputation.pkl
2024-05-24 08:20:04 [INFO]: Mean on PhysioNet-2012-seta: MAE=0.7024, MSE=1.0000
2024-05-24 08:20:04 [INFO]: Successfully created the given path "saved_results/round_3/Mean_physionet_2012_seta".
2024-05-24 08:20:04 [INFO]: Successfully saved to overlay_premask_post_norm_saved_results/round_3/Mean_physionet_2012_seta/imputation.pkl
2024-05-24 08:20:04 [INFO]: Have set the random seed as 2027 for numpy and pytorch.
2024-05-24 08:20:05 [INFO]: Using the given device: cuda:0
2024-05-24 08:20:05 [INFO]: Model files will be saved to overlay_premask_post_norm_saved_results/round_4/SAITS_physionet_2012_seta/20240524_T082005
2024-05-24 08:20:05 [INFO]: Tensorboard file will be saved to overlay_premask_post_norm_saved_results/round_4/SAITS_physionet_2012_seta/20240524_T082005/tensorboard
2024-05-24 08:20:05 [INFO]: SAITS initialized with the given hyperparameters, the number of trainable parameters: 5,322,806
2024-05-24 08:20:06 [INFO]: Epoch 001 - training loss: 1.1494, validation loss: 0.5071
2024-05-24 08:20:08 [INFO]: Epoch 002 - training loss: 0.8432, validation loss: 0.4219
2024-05-24 08:20:09 [INFO]: Epoch 003 - training loss: 0.7459, validation loss: 0.3799
2024-05-24 08:20:10 [INFO]: Epoch 004 - training loss: 0.6343, validation loss: 0.3620
2024-05-24 08:20:12 [INFO]: Epoch 005 - training loss: 0.5426, validation loss: 0.3484
2024-05-24 08:20:13 [INFO]: Epoch 006 - training loss: 0.4689, validation loss: 0.3373
2024-05-24 08:20:14 [INFO]: Epoch 007 - training loss: 0.4295, validation loss: 0.3333
2024-05-24 08:20:15 [INFO]: Epoch 008 - training loss: 0.3985, validation loss: 0.3255
2024-05-24 08:20:17 [INFO]: Epoch 009 - training loss: 0.3728, validation loss: 0.3141
2024-05-24 08:20:18 [INFO]: Epoch 010 - training loss: 0.3510, validation loss: 0.3175
2024-05-24 08:20:19 [INFO]: Epoch 011 - training loss: 0.3317, validation loss: 0.3089
2024-05-24 08:20:20 [INFO]: Epoch 012 - training loss: 0.3162, validation loss: 0.3049
2024-05-24 08:20:22 [INFO]: Epoch 013 - training loss: 0.3062, validation loss: 0.3001
2024-05-24 08:20:23 [INFO]: Epoch 014 - training loss: 0.2929, validation loss: 0.2953
2024-05-24 08:20:24 [INFO]: Epoch 015 - training loss: 0.2806, validation loss: 0.2977
2024-05-24 08:20:26 [INFO]: Epoch 016 - training loss: 0.2699, validation loss: 0.2930
2024-05-24 08:20:27 [INFO]: Epoch 017 - training loss: 0.2610, validation loss: 0.2935
2024-05-24 08:20:28 [INFO]: Epoch 018 - training loss: 0.2500, validation loss: 0.2899
2024-05-24 08:20:30 [INFO]: Epoch 019 - training loss: 0.2399, validation loss: 0.2898
2024-05-24 08:20:31 [INFO]: Epoch 020 - training loss: 0.2354, validation loss: 0.2896
2024-05-24 08:20:32 [INFO]: Epoch 021 - training loss: 0.2293, validation loss: 0.2916
2024-05-24 08:20:33 [INFO]: Epoch 022 - training loss: 0.2251, validation loss: 0.2901
2024-05-24 08:20:35 [INFO]: Epoch 023 - training loss: 0.2209, validation loss: 0.2891
2024-05-24 08:20:36 [INFO]: Epoch 024 - training loss: 0.2145, validation loss: 0.2951
2024-05-24 08:20:37 [INFO]: Epoch 025 - training loss: 0.2098, validation loss: 0.2925
2024-05-24 08:20:38 [INFO]: Epoch 026 - training loss: 0.2060, validation loss: 0.2904
2024-05-24 08:20:39 [INFO]: Epoch 027 - training loss: 0.1988, validation loss: 0.2906
2024-05-24 08:20:40 [INFO]: Epoch 028 - training loss: 0.1998, validation loss: 0.2922
2024-05-24 08:20:42 [INFO]: Epoch 029 - training loss: 0.2004, validation loss: 0.2980
2024-05-24 08:20:43 [INFO]: Epoch 030 - training loss: 0.1960, validation loss: 0.2915
2024-05-24 08:20:44 [INFO]: Epoch 031 - training loss: 0.1855, validation loss: 0.2909
2024-05-24 08:20:45 [INFO]: Epoch 032 - training loss: 0.1809, validation loss: 0.2931
2024-05-24 08:20:46 [INFO]: Epoch 033 - training loss: 0.1811, validation loss: 0.2908
2024-05-24 08:20:46 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-05-24 08:20:46 [INFO]: Finished training. The best model is from epoch#23.
2024-05-24 08:20:46 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_4/SAITS_physionet_2012_seta/20240524_T082005/SAITS.pypots
2024-05-24 08:20:46 [INFO]: SAITS on PhysioNet-2012-seta: MAE=0.2783, MSE=0.3300
2024-05-24 08:20:47 [INFO]: Successfully saved to overlay_premask_post_norm_saved_results/round_4/SAITS_physionet_2012_seta/imputation.pkl
2024-05-24 08:20:47 [INFO]: Using the given device: cuda:0
2024-05-24 08:20:47 [INFO]: Model files will be saved to overlay_premask_post_norm_saved_results/round_4/Transformer_physionet_2012_seta/20240524_T082047
2024-05-24 08:20:47 [INFO]: Tensorboard file will be saved to overlay_premask_post_norm_saved_results/round_4/Transformer_physionet_2012_seta/20240524_T082047/tensorboard
2024-05-24 08:20:47 [INFO]: Transformer initialized with the given hyperparameters, the number of trainable parameters: 2,152,613
2024-05-24 08:20:47 [INFO]: Epoch 001 - training loss: 1.1579, validation loss: 0.5685
2024-05-24 08:20:48 [INFO]: Epoch 002 - training loss: 0.7377, validation loss: 0.4650
2024-05-24 08:20:49 [INFO]: Epoch 003 - training loss: 0.6273, validation loss: 0.4315
2024-05-24 08:20:49 [INFO]: Epoch 004 - training loss: 0.5739, validation loss: 0.4267
2024-05-24 08:20:50 [INFO]: Epoch 005 - training loss: 0.5410, validation loss: 0.3979
2024-05-24 08:20:50 [INFO]: Epoch 006 - training loss: 0.5038, validation loss: 0.3952
2024-05-24 08:20:51 [INFO]: Epoch 007 - training loss: 0.4719, validation loss: 0.3815
2024-05-24 08:20:52 [INFO]: Epoch 008 - training loss: 0.4639, validation loss: 0.3745
2024-05-24 08:20:52 [INFO]: Epoch 009 - training loss: 0.4406, validation loss: 0.3647
2024-05-24 08:20:53 [INFO]: Epoch 010 - training loss: 0.4220, validation loss: 0.3581
2024-05-24 08:20:53 [INFO]: Epoch 011 - training loss: 0.4095, validation loss: 0.3516
2024-05-24 08:20:54 [INFO]: Epoch 012 - training loss: 0.3923, validation loss: 0.3432
2024-05-24 08:20:54 [INFO]: Epoch 013 - training loss: 0.3827, validation loss: 0.3413
2024-05-24 08:20:55 [INFO]: Epoch 014 - training loss: 0.3705, validation loss: 0.3439
2024-05-24 08:20:56 [INFO]: Epoch 015 - training loss: 0.3590, validation loss: 0.3412
2024-05-24 08:20:56 [INFO]: Epoch 016 - training loss: 0.3474, validation loss: 0.3347
2024-05-24 08:20:57 [INFO]: Epoch 017 - training loss: 0.3436, validation loss: 0.3305
2024-05-24 08:20:57 [INFO]: Epoch 018 - training loss: 0.3336, validation loss: 0.3281
2024-05-24 08:20:58 [INFO]: Epoch 019 - training loss: 0.3326, validation loss: 0.3270
2024-05-24 08:20:59 [INFO]: Epoch 020 - training loss: 0.3241, validation loss: 0.3246
2024-05-24 08:20:59 [INFO]: Epoch 021 - training loss: 0.3239, validation loss: 0.3273
2024-05-24 08:21:00 [INFO]: Epoch 022 - training loss: 0.3110, validation loss: 0.3238
2024-05-24 08:21:00 [INFO]: Epoch 023 - training loss: 0.3062, validation loss: 0.3218
2024-05-24 08:21:01 [INFO]: Epoch 024 - training loss: 0.2974, validation loss: 0.3218
2024-05-24 08:21:01 [INFO]: Epoch 025 - training loss: 0.2951, validation loss: 0.3191
2024-05-24 08:21:02 [INFO]: Epoch 026 - training loss: 0.2873, validation loss: 0.3207
2024-05-24 08:21:03 [INFO]: Epoch 027 - training loss: 0.2834, validation loss: 0.3143
2024-05-24 08:21:03 [INFO]: Epoch 028 - training loss: 0.2759, validation loss: 0.3165
2024-05-24 08:21:04 [INFO]: Epoch 029 - training loss: 0.2737, validation loss: 0.3140
2024-05-24 08:21:04 [INFO]: Epoch 030 - training loss: 0.2693, validation loss: 0.3140
2024-05-24 08:21:05 [INFO]: Epoch 031 - training loss: 0.2651, validation loss: 0.3152
2024-05-24 08:21:06 [INFO]: Epoch 032 - training loss: 0.2618, validation loss: 0.3199
2024-05-24 08:21:06 [INFO]: Epoch 033 - training loss: 0.2601, validation loss: 0.3115
2024-05-24 08:21:07 [INFO]: Epoch 034 - training loss: 0.2513, validation loss: 0.3124
2024-05-24 08:21:07 [INFO]: Epoch 035 - training loss: 0.2490, validation loss: 0.3133
2024-05-24 08:21:08 [INFO]: Epoch 036 - training loss: 0.2462, validation loss: 0.3108
2024-05-24 08:21:09 [INFO]: Epoch 037 - training loss: 0.2401, validation loss: 0.3112
2024-05-24 08:21:09 [INFO]: Epoch 038 - training loss: 0.2391, validation loss: 0.3156
2024-05-24 08:21:10 [INFO]: Epoch 039 - training loss: 0.2347, validation loss: 0.3112
2024-05-24 08:21:10 [INFO]: Epoch 040 - training loss: 0.2337, validation loss: 0.3144
2024-05-24 08:21:11 [INFO]: Epoch 041 - training loss: 0.2272, validation loss: 0.3122
2024-05-24 08:21:11 [INFO]: Epoch 042 - training loss: 0.2301, validation loss: 0.3140
2024-05-24 08:21:12 [INFO]: Epoch 043 - training loss: 0.2250, validation loss: 0.3206
2024-05-24 08:21:13 [INFO]: Epoch 044 - training loss: 0.2169, validation loss: 0.3150
2024-05-24 08:21:13 [INFO]: Epoch 045 - training loss: 0.2193, validation loss: 0.3133
2024-05-24 08:21:14 [INFO]: Epoch 046 - training loss: 0.2160, validation loss: 0.3149
2024-05-24 08:21:14 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-05-24 08:21:14 [INFO]: Finished training. The best model is from epoch#36.
2024-05-24 08:21:14 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_4/Transformer_physionet_2012_seta/20240524_T082047/Transformer.pypots
2024-05-24 08:21:14 [INFO]: Transformer on PhysioNet-2012-seta: MAE=0.2913, MSE=0.3379
2024-05-24 08:21:14 [INFO]: Successfully saved to overlay_premask_post_norm_saved_results/round_4/Transformer_physionet_2012_seta/imputation.pkl
2024-05-24 08:21:14 [INFO]: Using the given device: cuda:0
2024-05-24 08:21:14 [INFO]: Model files will be saved to overlay_premask_post_norm_saved_results/round_4/TimesNet_physionet_2012_seta/20240524_T082114
2024-05-24 08:21:14 [INFO]: Tensorboard file will be saved to overlay_premask_post_norm_saved_results/round_4/TimesNet_physionet_2012_seta/20240524_T082114/tensorboard
2024-05-24 08:21:14 [INFO]: TimesNet initialized with the given hyperparameters, the number of trainable parameters: 21,649,573
2024-05-24 08:21:15 [INFO]: Epoch 001 - training loss: 0.4832, validation loss: 0.3641
2024-05-24 08:21:16 [INFO]: Epoch 002 - training loss: 0.5967, validation loss: 0.3430
2024-05-24 08:21:16 [INFO]: Epoch 003 - training loss: 0.4538, validation loss: 0.3401
2024-05-24 08:21:17 [INFO]: Epoch 004 - training loss: 0.4563, validation loss: 0.3745
2024-05-24 08:21:18 [INFO]: Epoch 005 - training loss: 0.3376, validation loss: 0.3135
2024-05-24 08:21:19 [INFO]: Epoch 006 - training loss: 0.3100, validation loss: 0.3066
2024-05-24 08:21:19 [INFO]: Epoch 007 - training loss: 0.3039, validation loss: 0.3122
2024-05-24 08:21:20 [INFO]: Epoch 008 - training loss: 0.2906, validation loss: 0.3028
2024-05-24 08:21:21 [INFO]: Epoch 009 - training loss: 0.2828, validation loss: 0.2992
2024-05-24 08:21:21 [INFO]: Epoch 010 - training loss: 0.2772, validation loss: 0.3334
2024-05-24 08:21:22 [INFO]: Epoch 011 - training loss: 0.2800, validation loss: 0.2968
2024-05-24 08:21:23 [INFO]: Epoch 012 - training loss: 0.2641, validation loss: 0.2944
2024-05-24 08:21:23 [INFO]: Epoch 013 - training loss: 0.2566, validation loss: 0.2933
2024-05-24 08:21:24 [INFO]: Epoch 014 - training loss: 0.2504, validation loss: 0.2964
2024-05-24 08:21:25 [INFO]: Epoch 015 - training loss: 0.2524, validation loss: 0.2856
2024-05-24 08:21:26 [INFO]: Epoch 016 - training loss: 0.2427, validation loss: 0.2900
2024-05-24 08:21:26 [INFO]: Epoch 017 - training loss: 0.2375, validation loss: 0.2952
2024-05-24 08:21:27 [INFO]: Epoch 018 - training loss: 0.2564, validation loss: 0.2922
2024-05-24 08:21:28 [INFO]: Epoch 019 - training loss: 0.2518, validation loss: 0.2965
2024-05-24 08:21:28 [INFO]: Epoch 020 - training loss: 0.2551, validation loss: 0.2912
2024-05-24 08:21:29 [INFO]: Epoch 021 - training loss: 0.2323, validation loss: 0.2930
2024-05-24 08:21:30 [INFO]: Epoch 022 - training loss: 0.2242, validation loss: 0.2959
2024-05-24 08:21:31 [INFO]: Epoch 023 - training loss: 0.2234, validation loss: 0.3012
2024-05-24 08:21:31 [INFO]: Epoch 024 - training loss: 0.2289, validation loss: 0.2970
2024-05-24 08:21:32 [INFO]: Epoch 025 - training loss: 0.2118, validation loss: 0.2983
2024-05-24 08:21:32 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-05-24 08:21:32 [INFO]: Finished training. The best model is from epoch#15.
2024-05-24 08:21:32 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_4/TimesNet_physionet_2012_seta/20240524_T082114/TimesNet.pypots
2024-05-24 08:21:32 [INFO]: TimesNet on PhysioNet-2012-seta: MAE=0.2874, MSE=0.2822
2024-05-24 08:21:32 [INFO]: Successfully saved to overlay_premask_post_norm_saved_results/round_4/TimesNet_physionet_2012_seta/imputation.pkl
2024-05-24 08:21:32 [INFO]: Using the given device: cuda:0
2024-05-24 08:21:32 [INFO]: Model files will be saved to overlay_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T082132
2024-05-24 08:21:32 [INFO]: Tensorboard file will be saved to overlay_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T082132/tensorboard
2024-05-24 08:21:32 [INFO]: CSDI initialized with the given hyperparameters, the number of trainable parameters: 1,694,753
2024-05-24 08:22:16 [INFO]: Epoch 001 - training loss: 0.4115, validation loss: 0.3270
2024-05-24 08:22:16 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T082132/CSDI_epoch1_loss0.3270496815443039.pypots
2024-05-24 08:22:59 [INFO]: Epoch 002 - training loss: 0.3160, validation loss: 0.2984
2024-05-24 08:22:59 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T082132/CSDI_epoch2_loss0.29838950335979464.pypots
2024-05-24 08:23:42 [INFO]: Epoch 003 - training loss: 0.2770, validation loss: 0.2694
2024-05-24 08:23:42 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T082132/CSDI_epoch3_loss0.26939657628536223.pypots
2024-05-24 08:24:26 [INFO]: Epoch 004 - training loss: 0.2638, validation loss: 0.2543
2024-05-24 08:24:26 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T082132/CSDI_epoch4_loss0.25425489619374275.pypots
2024-05-24 08:25:09 [INFO]: Epoch 005 - training loss: 0.2587, validation loss: 0.2387
2024-05-24 08:25:09 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T082132/CSDI_epoch5_loss0.23870566338300706.pypots
2024-05-24 08:25:53 [INFO]: Epoch 006 - training loss: 0.2406, validation loss: 0.2304
2024-05-24 08:25:53 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T082132/CSDI_epoch6_loss0.23044428750872611.pypots
2024-05-24 08:26:36 [INFO]: Epoch 007 - training loss: 0.2367, validation loss: 0.2218
2024-05-24 08:26:36 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T082132/CSDI_epoch7_loss0.2218385875225067.pypots
2024-05-24 08:27:20 [INFO]: Epoch 008 - training loss: 0.2353, validation loss: 0.2196
2024-05-24 08:27:20 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T082132/CSDI_epoch8_loss0.21958105117082596.pypots
2024-05-24 08:28:04 [INFO]: Epoch 009 - training loss: 0.2261, validation loss: 0.2192
2024-05-24 08:28:04 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T082132/CSDI_epoch9_loss0.2191884398460388.pypots
2024-05-24 08:28:47 [INFO]: Epoch 010 - training loss: 0.2247, validation loss: 0.2131
2024-05-24 08:28:47 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T082132/CSDI_epoch10_loss0.2131244421005249.pypots
2024-05-24 08:29:31 [INFO]: Epoch 011 - training loss: 0.2253, validation loss: 0.2135
2024-05-24 08:29:31 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T082132/CSDI_epoch11_loss0.2135106109082699.pypots
2024-05-24 08:30:15 [INFO]: Epoch 012 - training loss: 0.2276, validation loss: 0.2136
2024-05-24 08:30:15 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T082132/CSDI_epoch12_loss0.21362804174423217.pypots
2024-05-24 08:30:59 [INFO]: Epoch 013 - training loss: 0.2264, validation loss: 0.2177
2024-05-24 08:30:59 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T082132/CSDI_epoch13_loss0.2177177280187607.pypots
2024-05-24 08:31:42 [INFO]: Epoch 014 - training loss: 0.2102, validation loss: 0.2043
2024-05-24 08:31:42 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T082132/CSDI_epoch14_loss0.20429400727152824.pypots
2024-05-24 08:32:26 [INFO]: Epoch 015 - training loss: 0.2231, validation loss: 0.2078
2024-05-24 08:32:26 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T082132/CSDI_epoch15_loss0.20776857286691666.pypots
2024-05-24 08:33:10 [INFO]: Epoch 016 - training loss: 0.2171, validation loss: 0.2061
2024-05-24 08:33:10 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T082132/CSDI_epoch16_loss0.20607124418020248.pypots
2024-05-24 08:33:53 [INFO]: Epoch 017 - training loss: 0.2172, validation loss: 0.2019
2024-05-24 08:33:53 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T082132/CSDI_epoch17_loss0.20192858576774597.pypots
2024-05-24 08:34:37 [INFO]: Epoch 018 - training loss: 0.2085, validation loss: 0.1997
2024-05-24 08:34:37 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T082132/CSDI_epoch18_loss0.19972895309329033.pypots
2024-05-24 08:35:21 [INFO]: Epoch 019 - training loss: 0.2149, validation loss: 0.1992
2024-05-24 08:35:21 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T082132/CSDI_epoch19_loss0.19919128268957137.pypots
2024-05-24 08:36:04 [INFO]: Epoch 020 - training loss: 0.2045, validation loss: 0.2002
2024-05-24 08:36:04 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T082132/CSDI_epoch20_loss0.2002154678106308.pypots
2024-05-24 08:36:48 [INFO]: Epoch 021 - training loss: 0.2031, validation loss: 0.2050
2024-05-24 08:36:48 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T082132/CSDI_epoch21_loss0.2050146736204624.pypots
2024-05-24 08:37:32 [INFO]: Epoch 022 - training loss: 0.2107, validation loss: 0.1945
2024-05-24 08:37:32 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T082132/CSDI_epoch22_loss0.19451554715633393.pypots
2024-05-24 08:38:15 [INFO]: Epoch 023 - training loss: 0.2136, validation loss: 0.1999
2024-05-24 08:38:15 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T082132/CSDI_epoch23_loss0.19992859810590743.pypots
2024-05-24 08:38:59 [INFO]: Epoch 024 - training loss: 0.2037, validation loss: 0.2042
2024-05-24 08:38:59 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T082132/CSDI_epoch24_loss0.20422757267951966.pypots
2024-05-24 08:39:43 [INFO]: Epoch 025 - training loss: 0.2010, validation loss: 0.1953
2024-05-24 08:39:43 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T082132/CSDI_epoch25_loss0.19531159698963166.pypots
2024-05-24 08:40:26 [INFO]: Epoch 026 - training loss: 0.1904, validation loss: 0.1946
2024-05-24 08:40:26 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T082132/CSDI_epoch26_loss0.1945721074938774.pypots
2024-05-24 08:41:10 [INFO]: Epoch 027 - training loss: 0.2068, validation loss: 0.1947
2024-05-24 08:41:10 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T082132/CSDI_epoch27_loss0.19465310946106912.pypots
2024-05-24 08:41:54 [INFO]: Epoch 028 - training loss: 0.2007, validation loss: 0.1958
2024-05-24 08:41:54 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T082132/CSDI_epoch28_loss0.19580115452408792.pypots
2024-05-24 08:42:37 [INFO]: Epoch 029 - training loss: 0.2019, validation loss: 0.1921
2024-05-24 08:42:37 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T082132/CSDI_epoch29_loss0.192122533172369.pypots
2024-05-24 08:43:21 [INFO]: Epoch 030 - training loss: 0.2027, validation loss: 0.1961
2024-05-24 08:43:21 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T082132/CSDI_epoch30_loss0.1960816986858845.pypots
2024-05-24 08:44:04 [INFO]: Epoch 031 - training loss: 0.2068, validation loss: 0.1945
2024-05-24 08:44:04 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T082132/CSDI_epoch31_loss0.1944929040968418.pypots
2024-05-24 08:44:48 [INFO]: Epoch 032 - training loss: 0.2003, validation loss: 0.1948
2024-05-24 08:44:48 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T082132/CSDI_epoch32_loss0.19477684497833253.pypots
2024-05-24 08:45:32 [INFO]: Epoch 033 - training loss: 0.2010, validation loss: 0.1900
2024-05-24 08:45:32 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T082132/CSDI_epoch33_loss0.19001837968826293.pypots
2024-05-24 08:46:16 [INFO]: Epoch 034 - training loss: 0.2063, validation loss: 0.1935
2024-05-24 08:46:16 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T082132/CSDI_epoch34_loss0.1934696689248085.pypots
2024-05-24 08:47:00 [INFO]: Epoch 035 - training loss: 0.1975, validation loss: 0.1933
2024-05-24 08:47:00 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T082132/CSDI_epoch35_loss0.19326842799782754.pypots
2024-05-24 08:47:43 [INFO]: Epoch 036 - training loss: 0.2114, validation loss: 0.1935
2024-05-24 08:47:43 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T082132/CSDI_epoch36_loss0.19347014427185058.pypots
2024-05-24 08:48:27 [INFO]: Epoch 037 - training loss: 0.2070, validation loss: 0.1935
2024-05-24 08:48:27 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T082132/CSDI_epoch37_loss0.19352566823363304.pypots
2024-05-24 08:49:11 [INFO]: Epoch 038 - training loss: 0.2013, validation loss: 0.1922
2024-05-24 08:49:11 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T082132/CSDI_epoch38_loss0.19219458773732184.pypots
2024-05-24 08:49:54 [INFO]: Epoch 039 - training loss: 0.1926, validation loss: 0.1901
2024-05-24 08:49:54 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T082132/CSDI_epoch39_loss0.19011592343449593.pypots
2024-05-24 08:50:38 [INFO]: Epoch 040 - training loss: 0.2070, validation loss: 0.1906
2024-05-24 08:50:38 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T082132/CSDI_epoch40_loss0.19064510986208916.pypots
2024-05-24 08:51:21 [INFO]: Epoch 041 - training loss: 0.2030, validation loss: 0.1870
2024-05-24 08:51:21 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T082132/CSDI_epoch41_loss0.18703650012612344.pypots
2024-05-24 08:52:05 [INFO]: Epoch 042 - training loss: 0.2085, validation loss: 0.1919
2024-05-24 08:52:05 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T082132/CSDI_epoch42_loss0.19193140640854836.pypots
2024-05-24 08:52:49 [INFO]: Epoch 043 - training loss: 0.1911, validation loss: 0.1907
2024-05-24 08:52:49 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T082132/CSDI_epoch43_loss0.19071232229471208.pypots
2024-05-24 08:53:33 [INFO]: Epoch 044 - training loss: 0.1992, validation loss: 0.1906
2024-05-24 08:53:33 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T082132/CSDI_epoch44_loss0.19060812667012214.pypots
2024-05-24 08:54:17 [INFO]: Epoch 045 - training loss: 0.1936, validation loss: 0.1869
2024-05-24 08:54:17 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T082132/CSDI_epoch45_loss0.18688153028488158.pypots
2024-05-24 08:55:00 [INFO]: Epoch 046 - training loss: 0.2012, validation loss: 0.1856
2024-05-24 08:55:00 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T082132/CSDI_epoch46_loss0.18557414636015893.pypots
2024-05-24 08:55:44 [INFO]: Epoch 047 - training loss: 0.1899, validation loss: 0.1871
2024-05-24 08:55:44 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T082132/CSDI_epoch47_loss0.18708627820014953.pypots
2024-05-24 08:56:27 [INFO]: Epoch 048 - training loss: 0.1908, validation loss: 0.1868
2024-05-24 08:56:27 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T082132/CSDI_epoch48_loss0.1868392378091812.pypots
2024-05-24 08:57:11 [INFO]: Epoch 049 - training loss: 0.1863, validation loss: 0.1893
2024-05-24 08:57:11 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T082132/CSDI_epoch49_loss0.18925179839134215.pypots
2024-05-24 08:57:55 [INFO]: Epoch 050 - training loss: 0.1933, validation loss: 0.1861
2024-05-24 08:57:55 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T082132/CSDI_epoch50_loss0.18611229360103607.pypots
2024-05-24 08:58:38 [INFO]: Epoch 051 - training loss: 0.2125, validation loss: 0.1864
2024-05-24 08:58:38 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T082132/CSDI_epoch51_loss0.18638132512569427.pypots
2024-05-24 08:59:22 [INFO]: Epoch 052 - training loss: 0.1884, validation loss: 0.1892
2024-05-24 08:59:22 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T082132/CSDI_epoch52_loss0.18924632519483567.pypots
2024-05-24 09:00:06 [INFO]: Epoch 053 - training loss: 0.1893, validation loss: 0.1858
2024-05-24 09:00:06 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T082132/CSDI_epoch53_loss0.18581054955720902.pypots
2024-05-24 09:00:49 [INFO]: Epoch 054 - training loss: 0.1876, validation loss: 0.1849
2024-05-24 09:00:49 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T082132/CSDI_epoch54_loss0.18488326370716096.pypots
2024-05-24 09:01:33 [INFO]: Epoch 055 - training loss: 0.1982, validation loss: 0.1876
2024-05-24 09:01:33 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T082132/CSDI_epoch55_loss0.18764206841588021.pypots
2024-05-24 09:02:17 [INFO]: Epoch 056 - training loss: 0.1994, validation loss: 0.1861
2024-05-24 09:02:17 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T082132/CSDI_epoch56_loss0.1860952652990818.pypots
2024-05-24 09:03:00 [INFO]: Epoch 057 - training loss: 0.2039, validation loss: 0.1875
2024-05-24 09:03:00 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T082132/CSDI_epoch57_loss0.18754056021571158.pypots
2024-05-24 09:03:44 [INFO]: Epoch 058 - training loss: 0.2051, validation loss: 0.1838
2024-05-24 09:03:44 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T082132/CSDI_epoch58_loss0.18380597084760666.pypots
2024-05-24 09:04:28 [INFO]: Epoch 059 - training loss: 0.1967, validation loss: 0.1863
2024-05-24 09:04:28 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T082132/CSDI_epoch59_loss0.18633929789066314.pypots
2024-05-24 09:05:11 [INFO]: Epoch 060 - training loss: 0.1935, validation loss: 0.1881
2024-05-24 09:05:11 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T082132/CSDI_epoch60_loss0.18812021017074584.pypots
2024-05-24 09:05:55 [INFO]: Epoch 061 - training loss: 0.1964, validation loss: 0.1872
2024-05-24 09:05:55 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T082132/CSDI_epoch61_loss0.187248358130455.pypots
2024-05-24 09:06:39 [INFO]: Epoch 062 - training loss: 0.1949, validation loss: 0.1841
2024-05-24 09:06:39 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T082132/CSDI_epoch62_loss0.18405766189098358.pypots
2024-05-24 09:07:22 [INFO]: Epoch 063 - training loss: 0.1962, validation loss: 0.1832
2024-05-24 09:07:22 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T082132/CSDI_epoch63_loss0.18324992582201957.pypots
2024-05-24 09:08:06 [INFO]: Epoch 064 - training loss: 0.1907, validation loss: 0.1853
2024-05-24 09:08:06 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T082132/CSDI_epoch64_loss0.18530700877308845.pypots
2024-05-24 09:08:50 [INFO]: Epoch 065 - training loss: 0.2050, validation loss: 0.1859
2024-05-24 09:08:50 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T082132/CSDI_epoch65_loss0.1858562022447586.pypots
2024-05-24 09:09:33 [INFO]: Epoch 066 - training loss: 0.1866, validation loss: 0.1821
2024-05-24 09:09:33 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T082132/CSDI_epoch66_loss0.1820693865418434.pypots
2024-05-24 09:10:17 [INFO]: Epoch 067 - training loss: 0.1939, validation loss: 0.1847
2024-05-24 09:10:17 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T082132/CSDI_epoch67_loss0.18468573018908502.pypots
2024-05-24 09:11:00 [INFO]: Epoch 068 - training loss: 0.1976, validation loss: 0.1832
2024-05-24 09:11:00 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T082132/CSDI_epoch68_loss0.1831919804215431.pypots
2024-05-24 09:11:44 [INFO]: Epoch 069 - training loss: 0.1839, validation loss: 0.1828
2024-05-24 09:11:44 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T082132/CSDI_epoch69_loss0.18277698904275894.pypots
2024-05-24 09:12:28 [INFO]: Epoch 070 - training loss: 0.1824, validation loss: 0.1850
2024-05-24 09:12:28 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T082132/CSDI_epoch70_loss0.18501925319433213.pypots
2024-05-24 09:13:11 [INFO]: Epoch 071 - training loss: 0.1999, validation loss: 0.1897
2024-05-24 09:13:11 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T082132/CSDI_epoch71_loss0.18974458053708076.pypots
2024-05-24 09:13:55 [INFO]: Epoch 072 - training loss: 0.1774, validation loss: 0.1846
2024-05-24 09:13:55 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T082132/CSDI_epoch72_loss0.1845658928155899.pypots
2024-05-24 09:14:39 [INFO]: Epoch 073 - training loss: 0.2009, validation loss: 0.1822
2024-05-24 09:14:39 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T082132/CSDI_epoch73_loss0.18218682259321212.pypots
2024-05-24 09:15:22 [INFO]: Epoch 074 - training loss: 0.1815, validation loss: 0.1831
2024-05-24 09:15:22 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T082132/CSDI_epoch74_loss0.18312012478709222.pypots
2024-05-24 09:16:06 [INFO]: Epoch 075 - training loss: 0.2001, validation loss: 0.1820
2024-05-24 09:16:06 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T082132/CSDI_epoch75_loss0.18201788440346717.pypots
2024-05-24 09:16:50 [INFO]: Epoch 076 - training loss: 0.1986, validation loss: 0.1817
2024-05-24 09:16:50 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T082132/CSDI_epoch76_loss0.18173960223793983.pypots
2024-05-24 09:17:33 [INFO]: Epoch 077 - training loss: 0.1868, validation loss: 0.1835
2024-05-24 09:17:33 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T082132/CSDI_epoch77_loss0.18353638797998428.pypots
2024-05-24 09:18:17 [INFO]: Epoch 078 - training loss: 0.1873, validation loss: 0.1813
2024-05-24 09:18:17 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T082132/CSDI_epoch78_loss0.18125682026147844.pypots
2024-05-24 09:19:01 [INFO]: Epoch 079 - training loss: 0.1859, validation loss: 0.1836
2024-05-24 09:19:01 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T082132/CSDI_epoch79_loss0.18364167734980583.pypots
2024-05-24 09:19:44 [INFO]: Epoch 080 - training loss: 0.1808, validation loss: 0.1780
2024-05-24 09:19:44 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T082132/CSDI_epoch80_loss0.1780473992228508.pypots
2024-05-24 09:20:28 [INFO]: Epoch 081 - training loss: 0.1850, validation loss: 0.1806
2024-05-24 09:20:28 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T082132/CSDI_epoch81_loss0.18059298545122146.pypots
2024-05-24 09:21:12 [INFO]: Epoch 082 - training loss: 0.1898, validation loss: 0.1833
2024-05-24 09:21:12 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T082132/CSDI_epoch82_loss0.18333625867962838.pypots
2024-05-24 09:21:55 [INFO]: Epoch 083 - training loss: 0.1893, validation loss: 0.1799
2024-05-24 09:21:55 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T082132/CSDI_epoch83_loss0.17988915964961052.pypots
2024-05-24 09:22:39 [INFO]: Epoch 084 - training loss: 0.1903, validation loss: 0.1817
2024-05-24 09:22:39 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T082132/CSDI_epoch84_loss0.18174246177077294.pypots
2024-05-24 09:23:23 [INFO]: Epoch 085 - training loss: 0.1893, validation loss: 0.1797
2024-05-24 09:23:23 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T082132/CSDI_epoch85_loss0.1797173023223877.pypots
2024-05-24 09:24:06 [INFO]: Epoch 086 - training loss: 0.1867, validation loss: 0.1869
2024-05-24 09:24:06 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T082132/CSDI_epoch86_loss0.18694087266921997.pypots
2024-05-24 09:24:50 [INFO]: Epoch 087 - training loss: 0.1820, validation loss: 0.1802
2024-05-24 09:24:50 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T082132/CSDI_epoch87_loss0.1801775202155113.pypots
2024-05-24 09:25:33 [INFO]: Epoch 088 - training loss: 0.1899, validation loss: 0.1799
2024-05-24 09:25:34 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T082132/CSDI_epoch88_loss0.17992266714572908.pypots
2024-05-24 09:26:17 [INFO]: Epoch 089 - training loss: 0.1864, validation loss: 0.1787
2024-05-24 09:26:17 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T082132/CSDI_epoch89_loss0.1786942645907402.pypots
2024-05-24 09:27:01 [INFO]: Epoch 090 - training loss: 0.1878, validation loss: 0.1811
2024-05-24 09:27:01 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T082132/CSDI_epoch90_loss0.18107956498861313.pypots
2024-05-24 09:27:01 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-05-24 09:27:01 [INFO]: Finished training. The best model is from epoch#80.
2024-05-24 09:27:01 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T082132/CSDI.pypots
2024-05-24 09:34:23 [INFO]: CSDI on PhysioNet-2012-seta: MAE=0.2392, MSE=0.2932
2024-05-24 10:03:45 [INFO]: Successfully saved to overlay_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/imputation.pkl
2024-05-24 10:03:45 [INFO]: Using the given device: cuda:0
2024-05-24 10:03:45 [INFO]: Model files will be saved to overlay_premask_post_norm_saved_results/round_4/GPVAE_physionet_2012_seta/20240524_T100345
2024-05-24 10:03:45 [INFO]: Tensorboard file will be saved to overlay_premask_post_norm_saved_results/round_4/GPVAE_physionet_2012_seta/20240524_T100345/tensorboard
2024-05-24 10:03:45 [INFO]: GPVAE initialized with the given hyperparameters, the number of trainable parameters: 1,028,244
2024-05-24 10:03:46 [INFO]: Epoch 001 - training loss: 42586.6362, validation loss: 0.9330
2024-05-24 10:03:47 [INFO]: Epoch 002 - training loss: 24427.0589, validation loss: 0.7449
2024-05-24 10:03:47 [INFO]: Epoch 003 - training loss: 23492.6986, validation loss: 0.7139
2024-05-24 10:03:48 [INFO]: Epoch 004 - training loss: 23188.6771, validation loss: 0.6808
2024-05-24 10:03:48 [INFO]: Epoch 005 - training loss: 23035.6562, validation loss: 0.6598
2024-05-24 10:03:49 [INFO]: Epoch 006 - training loss: 22952.3011, validation loss: 0.6447
2024-05-24 10:03:50 [INFO]: Epoch 007 - training loss: 22902.9675, validation loss: 0.6410
2024-05-24 10:03:50 [INFO]: Epoch 008 - training loss: 22871.3535, validation loss: 0.6424
2024-05-24 10:03:51 [INFO]: Epoch 009 - training loss: 22850.2891, validation loss: 0.6465
2024-05-24 10:03:51 [INFO]: Epoch 010 - training loss: 22835.4880, validation loss: 0.6367
2024-05-24 10:03:52 [INFO]: Epoch 011 - training loss: 22824.8669, validation loss: 0.6342
2024-05-24 10:03:53 [INFO]: Epoch 012 - training loss: 22817.1361, validation loss: 0.6391
2024-05-24 10:03:53 [INFO]: Epoch 013 - training loss: 22811.3694, validation loss: 0.6381
2024-05-24 10:03:54 [INFO]: Epoch 014 - training loss: 22806.7534, validation loss: 0.6303
2024-05-24 10:03:54 [INFO]: Epoch 015 - training loss: 22803.1882, validation loss: 0.6313
2024-05-24 10:03:55 [INFO]: Epoch 016 - training loss: 22799.9484, validation loss: 0.6257
2024-05-24 10:03:55 [INFO]: Epoch 017 - training loss: 22797.2194, validation loss: 0.6202
2024-05-24 10:03:56 [INFO]: Epoch 018 - training loss: 22795.1096, validation loss: 0.6188
2024-05-24 10:03:57 [INFO]: Epoch 019 - training loss: 22793.5245, validation loss: 0.6322
2024-05-24 10:03:57 [INFO]: Epoch 020 - training loss: 22792.3439, validation loss: 0.6158
2024-05-24 10:03:58 [INFO]: Epoch 021 - training loss: 22789.7585, validation loss: 0.6061
2024-05-24 10:03:58 [INFO]: Epoch 022 - training loss: 22788.2698, validation loss: 0.6021
2024-05-24 10:03:59 [INFO]: Epoch 023 - training loss: 22786.8058, validation loss: 0.5994
2024-05-24 10:04:00 [INFO]: Epoch 024 - training loss: 22786.7000, validation loss: 0.6050
2024-05-24 10:04:00 [INFO]: Epoch 025 - training loss: 22785.9503, validation loss: 0.6041
2024-05-24 10:04:01 [INFO]: Epoch 026 - training loss: 22785.6490, validation loss: 0.6008
2024-05-24 10:04:01 [INFO]: Epoch 027 - training loss: 22784.9723, validation loss: 0.5985
2024-05-24 10:04:02 [INFO]: Epoch 028 - training loss: 22784.1005, validation loss: 0.5978
2024-05-24 10:04:03 [INFO]: Epoch 029 - training loss: 22783.2115, validation loss: 0.6030
2024-05-24 10:04:03 [INFO]: Epoch 030 - training loss: 22782.8642, validation loss: 0.5981
2024-05-24 10:04:04 [INFO]: Epoch 031 - training loss: 22782.7079, validation loss: 0.5979
2024-05-24 10:04:04 [INFO]: Epoch 032 - training loss: 22782.7157, validation loss: 0.5950
2024-05-24 10:04:05 [INFO]: Epoch 033 - training loss: 22782.4235, validation loss: 0.5935
2024-05-24 10:04:06 [INFO]: Epoch 034 - training loss: 22781.4238, validation loss: 0.5910
2024-05-24 10:04:06 [INFO]: Epoch 035 - training loss: 22780.9979, validation loss: 0.6017
2024-05-24 10:04:07 [INFO]: Epoch 036 - training loss: 22780.2890, validation loss: 0.5905
2024-05-24 10:04:07 [INFO]: Epoch 037 - training loss: 22779.9620, validation loss: 0.5911
2024-05-24 10:04:08 [INFO]: Epoch 038 - training loss: 22779.7848, validation loss: 0.5945
2024-05-24 10:04:09 [INFO]: Epoch 039 - training loss: 22779.6308, validation loss: 0.5887
2024-05-24 10:04:09 [INFO]: Epoch 040 - training loss: 22779.5124, validation loss: 0.5928
2024-05-24 10:04:10 [INFO]: Epoch 041 - training loss: 22779.4094, validation loss: 0.5911
2024-05-24 10:04:10 [INFO]: Epoch 042 - training loss: 22778.7771, validation loss: 0.5953
2024-05-24 10:04:11 [INFO]: Epoch 043 - training loss: 22778.4495, validation loss: 0.5939
2024-05-24 10:04:12 [INFO]: Epoch 044 - training loss: 22778.8748, validation loss: 0.5880
2024-05-24 10:04:12 [INFO]: Epoch 045 - training loss: 22777.8682, validation loss: 0.5845
2024-05-24 10:04:13 [INFO]: Epoch 046 - training loss: 22777.3510, validation loss: 0.6031
2024-05-24 10:04:13 [INFO]: Epoch 047 - training loss: 22776.6190, validation loss: 0.5828
2024-05-24 10:04:14 [INFO]: Epoch 048 - training loss: 22777.2507, validation loss: 0.5824
2024-05-24 10:04:14 [INFO]: Epoch 049 - training loss: 22776.2412, validation loss: 0.5845
2024-05-24 10:04:15 [INFO]: Epoch 050 - training loss: 22775.5850, validation loss: 0.5778
2024-05-24 10:04:16 [INFO]: Epoch 051 - training loss: 22774.1368, validation loss: 0.5749
2024-05-24 10:04:16 [INFO]: Epoch 052 - training loss: 22773.5099, validation loss: 0.5640
2024-05-24 10:04:17 [INFO]: Epoch 053 - training loss: 22772.2297, validation loss: 0.5632
2024-05-24 10:04:17 [INFO]: Epoch 054 - training loss: 22770.5130, validation loss: 0.5565
2024-05-24 10:04:18 [INFO]: Epoch 055 - training loss: 22769.4371, validation loss: 0.5515
2024-05-24 10:04:19 [INFO]: Epoch 056 - training loss: 22768.3980, validation loss: 0.5552
2024-05-24 10:04:19 [INFO]: Epoch 057 - training loss: 22767.5040, validation loss: 0.5406
2024-05-24 10:04:20 [INFO]: Epoch 058 - training loss: 22766.0119, validation loss: 0.5350
2024-05-24 10:04:20 [INFO]: Epoch 059 - training loss: 22764.9666, validation loss: 0.5349
2024-05-24 10:04:21 [INFO]: Epoch 060 - training loss: 22764.0432, validation loss: 0.5262
2024-05-24 10:04:22 [INFO]: Epoch 061 - training loss: 22762.9177, validation loss: 0.5245
2024-05-24 10:04:22 [INFO]: Epoch 062 - training loss: 22762.3487, validation loss: 0.5209
2024-05-24 10:04:23 [INFO]: Epoch 063 - training loss: 22761.4265, validation loss: 0.5151
2024-05-24 10:04:23 [INFO]: Epoch 064 - training loss: 22760.0792, validation loss: 0.5379
2024-05-24 10:04:24 [INFO]: Epoch 065 - training loss: 22759.7990, validation loss: 0.5063
2024-05-24 10:04:25 [INFO]: Epoch 066 - training loss: 22758.4464, validation loss: 0.5083
2024-05-24 10:04:25 [INFO]: Epoch 067 - training loss: 22758.1144, validation loss: 0.5030
2024-05-24 10:04:26 [INFO]: Epoch 068 - training loss: 22758.0062, validation loss: 0.5047
2024-05-24 10:04:26 [INFO]: Epoch 069 - training loss: 22757.2173, validation loss: 0.4989
2024-05-24 10:04:27 [INFO]: Epoch 070 - training loss: 22758.7676, validation loss: 0.5136
2024-05-24 10:04:28 [INFO]: Epoch 071 - training loss: 22757.0826, validation loss: 0.4969
2024-05-24 10:04:28 [INFO]: Epoch 072 - training loss: 22757.0062, validation loss: 0.4993
2024-05-24 10:04:29 [INFO]: Epoch 073 - training loss: 22756.0020, validation loss: 0.4962
2024-05-24 10:04:29 [INFO]: Epoch 074 - training loss: 22755.9714, validation loss: 0.4977
2024-05-24 10:04:30 [INFO]: Epoch 075 - training loss: 22754.8096, validation loss: 0.4947
2024-05-24 10:04:30 [INFO]: Epoch 076 - training loss: 22754.9041, validation loss: 0.4926
2024-05-24 10:04:31 [INFO]: Epoch 077 - training loss: 22754.9750, validation loss: 0.4894
2024-05-24 10:04:32 [INFO]: Epoch 078 - training loss: 22754.0572, validation loss: 0.4962
2024-05-24 10:04:32 [INFO]: Epoch 079 - training loss: 22753.9009, validation loss: 0.4933
2024-05-24 10:04:33 [INFO]: Epoch 080 - training loss: 22753.4749, validation loss: 0.4934
2024-05-24 10:04:33 [INFO]: Epoch 081 - training loss: 22754.7861, validation loss: 0.5024
2024-05-24 10:04:34 [INFO]: Epoch 082 - training loss: 22754.7484, validation loss: 0.4918
2024-05-24 10:04:35 [INFO]: Epoch 083 - training loss: 22754.7480, validation loss: 0.4906
2024-05-24 10:04:35 [INFO]: Epoch 084 - training loss: 22752.9830, validation loss: 0.4903
2024-05-24 10:04:36 [INFO]: Epoch 085 - training loss: 22752.6255, validation loss: 0.4970
2024-05-24 10:04:36 [INFO]: Epoch 086 - training loss: 22752.1920, validation loss: 0.4876
2024-05-24 10:04:37 [INFO]: Epoch 087 - training loss: 22752.6229, validation loss: 0.4899
2024-05-24 10:04:38 [INFO]: Epoch 088 - training loss: 22752.0445, validation loss: 0.4896
2024-05-24 10:04:38 [INFO]: Epoch 089 - training loss: 22752.1905, validation loss: 0.4872
2024-05-24 10:04:39 [INFO]: Epoch 090 - training loss: 22752.4094, validation loss: 0.4860
2024-05-24 10:04:39 [INFO]: Epoch 091 - training loss: 22752.2496, validation loss: 0.4967
2024-05-24 10:04:40 [INFO]: Epoch 092 - training loss: 22752.1709, validation loss: 0.4883
2024-05-24 10:04:41 [INFO]: Epoch 093 - training loss: 22753.0470, validation loss: 0.4868
2024-05-24 10:04:41 [INFO]: Epoch 094 - training loss: 22751.2350, validation loss: 0.4812
2024-05-24 10:04:42 [INFO]: Epoch 095 - training loss: 22751.0982, validation loss: 0.4872
2024-05-24 10:04:42 [INFO]: Epoch 096 - training loss: 22751.2393, validation loss: 0.4929
2024-05-24 10:04:43 [INFO]: Epoch 097 - training loss: 22750.9032, validation loss: 0.4843
2024-05-24 10:04:44 [INFO]: Epoch 098 - training loss: 22750.7723, validation loss: 0.4874
2024-05-24 10:04:44 [INFO]: Epoch 099 - training loss: 22751.3097, validation loss: 0.4974
2024-05-24 10:04:45 [INFO]: Epoch 100 - training loss: 22751.6412, validation loss: 0.4888
2024-05-24 10:04:45 [INFO]: Epoch 101 - training loss: 22751.1696, validation loss: 0.4833
2024-05-24 10:04:46 [INFO]: Epoch 102 - training loss: 22750.2590, validation loss: 0.4874
2024-05-24 10:04:46 [INFO]: Epoch 103 - training loss: 22750.1120, validation loss: 0.4845
2024-05-24 10:04:47 [INFO]: Epoch 104 - training loss: 22750.4842, validation loss: 0.4816
2024-05-24 10:04:47 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-05-24 10:04:47 [INFO]: Finished training. The best model is from epoch#94.
2024-05-24 10:04:47 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_4/GPVAE_physionet_2012_seta/20240524_T100345/GPVAE.pypots
2024-05-24 10:04:47 [INFO]: GP-VAE on PhysioNet-2012-seta: MAE=0.4303, MSE=0.4995
2024-05-24 10:04:48 [INFO]: Successfully saved to overlay_premask_post_norm_saved_results/round_4/GPVAE_physionet_2012_seta/imputation.pkl
2024-05-24 10:04:48 [INFO]: Using the given device: cuda:0
2024-05-24 10:04:48 [INFO]: Model files will be saved to overlay_premask_post_norm_saved_results/round_4/USGAN_physionet_2012_seta/20240524_T100448
2024-05-24 10:04:48 [INFO]: Tensorboard file will be saved to overlay_premask_post_norm_saved_results/round_4/USGAN_physionet_2012_seta/20240524_T100448/tensorboard
2024-05-24 10:04:48 [INFO]: USGAN initialized with the given hyperparameters, the number of trainable parameters: 16,010,261
2024-05-24 10:05:09 [INFO]: Epoch 001 - generator training loss: 0.5952, discriminator training loss: 0.3831, validation loss: 0.6147
2024-05-24 10:05:28 [INFO]: Epoch 002 - generator training loss: 0.4799, discriminator training loss: 0.2731, validation loss: 0.5222
2024-05-24 10:05:47 [INFO]: Epoch 003 - generator training loss: 0.4315, discriminator training loss: 0.2381, validation loss: 0.4984
2024-05-24 10:06:05 [INFO]: Epoch 004 - generator training loss: 0.4484, discriminator training loss: 0.1913, validation loss: 0.4928
2024-05-24 10:06:24 [INFO]: Epoch 005 - generator training loss: 0.4461, discriminator training loss: 0.1601, validation loss: 0.4809
2024-05-24 10:06:43 [INFO]: Epoch 006 - generator training loss: 0.4364, discriminator training loss: 0.1412, validation loss: 0.4717
2024-05-24 10:07:01 [INFO]: Epoch 007 - generator training loss: 0.4239, discriminator training loss: 0.1270, validation loss: 0.4517
2024-05-24 10:07:20 [INFO]: Epoch 008 - generator training loss: 0.4086, discriminator training loss: 0.1161, validation loss: 0.4395
2024-05-24 10:07:39 [INFO]: Epoch 009 - generator training loss: 0.4005, discriminator training loss: 0.1076, validation loss: 0.4337
2024-05-24 10:07:57 [INFO]: Epoch 010 - generator training loss: 0.3927, discriminator training loss: 0.1003, validation loss: 0.4221
2024-05-24 10:08:16 [INFO]: Epoch 011 - generator training loss: 0.3861, discriminator training loss: 0.0944, validation loss: 0.4193
2024-05-24 10:08:35 [INFO]: Epoch 012 - generator training loss: 0.3822, discriminator training loss: 0.0891, validation loss: 0.4103
2024-05-24 10:08:53 [INFO]: Epoch 013 - generator training loss: 0.3767, discriminator training loss: 0.0844, validation loss: 0.4079
2024-05-24 10:09:12 [INFO]: Epoch 014 - generator training loss: 0.3723, discriminator training loss: 0.0805, validation loss: 0.4029
2024-05-24 10:09:30 [INFO]: Epoch 015 - generator training loss: 0.3682, discriminator training loss: 0.0767, validation loss: 0.3969
2024-05-24 10:09:49 [INFO]: Epoch 016 - generator training loss: 0.3628, discriminator training loss: 0.0736, validation loss: 0.3926
2024-05-24 10:10:08 [INFO]: Epoch 017 - generator training loss: 0.3590, discriminator training loss: 0.0709, validation loss: 0.3867
2024-05-24 10:10:26 [INFO]: Epoch 018 - generator training loss: 0.3540, discriminator training loss: 0.0681, validation loss: 0.3856
2024-05-24 10:10:45 [INFO]: Epoch 019 - generator training loss: 0.3545, discriminator training loss: 0.0659, validation loss: 0.3804
2024-05-24 10:11:03 [INFO]: Epoch 020 - generator training loss: 0.3489, discriminator training loss: 0.0638, validation loss: 0.3793
2024-05-24 10:11:22 [INFO]: Epoch 021 - generator training loss: 0.3447, discriminator training loss: 0.0617, validation loss: 0.3692
2024-05-24 10:11:40 [INFO]: Epoch 022 - generator training loss: 0.3411, discriminator training loss: 0.0602, validation loss: 0.3704
2024-05-24 10:11:59 [INFO]: Epoch 023 - generator training loss: 0.3362, discriminator training loss: 0.0584, validation loss: 0.3695
2024-05-24 10:12:18 [INFO]: Epoch 024 - generator training loss: 0.3319, discriminator training loss: 0.0571, validation loss: 0.3691
2024-05-24 10:12:36 [INFO]: Epoch 025 - generator training loss: 0.3294, discriminator training loss: 0.0558, validation loss: 0.3647
2024-05-24 10:12:55 [INFO]: Epoch 026 - generator training loss: 0.3270, discriminator training loss: 0.0546, validation loss: 0.3624
2024-05-24 10:13:13 [INFO]: Epoch 027 - generator training loss: 0.3266, discriminator training loss: 0.0540, validation loss: 0.3590
2024-05-24 10:13:32 [INFO]: Epoch 028 - generator training loss: 0.3222, discriminator training loss: 0.0529, validation loss: 0.3552
2024-05-24 10:13:51 [INFO]: Epoch 029 - generator training loss: 0.3173, discriminator training loss: 0.0523, validation loss: 0.3530
2024-05-24 10:14:09 [INFO]: Epoch 030 - generator training loss: 0.3115, discriminator training loss: 0.0512, validation loss: 0.3533
2024-05-24 10:14:28 [INFO]: Epoch 031 - generator training loss: 0.3099, discriminator training loss: 0.0505, validation loss: 0.3497
2024-05-24 10:14:46 [INFO]: Epoch 032 - generator training loss: 0.3093, discriminator training loss: 0.0502, validation loss: 0.3466
2024-05-24 10:15:05 [INFO]: Epoch 033 - generator training loss: 0.3054, discriminator training loss: 0.0497, validation loss: 0.3430
2024-05-24 10:15:24 [INFO]: Epoch 034 - generator training loss: 0.3020, discriminator training loss: 0.0490, validation loss: 0.3430
2024-05-24 10:15:43 [INFO]: Epoch 035 - generator training loss: 0.2997, discriminator training loss: 0.0486, validation loss: 0.3431
2024-05-24 10:16:01 [INFO]: Epoch 036 - generator training loss: 0.2963, discriminator training loss: 0.0481, validation loss: 0.3359
2024-05-24 10:16:20 [INFO]: Epoch 037 - generator training loss: 0.2898, discriminator training loss: 0.0477, validation loss: 0.3377
2024-05-24 10:16:39 [INFO]: Epoch 038 - generator training loss: 0.2848, discriminator training loss: 0.0473, validation loss: 0.3315
2024-05-24 10:16:57 [INFO]: Epoch 039 - generator training loss: 0.2803, discriminator training loss: 0.0470, validation loss: 0.3299
2024-05-24 10:17:16 [INFO]: Epoch 040 - generator training loss: 0.2759, discriminator training loss: 0.0464, validation loss: 0.3272
2024-05-24 10:17:35 [INFO]: Epoch 041 - generator training loss: 0.2785, discriminator training loss: 0.0464, validation loss: 0.3325
2024-05-24 10:17:54 [INFO]: Epoch 042 - generator training loss: 0.2791, discriminator training loss: 0.0460, validation loss: 0.3234
2024-05-24 10:18:12 [INFO]: Epoch 043 - generator training loss: 0.2695, discriminator training loss: 0.0457, validation loss: 0.3305
2024-05-24 10:18:31 [INFO]: Epoch 044 - generator training loss: 0.2642, discriminator training loss: 0.0453, validation loss: 0.3206
2024-05-24 10:18:50 [INFO]: Epoch 045 - generator training loss: 0.2638, discriminator training loss: 0.0450, validation loss: 0.3231
2024-05-24 10:19:09 [INFO]: Epoch 046 - generator training loss: 0.2597, discriminator training loss: 0.0448, validation loss: 0.3164
2024-05-24 10:19:27 [INFO]: Epoch 047 - generator training loss: 0.2622, discriminator training loss: 0.0447, validation loss: 0.3199
2024-05-24 10:19:46 [INFO]: Epoch 048 - generator training loss: 0.2564, discriminator training loss: 0.0445, validation loss: 0.3168
2024-05-24 10:20:05 [INFO]: Epoch 049 - generator training loss: 0.2522, discriminator training loss: 0.0443, validation loss: 0.3135
2024-05-24 10:20:23 [INFO]: Epoch 050 - generator training loss: 0.2494, discriminator training loss: 0.0438, validation loss: 0.3161
2024-05-24 10:20:42 [INFO]: Epoch 051 - generator training loss: 0.2524, discriminator training loss: 0.0439, validation loss: 0.3136
2024-05-24 10:21:01 [INFO]: Epoch 052 - generator training loss: 0.2481, discriminator training loss: 0.0436, validation loss: 0.3095
2024-05-24 10:21:19 [INFO]: Epoch 053 - generator training loss: 0.2386, discriminator training loss: 0.0433, validation loss: 0.3076
2024-05-24 10:21:38 [INFO]: Epoch 054 - generator training loss: 0.2365, discriminator training loss: 0.0434, validation loss: 0.3079
2024-05-24 10:21:57 [INFO]: Epoch 055 - generator training loss: 0.2351, discriminator training loss: 0.0431, validation loss: 0.3095
2024-05-24 10:22:15 [INFO]: Epoch 056 - generator training loss: 0.2332, discriminator training loss: 0.0431, validation loss: 0.3090
2024-05-24 10:22:34 [INFO]: Epoch 057 - generator training loss: 0.2337, discriminator training loss: 0.0428, validation loss: 0.3110
2024-05-24 10:22:53 [INFO]: Epoch 058 - generator training loss: 0.2353, discriminator training loss: 0.0432, validation loss: 0.3048
2024-05-24 10:23:12 [INFO]: Epoch 059 - generator training loss: 0.2281, discriminator training loss: 0.0426, validation loss: 0.3090
2024-05-24 10:23:30 [INFO]: Epoch 060 - generator training loss: 0.2306, discriminator training loss: 0.0427, validation loss: 0.3045
2024-05-24 10:23:49 [INFO]: Epoch 061 - generator training loss: 0.2285, discriminator training loss: 0.0426, validation loss: 0.3117
2024-05-24 10:24:08 [INFO]: Epoch 062 - generator training loss: 0.2261, discriminator training loss: 0.0426, validation loss: 0.3101
2024-05-24 10:24:27 [INFO]: Epoch 063 - generator training loss: 0.2221, discriminator training loss: 0.0426, validation loss: 0.3029
2024-05-24 10:24:45 [INFO]: Epoch 064 - generator training loss: 0.2282, discriminator training loss: 0.0425, validation loss: 0.3074
2024-05-24 10:25:04 [INFO]: Epoch 065 - generator training loss: 0.2233, discriminator training loss: 0.0422, validation loss: 0.3077
2024-05-24 10:25:23 [INFO]: Epoch 066 - generator training loss: 0.2184, discriminator training loss: 0.0422, validation loss: 0.3022
2024-05-24 10:25:41 [INFO]: Epoch 067 - generator training loss: 0.2115, discriminator training loss: 0.0422, validation loss: 0.3046
2024-05-24 10:26:00 [INFO]: Epoch 068 - generator training loss: 0.2105, discriminator training loss: 0.0418, validation loss: 0.3053
2024-05-24 10:26:19 [INFO]: Epoch 069 - generator training loss: 0.2086, discriminator training loss: 0.0419, validation loss: 0.3035
2024-05-24 10:26:37 [INFO]: Epoch 070 - generator training loss: 0.2052, discriminator training loss: 0.0420, validation loss: 0.3020
2024-05-24 10:26:56 [INFO]: Epoch 071 - generator training loss: 0.2082, discriminator training loss: 0.0419, validation loss: 0.3039
2024-05-24 10:27:15 [INFO]: Epoch 072 - generator training loss: 0.2031, discriminator training loss: 0.0416, validation loss: 0.3046
2024-05-24 10:27:34 [INFO]: Epoch 073 - generator training loss: 0.2013, discriminator training loss: 0.0416, validation loss: 0.3070
2024-05-24 10:27:52 [INFO]: Epoch 074 - generator training loss: 0.2035, discriminator training loss: 0.0414, validation loss: 0.3041
2024-05-24 10:28:11 [INFO]: Epoch 075 - generator training loss: 0.2022, discriminator training loss: 0.0415, validation loss: 0.3041
2024-05-24 10:28:29 [INFO]: Epoch 076 - generator training loss: 0.2022, discriminator training loss: 0.0414, validation loss: 0.3012
2024-05-24 10:28:48 [INFO]: Epoch 077 - generator training loss: 0.1991, discriminator training loss: 0.0412, validation loss: 0.3039
2024-05-24 10:29:07 [INFO]: Epoch 078 - generator training loss: 0.1973, discriminator training loss: 0.0413, validation loss: 0.3042
2024-05-24 10:29:26 [INFO]: Epoch 079 - generator training loss: 0.1909, discriminator training loss: 0.0412, validation loss: 0.3026
2024-05-24 10:29:44 [INFO]: Epoch 080 - generator training loss: 0.1869, discriminator training loss: 0.0409, validation loss: 0.3055
2024-05-24 10:30:03 [INFO]: Epoch 081 - generator training loss: 0.1849, discriminator training loss: 0.0409, validation loss: 0.3017
2024-05-24 10:30:22 [INFO]: Epoch 082 - generator training loss: 0.1843, discriminator training loss: 0.0408, validation loss: 0.3041
2024-05-24 10:30:40 [INFO]: Epoch 083 - generator training loss: 0.1835, discriminator training loss: 0.0409, validation loss: 0.3025
2024-05-24 10:30:59 [INFO]: Epoch 084 - generator training loss: 0.1810, discriminator training loss: 0.0409, validation loss: 0.3027
2024-05-24 10:31:18 [INFO]: Epoch 085 - generator training loss: 0.1804, discriminator training loss: 0.0408, validation loss: 0.3042
2024-05-24 10:31:36 [INFO]: Epoch 086 - generator training loss: 0.1775, discriminator training loss: 0.0408, validation loss: 0.3040
2024-05-24 10:31:36 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-05-24 10:31:36 [INFO]: Finished training. The best model is from epoch#76.
2024-05-24 10:31:36 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_4/USGAN_physionet_2012_seta/20240524_T100448/USGAN.pypots
2024-05-24 10:31:39 [INFO]: US-GAN on PhysioNet-2012-seta: MAE=0.2972, MSE=0.2843
2024-05-24 10:31:49 [INFO]: Successfully saved to overlay_premask_post_norm_saved_results/round_4/USGAN_physionet_2012_seta/imputation.pkl
2024-05-24 10:31:49 [INFO]: Using the given device: cuda:0
2024-05-24 10:31:49 [INFO]: Model files will be saved to overlay_premask_post_norm_saved_results/round_4/BRITS_physionet_2012_seta/20240524_T103149
2024-05-24 10:31:49 [INFO]: Tensorboard file will be saved to overlay_premask_post_norm_saved_results/round_4/BRITS_physionet_2012_seta/20240524_T103149/tensorboard
2024-05-24 10:31:49 [INFO]: BRITS initialized with the given hyperparameters, the number of trainable parameters: 9,176,048
2024-05-24 10:32:04 [INFO]: Epoch 001 - training loss: 1.1315, validation loss: 0.5248
2024-05-24 10:32:16 [INFO]: Epoch 002 - training loss: 0.9242, validation loss: 0.4670
2024-05-24 10:32:28 [INFO]: Epoch 003 - training loss: 0.8634, validation loss: 0.4383
2024-05-24 10:32:40 [INFO]: Epoch 004 - training loss: 0.8257, validation loss: 0.4117
2024-05-24 10:32:52 [INFO]: Epoch 005 - training loss: 0.7959, validation loss: 0.3952
2024-05-24 10:33:05 [INFO]: Epoch 006 - training loss: 0.7729, validation loss: 0.3805
2024-05-24 10:33:17 [INFO]: Epoch 007 - training loss: 0.7535, validation loss: 0.3691
2024-05-24 10:33:29 [INFO]: Epoch 008 - training loss: 0.7380, validation loss: 0.3617
2024-05-24 10:33:41 [INFO]: Epoch 009 - training loss: 0.7256, validation loss: 0.3540
2024-05-24 10:33:53 [INFO]: Epoch 010 - training loss: 0.7152, validation loss: 0.3498
2024-05-24 10:34:06 [INFO]: Epoch 011 - training loss: 0.7045, validation loss: 0.3472
2024-05-24 10:34:19 [INFO]: Epoch 012 - training loss: 0.6969, validation loss: 0.3428
2024-05-24 10:34:32 [INFO]: Epoch 013 - training loss: 0.6901, validation loss: 0.3421
2024-05-24 10:34:45 [INFO]: Epoch 014 - training loss: 0.6833, validation loss: 0.3390
2024-05-24 10:34:58 [INFO]: Epoch 015 - training loss: 0.6779, validation loss: 0.3391
2024-05-24 10:35:11 [INFO]: Epoch 016 - training loss: 0.6722, validation loss: 0.3360
2024-05-24 10:35:24 [INFO]: Epoch 017 - training loss: 0.6685, validation loss: 0.3380
2024-05-24 10:35:37 [INFO]: Epoch 018 - training loss: 0.6644, validation loss: 0.3365
2024-05-24 10:35:50 [INFO]: Epoch 019 - training loss: 0.6610, validation loss: 0.3345
2024-05-24 10:36:03 [INFO]: Epoch 020 - training loss: 0.6569, validation loss: 0.3346
2024-05-24 10:36:16 [INFO]: Epoch 021 - training loss: 0.6527, validation loss: 0.3341
2024-05-24 10:36:29 [INFO]: Epoch 022 - training loss: 0.6501, validation loss: 0.3343
2024-05-24 10:36:41 [INFO]: Epoch 023 - training loss: 0.6460, validation loss: 0.3348
2024-05-24 10:36:54 [INFO]: Epoch 024 - training loss: 0.6461, validation loss: 0.3338
2024-05-24 10:37:06 [INFO]: Epoch 025 - training loss: 0.6435, validation loss: 0.3330
2024-05-24 10:37:18 [INFO]: Epoch 026 - training loss: 0.6379, validation loss: 0.3319
2024-05-24 10:37:30 [INFO]: Epoch 027 - training loss: 0.6350, validation loss: 0.3321
2024-05-24 10:37:43 [INFO]: Epoch 028 - training loss: 0.6322, validation loss: 0.3323
2024-05-24 10:37:55 [INFO]: Epoch 029 - training loss: 0.6312, validation loss: 0.3332
2024-05-24 10:38:07 [INFO]: Epoch 030 - training loss: 0.6275, validation loss: 0.3324
2024-05-24 10:38:19 [INFO]: Epoch 031 - training loss: 0.6247, validation loss: 0.3315
2024-05-24 10:38:32 [INFO]: Epoch 032 - training loss: 0.6262, validation loss: 0.3341
2024-05-24 10:38:44 [INFO]: Epoch 033 - training loss: 0.6234, validation loss: 0.3323
2024-05-24 10:38:56 [INFO]: Epoch 034 - training loss: 0.6178, validation loss: 0.3330
2024-05-24 10:39:08 [INFO]: Epoch 035 - training loss: 0.6162, validation loss: 0.3331
2024-05-24 10:39:21 [INFO]: Epoch 036 - training loss: 0.6159, validation loss: 0.3324
2024-05-24 10:39:33 [INFO]: Epoch 037 - training loss: 0.6116, validation loss: 0.3348
2024-05-24 10:39:45 [INFO]: Epoch 038 - training loss: 0.6089, validation loss: 0.3339
2024-05-24 10:39:57 [INFO]: Epoch 039 - training loss: 0.6101, validation loss: 0.3342
2024-05-24 10:40:10 [INFO]: Epoch 040 - training loss: 0.6053, validation loss: 0.3342
2024-05-24 10:40:22 [INFO]: Epoch 041 - training loss: 0.6023, validation loss: 0.3349
2024-05-24 10:40:22 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-05-24 10:40:22 [INFO]: Finished training. The best model is from epoch#31.
2024-05-24 10:40:22 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_4/BRITS_physionet_2012_seta/20240524_T103149/BRITS.pypots
2024-05-24 10:40:24 [INFO]: BRITS on PhysioNet-2012-seta: MAE=0.2634, MSE=0.2935
2024-05-24 10:40:34 [INFO]: Successfully saved to overlay_premask_post_norm_saved_results/round_4/BRITS_physionet_2012_seta/imputation.pkl
2024-05-24 10:40:34 [INFO]: Using the given device: cuda:0
2024-05-24 10:40:34 [INFO]: Model files will be saved to overlay_premask_post_norm_saved_results/round_4/MRNN_physionet_2012_seta/20240524_T104034
2024-05-24 10:40:34 [INFO]: Tensorboard file will be saved to overlay_premask_post_norm_saved_results/round_4/MRNN_physionet_2012_seta/20240524_T104034/tensorboard
2024-05-24 10:40:34 [INFO]: MRNN initialized with the given hyperparameters, the number of trainable parameters: 7,599
2024-05-24 10:40:40 [INFO]: Epoch 001 - training loss: 1.2576, validation loss: 0.9988
2024-05-24 10:40:40 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_4/MRNN_physionet_2012_seta/20240524_T104034/MRNN_epoch1_loss0.9988329768180847.pypots
2024-05-24 10:40:43 [INFO]: Epoch 002 - training loss: 0.7624, validation loss: 0.9734
2024-05-24 10:40:43 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_4/MRNN_physionet_2012_seta/20240524_T104034/MRNN_epoch2_loss0.9733643591403961.pypots
2024-05-24 10:40:46 [INFO]: Epoch 003 - training loss: 0.6198, validation loss: 0.9480
2024-05-24 10:40:46 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_4/MRNN_physionet_2012_seta/20240524_T104034/MRNN_epoch3_loss0.9479633331298828.pypots
2024-05-24 10:40:48 [INFO]: Epoch 004 - training loss: 0.5687, validation loss: 0.9329
2024-05-24 10:40:48 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_4/MRNN_physionet_2012_seta/20240524_T104034/MRNN_epoch4_loss0.932851466536522.pypots
2024-05-24 10:40:51 [INFO]: Epoch 005 - training loss: 0.5435, validation loss: 0.9242
2024-05-24 10:40:51 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_4/MRNN_physionet_2012_seta/20240524_T104034/MRNN_epoch5_loss0.9241948664188385.pypots
2024-05-24 10:40:54 [INFO]: Epoch 006 - training loss: 0.5197, validation loss: 0.9188
2024-05-24 10:40:54 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_4/MRNN_physionet_2012_seta/20240524_T104034/MRNN_epoch6_loss0.9188035726547241.pypots
2024-05-24 10:40:57 [INFO]: Epoch 007 - training loss: 0.5055, validation loss: 0.9167
2024-05-24 10:40:57 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_4/MRNN_physionet_2012_seta/20240524_T104034/MRNN_epoch7_loss0.9167111247777939.pypots
2024-05-24 10:41:00 [INFO]: Epoch 008 - training loss: 0.4902, validation loss: 0.9133
2024-05-24 10:41:00 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_4/MRNN_physionet_2012_seta/20240524_T104034/MRNN_epoch8_loss0.9133226990699768.pypots
2024-05-24 10:41:03 [INFO]: Epoch 009 - training loss: 0.4895, validation loss: 0.9153
2024-05-24 10:41:03 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_4/MRNN_physionet_2012_seta/20240524_T104034/MRNN_epoch9_loss0.9153112411499024.pypots
2024-05-24 10:41:05 [INFO]: Epoch 010 - training loss: 0.4813, validation loss: 0.9110
2024-05-24 10:41:05 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_4/MRNN_physionet_2012_seta/20240524_T104034/MRNN_epoch10_loss0.9110247194766998.pypots
2024-05-24 10:41:08 [INFO]: Epoch 011 - training loss: 0.4740, validation loss: 0.9122
2024-05-24 10:41:08 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_4/MRNN_physionet_2012_seta/20240524_T104034/MRNN_epoch11_loss0.912214532494545.pypots
2024-05-24 10:41:11 [INFO]: Epoch 012 - training loss: 0.4675, validation loss: 0.9136
2024-05-24 10:41:11 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_4/MRNN_physionet_2012_seta/20240524_T104034/MRNN_epoch12_loss0.9136430889368057.pypots
2024-05-24 10:41:14 [INFO]: Epoch 013 - training loss: 0.4616, validation loss: 0.9156
2024-05-24 10:41:14 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_4/MRNN_physionet_2012_seta/20240524_T104034/MRNN_epoch13_loss0.9155927211046219.pypots
2024-05-24 10:41:17 [INFO]: Epoch 014 - training loss: 0.4561, validation loss: 0.9170
2024-05-24 10:41:17 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_4/MRNN_physionet_2012_seta/20240524_T104034/MRNN_epoch14_loss0.9170090049505234.pypots
2024-05-24 10:41:19 [INFO]: Epoch 015 - training loss: 0.4539, validation loss: 0.9190
2024-05-24 10:41:19 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_4/MRNN_physionet_2012_seta/20240524_T104034/MRNN_epoch15_loss0.9190206050872802.pypots
2024-05-24 10:41:22 [INFO]: Epoch 016 - training loss: 0.4638, validation loss: 0.9227
2024-05-24 10:41:22 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_4/MRNN_physionet_2012_seta/20240524_T104034/MRNN_epoch16_loss0.9226905226707458.pypots
2024-05-24 10:41:25 [INFO]: Epoch 017 - training loss: 0.4415, validation loss: 0.9221
2024-05-24 10:41:25 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_4/MRNN_physionet_2012_seta/20240524_T104034/MRNN_epoch17_loss0.9220993518829346.pypots
2024-05-24 10:41:28 [INFO]: Epoch 018 - training loss: 0.4440, validation loss: 0.9233
2024-05-24 10:41:28 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_4/MRNN_physionet_2012_seta/20240524_T104034/MRNN_epoch18_loss0.9232513278722763.pypots
2024-05-24 10:41:31 [INFO]: Epoch 019 - training loss: 0.4421, validation loss: 0.9243
2024-05-24 10:41:31 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_4/MRNN_physionet_2012_seta/20240524_T104034/MRNN_epoch19_loss0.9242584437131882.pypots
2024-05-24 10:41:34 [INFO]: Epoch 020 - training loss: 0.4413, validation loss: 0.9263
2024-05-24 10:41:34 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_4/MRNN_physionet_2012_seta/20240524_T104034/MRNN_epoch20_loss0.9262800604104996.pypots
2024-05-24 10:41:34 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-05-24 10:41:34 [INFO]: Finished training. The best model is from epoch#10.
2024-05-24 10:41:34 [INFO]: Saved the model to overlay_premask_post_norm_saved_results/round_4/MRNN_physionet_2012_seta/20240524_T104034/MRNN.pypots
2024-05-24 10:41:35 [INFO]: MRNN on PhysioNet-2012-seta: MAE=0.6836, MSE=0.9234
2024-05-24 10:41:39 [INFO]: Successfully saved to overlay_premask_post_norm_saved_results/round_4/MRNN_physionet_2012_seta/imputation.pkl
2024-05-24 10:41:39 [INFO]: Using the given device: cpu
2024-05-24 10:41:39 [INFO]: LOCF on PhysioNet-2012-seta: MAE=0.4085, MSE=0.5400
2024-05-24 10:41:39 [INFO]: Successfully created the given path "saved_results/round_4/LOCF_physionet_2012_seta".
2024-05-24 10:41:39 [INFO]: Successfully saved to overlay_premask_post_norm_saved_results/round_4/LOCF_physionet_2012_seta/imputation.pkl
2024-05-24 10:41:39 [INFO]: Median on PhysioNet-2012-seta: MAE=0.6857, MSE=1.0298
2024-05-24 10:41:39 [INFO]: Successfully created the given path "saved_results/round_4/Median_physionet_2012_seta".
2024-05-24 10:41:39 [INFO]: Successfully saved to overlay_premask_post_norm_saved_results/round_4/Median_physionet_2012_seta/imputation.pkl
2024-05-24 10:41:39 [INFO]: Mean on PhysioNet-2012-seta: MAE=0.7024, MSE=1.0000
2024-05-24 10:41:39 [INFO]: Successfully created the given path "saved_results/round_4/Mean_physionet_2012_seta".
2024-05-24 10:41:39 [INFO]: Successfully saved to overlay_premask_post_norm_saved_results/round_4/Mean_physionet_2012_seta/imputation.pkl
2024-05-24 10:41:39 [INFO]: 
SAITS on data/physionet_2012_seta: MAE=0.271±0.006343770597280529, MSE=0.325±0.004424743128431258
Transformer on data/physionet_2012_seta: MAE=0.290±0.001633553938410931, MSE=0.336±0.0033486778775762907
TimesNet on data/physionet_2012_seta: MAE=0.291±0.002634146595177344, MSE=0.288±0.007019922632927083
CSDI on data/physionet_2012_seta: MAE=0.239±0.006141355259114223, MSE=0.344±0.10936272923009731
GPVAE on data/physionet_2012_seta: MAE=0.420±0.009661296391179373, MSE=0.489±0.007961966707662362
USGAN on data/physionet_2012_seta: MAE=0.299±0.002953679939535955, MSE=0.287±0.004194035167043752
BRITS on data/physionet_2012_seta: MAE=0.263±0.0011554751694941592, MSE=0.294±0.0032264321406090535
MRNN on data/physionet_2012_seta: MAE=0.684±0.001334765434899775, MSE=0.923±0.0018218061508920211
LOCF on data/physionet_2012_seta: MAE=0.409±0.0, MSE=0.540±0.0
Median on data/physionet_2012_seta: MAE=0.686±0.0, MSE=1.030±0.0
Mean on data/physionet_2012_seta: MAE=0.702±0.0, MSE=1.000±0.0
