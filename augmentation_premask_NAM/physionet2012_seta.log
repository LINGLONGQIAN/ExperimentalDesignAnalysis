2024-05-23 17:23:21 [INFO]: Have set the random seed as 2023 for numpy and pytorch.
2024-05-23 17:23:22 [INFO]: Using the given device: cuda:0
2024-05-23 17:23:22 [INFO]: Model files will be saved to augmentation_premask_post_norm_saved_results/round_0/SAITS_physionet_2012_seta/20240523_T172322
2024-05-23 17:23:22 [INFO]: Tensorboard file will be saved to augmentation_premask_post_norm_saved_results/round_0/SAITS_physionet_2012_seta/20240523_T172322/tensorboard
2024-05-23 17:23:22 [INFO]: SAITS initialized with the given hyperparameters, the number of trainable parameters: 5,322,806
2024-05-23 17:23:24 [INFO]: Epoch 001 - training loss: 1.0698, validation loss: 0.5154
2024-05-23 17:23:25 [INFO]: Epoch 002 - training loss: 0.7022, validation loss: 0.4581
2024-05-23 17:23:27 [INFO]: Epoch 003 - training loss: 0.5801, validation loss: 0.4200
2024-05-23 17:23:28 [INFO]: Epoch 004 - training loss: 0.5112, validation loss: 0.4095
2024-05-23 17:23:29 [INFO]: Epoch 005 - training loss: 0.4690, validation loss: 0.3964
2024-05-23 17:23:30 [INFO]: Epoch 006 - training loss: 0.4427, validation loss: 0.3897
2024-05-23 17:23:31 [INFO]: Epoch 007 - training loss: 0.4140, validation loss: 0.3718
2024-05-23 17:23:32 [INFO]: Epoch 008 - training loss: 0.3900, validation loss: 0.3697
2024-05-23 17:23:34 [INFO]: Epoch 009 - training loss: 0.3709, validation loss: 0.3651
2024-05-23 17:23:35 [INFO]: Epoch 010 - training loss: 0.3620, validation loss: 0.3552
2024-05-23 17:23:36 [INFO]: Epoch 011 - training loss: 0.3378, validation loss: 0.3564
2024-05-23 17:23:37 [INFO]: Epoch 012 - training loss: 0.3239, validation loss: 0.3465
2024-05-23 17:23:38 [INFO]: Epoch 013 - training loss: 0.3060, validation loss: 0.3532
2024-05-23 17:23:39 [INFO]: Epoch 014 - training loss: 0.2962, validation loss: 0.3409
2024-05-23 17:23:40 [INFO]: Epoch 015 - training loss: 0.2848, validation loss: 0.3454
2024-05-23 17:23:42 [INFO]: Epoch 016 - training loss: 0.2780, validation loss: 0.3464
2024-05-23 17:23:43 [INFO]: Epoch 017 - training loss: 0.2698, validation loss: 0.3428
2024-05-23 17:23:44 [INFO]: Epoch 018 - training loss: 0.2604, validation loss: 0.3427
2024-05-23 17:23:45 [INFO]: Epoch 019 - training loss: 0.2552, validation loss: 0.3356
2024-05-23 17:23:46 [INFO]: Epoch 020 - training loss: 0.2478, validation loss: 0.3385
2024-05-23 17:23:47 [INFO]: Epoch 021 - training loss: 0.2395, validation loss: 0.3394
2024-05-23 17:23:49 [INFO]: Epoch 022 - training loss: 0.2277, validation loss: 0.3399
2024-05-23 17:23:50 [INFO]: Epoch 023 - training loss: 0.2269, validation loss: 0.3366
2024-05-23 17:23:51 [INFO]: Epoch 024 - training loss: 0.2209, validation loss: 0.3372
2024-05-23 17:23:52 [INFO]: Epoch 025 - training loss: 0.2171, validation loss: 0.3399
2024-05-23 17:23:53 [INFO]: Epoch 026 - training loss: 0.2151, validation loss: 0.3379
2024-05-23 17:23:55 [INFO]: Epoch 027 - training loss: 0.2073, validation loss: 0.3387
2024-05-23 17:23:56 [INFO]: Epoch 028 - training loss: 0.2049, validation loss: 0.3380
2024-05-23 17:23:57 [INFO]: Epoch 029 - training loss: 0.1976, validation loss: 0.3378
2024-05-23 17:23:57 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-05-23 17:23:57 [INFO]: Finished training. The best model is from epoch#19.
2024-05-23 17:23:57 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_0/SAITS_physionet_2012_seta/20240523_T172322/SAITS.pypots
2024-05-23 17:23:58 [INFO]: SAITS on PhysioNet-2012-seta: MAE=0.2747, MSE=0.2913
2024-05-23 17:23:58 [INFO]: Successfully saved to augmentation_premask_post_norm_saved_results/round_0/SAITS_physionet_2012_seta/imputation.pkl
2024-05-23 17:23:58 [INFO]: Using the given device: cuda:0
2024-05-23 17:23:58 [INFO]: Model files will be saved to augmentation_premask_post_norm_saved_results/round_0/Transformer_physionet_2012_seta/20240523_T172358
2024-05-23 17:23:58 [INFO]: Tensorboard file will be saved to augmentation_premask_post_norm_saved_results/round_0/Transformer_physionet_2012_seta/20240523_T172358/tensorboard
2024-05-23 17:23:58 [INFO]: Transformer initialized with the given hyperparameters, the number of trainable parameters: 2,152,613
2024-05-23 17:23:59 [INFO]: Epoch 001 - training loss: 1.1492, validation loss: 0.5662
2024-05-23 17:23:59 [INFO]: Epoch 002 - training loss: 0.7253, validation loss: 0.4895
2024-05-23 17:24:00 [INFO]: Epoch 003 - training loss: 0.6169, validation loss: 0.4759
2024-05-23 17:24:01 [INFO]: Epoch 004 - training loss: 0.5727, validation loss: 0.4639
2024-05-23 17:24:01 [INFO]: Epoch 005 - training loss: 0.5420, validation loss: 0.4508
2024-05-23 17:24:02 [INFO]: Epoch 006 - training loss: 0.5029, validation loss: 0.4270
2024-05-23 17:24:02 [INFO]: Epoch 007 - training loss: 0.4758, validation loss: 0.4196
2024-05-23 17:24:03 [INFO]: Epoch 008 - training loss: 0.4545, validation loss: 0.4149
2024-05-23 17:24:03 [INFO]: Epoch 009 - training loss: 0.4378, validation loss: 0.4092
2024-05-23 17:24:04 [INFO]: Epoch 010 - training loss: 0.4213, validation loss: 0.4020
2024-05-23 17:24:05 [INFO]: Epoch 011 - training loss: 0.4070, validation loss: 0.3905
2024-05-23 17:24:05 [INFO]: Epoch 012 - training loss: 0.3919, validation loss: 0.3878
2024-05-23 17:24:06 [INFO]: Epoch 013 - training loss: 0.3805, validation loss: 0.3854
2024-05-23 17:24:06 [INFO]: Epoch 014 - training loss: 0.3724, validation loss: 0.3757
2024-05-23 17:24:07 [INFO]: Epoch 015 - training loss: 0.3599, validation loss: 0.3771
2024-05-23 17:24:08 [INFO]: Epoch 016 - training loss: 0.3522, validation loss: 0.3704
2024-05-23 17:24:08 [INFO]: Epoch 017 - training loss: 0.3395, validation loss: 0.3715
2024-05-23 17:24:09 [INFO]: Epoch 018 - training loss: 0.3357, validation loss: 0.3665
2024-05-23 17:24:09 [INFO]: Epoch 019 - training loss: 0.3260, validation loss: 0.3689
2024-05-23 17:24:10 [INFO]: Epoch 020 - training loss: 0.3251, validation loss: 0.3668
2024-05-23 17:24:10 [INFO]: Epoch 021 - training loss: 0.3115, validation loss: 0.3589
2024-05-23 17:24:11 [INFO]: Epoch 022 - training loss: 0.3089, validation loss: 0.3591
2024-05-23 17:24:12 [INFO]: Epoch 023 - training loss: 0.3028, validation loss: 0.3584
2024-05-23 17:24:12 [INFO]: Epoch 024 - training loss: 0.2938, validation loss: 0.3570
2024-05-23 17:24:13 [INFO]: Epoch 025 - training loss: 0.2888, validation loss: 0.3540
2024-05-23 17:24:13 [INFO]: Epoch 026 - training loss: 0.2866, validation loss: 0.3606
2024-05-23 17:24:14 [INFO]: Epoch 027 - training loss: 0.2802, validation loss: 0.3564
2024-05-23 17:24:15 [INFO]: Epoch 028 - training loss: 0.2799, validation loss: 0.3576
2024-05-23 17:24:15 [INFO]: Epoch 029 - training loss: 0.2674, validation loss: 0.3563
2024-05-23 17:24:16 [INFO]: Epoch 030 - training loss: 0.2650, validation loss: 0.3554
2024-05-23 17:24:16 [INFO]: Epoch 031 - training loss: 0.2653, validation loss: 0.3496
2024-05-23 17:24:17 [INFO]: Epoch 032 - training loss: 0.2562, validation loss: 0.3535
2024-05-23 17:24:18 [INFO]: Epoch 033 - training loss: 0.2575, validation loss: 0.3505
2024-05-23 17:24:18 [INFO]: Epoch 034 - training loss: 0.2517, validation loss: 0.3559
2024-05-23 17:24:19 [INFO]: Epoch 035 - training loss: 0.2521, validation loss: 0.3501
2024-05-23 17:24:19 [INFO]: Epoch 036 - training loss: 0.2421, validation loss: 0.3529
2024-05-23 17:24:20 [INFO]: Epoch 037 - training loss: 0.2389, validation loss: 0.3553
2024-05-23 17:24:20 [INFO]: Epoch 038 - training loss: 0.2381, validation loss: 0.3515
2024-05-23 17:24:21 [INFO]: Epoch 039 - training loss: 0.2336, validation loss: 0.3524
2024-05-23 17:24:22 [INFO]: Epoch 040 - training loss: 0.2285, validation loss: 0.3529
2024-05-23 17:24:22 [INFO]: Epoch 041 - training loss: 0.2259, validation loss: 0.3553
2024-05-23 17:24:22 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-05-23 17:24:22 [INFO]: Finished training. The best model is from epoch#31.
2024-05-23 17:24:22 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_0/Transformer_physionet_2012_seta/20240523_T172358/Transformer.pypots
2024-05-23 17:24:22 [INFO]: Transformer on PhysioNet-2012-seta: MAE=0.2826, MSE=0.3026
2024-05-23 17:24:23 [INFO]: Successfully saved to augmentation_premask_post_norm_saved_results/round_0/Transformer_physionet_2012_seta/imputation.pkl
2024-05-23 17:24:23 [INFO]: Using the given device: cuda:0
2024-05-23 17:24:23 [INFO]: Model files will be saved to augmentation_premask_post_norm_saved_results/round_0/TimesNet_physionet_2012_seta/20240523_T172423
2024-05-23 17:24:23 [INFO]: Tensorboard file will be saved to augmentation_premask_post_norm_saved_results/round_0/TimesNet_physionet_2012_seta/20240523_T172423/tensorboard
2024-05-23 17:24:23 [INFO]: TimesNet initialized with the given hyperparameters, the number of trainable parameters: 21,649,573
2024-05-23 17:24:24 [INFO]: Epoch 001 - training loss: 0.4414, validation loss: 0.5732
2024-05-23 17:24:25 [INFO]: Epoch 002 - training loss: 0.6734, validation loss: 0.6404
2024-05-23 17:24:25 [INFO]: Epoch 003 - training loss: 0.4646, validation loss: 0.8707
2024-05-23 17:24:26 [INFO]: Epoch 004 - training loss: 0.4298, validation loss: 0.4276
2024-05-23 17:24:27 [INFO]: Epoch 005 - training loss: 0.3705, validation loss: 0.3803
2024-05-23 17:24:28 [INFO]: Epoch 006 - training loss: 0.3375, validation loss: 0.3489
2024-05-23 17:24:28 [INFO]: Epoch 007 - training loss: 0.3212, validation loss: 0.3265
2024-05-23 17:24:29 [INFO]: Epoch 008 - training loss: 0.3135, validation loss: 0.3360
2024-05-23 17:24:30 [INFO]: Epoch 009 - training loss: 0.2985, validation loss: 0.3099
2024-05-23 17:24:30 [INFO]: Epoch 010 - training loss: 0.2977, validation loss: 0.3292
2024-05-23 17:24:31 [INFO]: Epoch 011 - training loss: 0.2950, validation loss: 0.3253
2024-05-23 17:24:32 [INFO]: Epoch 012 - training loss: 0.2829, validation loss: 0.3342
2024-05-23 17:24:33 [INFO]: Epoch 013 - training loss: 0.2784, validation loss: 0.3205
2024-05-23 17:24:33 [INFO]: Epoch 014 - training loss: 0.2846, validation loss: 0.3138
2024-05-23 17:24:34 [INFO]: Epoch 015 - training loss: 0.2766, validation loss: 0.3252
2024-05-23 17:24:35 [INFO]: Epoch 016 - training loss: 0.2851, validation loss: 0.3110
2024-05-23 17:24:35 [INFO]: Epoch 017 - training loss: 0.2603, validation loss: 0.3217
2024-05-23 17:24:36 [INFO]: Epoch 018 - training loss: 0.2519, validation loss: 0.3216
2024-05-23 17:24:37 [INFO]: Epoch 019 - training loss: 0.2500, validation loss: 0.3211
2024-05-23 17:24:37 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-05-23 17:24:37 [INFO]: Finished training. The best model is from epoch#9.
2024-05-23 17:24:37 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_0/TimesNet_physionet_2012_seta/20240523_T172423/TimesNet.pypots
2024-05-23 17:24:37 [INFO]: TimesNet on PhysioNet-2012-seta: MAE=0.2896, MSE=0.2739
2024-05-23 17:24:37 [INFO]: Successfully saved to augmentation_premask_post_norm_saved_results/round_0/TimesNet_physionet_2012_seta/imputation.pkl
2024-05-23 17:24:37 [INFO]: Using the given device: cuda:0
2024-05-23 17:24:37 [INFO]: Model files will be saved to augmentation_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T172437
2024-05-23 17:24:37 [INFO]: Tensorboard file will be saved to augmentation_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T172437/tensorboard
2024-05-23 17:24:37 [INFO]: CSDI initialized with the given hyperparameters, the number of trainable parameters: 1,694,753
2024-05-23 17:25:21 [INFO]: Epoch 001 - training loss: 0.4300, validation loss: 0.3459
2024-05-23 17:25:21 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T172437/CSDI_epoch1_loss0.34589685052633284.pypots
2024-05-23 17:26:04 [INFO]: Epoch 002 - training loss: 0.3315, validation loss: 0.3176
2024-05-23 17:26:05 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T172437/CSDI_epoch2_loss0.31755942553281785.pypots
2024-05-23 17:26:48 [INFO]: Epoch 003 - training loss: 0.3049, validation loss: 0.2798
2024-05-23 17:26:48 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T172437/CSDI_epoch3_loss0.27982849776744845.pypots
2024-05-23 17:27:32 [INFO]: Epoch 004 - training loss: 0.2739, validation loss: 0.2656
2024-05-23 17:27:32 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T172437/CSDI_epoch4_loss0.2656295567750931.pypots
2024-05-23 17:28:16 [INFO]: Epoch 005 - training loss: 0.2703, validation loss: 0.2507
2024-05-23 17:28:16 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T172437/CSDI_epoch5_loss0.2507073998451233.pypots
2024-05-23 17:29:00 [INFO]: Epoch 006 - training loss: 0.2564, validation loss: 0.2411
2024-05-23 17:29:00 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T172437/CSDI_epoch6_loss0.24112414866685866.pypots
2024-05-23 17:29:43 [INFO]: Epoch 007 - training loss: 0.2437, validation loss: 0.2323
2024-05-23 17:29:43 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T172437/CSDI_epoch7_loss0.23233050853013992.pypots
2024-05-23 17:30:27 [INFO]: Epoch 008 - training loss: 0.2514, validation loss: 0.2309
2024-05-23 17:30:27 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T172437/CSDI_epoch8_loss0.23087807223200799.pypots
2024-05-23 17:31:11 [INFO]: Epoch 009 - training loss: 0.2403, validation loss: 0.2273
2024-05-23 17:31:11 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T172437/CSDI_epoch9_loss0.22730019316077232.pypots
2024-05-23 17:31:55 [INFO]: Epoch 010 - training loss: 0.2222, validation loss: 0.2215
2024-05-23 17:31:55 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T172437/CSDI_epoch10_loss0.22152804359793662.pypots
2024-05-23 17:32:39 [INFO]: Epoch 011 - training loss: 0.2299, validation loss: 0.2158
2024-05-23 17:32:39 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T172437/CSDI_epoch11_loss0.2158001773059368.pypots
2024-05-23 17:33:23 [INFO]: Epoch 012 - training loss: 0.2302, validation loss: 0.2116
2024-05-23 17:33:23 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T172437/CSDI_epoch12_loss0.21164182648062707.pypots
2024-05-23 17:34:07 [INFO]: Epoch 013 - training loss: 0.2224, validation loss: 0.2186
2024-05-23 17:34:07 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T172437/CSDI_epoch13_loss0.21860293447971343.pypots
2024-05-23 17:34:51 [INFO]: Epoch 014 - training loss: 0.2267, validation loss: 0.2114
2024-05-23 17:34:51 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T172437/CSDI_epoch14_loss0.21136772856116295.pypots
2024-05-23 17:35:35 [INFO]: Epoch 015 - training loss: 0.2224, validation loss: 0.2063
2024-05-23 17:35:35 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T172437/CSDI_epoch15_loss0.20633761435747147.pypots
2024-05-23 17:36:18 [INFO]: Epoch 016 - training loss: 0.2084, validation loss: 0.2060
2024-05-23 17:36:19 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T172437/CSDI_epoch16_loss0.20600995421409607.pypots
2024-05-23 17:37:02 [INFO]: Epoch 017 - training loss: 0.1980, validation loss: 0.2091
2024-05-23 17:37:02 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T172437/CSDI_epoch17_loss0.20905377641320227.pypots
2024-05-23 17:37:46 [INFO]: Epoch 018 - training loss: 0.2101, validation loss: 0.2017
2024-05-23 17:37:46 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T172437/CSDI_epoch18_loss0.20174721851944924.pypots
2024-05-23 17:38:30 [INFO]: Epoch 019 - training loss: 0.2121, validation loss: 0.2073
2024-05-23 17:38:30 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T172437/CSDI_epoch19_loss0.20730973929166793.pypots
2024-05-23 17:39:14 [INFO]: Epoch 020 - training loss: 0.2234, validation loss: 0.2072
2024-05-23 17:39:14 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T172437/CSDI_epoch20_loss0.20721251890063286.pypots
2024-05-23 17:39:58 [INFO]: Epoch 021 - training loss: 0.2043, validation loss: 0.2064
2024-05-23 17:39:58 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T172437/CSDI_epoch21_loss0.2064185030758381.pypots
2024-05-23 17:40:41 [INFO]: Epoch 022 - training loss: 0.2068, validation loss: 0.2050
2024-05-23 17:40:41 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T172437/CSDI_epoch22_loss0.20496409609913827.pypots
2024-05-23 17:41:25 [INFO]: Epoch 023 - training loss: 0.2004, validation loss: 0.2045
2024-05-23 17:41:25 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T172437/CSDI_epoch23_loss0.20447880178689956.pypots
2024-05-23 17:42:09 [INFO]: Epoch 024 - training loss: 0.2092, validation loss: 0.1984
2024-05-23 17:42:09 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T172437/CSDI_epoch24_loss0.19836042448878288.pypots
2024-05-23 17:42:53 [INFO]: Epoch 025 - training loss: 0.2115, validation loss: 0.1965
2024-05-23 17:42:53 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T172437/CSDI_epoch25_loss0.19651731625199317.pypots
2024-05-23 17:43:36 [INFO]: Epoch 026 - training loss: 0.2172, validation loss: 0.2034
2024-05-23 17:43:36 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T172437/CSDI_epoch26_loss0.20335137322545052.pypots
2024-05-23 17:44:20 [INFO]: Epoch 027 - training loss: 0.2024, validation loss: 0.1959
2024-05-23 17:44:20 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T172437/CSDI_epoch27_loss0.19594522342085838.pypots
2024-05-23 17:45:04 [INFO]: Epoch 028 - training loss: 0.2067, validation loss: 0.1975
2024-05-23 17:45:04 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T172437/CSDI_epoch28_loss0.19750414565205573.pypots
2024-05-23 17:45:48 [INFO]: Epoch 029 - training loss: 0.2169, validation loss: 0.2001
2024-05-23 17:45:48 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T172437/CSDI_epoch29_loss0.20007686987519263.pypots
2024-05-23 17:46:31 [INFO]: Epoch 030 - training loss: 0.2116, validation loss: 0.1945
2024-05-23 17:46:31 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T172437/CSDI_epoch30_loss0.19446979090571404.pypots
2024-05-23 17:47:15 [INFO]: Epoch 031 - training loss: 0.1981, validation loss: 0.1998
2024-05-23 17:47:15 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T172437/CSDI_epoch31_loss0.199756770581007.pypots
2024-05-23 17:47:59 [INFO]: Epoch 032 - training loss: 0.2038, validation loss: 0.1977
2024-05-23 17:47:59 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T172437/CSDI_epoch32_loss0.19771372750401497.pypots
2024-05-23 17:48:43 [INFO]: Epoch 033 - training loss: 0.1967, validation loss: 0.1971
2024-05-23 17:48:43 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T172437/CSDI_epoch33_loss0.1970866620540619.pypots
2024-05-23 17:49:26 [INFO]: Epoch 034 - training loss: 0.1991, validation loss: 0.1960
2024-05-23 17:49:26 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T172437/CSDI_epoch34_loss0.1960315987467766.pypots
2024-05-23 17:50:10 [INFO]: Epoch 035 - training loss: 0.2056, validation loss: 0.1952
2024-05-23 17:50:10 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T172437/CSDI_epoch35_loss0.19523564279079436.pypots
2024-05-23 17:50:54 [INFO]: Epoch 036 - training loss: 0.1970, validation loss: 0.1947
2024-05-23 17:50:54 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T172437/CSDI_epoch36_loss0.1947263553738594.pypots
2024-05-23 17:51:38 [INFO]: Epoch 037 - training loss: 0.1963, validation loss: 0.1963
2024-05-23 17:51:38 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T172437/CSDI_epoch37_loss0.19632234945893287.pypots
2024-05-23 17:52:22 [INFO]: Epoch 038 - training loss: 0.2044, validation loss: 0.1934
2024-05-23 17:52:22 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T172437/CSDI_epoch38_loss0.19335527271032332.pypots
2024-05-23 17:53:05 [INFO]: Epoch 039 - training loss: 0.1950, validation loss: 0.1924
2024-05-23 17:53:05 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T172437/CSDI_epoch39_loss0.1923642598092556.pypots
2024-05-23 17:53:49 [INFO]: Epoch 040 - training loss: 0.1927, validation loss: 0.1938
2024-05-23 17:53:49 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T172437/CSDI_epoch40_loss0.19382292479276658.pypots
2024-05-23 17:54:33 [INFO]: Epoch 041 - training loss: 0.1977, validation loss: 0.1956
2024-05-23 17:54:33 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T172437/CSDI_epoch41_loss0.1956159509718418.pypots
2024-05-23 17:55:17 [INFO]: Epoch 042 - training loss: 0.1944, validation loss: 0.1945
2024-05-23 17:55:17 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T172437/CSDI_epoch42_loss0.19447644650936127.pypots
2024-05-23 17:56:00 [INFO]: Epoch 043 - training loss: 0.2069, validation loss: 0.1947
2024-05-23 17:56:00 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T172437/CSDI_epoch43_loss0.19466600939631462.pypots
2024-05-23 17:56:44 [INFO]: Epoch 044 - training loss: 0.2023, validation loss: 0.1976
2024-05-23 17:56:44 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T172437/CSDI_epoch44_loss0.19763298705220222.pypots
2024-05-23 17:57:28 [INFO]: Epoch 045 - training loss: 0.1989, validation loss: 0.1915
2024-05-23 17:57:28 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T172437/CSDI_epoch45_loss0.19148371145129203.pypots
2024-05-23 17:58:12 [INFO]: Epoch 046 - training loss: 0.1970, validation loss: 0.1920
2024-05-23 17:58:12 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T172437/CSDI_epoch46_loss0.19197358563542366.pypots
2024-05-23 17:58:56 [INFO]: Epoch 047 - training loss: 0.2016, validation loss: 0.1921
2024-05-23 17:58:56 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T172437/CSDI_epoch47_loss0.19205400794744493.pypots
2024-05-23 17:59:39 [INFO]: Epoch 048 - training loss: 0.1962, validation loss: 0.1926
2024-05-23 17:59:39 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T172437/CSDI_epoch48_loss0.19258907660841942.pypots
2024-05-23 18:00:23 [INFO]: Epoch 049 - training loss: 0.1938, validation loss: 0.1882
2024-05-23 18:00:23 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T172437/CSDI_epoch49_loss0.1882480949163437.pypots
2024-05-23 18:01:07 [INFO]: Epoch 050 - training loss: 0.2032, validation loss: 0.1890
2024-05-23 18:01:07 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T172437/CSDI_epoch50_loss0.1890418380498886.pypots
2024-05-23 18:01:51 [INFO]: Epoch 051 - training loss: 0.1951, validation loss: 0.1941
2024-05-23 18:01:51 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T172437/CSDI_epoch51_loss0.1941085547208786.pypots
2024-05-23 18:02:35 [INFO]: Epoch 052 - training loss: 0.1895, validation loss: 0.1872
2024-05-23 18:02:35 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T172437/CSDI_epoch52_loss0.18716877326369286.pypots
2024-05-23 18:03:18 [INFO]: Epoch 053 - training loss: 0.1886, validation loss: 0.1892
2024-05-23 18:03:18 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T172437/CSDI_epoch53_loss0.18919053822755813.pypots
2024-05-23 18:04:02 [INFO]: Epoch 054 - training loss: 0.1970, validation loss: 0.1892
2024-05-23 18:04:02 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T172437/CSDI_epoch54_loss0.18922236263751985.pypots
2024-05-23 18:04:46 [INFO]: Epoch 055 - training loss: 0.1894, validation loss: 0.1874
2024-05-23 18:04:46 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T172437/CSDI_epoch55_loss0.187362140417099.pypots
2024-05-23 18:05:30 [INFO]: Epoch 056 - training loss: 0.1817, validation loss: 0.1884
2024-05-23 18:05:30 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T172437/CSDI_epoch56_loss0.18838144093751907.pypots
2024-05-23 18:06:14 [INFO]: Epoch 057 - training loss: 0.2035, validation loss: 0.1869
2024-05-23 18:06:14 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T172437/CSDI_epoch57_loss0.18690953478217126.pypots
2024-05-23 18:06:58 [INFO]: Epoch 058 - training loss: 0.1963, validation loss: 0.1889
2024-05-23 18:06:58 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T172437/CSDI_epoch58_loss0.18886867687106132.pypots
2024-05-23 18:07:42 [INFO]: Epoch 059 - training loss: 0.2004, validation loss: 0.1900
2024-05-23 18:07:42 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T172437/CSDI_epoch59_loss0.18999204337596892.pypots
2024-05-23 18:08:26 [INFO]: Epoch 060 - training loss: 0.1954, validation loss: 0.1892
2024-05-23 18:08:26 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T172437/CSDI_epoch60_loss0.1891597755253315.pypots
2024-05-23 18:09:10 [INFO]: Epoch 061 - training loss: 0.1838, validation loss: 0.1879
2024-05-23 18:09:10 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T172437/CSDI_epoch61_loss0.18790288642048836.pypots
2024-05-23 18:09:54 [INFO]: Epoch 062 - training loss: 0.1999, validation loss: 0.1898
2024-05-23 18:09:54 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T172437/CSDI_epoch62_loss0.18975080102682113.pypots
2024-05-23 18:10:37 [INFO]: Epoch 063 - training loss: 0.1944, validation loss: 0.1870
2024-05-23 18:10:37 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T172437/CSDI_epoch63_loss0.18703926354646683.pypots
2024-05-23 18:11:21 [INFO]: Epoch 064 - training loss: 0.1905, validation loss: 0.1896
2024-05-23 18:11:21 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T172437/CSDI_epoch64_loss0.18957482203841208.pypots
2024-05-23 18:12:05 [INFO]: Epoch 065 - training loss: 0.1902, validation loss: 0.1915
2024-05-23 18:12:05 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T172437/CSDI_epoch65_loss0.1915324106812477.pypots
2024-05-23 18:12:49 [INFO]: Epoch 066 - training loss: 0.1891, validation loss: 0.1869
2024-05-23 18:12:49 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T172437/CSDI_epoch66_loss0.18687611147761346.pypots
2024-05-23 18:13:33 [INFO]: Epoch 067 - training loss: 0.1862, validation loss: 0.1887
2024-05-23 18:13:33 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T172437/CSDI_epoch67_loss0.18867803364992142.pypots
2024-05-23 18:14:16 [INFO]: Epoch 068 - training loss: 0.1914, validation loss: 0.1875
2024-05-23 18:14:16 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T172437/CSDI_epoch68_loss0.1874849818646908.pypots
2024-05-23 18:15:00 [INFO]: Epoch 069 - training loss: 0.1895, validation loss: 0.1885
2024-05-23 18:15:00 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T172437/CSDI_epoch69_loss0.18850145861506462.pypots
2024-05-23 18:15:44 [INFO]: Epoch 070 - training loss: 0.1957, validation loss: 0.1862
2024-05-23 18:15:44 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T172437/CSDI_epoch70_loss0.1861714504659176.pypots
2024-05-23 18:16:28 [INFO]: Epoch 071 - training loss: 0.1926, validation loss: 0.1876
2024-05-23 18:16:28 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T172437/CSDI_epoch71_loss0.18763632327318192.pypots
2024-05-23 18:17:12 [INFO]: Epoch 072 - training loss: 0.1810, validation loss: 0.1855
2024-05-23 18:17:12 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T172437/CSDI_epoch72_loss0.1855457179248333.pypots
2024-05-23 18:17:56 [INFO]: Epoch 073 - training loss: 0.1904, validation loss: 0.1852
2024-05-23 18:17:56 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T172437/CSDI_epoch73_loss0.18517670184373855.pypots
2024-05-23 18:18:40 [INFO]: Epoch 074 - training loss: 0.2004, validation loss: 0.1889
2024-05-23 18:18:40 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T172437/CSDI_epoch74_loss0.1889106549322605.pypots
2024-05-23 18:19:24 [INFO]: Epoch 075 - training loss: 0.1856, validation loss: 0.1906
2024-05-23 18:19:24 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T172437/CSDI_epoch75_loss0.1905843622982502.pypots
2024-05-23 18:20:08 [INFO]: Epoch 076 - training loss: 0.1873, validation loss: 0.1860
2024-05-23 18:20:08 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T172437/CSDI_epoch76_loss0.185956259816885.pypots
2024-05-23 18:20:51 [INFO]: Epoch 077 - training loss: 0.1943, validation loss: 0.1852
2024-05-23 18:20:51 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T172437/CSDI_epoch77_loss0.18520994037389754.pypots
2024-05-23 18:21:35 [INFO]: Epoch 078 - training loss: 0.1920, validation loss: 0.1878
2024-05-23 18:21:35 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T172437/CSDI_epoch78_loss0.18776979595422744.pypots
2024-05-23 18:22:19 [INFO]: Epoch 079 - training loss: 0.1896, validation loss: 0.1860
2024-05-23 18:22:19 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T172437/CSDI_epoch79_loss0.1859663061797619.pypots
2024-05-23 18:23:03 [INFO]: Epoch 080 - training loss: 0.1910, validation loss: 0.1870
2024-05-23 18:23:03 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T172437/CSDI_epoch80_loss0.18703592494130133.pypots
2024-05-23 18:23:47 [INFO]: Epoch 081 - training loss: 0.1900, validation loss: 0.1874
2024-05-23 18:23:47 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T172437/CSDI_epoch81_loss0.18744964078068732.pypots
2024-05-23 18:24:31 [INFO]: Epoch 082 - training loss: 0.1862, validation loss: 0.1839
2024-05-23 18:24:31 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T172437/CSDI_epoch82_loss0.18393489643931388.pypots
2024-05-23 18:25:14 [INFO]: Epoch 083 - training loss: 0.1975, validation loss: 0.1911
2024-05-23 18:25:14 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T172437/CSDI_epoch83_loss0.19110696464776994.pypots
2024-05-23 18:25:58 [INFO]: Epoch 084 - training loss: 0.1867, validation loss: 0.1847
2024-05-23 18:25:58 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T172437/CSDI_epoch84_loss0.18473797738552095.pypots
2024-05-23 18:26:42 [INFO]: Epoch 085 - training loss: 0.1831, validation loss: 0.1858
2024-05-23 18:26:42 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T172437/CSDI_epoch85_loss0.185819873213768.pypots
2024-05-23 18:27:26 [INFO]: Epoch 086 - training loss: 0.1888, validation loss: 0.1850
2024-05-23 18:27:26 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T172437/CSDI_epoch86_loss0.1849512130022049.pypots
2024-05-23 18:28:10 [INFO]: Epoch 087 - training loss: 0.1905, validation loss: 0.1832
2024-05-23 18:28:10 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T172437/CSDI_epoch87_loss0.18317707031965255.pypots
2024-05-23 18:28:54 [INFO]: Epoch 088 - training loss: 0.1926, validation loss: 0.1877
2024-05-23 18:28:54 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T172437/CSDI_epoch88_loss0.18774390071630478.pypots
2024-05-23 18:29:37 [INFO]: Epoch 089 - training loss: 0.1841, validation loss: 0.1832
2024-05-23 18:29:38 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T172437/CSDI_epoch89_loss0.1831868052482605.pypots
2024-05-23 18:30:21 [INFO]: Epoch 090 - training loss: 0.1945, validation loss: 0.1830
2024-05-23 18:30:21 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T172437/CSDI_epoch90_loss0.18299977108836174.pypots
2024-05-23 18:31:05 [INFO]: Epoch 091 - training loss: 0.1812, validation loss: 0.1851
2024-05-23 18:31:05 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T172437/CSDI_epoch91_loss0.18506030291318892.pypots
2024-05-23 18:31:49 [INFO]: Epoch 092 - training loss: 0.1851, validation loss: 0.1823
2024-05-23 18:31:49 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T172437/CSDI_epoch92_loss0.1823361821472645.pypots
2024-05-23 18:32:33 [INFO]: Epoch 093 - training loss: 0.1862, validation loss: 0.1837
2024-05-23 18:32:33 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T172437/CSDI_epoch93_loss0.18369534462690354.pypots
2024-05-23 18:33:17 [INFO]: Epoch 094 - training loss: 0.1838, validation loss: 0.1862
2024-05-23 18:33:17 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T172437/CSDI_epoch94_loss0.18622886165976524.pypots
2024-05-23 18:34:01 [INFO]: Epoch 095 - training loss: 0.1882, validation loss: 0.1841
2024-05-23 18:34:01 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T172437/CSDI_epoch95_loss0.1840778797864914.pypots
2024-05-23 18:34:45 [INFO]: Epoch 096 - training loss: 0.1843, validation loss: 0.1845
2024-05-23 18:34:45 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T172437/CSDI_epoch96_loss0.18446452915668488.pypots
2024-05-23 18:35:29 [INFO]: Epoch 097 - training loss: 0.1859, validation loss: 0.1837
2024-05-23 18:35:29 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T172437/CSDI_epoch97_loss0.18370492607355118.pypots
2024-05-23 18:36:12 [INFO]: Epoch 098 - training loss: 0.1799, validation loss: 0.1915
2024-05-23 18:36:12 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T172437/CSDI_epoch98_loss0.19151151180267334.pypots
2024-05-23 18:36:56 [INFO]: Epoch 099 - training loss: 0.1848, validation loss: 0.1841
2024-05-23 18:36:56 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T172437/CSDI_epoch99_loss0.18411923348903655.pypots
2024-05-23 18:37:40 [INFO]: Epoch 100 - training loss: 0.1848, validation loss: 0.1818
2024-05-23 18:37:40 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T172437/CSDI_epoch100_loss0.18180632442235947.pypots
2024-05-23 18:38:24 [INFO]: Epoch 101 - training loss: 0.1737, validation loss: 0.1831
2024-05-23 18:38:24 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T172437/CSDI_epoch101_loss0.18307316452264785.pypots
2024-05-23 18:39:08 [INFO]: Epoch 102 - training loss: 0.1837, validation loss: 0.1830
2024-05-23 18:39:08 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T172437/CSDI_epoch102_loss0.18299116119742392.pypots
2024-05-23 18:39:52 [INFO]: Epoch 103 - training loss: 0.1791, validation loss: 0.1838
2024-05-23 18:39:52 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T172437/CSDI_epoch103_loss0.18384912610054016.pypots
2024-05-23 18:40:36 [INFO]: Epoch 104 - training loss: 0.1871, validation loss: 0.1824
2024-05-23 18:40:36 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T172437/CSDI_epoch104_loss0.1823924481868744.pypots
2024-05-23 18:41:20 [INFO]: Epoch 105 - training loss: 0.1839, validation loss: 0.1849
2024-05-23 18:41:20 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T172437/CSDI_epoch105_loss0.18485099747776984.pypots
2024-05-23 18:42:03 [INFO]: Epoch 106 - training loss: 0.1830, validation loss: 0.1819
2024-05-23 18:42:03 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T172437/CSDI_epoch106_loss0.18186374232172967.pypots
2024-05-23 18:42:47 [INFO]: Epoch 107 - training loss: 0.1850, validation loss: 0.1818
2024-05-23 18:42:47 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T172437/CSDI_epoch107_loss0.18175244182348252.pypots
2024-05-23 18:43:31 [INFO]: Epoch 108 - training loss: 0.1896, validation loss: 0.1818
2024-05-23 18:43:31 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T172437/CSDI_epoch108_loss0.18178783133625984.pypots
2024-05-23 18:44:15 [INFO]: Epoch 109 - training loss: 0.1895, validation loss: 0.1823
2024-05-23 18:44:15 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T172437/CSDI_epoch109_loss0.18225013166666032.pypots
2024-05-23 18:44:59 [INFO]: Epoch 110 - training loss: 0.1806, validation loss: 0.1827
2024-05-23 18:44:59 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T172437/CSDI_epoch110_loss0.18266667276620865.pypots
2024-05-23 18:45:43 [INFO]: Epoch 111 - training loss: 0.1807, validation loss: 0.1843
2024-05-23 18:45:43 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T172437/CSDI_epoch111_loss0.18434790447354316.pypots
2024-05-23 18:46:27 [INFO]: Epoch 112 - training loss: 0.1767, validation loss: 0.1822
2024-05-23 18:46:27 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T172437/CSDI_epoch112_loss0.1822073094546795.pypots
2024-05-23 18:47:11 [INFO]: Epoch 113 - training loss: 0.1821, validation loss: 0.1815
2024-05-23 18:47:11 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T172437/CSDI_epoch113_loss0.1814889244735241.pypots
2024-05-23 18:47:55 [INFO]: Epoch 114 - training loss: 0.1794, validation loss: 0.1807
2024-05-23 18:47:55 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T172437/CSDI_epoch114_loss0.1807243585586548.pypots
2024-05-23 18:48:39 [INFO]: Epoch 115 - training loss: 0.1733, validation loss: 0.1819
2024-05-23 18:48:39 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T172437/CSDI_epoch115_loss0.181895013153553.pypots
2024-05-23 18:49:23 [INFO]: Epoch 116 - training loss: 0.1954, validation loss: 0.1851
2024-05-23 18:49:23 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T172437/CSDI_epoch116_loss0.1851493887603283.pypots
2024-05-23 18:50:07 [INFO]: Epoch 117 - training loss: 0.1863, validation loss: 0.1839
2024-05-23 18:50:07 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T172437/CSDI_epoch117_loss0.1838902659714222.pypots
2024-05-23 18:50:51 [INFO]: Epoch 118 - training loss: 0.1780, validation loss: 0.1813
2024-05-23 18:50:51 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T172437/CSDI_epoch118_loss0.1812548577785492.pypots
2024-05-23 18:51:35 [INFO]: Epoch 119 - training loss: 0.1862, validation loss: 0.1809
2024-05-23 18:51:35 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T172437/CSDI_epoch119_loss0.18092697337269784.pypots
2024-05-23 18:52:19 [INFO]: Epoch 120 - training loss: 0.1825, validation loss: 0.1813
2024-05-23 18:52:19 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T172437/CSDI_epoch120_loss0.1812748670578003.pypots
2024-05-23 18:53:03 [INFO]: Epoch 121 - training loss: 0.1779, validation loss: 0.1822
2024-05-23 18:53:03 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T172437/CSDI_epoch121_loss0.18223099261522294.pypots
2024-05-23 18:53:47 [INFO]: Epoch 122 - training loss: 0.1673, validation loss: 0.1803
2024-05-23 18:53:47 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T172437/CSDI_epoch122_loss0.18029276430606841.pypots
2024-05-23 18:54:31 [INFO]: Epoch 123 - training loss: 0.1838, validation loss: 0.1837
2024-05-23 18:54:31 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T172437/CSDI_epoch123_loss0.1836920753121376.pypots
2024-05-23 18:55:15 [INFO]: Epoch 124 - training loss: 0.1884, validation loss: 0.1782
2024-05-23 18:55:15 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T172437/CSDI_epoch124_loss0.17821418419480323.pypots
2024-05-23 18:55:59 [INFO]: Epoch 125 - training loss: 0.1837, validation loss: 0.1820
2024-05-23 18:55:59 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T172437/CSDI_epoch125_loss0.18200602158904075.pypots
2024-05-23 18:56:43 [INFO]: Epoch 126 - training loss: 0.1779, validation loss: 0.1816
2024-05-23 18:56:43 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T172437/CSDI_epoch126_loss0.1816486582159996.pypots
2024-05-23 18:57:27 [INFO]: Epoch 127 - training loss: 0.1798, validation loss: 0.1804
2024-05-23 18:57:27 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T172437/CSDI_epoch127_loss0.18044862523674965.pypots
2024-05-23 18:58:11 [INFO]: Epoch 128 - training loss: 0.1748, validation loss: 0.1815
2024-05-23 18:58:11 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T172437/CSDI_epoch128_loss0.1814751349389553.pypots
2024-05-23 18:58:55 [INFO]: Epoch 129 - training loss: 0.1823, validation loss: 0.1826
2024-05-23 18:58:55 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T172437/CSDI_epoch129_loss0.18255942314863205.pypots
2024-05-23 18:59:38 [INFO]: Epoch 130 - training loss: 0.1963, validation loss: 0.1795
2024-05-23 18:59:39 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T172437/CSDI_epoch130_loss0.1795030191540718.pypots
2024-05-23 19:00:22 [INFO]: Epoch 131 - training loss: 0.1809, validation loss: 0.1803
2024-05-23 19:00:22 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T172437/CSDI_epoch131_loss0.18029647395014764.pypots
2024-05-23 19:01:06 [INFO]: Epoch 132 - training loss: 0.1788, validation loss: 0.1811
2024-05-23 19:01:06 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T172437/CSDI_epoch132_loss0.18107866793870925.pypots
2024-05-23 19:01:50 [INFO]: Epoch 133 - training loss: 0.1786, validation loss: 0.1817
2024-05-23 19:01:50 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T172437/CSDI_epoch133_loss0.18170161172747612.pypots
2024-05-23 19:02:34 [INFO]: Epoch 134 - training loss: 0.1819, validation loss: 0.1812
2024-05-23 19:02:34 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T172437/CSDI_epoch134_loss0.1812480479478836.pypots
2024-05-23 19:02:34 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-05-23 19:02:34 [INFO]: Finished training. The best model is from epoch#124.
2024-05-23 19:02:34 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/20240523_T172437/CSDI.pypots
2024-05-23 19:09:56 [INFO]: CSDI on PhysioNet-2012-seta: MAE=0.2225, MSE=0.2500
2024-05-23 19:39:22 [INFO]: Successfully saved to augmentation_premask_post_norm_saved_results/round_0/CSDI_physionet_2012_seta/imputation.pkl
2024-05-23 19:39:22 [INFO]: Using the given device: cuda:0
2024-05-23 19:39:22 [INFO]: Model files will be saved to augmentation_premask_post_norm_saved_results/round_0/GPVAE_physionet_2012_seta/20240523_T193922
2024-05-23 19:39:22 [INFO]: Tensorboard file will be saved to augmentation_premask_post_norm_saved_results/round_0/GPVAE_physionet_2012_seta/20240523_T193922/tensorboard
2024-05-23 19:39:22 [INFO]: GPVAE initialized with the given hyperparameters, the number of trainable parameters: 1,028,244
2024-05-23 19:39:23 [INFO]: Epoch 001 - training loss: 42822.8376, validation loss: 0.9422
2024-05-23 19:39:24 [INFO]: Epoch 002 - training loss: 24402.3493, validation loss: 0.7493
2024-05-23 19:39:24 [INFO]: Epoch 003 - training loss: 23457.2921, validation loss: 0.7029
2024-05-23 19:39:25 [INFO]: Epoch 004 - training loss: 23165.0758, validation loss: 0.6796
2024-05-23 19:39:25 [INFO]: Epoch 005 - training loss: 23025.3373, validation loss: 0.6771
2024-05-23 19:39:26 [INFO]: Epoch 006 - training loss: 22948.6084, validation loss: 0.6775
2024-05-23 19:39:27 [INFO]: Epoch 007 - training loss: 22901.3339, validation loss: 0.6756
2024-05-23 19:39:27 [INFO]: Epoch 008 - training loss: 22871.2359, validation loss: 0.6726
2024-05-23 19:39:28 [INFO]: Epoch 009 - training loss: 22850.7274, validation loss: 0.6747
2024-05-23 19:39:28 [INFO]: Epoch 010 - training loss: 22836.6558, validation loss: 0.6689
2024-05-23 19:39:29 [INFO]: Epoch 011 - training loss: 22826.5734, validation loss: 0.6658
2024-05-23 19:39:30 [INFO]: Epoch 012 - training loss: 22818.5533, validation loss: 0.6662
2024-05-23 19:39:30 [INFO]: Epoch 013 - training loss: 22812.6838, validation loss: 0.6637
2024-05-23 19:39:31 [INFO]: Epoch 014 - training loss: 22807.9468, validation loss: 0.6628
2024-05-23 19:39:31 [INFO]: Epoch 015 - training loss: 22804.5546, validation loss: 0.6643
2024-05-23 19:39:32 [INFO]: Epoch 016 - training loss: 22801.2436, validation loss: 0.6562
2024-05-23 19:39:33 [INFO]: Epoch 017 - training loss: 22798.5002, validation loss: 0.6541
2024-05-23 19:39:33 [INFO]: Epoch 018 - training loss: 22796.2793, validation loss: 0.6519
2024-05-23 19:39:34 [INFO]: Epoch 019 - training loss: 22794.5237, validation loss: 0.6586
2024-05-23 19:39:34 [INFO]: Epoch 020 - training loss: 22792.3047, validation loss: 0.6457
2024-05-23 19:39:35 [INFO]: Epoch 021 - training loss: 22791.0951, validation loss: 0.6432
2024-05-23 19:39:36 [INFO]: Epoch 022 - training loss: 22789.2148, validation loss: 0.6400
2024-05-23 19:39:36 [INFO]: Epoch 023 - training loss: 22788.5033, validation loss: 0.6381
2024-05-23 19:39:37 [INFO]: Epoch 024 - training loss: 22787.2065, validation loss: 0.6421
2024-05-23 19:39:37 [INFO]: Epoch 025 - training loss: 22785.9298, validation loss: 0.6392
2024-05-23 19:39:38 [INFO]: Epoch 026 - training loss: 22785.8131, validation loss: 0.6408
2024-05-23 19:39:39 [INFO]: Epoch 027 - training loss: 22785.1137, validation loss: 0.6342
2024-05-23 19:39:39 [INFO]: Epoch 028 - training loss: 22783.4797, validation loss: 0.6495
2024-05-23 19:39:40 [INFO]: Epoch 029 - training loss: 22784.0499, validation loss: 0.6386
2024-05-23 19:39:40 [INFO]: Epoch 030 - training loss: 22782.8917, validation loss: 0.6310
2024-05-23 19:39:41 [INFO]: Epoch 031 - training loss: 22782.0705, validation loss: 0.6289
2024-05-23 19:39:42 [INFO]: Epoch 032 - training loss: 22781.4780, validation loss: 0.6483
2024-05-23 19:39:42 [INFO]: Epoch 033 - training loss: 22781.3996, validation loss: 0.6301
2024-05-23 19:39:43 [INFO]: Epoch 034 - training loss: 22780.4888, validation loss: 0.6238
2024-05-23 19:39:43 [INFO]: Epoch 035 - training loss: 22778.7734, validation loss: 0.6202
2024-05-23 19:39:44 [INFO]: Epoch 036 - training loss: 22776.9911, validation loss: 0.6224
2024-05-23 19:39:45 [INFO]: Epoch 037 - training loss: 22776.5279, validation loss: 0.6123
2024-05-23 19:39:45 [INFO]: Epoch 038 - training loss: 22775.3718, validation loss: 0.6274
2024-05-23 19:39:46 [INFO]: Epoch 039 - training loss: 22774.6785, validation loss: 0.6059
2024-05-23 19:39:46 [INFO]: Epoch 040 - training loss: 22775.0594, validation loss: 0.6118
2024-05-23 19:39:47 [INFO]: Epoch 041 - training loss: 22773.8990, validation loss: 0.6057
2024-05-23 19:39:48 [INFO]: Epoch 042 - training loss: 22772.9147, validation loss: 0.6004
2024-05-23 19:39:48 [INFO]: Epoch 043 - training loss: 22772.3154, validation loss: 0.5941
2024-05-23 19:39:49 [INFO]: Epoch 044 - training loss: 22772.7111, validation loss: 0.6018
2024-05-23 19:39:49 [INFO]: Epoch 045 - training loss: 22771.2233, validation loss: 0.5945
2024-05-23 19:39:50 [INFO]: Epoch 046 - training loss: 22770.5339, validation loss: 0.5918
2024-05-23 19:39:51 [INFO]: Epoch 047 - training loss: 22769.7044, validation loss: 0.5895
2024-05-23 19:39:51 [INFO]: Epoch 048 - training loss: 22768.9552, validation loss: 0.5918
2024-05-23 19:39:52 [INFO]: Epoch 049 - training loss: 22768.6321, validation loss: 0.5894
2024-05-23 19:39:52 [INFO]: Epoch 050 - training loss: 22768.0691, validation loss: 0.5877
2024-05-23 19:39:53 [INFO]: Epoch 051 - training loss: 22767.6793, validation loss: 0.5880
2024-05-23 19:39:54 [INFO]: Epoch 052 - training loss: 22767.0959, validation loss: 0.5858
2024-05-23 19:39:54 [INFO]: Epoch 053 - training loss: 22766.5323, validation loss: 0.5911
2024-05-23 19:39:55 [INFO]: Epoch 054 - training loss: 22766.0835, validation loss: 0.5837
2024-05-23 19:39:55 [INFO]: Epoch 055 - training loss: 22766.0959, validation loss: 0.5837
2024-05-23 19:39:56 [INFO]: Epoch 056 - training loss: 22764.8695, validation loss: 0.5754
2024-05-23 19:39:57 [INFO]: Epoch 057 - training loss: 22764.2970, validation loss: 0.5744
2024-05-23 19:39:57 [INFO]: Epoch 058 - training loss: 22763.6876, validation loss: 0.5743
2024-05-23 19:39:58 [INFO]: Epoch 059 - training loss: 22763.3352, validation loss: 0.5743
2024-05-23 19:39:58 [INFO]: Epoch 060 - training loss: 22762.4894, validation loss: 0.5689
2024-05-23 19:39:59 [INFO]: Epoch 061 - training loss: 22762.4522, validation loss: 0.5705
2024-05-23 19:40:00 [INFO]: Epoch 062 - training loss: 22761.8203, validation loss: 0.5643
2024-05-23 19:40:00 [INFO]: Epoch 063 - training loss: 22762.6511, validation loss: 0.5697
2024-05-23 19:40:01 [INFO]: Epoch 064 - training loss: 22761.1040, validation loss: 0.5633
2024-05-23 19:40:01 [INFO]: Epoch 065 - training loss: 22761.2186, validation loss: 0.5713
2024-05-23 19:40:02 [INFO]: Epoch 066 - training loss: 22760.6994, validation loss: 0.5579
2024-05-23 19:40:03 [INFO]: Epoch 067 - training loss: 22760.5738, validation loss: 0.5591
2024-05-23 19:40:03 [INFO]: Epoch 068 - training loss: 22758.9913, validation loss: 0.5527
2024-05-23 19:40:04 [INFO]: Epoch 069 - training loss: 22758.2243, validation loss: 0.5469
2024-05-23 19:40:04 [INFO]: Epoch 070 - training loss: 22758.7388, validation loss: 0.5495
2024-05-23 19:40:05 [INFO]: Epoch 071 - training loss: 22758.7437, validation loss: 0.5432
2024-05-23 19:40:06 [INFO]: Epoch 072 - training loss: 22759.4856, validation loss: 0.5410
2024-05-23 19:40:06 [INFO]: Epoch 073 - training loss: 22756.7079, validation loss: 0.5408
2024-05-23 19:40:07 [INFO]: Epoch 074 - training loss: 22756.1388, validation loss: 0.5449
2024-05-23 19:40:08 [INFO]: Epoch 075 - training loss: 22756.3070, validation loss: 0.5385
2024-05-23 19:40:08 [INFO]: Epoch 076 - training loss: 22755.8420, validation loss: 0.5389
2024-05-23 19:40:09 [INFO]: Epoch 077 - training loss: 22754.6863, validation loss: 0.5370
2024-05-23 19:40:09 [INFO]: Epoch 078 - training loss: 22754.9008, validation loss: 0.5309
2024-05-23 19:40:10 [INFO]: Epoch 079 - training loss: 22754.2575, validation loss: 0.5358
2024-05-23 19:40:11 [INFO]: Epoch 080 - training loss: 22754.0061, validation loss: 0.5313
2024-05-23 19:40:11 [INFO]: Epoch 081 - training loss: 22753.8921, validation loss: 0.5278
2024-05-23 19:40:12 [INFO]: Epoch 082 - training loss: 22754.1637, validation loss: 0.5306
2024-05-23 19:40:12 [INFO]: Epoch 083 - training loss: 22753.5674, validation loss: 0.5272
2024-05-23 19:40:13 [INFO]: Epoch 084 - training loss: 22754.0803, validation loss: 0.5285
2024-05-23 19:40:14 [INFO]: Epoch 085 - training loss: 22753.2282, validation loss: 0.5262
2024-05-23 19:40:14 [INFO]: Epoch 086 - training loss: 22753.2753, validation loss: 0.5416
2024-05-23 19:40:15 [INFO]: Epoch 087 - training loss: 22753.4846, validation loss: 0.5292
2024-05-23 19:40:15 [INFO]: Epoch 088 - training loss: 22752.7386, validation loss: 0.5361
2024-05-23 19:40:16 [INFO]: Epoch 089 - training loss: 22753.1339, validation loss: 0.5235
2024-05-23 19:40:17 [INFO]: Epoch 090 - training loss: 22752.2363, validation loss: 0.5355
2024-05-23 19:40:17 [INFO]: Epoch 091 - training loss: 22752.3239, validation loss: 0.5303
2024-05-23 19:40:18 [INFO]: Epoch 092 - training loss: 22751.9833, validation loss: 0.5254
2024-05-23 19:40:18 [INFO]: Epoch 093 - training loss: 22751.2646, validation loss: 0.5235
2024-05-23 19:40:19 [INFO]: Epoch 094 - training loss: 22750.9134, validation loss: 0.5206
2024-05-23 19:40:20 [INFO]: Epoch 095 - training loss: 22750.8126, validation loss: 0.5197
2024-05-23 19:40:20 [INFO]: Epoch 096 - training loss: 22750.5535, validation loss: 0.5201
2024-05-23 19:40:21 [INFO]: Epoch 097 - training loss: 22750.4604, validation loss: 0.5215
2024-05-23 19:40:21 [INFO]: Epoch 098 - training loss: 22751.0996, validation loss: 0.5205
2024-05-23 19:40:22 [INFO]: Epoch 099 - training loss: 22751.1820, validation loss: 0.5204
2024-05-23 19:40:23 [INFO]: Epoch 100 - training loss: 22750.3811, validation loss: 0.5172
2024-05-23 19:40:23 [INFO]: Epoch 101 - training loss: 22751.0217, validation loss: 0.5195
2024-05-23 19:40:24 [INFO]: Epoch 102 - training loss: 22750.3228, validation loss: 0.5165
2024-05-23 19:40:25 [INFO]: Epoch 103 - training loss: 22750.7573, validation loss: 0.5147
2024-05-23 19:40:25 [INFO]: Epoch 104 - training loss: 22750.5007, validation loss: 0.5148
2024-05-23 19:40:26 [INFO]: Epoch 105 - training loss: 22750.6459, validation loss: 0.5115
2024-05-23 19:40:26 [INFO]: Epoch 106 - training loss: 22749.3611, validation loss: 0.5118
2024-05-23 19:40:27 [INFO]: Epoch 107 - training loss: 22749.1726, validation loss: 0.5118
2024-05-23 19:40:28 [INFO]: Epoch 108 - training loss: 22749.2347, validation loss: 0.5089
2024-05-23 19:40:28 [INFO]: Epoch 109 - training loss: 22748.6075, validation loss: 0.5097
2024-05-23 19:40:29 [INFO]: Epoch 110 - training loss: 22748.3240, validation loss: 0.5091
2024-05-23 19:40:29 [INFO]: Epoch 111 - training loss: 22748.2707, validation loss: 0.5065
2024-05-23 19:40:30 [INFO]: Epoch 112 - training loss: 22748.2139, validation loss: 0.5084
2024-05-23 19:40:31 [INFO]: Epoch 113 - training loss: 22747.8182, validation loss: 0.5054
2024-05-23 19:40:31 [INFO]: Epoch 114 - training loss: 22747.6108, validation loss: 0.5077
2024-05-23 19:40:32 [INFO]: Epoch 115 - training loss: 22747.6273, validation loss: 0.5058
2024-05-23 19:40:32 [INFO]: Epoch 116 - training loss: 22747.6891, validation loss: 0.5049
2024-05-23 19:40:33 [INFO]: Epoch 117 - training loss: 22747.6022, validation loss: 0.5050
2024-05-23 19:40:34 [INFO]: Epoch 118 - training loss: 22747.6630, validation loss: 0.5035
2024-05-23 19:40:34 [INFO]: Epoch 119 - training loss: 22747.8840, validation loss: 0.5047
2024-05-23 19:40:35 [INFO]: Epoch 120 - training loss: 22747.5508, validation loss: 0.5073
2024-05-23 19:40:35 [INFO]: Epoch 121 - training loss: 22748.3512, validation loss: 0.5018
2024-05-23 19:40:36 [INFO]: Epoch 122 - training loss: 22747.1743, validation loss: 0.5210
2024-05-23 19:40:37 [INFO]: Epoch 123 - training loss: 22748.6605, validation loss: 0.4999
2024-05-23 19:40:37 [INFO]: Epoch 124 - training loss: 22747.1704, validation loss: 0.5093
2024-05-23 19:40:38 [INFO]: Epoch 125 - training loss: 22749.7053, validation loss: 0.4999
2024-05-23 19:40:38 [INFO]: Epoch 126 - training loss: 22748.0234, validation loss: 0.5156
2024-05-23 19:40:39 [INFO]: Epoch 127 - training loss: 22747.4623, validation loss: 0.4979
2024-05-23 19:40:40 [INFO]: Epoch 128 - training loss: 22746.4406, validation loss: 0.5003
2024-05-23 19:40:40 [INFO]: Epoch 129 - training loss: 22746.1161, validation loss: 0.4942
2024-05-23 19:40:41 [INFO]: Epoch 130 - training loss: 22745.8423, validation loss: 0.4943
2024-05-23 19:40:42 [INFO]: Epoch 131 - training loss: 22746.2224, validation loss: 0.4934
2024-05-23 19:40:42 [INFO]: Epoch 132 - training loss: 22745.5485, validation loss: 0.4969
2024-05-23 19:40:43 [INFO]: Epoch 133 - training loss: 22746.2762, validation loss: 0.4931
2024-05-23 19:40:43 [INFO]: Epoch 134 - training loss: 22746.5578, validation loss: 0.4951
2024-05-23 19:40:44 [INFO]: Epoch 135 - training loss: 22746.7938, validation loss: 0.4960
2024-05-23 19:40:45 [INFO]: Epoch 136 - training loss: 22745.8797, validation loss: 0.4968
2024-05-23 19:40:45 [INFO]: Epoch 137 - training loss: 22745.7522, validation loss: 0.4953
2024-05-23 19:40:46 [INFO]: Epoch 138 - training loss: 22745.7314, validation loss: 0.4923
2024-05-23 19:40:46 [INFO]: Epoch 139 - training loss: 22744.9957, validation loss: 0.4911
2024-05-23 19:40:47 [INFO]: Epoch 140 - training loss: 22745.0559, validation loss: 0.4914
2024-05-23 19:40:48 [INFO]: Epoch 141 - training loss: 22745.6756, validation loss: 0.4894
2024-05-23 19:40:48 [INFO]: Epoch 142 - training loss: 22744.9643, validation loss: 0.4882
2024-05-23 19:40:49 [INFO]: Epoch 143 - training loss: 22744.8244, validation loss: 0.4896
2024-05-23 19:40:49 [INFO]: Epoch 144 - training loss: 22745.7714, validation loss: 0.4913
2024-05-23 19:40:50 [INFO]: Epoch 145 - training loss: 22745.0661, validation loss: 0.4883
2024-05-23 19:40:51 [INFO]: Epoch 146 - training loss: 22744.9416, validation loss: 0.4915
2024-05-23 19:40:51 [INFO]: Epoch 147 - training loss: 22745.2224, validation loss: 0.4893
2024-05-23 19:40:52 [INFO]: Epoch 148 - training loss: 22745.0197, validation loss: 0.4872
2024-05-23 19:40:52 [INFO]: Epoch 149 - training loss: 22744.2196, validation loss: 0.4909
2024-05-23 19:40:53 [INFO]: Epoch 150 - training loss: 22744.4298, validation loss: 0.4861
2024-05-23 19:40:54 [INFO]: Epoch 151 - training loss: 22744.6945, validation loss: 0.4834
2024-05-23 19:40:54 [INFO]: Epoch 152 - training loss: 22744.5594, validation loss: 0.4845
2024-05-23 19:40:55 [INFO]: Epoch 153 - training loss: 22744.1712, validation loss: 0.4883
2024-05-23 19:40:56 [INFO]: Epoch 154 - training loss: 22744.1383, validation loss: 0.4843
2024-05-23 19:40:56 [INFO]: Epoch 155 - training loss: 22744.2930, validation loss: 0.4849
2024-05-23 19:40:57 [INFO]: Epoch 156 - training loss: 22743.9221, validation loss: 0.4857
2024-05-23 19:40:57 [INFO]: Epoch 157 - training loss: 22743.8226, validation loss: 0.4867
2024-05-23 19:40:58 [INFO]: Epoch 158 - training loss: 22744.2235, validation loss: 0.4787
2024-05-23 19:40:59 [INFO]: Epoch 159 - training loss: 22743.9648, validation loss: 0.4846
2024-05-23 19:40:59 [INFO]: Epoch 160 - training loss: 22744.3516, validation loss: 0.4780
2024-05-23 19:41:00 [INFO]: Epoch 161 - training loss: 22743.9771, validation loss: 0.4788
2024-05-23 19:41:00 [INFO]: Epoch 162 - training loss: 22743.3859, validation loss: 0.4841
2024-05-23 19:41:01 [INFO]: Epoch 163 - training loss: 22744.6511, validation loss: 0.4851
2024-05-23 19:41:02 [INFO]: Epoch 164 - training loss: 22743.6685, validation loss: 0.4773
2024-05-23 19:41:02 [INFO]: Epoch 165 - training loss: 22743.4494, validation loss: 0.4786
2024-05-23 19:41:03 [INFO]: Epoch 166 - training loss: 22743.9522, validation loss: 0.4756
2024-05-23 19:41:03 [INFO]: Epoch 167 - training loss: 22743.0071, validation loss: 0.4781
2024-05-23 19:41:04 [INFO]: Epoch 168 - training loss: 22742.8940, validation loss: 0.4759
2024-05-23 19:41:05 [INFO]: Epoch 169 - training loss: 22742.7887, validation loss: 0.4790
2024-05-23 19:41:05 [INFO]: Epoch 170 - training loss: 22742.5107, validation loss: 0.4739
2024-05-23 19:41:06 [INFO]: Epoch 171 - training loss: 22742.6229, validation loss: 0.4762
2024-05-23 19:41:06 [INFO]: Epoch 172 - training loss: 22742.5564, validation loss: 0.4749
2024-05-23 19:41:07 [INFO]: Epoch 173 - training loss: 22742.8420, validation loss: 0.4732
2024-05-23 19:41:08 [INFO]: Epoch 174 - training loss: 22743.0094, validation loss: 0.4752
2024-05-23 19:41:08 [INFO]: Epoch 175 - training loss: 22743.0201, validation loss: 0.4726
2024-05-23 19:41:09 [INFO]: Epoch 176 - training loss: 22742.4537, validation loss: 0.4757
2024-05-23 19:41:09 [INFO]: Epoch 177 - training loss: 22743.3594, validation loss: 0.4710
2024-05-23 19:41:10 [INFO]: Epoch 178 - training loss: 22742.8861, validation loss: 0.4786
2024-05-23 19:41:11 [INFO]: Epoch 179 - training loss: 22743.3548, validation loss: 0.4693
2024-05-23 19:41:11 [INFO]: Epoch 180 - training loss: 22742.8087, validation loss: 0.4749
2024-05-23 19:41:12 [INFO]: Epoch 181 - training loss: 22742.2488, validation loss: 0.4696
2024-05-23 19:41:13 [INFO]: Epoch 182 - training loss: 22742.5160, validation loss: 0.4735
2024-05-23 19:41:13 [INFO]: Epoch 183 - training loss: 22741.6102, validation loss: 0.4732
2024-05-23 19:41:14 [INFO]: Epoch 184 - training loss: 22741.4882, validation loss: 0.4692
2024-05-23 19:41:14 [INFO]: Epoch 185 - training loss: 22741.5849, validation loss: 0.4700
2024-05-23 19:41:15 [INFO]: Epoch 186 - training loss: 22741.4662, validation loss: 0.4694
2024-05-23 19:41:16 [INFO]: Epoch 187 - training loss: 22741.4940, validation loss: 0.4662
2024-05-23 19:41:16 [INFO]: Epoch 188 - training loss: 22741.3582, validation loss: 0.4667
2024-05-23 19:41:17 [INFO]: Epoch 189 - training loss: 22741.8644, validation loss: 0.4710
2024-05-23 19:41:17 [INFO]: Epoch 190 - training loss: 22742.3071, validation loss: 0.4688
2024-05-23 19:41:18 [INFO]: Epoch 191 - training loss: 22741.9202, validation loss: 0.4821
2024-05-23 19:41:19 [INFO]: Epoch 192 - training loss: 22742.9627, validation loss: 0.4629
2024-05-23 19:41:19 [INFO]: Epoch 193 - training loss: 22741.3546, validation loss: 0.4690
2024-05-23 19:41:20 [INFO]: Epoch 194 - training loss: 22740.7556, validation loss: 0.4675
2024-05-23 19:41:20 [INFO]: Epoch 195 - training loss: 22741.3775, validation loss: 0.4656
2024-05-23 19:41:21 [INFO]: Epoch 196 - training loss: 22741.3251, validation loss: 0.4625
2024-05-23 19:41:22 [INFO]: Epoch 197 - training loss: 22741.3523, validation loss: 0.4676
2024-05-23 19:41:22 [INFO]: Epoch 198 - training loss: 22741.8043, validation loss: 0.4628
2024-05-23 19:41:23 [INFO]: Epoch 199 - training loss: 22740.9820, validation loss: 0.4638
2024-05-23 19:41:23 [INFO]: Epoch 200 - training loss: 22740.4229, validation loss: 0.4640
2024-05-23 19:41:24 [INFO]: Epoch 201 - training loss: 22740.7585, validation loss: 0.4628
2024-05-23 19:41:25 [INFO]: Epoch 202 - training loss: 22740.5093, validation loss: 0.4644
2024-05-23 19:41:25 [INFO]: Epoch 203 - training loss: 22740.5823, validation loss: 0.4630
2024-05-23 19:41:26 [INFO]: Epoch 204 - training loss: 22740.4908, validation loss: 0.4613
2024-05-23 19:41:27 [INFO]: Epoch 205 - training loss: 22740.1332, validation loss: 0.4613
2024-05-23 19:41:27 [INFO]: Epoch 206 - training loss: 22740.5907, validation loss: 0.4631
2024-05-23 19:41:28 [INFO]: Epoch 207 - training loss: 22740.0055, validation loss: 0.4613
2024-05-23 19:41:28 [INFO]: Epoch 208 - training loss: 22739.9304, validation loss: 0.4623
2024-05-23 19:41:29 [INFO]: Epoch 209 - training loss: 22740.7064, validation loss: 0.4633
2024-05-23 19:41:30 [INFO]: Epoch 210 - training loss: 22740.2667, validation loss: 0.4589
2024-05-23 19:41:30 [INFO]: Epoch 211 - training loss: 22740.1364, validation loss: 0.4602
2024-05-23 19:41:31 [INFO]: Epoch 212 - training loss: 22739.9581, validation loss: 0.4600
2024-05-23 19:41:31 [INFO]: Epoch 213 - training loss: 22739.6934, validation loss: 0.4584
2024-05-23 19:41:32 [INFO]: Epoch 214 - training loss: 22739.7492, validation loss: 0.4611
2024-05-23 19:41:33 [INFO]: Epoch 215 - training loss: 22739.8394, validation loss: 0.4632
2024-05-23 19:41:33 [INFO]: Epoch 216 - training loss: 22739.3960, validation loss: 0.4602
2024-05-23 19:41:34 [INFO]: Epoch 217 - training loss: 22739.5470, validation loss: 0.4562
2024-05-23 19:41:34 [INFO]: Epoch 218 - training loss: 22739.3071, validation loss: 0.4607
2024-05-23 19:41:35 [INFO]: Epoch 219 - training loss: 22739.4574, validation loss: 0.4635
2024-05-23 19:41:36 [INFO]: Epoch 220 - training loss: 22739.5609, validation loss: 0.4549
2024-05-23 19:41:36 [INFO]: Epoch 221 - training loss: 22739.9363, validation loss: 0.4588
2024-05-23 19:41:37 [INFO]: Epoch 222 - training loss: 22739.3540, validation loss: 0.4597
2024-05-23 19:41:37 [INFO]: Epoch 223 - training loss: 22739.1965, validation loss: 0.4586
2024-05-23 19:41:38 [INFO]: Epoch 224 - training loss: 22740.1057, validation loss: 0.4587
2024-05-23 19:41:39 [INFO]: Epoch 225 - training loss: 22739.2857, validation loss: 0.4588
2024-05-23 19:41:39 [INFO]: Epoch 226 - training loss: 22739.0662, validation loss: 0.4567
2024-05-23 19:41:40 [INFO]: Epoch 227 - training loss: 22739.0661, validation loss: 0.4588
2024-05-23 19:41:40 [INFO]: Epoch 228 - training loss: 22738.7935, validation loss: 0.4564
2024-05-23 19:41:41 [INFO]: Epoch 229 - training loss: 22738.6870, validation loss: 0.4571
2024-05-23 19:41:42 [INFO]: Epoch 230 - training loss: 22738.9470, validation loss: 0.4595
2024-05-23 19:41:42 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-05-23 19:41:42 [INFO]: Finished training. The best model is from epoch#220.
2024-05-23 19:41:42 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_0/GPVAE_physionet_2012_seta/20240523_T193922/GPVAE.pypots
2024-05-23 19:41:42 [INFO]: GP-VAE on PhysioNet-2012-seta: MAE=0.3976, MSE=0.4057
2024-05-23 19:41:42 [INFO]: Successfully saved to augmentation_premask_post_norm_saved_results/round_0/GPVAE_physionet_2012_seta/imputation.pkl
2024-05-23 19:41:42 [INFO]: Using the given device: cuda:0
2024-05-23 19:41:42 [INFO]: Model files will be saved to augmentation_premask_post_norm_saved_results/round_0/USGAN_physionet_2012_seta/20240523_T194142
2024-05-23 19:41:42 [INFO]: Tensorboard file will be saved to augmentation_premask_post_norm_saved_results/round_0/USGAN_physionet_2012_seta/20240523_T194142/tensorboard
2024-05-23 19:41:42 [INFO]: USGAN initialized with the given hyperparameters, the number of trainable parameters: 16,010,261
2024-05-23 19:42:04 [INFO]: Epoch 001 - generator training loss: 0.6106, discriminator training loss: 0.3830, validation loss: 0.6500
2024-05-23 19:42:23 [INFO]: Epoch 002 - generator training loss: 0.4870, discriminator training loss: 0.2713, validation loss: 0.5583
2024-05-23 19:42:42 [INFO]: Epoch 003 - generator training loss: 0.4432, discriminator training loss: 0.2354, validation loss: 0.5373
2024-05-23 19:43:01 [INFO]: Epoch 004 - generator training loss: 0.4558, discriminator training loss: 0.1898, validation loss: 0.5347
2024-05-23 19:43:20 [INFO]: Epoch 005 - generator training loss: 0.4513, discriminator training loss: 0.1588, validation loss: 0.5130
2024-05-23 19:43:39 [INFO]: Epoch 006 - generator training loss: 0.4389, discriminator training loss: 0.1396, validation loss: 0.5006
2024-05-23 19:43:58 [INFO]: Epoch 007 - generator training loss: 0.4248, discriminator training loss: 0.1259, validation loss: 0.4893
2024-05-23 19:44:17 [INFO]: Epoch 008 - generator training loss: 0.4139, discriminator training loss: 0.1149, validation loss: 0.4789
2024-05-23 19:44:35 [INFO]: Epoch 009 - generator training loss: 0.4089, discriminator training loss: 0.1067, validation loss: 0.4734
2024-05-23 19:44:54 [INFO]: Epoch 010 - generator training loss: 0.4012, discriminator training loss: 0.0994, validation loss: 0.4665
2024-05-23 19:45:13 [INFO]: Epoch 011 - generator training loss: 0.3977, discriminator training loss: 0.0935, validation loss: 0.4615
2024-05-23 19:45:32 [INFO]: Epoch 012 - generator training loss: 0.3936, discriminator training loss: 0.0885, validation loss: 0.4547
2024-05-23 19:45:51 [INFO]: Epoch 013 - generator training loss: 0.3872, discriminator training loss: 0.0836, validation loss: 0.4486
2024-05-23 19:46:10 [INFO]: Epoch 014 - generator training loss: 0.3813, discriminator training loss: 0.0799, validation loss: 0.4411
2024-05-23 19:46:29 [INFO]: Epoch 015 - generator training loss: 0.3775, discriminator training loss: 0.0763, validation loss: 0.4386
2024-05-23 19:46:48 [INFO]: Epoch 016 - generator training loss: 0.3755, discriminator training loss: 0.0733, validation loss: 0.4342
2024-05-23 19:47:07 [INFO]: Epoch 017 - generator training loss: 0.3694, discriminator training loss: 0.0706, validation loss: 0.4328
2024-05-23 19:47:26 [INFO]: Epoch 018 - generator training loss: 0.3659, discriminator training loss: 0.0681, validation loss: 0.4312
2024-05-23 19:47:44 [INFO]: Epoch 019 - generator training loss: 0.3619, discriminator training loss: 0.0659, validation loss: 0.4247
2024-05-23 19:48:03 [INFO]: Epoch 020 - generator training loss: 0.3595, discriminator training loss: 0.0638, validation loss: 0.4203
2024-05-23 19:48:22 [INFO]: Epoch 021 - generator training loss: 0.3531, discriminator training loss: 0.0619, validation loss: 0.4182
2024-05-23 19:48:41 [INFO]: Epoch 022 - generator training loss: 0.3505, discriminator training loss: 0.0599, validation loss: 0.4164
2024-05-23 19:49:00 [INFO]: Epoch 023 - generator training loss: 0.3482, discriminator training loss: 0.0584, validation loss: 0.4092
2024-05-23 19:49:19 [INFO]: Epoch 024 - generator training loss: 0.3438, discriminator training loss: 0.0570, validation loss: 0.4096
2024-05-23 19:49:38 [INFO]: Epoch 025 - generator training loss: 0.3375, discriminator training loss: 0.0557, validation loss: 0.4059
2024-05-23 19:49:57 [INFO]: Epoch 026 - generator training loss: 0.3344, discriminator training loss: 0.0545, validation loss: 0.4060
2024-05-23 19:50:17 [INFO]: Epoch 027 - generator training loss: 0.3296, discriminator training loss: 0.0535, validation loss: 0.4031
2024-05-23 19:50:36 [INFO]: Epoch 028 - generator training loss: 0.3269, discriminator training loss: 0.0527, validation loss: 0.3977
2024-05-23 19:50:55 [INFO]: Epoch 029 - generator training loss: 0.3243, discriminator training loss: 0.0517, validation loss: 0.3943
2024-05-23 19:51:14 [INFO]: Epoch 030 - generator training loss: 0.3228, discriminator training loss: 0.0512, validation loss: 0.3904
2024-05-23 19:51:33 [INFO]: Epoch 031 - generator training loss: 0.3177, discriminator training loss: 0.0502, validation loss: 0.3892
2024-05-23 19:51:52 [INFO]: Epoch 032 - generator training loss: 0.3121, discriminator training loss: 0.0495, validation loss: 0.3851
2024-05-23 19:52:11 [INFO]: Epoch 033 - generator training loss: 0.3080, discriminator training loss: 0.0489, validation loss: 0.3833
2024-05-23 19:52:30 [INFO]: Epoch 034 - generator training loss: 0.3070, discriminator training loss: 0.0485, validation loss: 0.3801
2024-05-23 19:52:49 [INFO]: Epoch 035 - generator training loss: 0.3098, discriminator training loss: 0.0480, validation loss: 0.3808
2024-05-23 19:53:08 [INFO]: Epoch 036 - generator training loss: 0.3004, discriminator training loss: 0.0475, validation loss: 0.3830
2024-05-23 19:53:27 [INFO]: Epoch 037 - generator training loss: 0.3018, discriminator training loss: 0.0470, validation loss: 0.3805
2024-05-23 19:53:46 [INFO]: Epoch 038 - generator training loss: 0.2978, discriminator training loss: 0.0466, validation loss: 0.3707
2024-05-23 19:54:05 [INFO]: Epoch 039 - generator training loss: 0.2947, discriminator training loss: 0.0461, validation loss: 0.3770
2024-05-23 19:54:24 [INFO]: Epoch 040 - generator training loss: 0.2863, discriminator training loss: 0.0458, validation loss: 0.3699
2024-05-23 19:54:43 [INFO]: Epoch 041 - generator training loss: 0.2796, discriminator training loss: 0.0452, validation loss: 0.3652
2024-05-23 19:55:02 [INFO]: Epoch 042 - generator training loss: 0.2778, discriminator training loss: 0.0453, validation loss: 0.3629
2024-05-23 19:55:21 [INFO]: Epoch 043 - generator training loss: 0.2767, discriminator training loss: 0.0448, validation loss: 0.3630
2024-05-23 19:55:40 [INFO]: Epoch 044 - generator training loss: 0.2741, discriminator training loss: 0.0445, validation loss: 0.3603
2024-05-23 19:55:59 [INFO]: Epoch 045 - generator training loss: 0.2705, discriminator training loss: 0.0441, validation loss: 0.3630
2024-05-23 19:56:18 [INFO]: Epoch 046 - generator training loss: 0.2687, discriminator training loss: 0.0440, validation loss: 0.3630
2024-05-23 19:56:37 [INFO]: Epoch 047 - generator training loss: 0.2646, discriminator training loss: 0.0438, validation loss: 0.3559
2024-05-23 19:56:56 [INFO]: Epoch 048 - generator training loss: 0.2615, discriminator training loss: 0.0438, validation loss: 0.3524
2024-05-23 19:57:15 [INFO]: Epoch 049 - generator training loss: 0.2572, discriminator training loss: 0.0432, validation loss: 0.3496
2024-05-23 19:57:34 [INFO]: Epoch 050 - generator training loss: 0.2559, discriminator training loss: 0.0433, validation loss: 0.3511
2024-05-23 19:57:53 [INFO]: Epoch 051 - generator training loss: 0.2600, discriminator training loss: 0.0430, validation loss: 0.3539
2024-05-23 19:58:12 [INFO]: Epoch 052 - generator training loss: 0.2501, discriminator training loss: 0.0430, validation loss: 0.3430
2024-05-23 19:58:31 [INFO]: Epoch 053 - generator training loss: 0.2449, discriminator training loss: 0.0427, validation loss: 0.3433
2024-05-23 19:58:50 [INFO]: Epoch 054 - generator training loss: 0.2428, discriminator training loss: 0.0426, validation loss: 0.3401
2024-05-23 19:59:09 [INFO]: Epoch 055 - generator training loss: 0.2382, discriminator training loss: 0.0425, validation loss: 0.3434
2024-05-23 19:59:28 [INFO]: Epoch 056 - generator training loss: 0.2350, discriminator training loss: 0.0425, validation loss: 0.3467
2024-05-23 19:59:47 [INFO]: Epoch 057 - generator training loss: 0.2417, discriminator training loss: 0.0422, validation loss: 0.3510
2024-05-23 20:00:06 [INFO]: Epoch 058 - generator training loss: 0.2463, discriminator training loss: 0.0422, validation loss: 0.3496
2024-05-23 20:00:24 [INFO]: Epoch 059 - generator training loss: 0.2525, discriminator training loss: 0.0421, validation loss: 0.3405
2024-05-23 20:00:43 [INFO]: Epoch 060 - generator training loss: 0.2352, discriminator training loss: 0.0420, validation loss: 0.3361
2024-05-23 20:01:02 [INFO]: Epoch 061 - generator training loss: 0.2294, discriminator training loss: 0.0417, validation loss: 0.3368
2024-05-23 20:01:21 [INFO]: Epoch 062 - generator training loss: 0.2248, discriminator training loss: 0.0416, validation loss: 0.3349
2024-05-23 20:01:40 [INFO]: Epoch 063 - generator training loss: 0.2211, discriminator training loss: 0.0416, validation loss: 0.3365
2024-05-23 20:01:59 [INFO]: Epoch 064 - generator training loss: 0.2207, discriminator training loss: 0.0417, validation loss: 0.3364
2024-05-23 20:02:17 [INFO]: Epoch 065 - generator training loss: 0.2235, discriminator training loss: 0.0415, validation loss: 0.3383
2024-05-23 20:02:36 [INFO]: Epoch 066 - generator training loss: 0.2216, discriminator training loss: 0.0415, validation loss: 0.3337
2024-05-23 20:02:55 [INFO]: Epoch 067 - generator training loss: 0.2194, discriminator training loss: 0.0414, validation loss: 0.3396
2024-05-23 20:03:14 [INFO]: Epoch 068 - generator training loss: 0.2179, discriminator training loss: 0.0411, validation loss: 0.3351
2024-05-23 20:03:33 [INFO]: Epoch 069 - generator training loss: 0.2166, discriminator training loss: 0.0412, validation loss: 0.3425
2024-05-23 20:03:52 [INFO]: Epoch 070 - generator training loss: 0.2156, discriminator training loss: 0.0412, validation loss: 0.3365
2024-05-23 20:04:10 [INFO]: Epoch 071 - generator training loss: 0.2160, discriminator training loss: 0.0411, validation loss: 0.3413
2024-05-23 20:04:29 [INFO]: Epoch 072 - generator training loss: 0.2162, discriminator training loss: 0.0410, validation loss: 0.3412
2024-05-23 20:04:48 [INFO]: Epoch 073 - generator training loss: 0.2098, discriminator training loss: 0.0411, validation loss: 0.3385
2024-05-23 20:05:07 [INFO]: Epoch 074 - generator training loss: 0.2067, discriminator training loss: 0.0408, validation loss: 0.3405
2024-05-23 20:05:26 [INFO]: Epoch 075 - generator training loss: 0.2041, discriminator training loss: 0.0409, validation loss: 0.3353
2024-05-23 20:05:45 [INFO]: Epoch 076 - generator training loss: 0.2023, discriminator training loss: 0.0407, validation loss: 0.3338
2024-05-23 20:05:45 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-05-23 20:05:45 [INFO]: Finished training. The best model is from epoch#66.
2024-05-23 20:05:45 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_0/USGAN_physionet_2012_seta/20240523_T194142/USGAN.pypots
2024-05-23 20:05:47 [INFO]: US-GAN on PhysioNet-2012-seta: MAE=0.2938, MSE=0.2609
2024-05-23 20:05:57 [INFO]: Successfully saved to augmentation_premask_post_norm_saved_results/round_0/USGAN_physionet_2012_seta/imputation.pkl
2024-05-23 20:05:57 [INFO]: Using the given device: cuda:0
2024-05-23 20:05:57 [INFO]: Model files will be saved to augmentation_premask_post_norm_saved_results/round_0/BRITS_physionet_2012_seta/20240523_T200557
2024-05-23 20:05:57 [INFO]: Tensorboard file will be saved to augmentation_premask_post_norm_saved_results/round_0/BRITS_physionet_2012_seta/20240523_T200557/tensorboard
2024-05-23 20:05:57 [INFO]: BRITS initialized with the given hyperparameters, the number of trainable parameters: 9,176,048
2024-05-23 20:06:13 [INFO]: Epoch 001 - training loss: 1.1371, validation loss: 0.5556
2024-05-23 20:06:25 [INFO]: Epoch 002 - training loss: 0.9278, validation loss: 0.4994
2024-05-23 20:06:38 [INFO]: Epoch 003 - training loss: 0.8692, validation loss: 0.4712
2024-05-23 20:06:50 [INFO]: Epoch 004 - training loss: 0.8320, validation loss: 0.4484
2024-05-23 20:07:02 [INFO]: Epoch 005 - training loss: 0.8043, validation loss: 0.4301
2024-05-23 20:07:15 [INFO]: Epoch 006 - training loss: 0.7822, validation loss: 0.4182
2024-05-23 20:07:27 [INFO]: Epoch 007 - training loss: 0.7620, validation loss: 0.4090
2024-05-23 20:07:39 [INFO]: Epoch 008 - training loss: 0.7455, validation loss: 0.3989
2024-05-23 20:07:52 [INFO]: Epoch 009 - training loss: 0.7311, validation loss: 0.3913
2024-05-23 20:08:04 [INFO]: Epoch 010 - training loss: 0.7202, validation loss: 0.3903
2024-05-23 20:08:16 [INFO]: Epoch 011 - training loss: 0.7098, validation loss: 0.3870
2024-05-23 20:08:29 [INFO]: Epoch 012 - training loss: 0.7014, validation loss: 0.3832
2024-05-23 20:08:41 [INFO]: Epoch 013 - training loss: 0.6934, validation loss: 0.3836
2024-05-23 20:08:53 [INFO]: Epoch 014 - training loss: 0.6863, validation loss: 0.3807
2024-05-23 20:09:06 [INFO]: Epoch 015 - training loss: 0.6815, validation loss: 0.3789
2024-05-23 20:09:18 [INFO]: Epoch 016 - training loss: 0.6754, validation loss: 0.3818
2024-05-23 20:09:30 [INFO]: Epoch 017 - training loss: 0.6709, validation loss: 0.3792
2024-05-23 20:09:43 [INFO]: Epoch 018 - training loss: 0.6659, validation loss: 0.3783
2024-05-23 20:09:55 [INFO]: Epoch 019 - training loss: 0.6623, validation loss: 0.3763
2024-05-23 20:10:08 [INFO]: Epoch 020 - training loss: 0.6592, validation loss: 0.3776
2024-05-23 20:10:20 [INFO]: Epoch 021 - training loss: 0.6546, validation loss: 0.3769
2024-05-23 20:10:32 [INFO]: Epoch 022 - training loss: 0.6519, validation loss: 0.3764
2024-05-23 20:10:45 [INFO]: Epoch 023 - training loss: 0.6488, validation loss: 0.3767
2024-05-23 20:10:57 [INFO]: Epoch 024 - training loss: 0.6452, validation loss: 0.3764
2024-05-23 20:11:10 [INFO]: Epoch 025 - training loss: 0.6433, validation loss: 0.3757
2024-05-23 20:11:22 [INFO]: Epoch 026 - training loss: 0.6407, validation loss: 0.3752
2024-05-23 20:11:34 [INFO]: Epoch 027 - training loss: 0.6377, validation loss: 0.3766
2024-05-23 20:11:47 [INFO]: Epoch 028 - training loss: 0.6351, validation loss: 0.3759
2024-05-23 20:11:59 [INFO]: Epoch 029 - training loss: 0.6317, validation loss: 0.3756
2024-05-23 20:12:12 [INFO]: Epoch 030 - training loss: 0.6293, validation loss: 0.3746
2024-05-23 20:12:24 [INFO]: Epoch 031 - training loss: 0.6272, validation loss: 0.3753
2024-05-23 20:12:37 [INFO]: Epoch 032 - training loss: 0.6251, validation loss: 0.3727
2024-05-23 20:12:49 [INFO]: Epoch 033 - training loss: 0.6218, validation loss: 0.3734
2024-05-23 20:13:02 [INFO]: Epoch 034 - training loss: 0.6180, validation loss: 0.3724
2024-05-23 20:13:14 [INFO]: Epoch 035 - training loss: 0.6151, validation loss: 0.3725
2024-05-23 20:13:27 [INFO]: Epoch 036 - training loss: 0.6125, validation loss: 0.3724
2024-05-23 20:13:39 [INFO]: Epoch 037 - training loss: 0.6096, validation loss: 0.3724
2024-05-23 20:13:52 [INFO]: Epoch 038 - training loss: 0.6069, validation loss: 0.3735
2024-05-23 20:14:04 [INFO]: Epoch 039 - training loss: 0.6048, validation loss: 0.3719
2024-05-23 20:14:17 [INFO]: Epoch 040 - training loss: 0.6022, validation loss: 0.3772
2024-05-23 20:14:29 [INFO]: Epoch 041 - training loss: 0.6062, validation loss: 0.3743
2024-05-23 20:14:41 [INFO]: Epoch 042 - training loss: 0.6024, validation loss: 0.3754
2024-05-23 20:14:54 [INFO]: Epoch 043 - training loss: 0.5963, validation loss: 0.3738
2024-05-23 20:15:06 [INFO]: Epoch 044 - training loss: 0.5937, validation loss: 0.3754
2024-05-23 20:15:19 [INFO]: Epoch 045 - training loss: 0.5902, validation loss: 0.3759
2024-05-23 20:15:31 [INFO]: Epoch 046 - training loss: 0.5894, validation loss: 0.3765
2024-05-23 20:15:43 [INFO]: Epoch 047 - training loss: 0.5854, validation loss: 0.3763
2024-05-23 20:15:56 [INFO]: Epoch 048 - training loss: 0.5802, validation loss: 0.3767
2024-05-23 20:16:08 [INFO]: Epoch 049 - training loss: 0.5778, validation loss: 0.3772
2024-05-23 20:16:08 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-05-23 20:16:08 [INFO]: Finished training. The best model is from epoch#39.
2024-05-23 20:16:08 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_0/BRITS_physionet_2012_seta/20240523_T200557/BRITS.pypots
2024-05-23 20:16:11 [INFO]: BRITS on PhysioNet-2012-seta: MAE=0.2574, MSE=0.2608
2024-05-23 20:16:21 [INFO]: Successfully saved to augmentation_premask_post_norm_saved_results/round_0/BRITS_physionet_2012_seta/imputation.pkl
2024-05-23 20:16:21 [INFO]: Using the given device: cuda:0
2024-05-23 20:16:21 [INFO]: Model files will be saved to augmentation_premask_post_norm_saved_results/round_0/MRNN_physionet_2012_seta/20240523_T201621
2024-05-23 20:16:21 [INFO]: Tensorboard file will be saved to augmentation_premask_post_norm_saved_results/round_0/MRNN_physionet_2012_seta/20240523_T201621/tensorboard
2024-05-23 20:16:21 [INFO]: MRNN initialized with the given hyperparameters, the number of trainable parameters: 7,599
2024-05-23 20:16:26 [INFO]: Epoch 001 - training loss: 1.1970, validation loss: 0.9985
2024-05-23 20:16:26 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_0/MRNN_physionet_2012_seta/20240523_T201621/MRNN_epoch1_loss0.9984581947326661.pypots
2024-05-23 20:16:29 [INFO]: Epoch 002 - training loss: 0.7118, validation loss: 0.9780
2024-05-23 20:16:29 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_0/MRNN_physionet_2012_seta/20240523_T201621/MRNN_epoch2_loss0.9780262023210525.pypots
2024-05-23 20:16:32 [INFO]: Epoch 003 - training loss: 0.6006, validation loss: 0.9522
2024-05-23 20:16:32 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_0/MRNN_physionet_2012_seta/20240523_T201621/MRNN_epoch3_loss0.9522356063127517.pypots
2024-05-23 20:16:35 [INFO]: Epoch 004 - training loss: 0.5541, validation loss: 0.9398
2024-05-23 20:16:35 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_0/MRNN_physionet_2012_seta/20240523_T201621/MRNN_epoch4_loss0.9397684097290039.pypots
2024-05-23 20:16:38 [INFO]: Epoch 005 - training loss: 0.5266, validation loss: 0.9312
2024-05-23 20:16:38 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_0/MRNN_physionet_2012_seta/20240523_T201621/MRNN_epoch5_loss0.9311633974313736.pypots
2024-05-23 20:16:40 [INFO]: Epoch 006 - training loss: 0.5108, validation loss: 0.9267
2024-05-23 20:16:40 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_0/MRNN_physionet_2012_seta/20240523_T201621/MRNN_epoch6_loss0.926659905910492.pypots
2024-05-23 20:16:43 [INFO]: Epoch 007 - training loss: 0.5022, validation loss: 0.9230
2024-05-23 20:16:43 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_0/MRNN_physionet_2012_seta/20240523_T201621/MRNN_epoch7_loss0.9230197638273239.pypots
2024-05-23 20:16:46 [INFO]: Epoch 008 - training loss: 0.4902, validation loss: 0.9209
2024-05-23 20:16:46 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_0/MRNN_physionet_2012_seta/20240523_T201621/MRNN_epoch8_loss0.9209349036216736.pypots
2024-05-23 20:16:49 [INFO]: Epoch 009 - training loss: 0.4752, validation loss: 0.9199
2024-05-23 20:16:49 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_0/MRNN_physionet_2012_seta/20240523_T201621/MRNN_epoch9_loss0.9199366211891175.pypots
2024-05-23 20:16:52 [INFO]: Epoch 010 - training loss: 0.4726, validation loss: 0.9183
2024-05-23 20:16:52 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_0/MRNN_physionet_2012_seta/20240523_T201621/MRNN_epoch10_loss0.9183095782995224.pypots
2024-05-23 20:16:55 [INFO]: Epoch 011 - training loss: 0.4699, validation loss: 0.9205
2024-05-23 20:16:55 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_0/MRNN_physionet_2012_seta/20240523_T201621/MRNN_epoch11_loss0.9205287903547287.pypots
2024-05-23 20:16:57 [INFO]: Epoch 012 - training loss: 0.4602, validation loss: 0.9200
2024-05-23 20:16:57 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_0/MRNN_physionet_2012_seta/20240523_T201621/MRNN_epoch12_loss0.9199595928192139.pypots
2024-05-23 20:17:00 [INFO]: Epoch 013 - training loss: 0.4584, validation loss: 0.9229
2024-05-23 20:17:00 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_0/MRNN_physionet_2012_seta/20240523_T201621/MRNN_epoch13_loss0.9229348510503769.pypots
2024-05-23 20:17:03 [INFO]: Epoch 014 - training loss: 0.4520, validation loss: 0.9261
2024-05-23 20:17:03 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_0/MRNN_physionet_2012_seta/20240523_T201621/MRNN_epoch14_loss0.9261326044797897.pypots
2024-05-23 20:17:06 [INFO]: Epoch 015 - training loss: 0.4481, validation loss: 0.9264
2024-05-23 20:17:06 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_0/MRNN_physionet_2012_seta/20240523_T201621/MRNN_epoch15_loss0.9263600170612335.pypots
2024-05-23 20:17:09 [INFO]: Epoch 016 - training loss: 0.4355, validation loss: 0.9276
2024-05-23 20:17:09 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_0/MRNN_physionet_2012_seta/20240523_T201621/MRNN_epoch16_loss0.9276053816080093.pypots
2024-05-23 20:17:12 [INFO]: Epoch 017 - training loss: 0.4383, validation loss: 0.9301
2024-05-23 20:17:12 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_0/MRNN_physionet_2012_seta/20240523_T201621/MRNN_epoch17_loss0.9300795763731002.pypots
2024-05-23 20:17:14 [INFO]: Epoch 018 - training loss: 0.4434, validation loss: 0.9332
2024-05-23 20:17:14 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_0/MRNN_physionet_2012_seta/20240523_T201621/MRNN_epoch18_loss0.9331515401601791.pypots
2024-05-23 20:17:17 [INFO]: Epoch 019 - training loss: 0.4360, validation loss: 0.9341
2024-05-23 20:17:17 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_0/MRNN_physionet_2012_seta/20240523_T201621/MRNN_epoch19_loss0.9341214537620545.pypots
2024-05-23 20:17:20 [INFO]: Epoch 020 - training loss: 0.4328, validation loss: 0.9353
2024-05-23 20:17:20 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_0/MRNN_physionet_2012_seta/20240523_T201621/MRNN_epoch20_loss0.9353066384792328.pypots
2024-05-23 20:17:20 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-05-23 20:17:20 [INFO]: Finished training. The best model is from epoch#10.
2024-05-23 20:17:20 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_0/MRNN_physionet_2012_seta/20240523_T201621/MRNN.pypots
2024-05-23 20:17:21 [INFO]: MRNN on PhysioNet-2012-seta: MAE=0.6900, MSE=0.9019
2024-05-23 20:17:25 [INFO]: Successfully saved to augmentation_premask_post_norm_saved_results/round_0/MRNN_physionet_2012_seta/imputation.pkl
2024-05-23 20:17:25 [INFO]: Using the given device: cpu
2024-05-23 20:17:25 [INFO]: LOCF on PhysioNet-2012-seta: MAE=0.4038, MSE=0.5072
2024-05-23 20:17:25 [INFO]: Successfully created the given path "saved_results/round_0/LOCF_physionet_2012_seta".
2024-05-23 20:17:25 [INFO]: Successfully saved to augmentation_premask_post_norm_saved_results/round_0/LOCF_physionet_2012_seta/imputation.pkl
2024-05-23 20:17:26 [INFO]: Median on PhysioNet-2012-seta: MAE=0.6908, MSE=1.0216
2024-05-23 20:17:26 [INFO]: Successfully created the given path "saved_results/round_0/Median_physionet_2012_seta".
2024-05-23 20:17:26 [INFO]: Successfully saved to augmentation_premask_post_norm_saved_results/round_0/Median_physionet_2012_seta/imputation.pkl
2024-05-23 20:17:26 [INFO]: Mean on PhysioNet-2012-seta: MAE=0.7063, MSE=0.9786
2024-05-23 20:17:26 [INFO]: Successfully created the given path "saved_results/round_0/Mean_physionet_2012_seta".
2024-05-23 20:17:26 [INFO]: Successfully saved to augmentation_premask_post_norm_saved_results/round_0/Mean_physionet_2012_seta/imputation.pkl
2024-05-23 20:17:26 [INFO]: Have set the random seed as 2024 for numpy and pytorch.
2024-05-23 20:17:26 [INFO]: Using the given device: cuda:0
2024-05-23 20:17:26 [INFO]: Model files will be saved to augmentation_premask_post_norm_saved_results/round_1/SAITS_physionet_2012_seta/20240523_T201726
2024-05-23 20:17:26 [INFO]: Tensorboard file will be saved to augmentation_premask_post_norm_saved_results/round_1/SAITS_physionet_2012_seta/20240523_T201726/tensorboard
2024-05-23 20:17:26 [INFO]: SAITS initialized with the given hyperparameters, the number of trainable parameters: 5,322,806
2024-05-23 20:17:27 [INFO]: Epoch 001 - training loss: 1.0767, validation loss: 0.5103
2024-05-23 20:17:28 [INFO]: Epoch 002 - training loss: 0.7042, validation loss: 0.4493
2024-05-23 20:17:29 [INFO]: Epoch 003 - training loss: 0.5796, validation loss: 0.4171
2024-05-23 20:17:31 [INFO]: Epoch 004 - training loss: 0.5162, validation loss: 0.4028
2024-05-23 20:17:32 [INFO]: Epoch 005 - training loss: 0.4755, validation loss: 0.3917
2024-05-23 20:17:33 [INFO]: Epoch 006 - training loss: 0.4403, validation loss: 0.3858
2024-05-23 20:17:34 [INFO]: Epoch 007 - training loss: 0.4213, validation loss: 0.3923
2024-05-23 20:17:35 [INFO]: Epoch 008 - training loss: 0.3961, validation loss: 0.3709
2024-05-23 20:17:36 [INFO]: Epoch 009 - training loss: 0.3755, validation loss: 0.3676
2024-05-23 20:17:38 [INFO]: Epoch 010 - training loss: 0.3589, validation loss: 0.3565
2024-05-23 20:17:39 [INFO]: Epoch 011 - training loss: 0.3461, validation loss: 0.3463
2024-05-23 20:17:40 [INFO]: Epoch 012 - training loss: 0.3230, validation loss: 0.3474
2024-05-23 20:17:41 [INFO]: Epoch 013 - training loss: 0.3081, validation loss: 0.3456
2024-05-23 20:17:42 [INFO]: Epoch 014 - training loss: 0.3036, validation loss: 0.3418
2024-05-23 20:17:44 [INFO]: Epoch 015 - training loss: 0.2896, validation loss: 0.3451
2024-05-23 20:17:45 [INFO]: Epoch 016 - training loss: 0.2791, validation loss: 0.3433
2024-05-23 20:17:46 [INFO]: Epoch 017 - training loss: 0.2689, validation loss: 0.3407
2024-05-23 20:17:47 [INFO]: Epoch 018 - training loss: 0.2603, validation loss: 0.3411
2024-05-23 20:17:48 [INFO]: Epoch 019 - training loss: 0.2542, validation loss: 0.3445
2024-05-23 20:17:49 [INFO]: Epoch 020 - training loss: 0.2457, validation loss: 0.3362
2024-05-23 20:17:51 [INFO]: Epoch 021 - training loss: 0.2392, validation loss: 0.3390
2024-05-23 20:17:52 [INFO]: Epoch 022 - training loss: 0.2291, validation loss: 0.3399
2024-05-23 20:17:53 [INFO]: Epoch 023 - training loss: 0.2292, validation loss: 0.3393
2024-05-23 20:17:54 [INFO]: Epoch 024 - training loss: 0.2214, validation loss: 0.3445
2024-05-23 20:17:55 [INFO]: Epoch 025 - training loss: 0.2170, validation loss: 0.3413
2024-05-23 20:17:56 [INFO]: Epoch 026 - training loss: 0.2116, validation loss: 0.3365
2024-05-23 20:17:58 [INFO]: Epoch 027 - training loss: 0.2053, validation loss: 0.3333
2024-05-23 20:17:59 [INFO]: Epoch 028 - training loss: 0.1974, validation loss: 0.3369
2024-05-23 20:18:00 [INFO]: Epoch 029 - training loss: 0.1992, validation loss: 0.3365
2024-05-23 20:18:01 [INFO]: Epoch 030 - training loss: 0.1940, validation loss: 0.3355
2024-05-23 20:18:02 [INFO]: Epoch 031 - training loss: 0.1876, validation loss: 0.3354
2024-05-23 20:18:03 [INFO]: Epoch 032 - training loss: 0.1846, validation loss: 0.3331
2024-05-23 20:18:05 [INFO]: Epoch 033 - training loss: 0.1826, validation loss: 0.3380
2024-05-23 20:18:06 [INFO]: Epoch 034 - training loss: 0.1800, validation loss: 0.3332
2024-05-23 20:18:07 [INFO]: Epoch 035 - training loss: 0.1752, validation loss: 0.3359
2024-05-23 20:18:08 [INFO]: Epoch 036 - training loss: 0.1708, validation loss: 0.3292
2024-05-23 20:18:09 [INFO]: Epoch 037 - training loss: 0.1709, validation loss: 0.3330
2024-05-23 20:18:11 [INFO]: Epoch 038 - training loss: 0.1672, validation loss: 0.3371
2024-05-23 20:18:12 [INFO]: Epoch 039 - training loss: 0.1622, validation loss: 0.3318
2024-05-23 20:18:13 [INFO]: Epoch 040 - training loss: 0.1643, validation loss: 0.3342
2024-05-23 20:18:14 [INFO]: Epoch 041 - training loss: 0.1604, validation loss: 0.3352
2024-05-23 20:18:15 [INFO]: Epoch 042 - training loss: 0.1601, validation loss: 0.3342
2024-05-23 20:18:16 [INFO]: Epoch 043 - training loss: 0.1573, validation loss: 0.3251
2024-05-23 20:18:18 [INFO]: Epoch 044 - training loss: 0.1530, validation loss: 0.3324
2024-05-23 20:18:19 [INFO]: Epoch 045 - training loss: 0.1523, validation loss: 0.3297
2024-05-23 20:18:20 [INFO]: Epoch 046 - training loss: 0.1502, validation loss: 0.3362
2024-05-23 20:18:21 [INFO]: Epoch 047 - training loss: 0.1520, validation loss: 0.3334
2024-05-23 20:18:22 [INFO]: Epoch 048 - training loss: 0.1481, validation loss: 0.3315
2024-05-23 20:18:23 [INFO]: Epoch 049 - training loss: 0.1449, validation loss: 0.3376
2024-05-23 20:18:25 [INFO]: Epoch 050 - training loss: 0.1441, validation loss: 0.3329
2024-05-23 20:18:26 [INFO]: Epoch 051 - training loss: 0.1438, validation loss: 0.3336
2024-05-23 20:18:27 [INFO]: Epoch 052 - training loss: 0.1414, validation loss: 0.3385
2024-05-23 20:18:28 [INFO]: Epoch 053 - training loss: 0.1418, validation loss: 0.3340
2024-05-23 20:18:28 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-05-23 20:18:28 [INFO]: Finished training. The best model is from epoch#43.
2024-05-23 20:18:28 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_1/SAITS_physionet_2012_seta/20240523_T201726/SAITS.pypots
2024-05-23 20:18:28 [INFO]: SAITS on PhysioNet-2012-seta: MAE=0.2619, MSE=0.2886
2024-05-23 20:18:29 [INFO]: Successfully saved to augmentation_premask_post_norm_saved_results/round_1/SAITS_physionet_2012_seta/imputation.pkl
2024-05-23 20:18:29 [INFO]: Using the given device: cuda:0
2024-05-23 20:18:29 [INFO]: Model files will be saved to augmentation_premask_post_norm_saved_results/round_1/Transformer_physionet_2012_seta/20240523_T201829
2024-05-23 20:18:29 [INFO]: Tensorboard file will be saved to augmentation_premask_post_norm_saved_results/round_1/Transformer_physionet_2012_seta/20240523_T201829/tensorboard
2024-05-23 20:18:29 [INFO]: Transformer initialized with the given hyperparameters, the number of trainable parameters: 2,152,613
2024-05-23 20:18:29 [INFO]: Epoch 001 - training loss: 1.0854, validation loss: 0.5575
2024-05-23 20:18:30 [INFO]: Epoch 002 - training loss: 0.7129, validation loss: 0.5024
2024-05-23 20:18:31 [INFO]: Epoch 003 - training loss: 0.6303, validation loss: 0.4827
2024-05-23 20:18:31 [INFO]: Epoch 004 - training loss: 0.5823, validation loss: 0.4563
2024-05-23 20:18:32 [INFO]: Epoch 005 - training loss: 0.5344, validation loss: 0.4328
2024-05-23 20:18:32 [INFO]: Epoch 006 - training loss: 0.5071, validation loss: 0.4291
2024-05-23 20:18:33 [INFO]: Epoch 007 - training loss: 0.4756, validation loss: 0.4160
2024-05-23 20:18:34 [INFO]: Epoch 008 - training loss: 0.4592, validation loss: 0.4080
2024-05-23 20:18:34 [INFO]: Epoch 009 - training loss: 0.4372, validation loss: 0.3987
2024-05-23 20:18:35 [INFO]: Epoch 010 - training loss: 0.4240, validation loss: 0.3980
2024-05-23 20:18:35 [INFO]: Epoch 011 - training loss: 0.4082, validation loss: 0.3841
2024-05-23 20:18:36 [INFO]: Epoch 012 - training loss: 0.3919, validation loss: 0.3828
2024-05-23 20:18:37 [INFO]: Epoch 013 - training loss: 0.3787, validation loss: 0.3863
2024-05-23 20:18:37 [INFO]: Epoch 014 - training loss: 0.3695, validation loss: 0.3760
2024-05-23 20:18:38 [INFO]: Epoch 015 - training loss: 0.3615, validation loss: 0.3713
2024-05-23 20:18:39 [INFO]: Epoch 016 - training loss: 0.3502, validation loss: 0.3686
2024-05-23 20:18:39 [INFO]: Epoch 017 - training loss: 0.3488, validation loss: 0.3731
2024-05-23 20:18:40 [INFO]: Epoch 018 - training loss: 0.3298, validation loss: 0.3704
2024-05-23 20:18:40 [INFO]: Epoch 019 - training loss: 0.3252, validation loss: 0.3601
2024-05-23 20:18:41 [INFO]: Epoch 020 - training loss: 0.3188, validation loss: 0.3633
2024-05-23 20:18:42 [INFO]: Epoch 021 - training loss: 0.3083, validation loss: 0.3617
2024-05-23 20:18:42 [INFO]: Epoch 022 - training loss: 0.3093, validation loss: 0.3560
2024-05-23 20:18:43 [INFO]: Epoch 023 - training loss: 0.2992, validation loss: 0.3616
2024-05-23 20:18:43 [INFO]: Epoch 024 - training loss: 0.2921, validation loss: 0.3561
2024-05-23 20:18:44 [INFO]: Epoch 025 - training loss: 0.2889, validation loss: 0.3610
2024-05-23 20:18:45 [INFO]: Epoch 026 - training loss: 0.2880, validation loss: 0.3580
2024-05-23 20:18:45 [INFO]: Epoch 027 - training loss: 0.2796, validation loss: 0.3553
2024-05-23 20:18:46 [INFO]: Epoch 028 - training loss: 0.2788, validation loss: 0.3539
2024-05-23 20:18:46 [INFO]: Epoch 029 - training loss: 0.2703, validation loss: 0.3564
2024-05-23 20:18:47 [INFO]: Epoch 030 - training loss: 0.2638, validation loss: 0.3537
2024-05-23 20:18:48 [INFO]: Epoch 031 - training loss: 0.2623, validation loss: 0.3524
2024-05-23 20:18:48 [INFO]: Epoch 032 - training loss: 0.2559, validation loss: 0.3585
2024-05-23 20:18:49 [INFO]: Epoch 033 - training loss: 0.2541, validation loss: 0.3565
2024-05-23 20:18:50 [INFO]: Epoch 034 - training loss: 0.2521, validation loss: 0.3531
2024-05-23 20:18:50 [INFO]: Epoch 035 - training loss: 0.2437, validation loss: 0.3545
2024-05-23 20:18:51 [INFO]: Epoch 036 - training loss: 0.2410, validation loss: 0.3559
2024-05-23 20:18:51 [INFO]: Epoch 037 - training loss: 0.2388, validation loss: 0.3568
2024-05-23 20:18:52 [INFO]: Epoch 038 - training loss: 0.2376, validation loss: 0.3537
2024-05-23 20:18:53 [INFO]: Epoch 039 - training loss: 0.2313, validation loss: 0.3544
2024-05-23 20:18:54 [INFO]: Epoch 040 - training loss: 0.2306, validation loss: 0.3528
2024-05-23 20:18:54 [INFO]: Epoch 041 - training loss: 0.2289, validation loss: 0.3594
2024-05-23 20:18:54 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-05-23 20:18:54 [INFO]: Finished training. The best model is from epoch#31.
2024-05-23 20:18:54 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_1/Transformer_physionet_2012_seta/20240523_T201829/Transformer.pypots
2024-05-23 20:18:54 [INFO]: Transformer on PhysioNet-2012-seta: MAE=0.2857, MSE=0.3000
2024-05-23 20:18:54 [INFO]: Successfully saved to augmentation_premask_post_norm_saved_results/round_1/Transformer_physionet_2012_seta/imputation.pkl
2024-05-23 20:18:54 [INFO]: Using the given device: cuda:0
2024-05-23 20:18:54 [INFO]: Model files will be saved to augmentation_premask_post_norm_saved_results/round_1/TimesNet_physionet_2012_seta/20240523_T201854
2024-05-23 20:18:54 [INFO]: Tensorboard file will be saved to augmentation_premask_post_norm_saved_results/round_1/TimesNet_physionet_2012_seta/20240523_T201854/tensorboard
2024-05-23 20:18:55 [INFO]: TimesNet initialized with the given hyperparameters, the number of trainable parameters: 21,649,573
2024-05-23 20:18:55 [INFO]: Epoch 001 - training loss: 0.4579, validation loss: 0.4099
2024-05-23 20:18:56 [INFO]: Epoch 002 - training loss: 0.3786, validation loss: 1.1797
2024-05-23 20:18:57 [INFO]: Epoch 003 - training loss: 0.5308, validation loss: 0.8233
2024-05-23 20:18:58 [INFO]: Epoch 004 - training loss: 0.4283, validation loss: 1.1617
2024-05-23 20:18:58 [INFO]: Epoch 005 - training loss: 0.5042, validation loss: 0.4619
2024-05-23 20:18:59 [INFO]: Epoch 006 - training loss: 0.3416, validation loss: 0.3389
2024-05-23 20:19:00 [INFO]: Epoch 007 - training loss: 0.3085, validation loss: 0.3439
2024-05-23 20:19:00 [INFO]: Epoch 008 - training loss: 0.3003, validation loss: 0.3286
2024-05-23 20:19:01 [INFO]: Epoch 009 - training loss: 0.2927, validation loss: 0.3215
2024-05-23 20:19:02 [INFO]: Epoch 010 - training loss: 0.2857, validation loss: 0.3249
2024-05-23 20:19:03 [INFO]: Epoch 011 - training loss: 0.2835, validation loss: 0.3241
2024-05-23 20:19:03 [INFO]: Epoch 012 - training loss: 0.2752, validation loss: 0.3198
2024-05-23 20:19:04 [INFO]: Epoch 013 - training loss: 0.2829, validation loss: 0.3284
2024-05-23 20:19:05 [INFO]: Epoch 014 - training loss: 0.2788, validation loss: 0.3207
2024-05-23 20:19:05 [INFO]: Epoch 015 - training loss: 0.2607, validation loss: 0.3200
2024-05-23 20:19:06 [INFO]: Epoch 016 - training loss: 0.2572, validation loss: 0.3244
2024-05-23 20:19:07 [INFO]: Epoch 017 - training loss: 0.2563, validation loss: 0.3214
2024-05-23 20:19:08 [INFO]: Epoch 018 - training loss: 0.2488, validation loss: 0.3233
2024-05-23 20:19:08 [INFO]: Epoch 019 - training loss: 0.2437, validation loss: 0.3304
2024-05-23 20:19:09 [INFO]: Epoch 020 - training loss: 0.2382, validation loss: 0.3281
2024-05-23 20:19:10 [INFO]: Epoch 021 - training loss: 0.2424, validation loss: 0.3282
2024-05-23 20:19:10 [INFO]: Epoch 022 - training loss: 0.2402, validation loss: 0.3152
2024-05-23 20:19:11 [INFO]: Epoch 023 - training loss: 0.2327, validation loss: 0.3285
2024-05-23 20:19:12 [INFO]: Epoch 024 - training loss: 0.2337, validation loss: 0.3174
2024-05-23 20:19:13 [INFO]: Epoch 025 - training loss: 0.2219, validation loss: 0.3369
2024-05-23 20:19:13 [INFO]: Epoch 026 - training loss: 0.2246, validation loss: 0.3264
2024-05-23 20:19:14 [INFO]: Epoch 027 - training loss: 0.2205, validation loss: 0.3317
2024-05-23 20:19:15 [INFO]: Epoch 028 - training loss: 0.2206, validation loss: 0.3314
2024-05-23 20:19:15 [INFO]: Epoch 029 - training loss: 0.2070, validation loss: 0.3291
2024-05-23 20:19:16 [INFO]: Epoch 030 - training loss: 0.2096, validation loss: 0.3416
2024-05-23 20:19:17 [INFO]: Epoch 031 - training loss: 0.2058, validation loss: 0.3174
2024-05-23 20:19:18 [INFO]: Epoch 032 - training loss: 0.2043, validation loss: 0.3357
2024-05-23 20:19:18 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-05-23 20:19:18 [INFO]: Finished training. The best model is from epoch#22.
2024-05-23 20:19:18 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_1/TimesNet_physionet_2012_seta/20240523_T201854/TimesNet.pypots
2024-05-23 20:19:18 [INFO]: TimesNet on PhysioNet-2012-seta: MAE=0.2855, MSE=0.2742
2024-05-23 20:19:18 [INFO]: Successfully saved to augmentation_premask_post_norm_saved_results/round_1/TimesNet_physionet_2012_seta/imputation.pkl
2024-05-23 20:19:18 [INFO]: Using the given device: cuda:0
2024-05-23 20:19:18 [INFO]: Model files will be saved to augmentation_premask_post_norm_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T201918
2024-05-23 20:19:18 [INFO]: Tensorboard file will be saved to augmentation_premask_post_norm_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T201918/tensorboard
2024-05-23 20:19:18 [INFO]: CSDI initialized with the given hyperparameters, the number of trainable parameters: 1,694,753
2024-05-23 20:20:01 [INFO]: Epoch 001 - training loss: 0.4150, validation loss: 0.3332
2024-05-23 20:20:01 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T201918/CSDI_epoch1_loss0.33317124992609026.pypots
2024-05-23 20:20:45 [INFO]: Epoch 002 - training loss: 0.3152, validation loss: 0.2965
2024-05-23 20:20:45 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T201918/CSDI_epoch2_loss0.2964915543794632.pypots
2024-05-23 20:21:28 [INFO]: Epoch 003 - training loss: 0.2912, validation loss: 0.2720
2024-05-23 20:21:28 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T201918/CSDI_epoch3_loss0.27198711782693863.pypots
2024-05-23 20:22:12 [INFO]: Epoch 004 - training loss: 0.2739, validation loss: 0.2557
2024-05-23 20:22:12 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T201918/CSDI_epoch4_loss0.2556601300835609.pypots
2024-05-23 20:22:56 [INFO]: Epoch 005 - training loss: 0.2511, validation loss: 0.2446
2024-05-23 20:22:56 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T201918/CSDI_epoch5_loss0.2445674270391464.pypots
2024-05-23 20:23:40 [INFO]: Epoch 006 - training loss: 0.2476, validation loss: 0.2308
2024-05-23 20:23:40 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T201918/CSDI_epoch6_loss0.23081952705979347.pypots
2024-05-23 20:24:23 [INFO]: Epoch 007 - training loss: 0.2339, validation loss: 0.2338
2024-05-23 20:24:23 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T201918/CSDI_epoch7_loss0.23383560106158258.pypots
2024-05-23 20:25:07 [INFO]: Epoch 008 - training loss: 0.2378, validation loss: 0.2252
2024-05-23 20:25:07 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T201918/CSDI_epoch8_loss0.22515623047947883.pypots
2024-05-23 20:25:51 [INFO]: Epoch 009 - training loss: 0.2251, validation loss: 0.2216
2024-05-23 20:25:51 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T201918/CSDI_epoch9_loss0.22161099463701248.pypots
2024-05-23 20:26:35 [INFO]: Epoch 010 - training loss: 0.2279, validation loss: 0.2160
2024-05-23 20:26:35 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T201918/CSDI_epoch10_loss0.21595662757754325.pypots
2024-05-23 20:27:19 [INFO]: Epoch 011 - training loss: 0.2165, validation loss: 0.2130
2024-05-23 20:27:19 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T201918/CSDI_epoch11_loss0.213029208779335.pypots
2024-05-23 20:28:02 [INFO]: Epoch 012 - training loss: 0.2192, validation loss: 0.2118
2024-05-23 20:28:02 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T201918/CSDI_epoch12_loss0.2118256501853466.pypots
2024-05-23 20:28:46 [INFO]: Epoch 013 - training loss: 0.2127, validation loss: 0.2128
2024-05-23 20:28:46 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T201918/CSDI_epoch13_loss0.2128249280154705.pypots
2024-05-23 20:29:30 [INFO]: Epoch 014 - training loss: 0.2210, validation loss: 0.2125
2024-05-23 20:29:30 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T201918/CSDI_epoch14_loss0.21254517287015914.pypots
2024-05-23 20:30:14 [INFO]: Epoch 015 - training loss: 0.2186, validation loss: 0.2110
2024-05-23 20:30:14 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T201918/CSDI_epoch15_loss0.2110294669866562.pypots
2024-05-23 20:30:58 [INFO]: Epoch 016 - training loss: 0.2106, validation loss: 0.2083
2024-05-23 20:30:58 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T201918/CSDI_epoch16_loss0.20826387032866478.pypots
2024-05-23 20:31:41 [INFO]: Epoch 017 - training loss: 0.2163, validation loss: 0.2069
2024-05-23 20:31:42 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T201918/CSDI_epoch17_loss0.2068631775677204.pypots
2024-05-23 20:32:25 [INFO]: Epoch 018 - training loss: 0.2160, validation loss: 0.2051
2024-05-23 20:32:25 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T201918/CSDI_epoch18_loss0.20507937669754028.pypots
2024-05-23 20:33:09 [INFO]: Epoch 019 - training loss: 0.2062, validation loss: 0.2063
2024-05-23 20:33:09 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T201918/CSDI_epoch19_loss0.2063206546008587.pypots
2024-05-23 20:33:53 [INFO]: Epoch 020 - training loss: 0.2031, validation loss: 0.2040
2024-05-23 20:33:53 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T201918/CSDI_epoch20_loss0.20403526872396469.pypots
2024-05-23 20:34:37 [INFO]: Epoch 021 - training loss: 0.2086, validation loss: 0.2004
2024-05-23 20:34:37 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T201918/CSDI_epoch21_loss0.20043038278818132.pypots
2024-05-23 20:35:21 [INFO]: Epoch 022 - training loss: 0.2050, validation loss: 0.1996
2024-05-23 20:35:21 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T201918/CSDI_epoch22_loss0.19963252693414688.pypots
2024-05-23 20:36:05 [INFO]: Epoch 023 - training loss: 0.2054, validation loss: 0.2017
2024-05-23 20:36:05 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T201918/CSDI_epoch23_loss0.20174964219331742.pypots
2024-05-23 20:36:48 [INFO]: Epoch 024 - training loss: 0.2168, validation loss: 0.2016
2024-05-23 20:36:48 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T201918/CSDI_epoch24_loss0.2016075797379017.pypots
2024-05-23 20:37:32 [INFO]: Epoch 025 - training loss: 0.2115, validation loss: 0.1985
2024-05-23 20:37:32 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T201918/CSDI_epoch25_loss0.19853003397583963.pypots
2024-05-23 20:38:16 [INFO]: Epoch 026 - training loss: 0.2079, validation loss: 0.1962
2024-05-23 20:38:16 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T201918/CSDI_epoch26_loss0.19619343280792237.pypots
2024-05-23 20:39:00 [INFO]: Epoch 027 - training loss: 0.2075, validation loss: 0.2030
2024-05-23 20:39:00 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T201918/CSDI_epoch27_loss0.20301936566829681.pypots
2024-05-23 20:39:44 [INFO]: Epoch 028 - training loss: 0.1921, validation loss: 0.1988
2024-05-23 20:39:44 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T201918/CSDI_epoch28_loss0.19882090538740158.pypots
2024-05-23 20:40:27 [INFO]: Epoch 029 - training loss: 0.2062, validation loss: 0.1966
2024-05-23 20:40:27 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T201918/CSDI_epoch29_loss0.19661800414323807.pypots
2024-05-23 20:41:11 [INFO]: Epoch 030 - training loss: 0.1988, validation loss: 0.1984
2024-05-23 20:41:11 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T201918/CSDI_epoch30_loss0.1984127640724182.pypots
2024-05-23 20:41:55 [INFO]: Epoch 031 - training loss: 0.2106, validation loss: 0.1948
2024-05-23 20:41:55 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T201918/CSDI_epoch31_loss0.19483639895915986.pypots
2024-05-23 20:42:39 [INFO]: Epoch 032 - training loss: 0.2070, validation loss: 0.1984
2024-05-23 20:42:39 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T201918/CSDI_epoch32_loss0.1983666442334652.pypots
2024-05-23 20:43:23 [INFO]: Epoch 033 - training loss: 0.2087, validation loss: 0.1967
2024-05-23 20:43:23 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T201918/CSDI_epoch33_loss0.1967199943959713.pypots
2024-05-23 20:44:06 [INFO]: Epoch 034 - training loss: 0.1931, validation loss: 0.1955
2024-05-23 20:44:06 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T201918/CSDI_epoch34_loss0.1955124020576477.pypots
2024-05-23 20:44:50 [INFO]: Epoch 035 - training loss: 0.1954, validation loss: 0.1968
2024-05-23 20:44:50 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T201918/CSDI_epoch35_loss0.1967777580022812.pypots
2024-05-23 20:45:34 [INFO]: Epoch 036 - training loss: 0.1919, validation loss: 0.1961
2024-05-23 20:45:34 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T201918/CSDI_epoch36_loss0.1960570089519024.pypots
2024-05-23 20:46:18 [INFO]: Epoch 037 - training loss: 0.2052, validation loss: 0.1959
2024-05-23 20:46:18 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T201918/CSDI_epoch37_loss0.19590816348791124.pypots
2024-05-23 20:47:02 [INFO]: Epoch 038 - training loss: 0.1990, validation loss: 0.1941
2024-05-23 20:47:02 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T201918/CSDI_epoch38_loss0.19408514276146888.pypots
2024-05-23 20:47:46 [INFO]: Epoch 039 - training loss: 0.1927, validation loss: 0.1957
2024-05-23 20:47:46 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T201918/CSDI_epoch39_loss0.19568378999829292.pypots
2024-05-23 20:48:30 [INFO]: Epoch 040 - training loss: 0.2024, validation loss: 0.1920
2024-05-23 20:48:30 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T201918/CSDI_epoch40_loss0.19200173541903495.pypots
2024-05-23 20:49:13 [INFO]: Epoch 041 - training loss: 0.1992, validation loss: 0.1939
2024-05-23 20:49:13 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T201918/CSDI_epoch41_loss0.19390034303069115.pypots
2024-05-23 20:49:57 [INFO]: Epoch 042 - training loss: 0.2025, validation loss: 0.1918
2024-05-23 20:49:57 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T201918/CSDI_epoch42_loss0.19176225364208221.pypots
2024-05-23 20:50:41 [INFO]: Epoch 043 - training loss: 0.1908, validation loss: 0.1934
2024-05-23 20:50:41 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T201918/CSDI_epoch43_loss0.1934454582631588.pypots
2024-05-23 20:51:25 [INFO]: Epoch 044 - training loss: 0.1976, validation loss: 0.1930
2024-05-23 20:51:25 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T201918/CSDI_epoch44_loss0.19297600090503692.pypots
2024-05-23 20:52:09 [INFO]: Epoch 045 - training loss: 0.1987, validation loss: 0.1888
2024-05-23 20:52:09 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T201918/CSDI_epoch45_loss0.18882775604724883.pypots
2024-05-23 20:52:53 [INFO]: Epoch 046 - training loss: 0.1870, validation loss: 0.1941
2024-05-23 20:52:53 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T201918/CSDI_epoch46_loss0.19409252777695657.pypots
2024-05-23 20:53:37 [INFO]: Epoch 047 - training loss: 0.1947, validation loss: 0.1895
2024-05-23 20:53:37 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T201918/CSDI_epoch47_loss0.189506209641695.pypots
2024-05-23 20:54:21 [INFO]: Epoch 048 - training loss: 0.2032, validation loss: 0.1887
2024-05-23 20:54:21 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T201918/CSDI_epoch48_loss0.18868361935019493.pypots
2024-05-23 20:55:05 [INFO]: Epoch 049 - training loss: 0.1950, validation loss: 0.1931
2024-05-23 20:55:05 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T201918/CSDI_epoch49_loss0.1931316763162613.pypots
2024-05-23 20:55:49 [INFO]: Epoch 050 - training loss: 0.2001, validation loss: 0.1896
2024-05-23 20:55:49 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T201918/CSDI_epoch50_loss0.18955680429935456.pypots
2024-05-23 20:56:33 [INFO]: Epoch 051 - training loss: 0.1926, validation loss: 0.1914
2024-05-23 20:56:33 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T201918/CSDI_epoch51_loss0.1913723260164261.pypots
2024-05-23 20:57:17 [INFO]: Epoch 052 - training loss: 0.1895, validation loss: 0.1957
2024-05-23 20:57:17 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T201918/CSDI_epoch52_loss0.19573275744915009.pypots
2024-05-23 20:58:01 [INFO]: Epoch 053 - training loss: 0.1871, validation loss: 0.1927
2024-05-23 20:58:01 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T201918/CSDI_epoch53_loss0.19267704859375953.pypots
2024-05-23 20:58:45 [INFO]: Epoch 054 - training loss: 0.1943, validation loss: 0.1932
2024-05-23 20:58:45 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T201918/CSDI_epoch54_loss0.19323992729187012.pypots
2024-05-23 20:59:29 [INFO]: Epoch 055 - training loss: 0.1849, validation loss: 0.1904
2024-05-23 20:59:29 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T201918/CSDI_epoch55_loss0.19038453102111816.pypots
2024-05-23 21:00:13 [INFO]: Epoch 056 - training loss: 0.1983, validation loss: 0.1935
2024-05-23 21:00:13 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T201918/CSDI_epoch56_loss0.1934744469821453.pypots
2024-05-23 21:00:57 [INFO]: Epoch 057 - training loss: 0.2051, validation loss: 0.1917
2024-05-23 21:00:57 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T201918/CSDI_epoch57_loss0.19173441976308822.pypots
2024-05-23 21:01:41 [INFO]: Epoch 058 - training loss: 0.1884, validation loss: 0.1896
2024-05-23 21:01:41 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T201918/CSDI_epoch58_loss0.18956671059131622.pypots
2024-05-23 21:01:41 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-05-23 21:01:41 [INFO]: Finished training. The best model is from epoch#48.
2024-05-23 21:01:41 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_1/CSDI_physionet_2012_seta/20240523_T201918/CSDI.pypots
2024-05-23 21:09:04 [INFO]: CSDI on PhysioNet-2012-seta: MAE=0.2601, MSE=0.4907
2024-05-23 21:38:30 [INFO]: Successfully saved to augmentation_premask_post_norm_saved_results/round_1/CSDI_physionet_2012_seta/imputation.pkl
2024-05-23 21:38:30 [INFO]: Using the given device: cuda:0
2024-05-23 21:38:30 [INFO]: Model files will be saved to augmentation_premask_post_norm_saved_results/round_1/GPVAE_physionet_2012_seta/20240523_T213830
2024-05-23 21:38:30 [INFO]: Tensorboard file will be saved to augmentation_premask_post_norm_saved_results/round_1/GPVAE_physionet_2012_seta/20240523_T213830/tensorboard
2024-05-23 21:38:30 [INFO]: GPVAE initialized with the given hyperparameters, the number of trainable parameters: 1,028,244
2024-05-23 21:38:30 [INFO]: Epoch 001 - training loss: 42685.1001, validation loss: 0.9338
2024-05-23 21:38:31 [INFO]: Epoch 002 - training loss: 24487.0683, validation loss: 0.7591
2024-05-23 21:38:32 [INFO]: Epoch 003 - training loss: 23509.0108, validation loss: 0.7421
2024-05-23 21:38:32 [INFO]: Epoch 004 - training loss: 23201.6087, validation loss: 0.7184
2024-05-23 21:38:33 [INFO]: Epoch 005 - training loss: 23048.6188, validation loss: 0.6832
2024-05-23 21:38:34 [INFO]: Epoch 006 - training loss: 22962.1375, validation loss: 0.6894
2024-05-23 21:38:34 [INFO]: Epoch 007 - training loss: 22910.2611, validation loss: 0.6744
2024-05-23 21:38:35 [INFO]: Epoch 008 - training loss: 22877.0266, validation loss: 0.6696
2024-05-23 21:38:35 [INFO]: Epoch 009 - training loss: 22854.6782, validation loss: 0.6696
2024-05-23 21:38:36 [INFO]: Epoch 010 - training loss: 22839.3459, validation loss: 0.6714
2024-05-23 21:38:37 [INFO]: Epoch 011 - training loss: 22828.0319, validation loss: 0.6695
2024-05-23 21:38:37 [INFO]: Epoch 012 - training loss: 22819.5268, validation loss: 0.6691
2024-05-23 21:38:38 [INFO]: Epoch 013 - training loss: 22813.6593, validation loss: 0.6685
2024-05-23 21:38:38 [INFO]: Epoch 014 - training loss: 22808.8163, validation loss: 0.6647
2024-05-23 21:38:39 [INFO]: Epoch 015 - training loss: 22805.3325, validation loss: 0.6642
2024-05-23 21:38:40 [INFO]: Epoch 016 - training loss: 22801.5886, validation loss: 0.6590
2024-05-23 21:38:40 [INFO]: Epoch 017 - training loss: 22799.3294, validation loss: 0.6612
2024-05-23 21:38:41 [INFO]: Epoch 018 - training loss: 22796.9101, validation loss: 0.6530
2024-05-23 21:38:42 [INFO]: Epoch 019 - training loss: 22795.5975, validation loss: 0.6526
2024-05-23 21:38:42 [INFO]: Epoch 020 - training loss: 22792.5401, validation loss: 0.6461
2024-05-23 21:38:43 [INFO]: Epoch 021 - training loss: 22790.5735, validation loss: 0.6539
2024-05-23 21:38:43 [INFO]: Epoch 022 - training loss: 22789.4022, validation loss: 0.6409
2024-05-23 21:38:44 [INFO]: Epoch 023 - training loss: 22788.6031, validation loss: 0.6384
2024-05-23 21:38:45 [INFO]: Epoch 024 - training loss: 22787.1974, validation loss: 0.6407
2024-05-23 21:38:45 [INFO]: Epoch 025 - training loss: 22787.0275, validation loss: 0.6348
2024-05-23 21:38:46 [INFO]: Epoch 026 - training loss: 22786.5540, validation loss: 0.6459
2024-05-23 21:38:46 [INFO]: Epoch 027 - training loss: 22785.2820, validation loss: 0.6351
2024-05-23 21:38:47 [INFO]: Epoch 028 - training loss: 22784.2759, validation loss: 0.6342
2024-05-23 21:38:48 [INFO]: Epoch 029 - training loss: 22783.7516, validation loss: 0.6406
2024-05-23 21:38:48 [INFO]: Epoch 030 - training loss: 22783.1168, validation loss: 0.6513
2024-05-23 21:38:49 [INFO]: Epoch 031 - training loss: 22782.9493, validation loss: 0.6324
2024-05-23 21:38:50 [INFO]: Epoch 032 - training loss: 22782.1775, validation loss: 0.6306
2024-05-23 21:38:50 [INFO]: Epoch 033 - training loss: 22781.6063, validation loss: 0.6305
2024-05-23 21:38:51 [INFO]: Epoch 034 - training loss: 22781.7579, validation loss: 0.6360
2024-05-23 21:38:51 [INFO]: Epoch 035 - training loss: 22782.3563, validation loss: 0.6314
2024-05-23 21:38:52 [INFO]: Epoch 036 - training loss: 22781.1368, validation loss: 0.6356
2024-05-23 21:38:53 [INFO]: Epoch 037 - training loss: 22780.4068, validation loss: 0.6334
2024-05-23 21:38:53 [INFO]: Epoch 038 - training loss: 22779.8243, validation loss: 0.6288
2024-05-23 21:38:54 [INFO]: Epoch 039 - training loss: 22778.3656, validation loss: 0.6245
2024-05-23 21:38:55 [INFO]: Epoch 040 - training loss: 22778.2499, validation loss: 0.6287
2024-05-23 21:38:55 [INFO]: Epoch 041 - training loss: 22776.3285, validation loss: 0.6176
2024-05-23 21:38:56 [INFO]: Epoch 042 - training loss: 22775.1860, validation loss: 0.6155
2024-05-23 21:38:56 [INFO]: Epoch 043 - training loss: 22774.5228, validation loss: 0.6146
2024-05-23 21:38:57 [INFO]: Epoch 044 - training loss: 22774.6902, validation loss: 0.6113
2024-05-23 21:38:58 [INFO]: Epoch 045 - training loss: 22773.8631, validation loss: 0.6113
2024-05-23 21:38:58 [INFO]: Epoch 046 - training loss: 22772.4636, validation loss: 0.6054
2024-05-23 21:38:59 [INFO]: Epoch 047 - training loss: 22773.5540, validation loss: 0.6081
2024-05-23 21:39:00 [INFO]: Epoch 048 - training loss: 22771.3535, validation loss: 0.6003
2024-05-23 21:39:00 [INFO]: Epoch 049 - training loss: 22770.6190, validation loss: 0.6007
2024-05-23 21:39:01 [INFO]: Epoch 050 - training loss: 22769.8125, validation loss: 0.5995
2024-05-23 21:39:01 [INFO]: Epoch 051 - training loss: 22769.8371, validation loss: 0.6077
2024-05-23 21:39:02 [INFO]: Epoch 052 - training loss: 22769.2078, validation loss: 0.5947
2024-05-23 21:39:03 [INFO]: Epoch 053 - training loss: 22768.9031, validation loss: 0.5981
2024-05-23 21:39:03 [INFO]: Epoch 054 - training loss: 22768.1167, validation loss: 0.5966
2024-05-23 21:39:04 [INFO]: Epoch 055 - training loss: 22767.7597, validation loss: 0.5951
2024-05-23 21:39:05 [INFO]: Epoch 056 - training loss: 22767.2241, validation loss: 0.5924
2024-05-23 21:39:05 [INFO]: Epoch 057 - training loss: 22766.7311, validation loss: 0.5932
2024-05-23 21:39:06 [INFO]: Epoch 058 - training loss: 22766.9869, validation loss: 0.5952
2024-05-23 21:39:06 [INFO]: Epoch 059 - training loss: 22766.6708, validation loss: 0.6034
2024-05-23 21:39:07 [INFO]: Epoch 060 - training loss: 22766.0229, validation loss: 0.5939
2024-05-23 21:39:08 [INFO]: Epoch 061 - training loss: 22765.3512, validation loss: 0.5886
2024-05-23 21:39:08 [INFO]: Epoch 062 - training loss: 22764.7535, validation loss: 0.5897
2024-05-23 21:39:09 [INFO]: Epoch 063 - training loss: 22765.3985, validation loss: 0.5846
2024-05-23 21:39:10 [INFO]: Epoch 064 - training loss: 22765.6813, validation loss: 0.5764
2024-05-23 21:39:10 [INFO]: Epoch 065 - training loss: 22764.9076, validation loss: 0.5802
2024-05-23 21:39:11 [INFO]: Epoch 066 - training loss: 22762.9453, validation loss: 0.5717
2024-05-23 21:39:11 [INFO]: Epoch 067 - training loss: 22761.8793, validation loss: 0.5763
2024-05-23 21:39:12 [INFO]: Epoch 068 - training loss: 22761.4107, validation loss: 0.5622
2024-05-23 21:39:13 [INFO]: Epoch 069 - training loss: 22759.7001, validation loss: 0.5637
2024-05-23 21:39:13 [INFO]: Epoch 070 - training loss: 22760.2293, validation loss: 0.5535
2024-05-23 21:39:14 [INFO]: Epoch 071 - training loss: 22758.8447, validation loss: 0.5557
2024-05-23 21:39:14 [INFO]: Epoch 072 - training loss: 22758.8145, validation loss: 0.5488
2024-05-23 21:39:15 [INFO]: Epoch 073 - training loss: 22757.5554, validation loss: 0.5439
2024-05-23 21:39:16 [INFO]: Epoch 074 - training loss: 22757.2748, validation loss: 0.5509
2024-05-23 21:39:16 [INFO]: Epoch 075 - training loss: 22756.7054, validation loss: 0.5439
2024-05-23 21:39:17 [INFO]: Epoch 076 - training loss: 22756.5401, validation loss: 0.5400
2024-05-23 21:39:17 [INFO]: Epoch 077 - training loss: 22755.8758, validation loss: 0.5404
2024-05-23 21:39:18 [INFO]: Epoch 078 - training loss: 22755.8221, validation loss: 0.5401
2024-05-23 21:39:19 [INFO]: Epoch 079 - training loss: 22755.6451, validation loss: 0.5413
2024-05-23 21:39:19 [INFO]: Epoch 080 - training loss: 22756.6558, validation loss: 0.5386
2024-05-23 21:39:20 [INFO]: Epoch 081 - training loss: 22754.8689, validation loss: 0.5375
2024-05-23 21:39:21 [INFO]: Epoch 082 - training loss: 22754.8649, validation loss: 0.5380
2024-05-23 21:39:21 [INFO]: Epoch 083 - training loss: 22754.2570, validation loss: 0.5378
2024-05-23 21:39:22 [INFO]: Epoch 084 - training loss: 22754.0520, validation loss: 0.5362
2024-05-23 21:39:22 [INFO]: Epoch 085 - training loss: 22753.5170, validation loss: 0.5324
2024-05-23 21:39:23 [INFO]: Epoch 086 - training loss: 22753.5933, validation loss: 0.5335
2024-05-23 21:39:24 [INFO]: Epoch 087 - training loss: 22753.7061, validation loss: 0.5542
2024-05-23 21:39:24 [INFO]: Epoch 088 - training loss: 22757.3286, validation loss: 0.5317
2024-05-23 21:39:25 [INFO]: Epoch 089 - training loss: 22753.4181, validation loss: 0.5619
2024-05-23 21:39:25 [INFO]: Epoch 090 - training loss: 22754.2751, validation loss: 0.5369
2024-05-23 21:39:26 [INFO]: Epoch 091 - training loss: 22753.3242, validation loss: 0.5291
2024-05-23 21:39:27 [INFO]: Epoch 092 - training loss: 22752.1084, validation loss: 0.5273
2024-05-23 21:39:27 [INFO]: Epoch 093 - training loss: 22751.8842, validation loss: 0.5280
2024-05-23 21:39:28 [INFO]: Epoch 094 - training loss: 22751.6944, validation loss: 0.5278
2024-05-23 21:39:29 [INFO]: Epoch 095 - training loss: 22751.2042, validation loss: 0.5256
2024-05-23 21:39:29 [INFO]: Epoch 096 - training loss: 22751.0992, validation loss: 0.5247
2024-05-23 21:39:30 [INFO]: Epoch 097 - training loss: 22751.6655, validation loss: 0.5215
2024-05-23 21:39:30 [INFO]: Epoch 098 - training loss: 22750.9337, validation loss: 0.5226
2024-05-23 21:39:31 [INFO]: Epoch 099 - training loss: 22750.7160, validation loss: 0.5190
2024-05-23 21:39:32 [INFO]: Epoch 100 - training loss: 22751.0013, validation loss: 0.5191
2024-05-23 21:39:32 [INFO]: Epoch 101 - training loss: 22750.5335, validation loss: 0.5204
2024-05-23 21:39:33 [INFO]: Epoch 102 - training loss: 22750.7089, validation loss: 0.5167
2024-05-23 21:39:33 [INFO]: Epoch 103 - training loss: 22750.2556, validation loss: 0.5206
2024-05-23 21:39:34 [INFO]: Epoch 104 - training loss: 22750.4283, validation loss: 0.5153
2024-05-23 21:39:35 [INFO]: Epoch 105 - training loss: 22749.7667, validation loss: 0.5190
2024-05-23 21:39:35 [INFO]: Epoch 106 - training loss: 22749.7989, validation loss: 0.5158
2024-05-23 21:39:36 [INFO]: Epoch 107 - training loss: 22749.3630, validation loss: 0.5110
2024-05-23 21:39:37 [INFO]: Epoch 108 - training loss: 22750.1214, validation loss: 0.5176
2024-05-23 21:39:37 [INFO]: Epoch 109 - training loss: 22749.9490, validation loss: 0.5119
2024-05-23 21:39:38 [INFO]: Epoch 110 - training loss: 22749.6720, validation loss: 0.5119
2024-05-23 21:39:38 [INFO]: Epoch 111 - training loss: 22748.9419, validation loss: 0.5130
2024-05-23 21:39:39 [INFO]: Epoch 112 - training loss: 22749.0236, validation loss: 0.5126
2024-05-23 21:39:40 [INFO]: Epoch 113 - training loss: 22748.3651, validation loss: 0.5120
2024-05-23 21:39:40 [INFO]: Epoch 114 - training loss: 22748.5085, validation loss: 0.5061
2024-05-23 21:39:41 [INFO]: Epoch 115 - training loss: 22748.2257, validation loss: 0.5088
2024-05-23 21:39:41 [INFO]: Epoch 116 - training loss: 22748.6862, validation loss: 0.5128
2024-05-23 21:39:42 [INFO]: Epoch 117 - training loss: 22749.4423, validation loss: 0.5129
2024-05-23 21:39:43 [INFO]: Epoch 118 - training loss: 22753.6623, validation loss: 0.5110
2024-05-23 21:39:43 [INFO]: Epoch 119 - training loss: 22750.1356, validation loss: 0.5040
2024-05-23 21:39:44 [INFO]: Epoch 120 - training loss: 22748.8217, validation loss: 0.5144
2024-05-23 21:39:44 [INFO]: Epoch 121 - training loss: 22750.3010, validation loss: 0.5047
2024-05-23 21:39:45 [INFO]: Epoch 122 - training loss: 22749.1656, validation loss: 0.5071
2024-05-23 21:39:46 [INFO]: Epoch 123 - training loss: 22747.8194, validation loss: 0.5029
2024-05-23 21:39:46 [INFO]: Epoch 124 - training loss: 22747.4980, validation loss: 0.5083
2024-05-23 21:39:47 [INFO]: Epoch 125 - training loss: 22747.2887, validation loss: 0.5011
2024-05-23 21:39:48 [INFO]: Epoch 126 - training loss: 22746.9887, validation loss: 0.5006
2024-05-23 21:39:48 [INFO]: Epoch 127 - training loss: 22746.9121, validation loss: 0.4982
2024-05-23 21:39:49 [INFO]: Epoch 128 - training loss: 22746.4836, validation loss: 0.5036
2024-05-23 21:39:49 [INFO]: Epoch 129 - training loss: 22746.5155, validation loss: 0.4981
2024-05-23 21:39:50 [INFO]: Epoch 130 - training loss: 22746.6298, validation loss: 0.5022
2024-05-23 21:39:51 [INFO]: Epoch 131 - training loss: 22747.9380, validation loss: 0.5042
2024-05-23 21:39:51 [INFO]: Epoch 132 - training loss: 22747.5496, validation loss: 0.5026
2024-05-23 21:39:52 [INFO]: Epoch 133 - training loss: 22746.7034, validation loss: 0.4953
2024-05-23 21:39:52 [INFO]: Epoch 134 - training loss: 22746.0260, validation loss: 0.5013
2024-05-23 21:39:53 [INFO]: Epoch 135 - training loss: 22746.0371, validation loss: 0.4950
2024-05-23 21:39:54 [INFO]: Epoch 136 - training loss: 22746.1585, validation loss: 0.5006
2024-05-23 21:39:54 [INFO]: Epoch 137 - training loss: 22746.7457, validation loss: 0.4909
2024-05-23 21:39:55 [INFO]: Epoch 138 - training loss: 22746.4903, validation loss: 0.4960
2024-05-23 21:39:56 [INFO]: Epoch 139 - training loss: 22746.2862, validation loss: 0.4964
2024-05-23 21:39:56 [INFO]: Epoch 140 - training loss: 22746.1107, validation loss: 0.4903
2024-05-23 21:39:57 [INFO]: Epoch 141 - training loss: 22745.8897, validation loss: 0.4874
2024-05-23 21:39:57 [INFO]: Epoch 142 - training loss: 22745.6084, validation loss: 0.4914
2024-05-23 21:39:58 [INFO]: Epoch 143 - training loss: 22744.8189, validation loss: 0.4904
2024-05-23 21:39:59 [INFO]: Epoch 144 - training loss: 22744.9557, validation loss: 0.4896
2024-05-23 21:39:59 [INFO]: Epoch 145 - training loss: 22744.6537, validation loss: 0.4876
2024-05-23 21:40:00 [INFO]: Epoch 146 - training loss: 22744.8964, validation loss: 0.4875
2024-05-23 21:40:00 [INFO]: Epoch 147 - training loss: 22744.6202, validation loss: 0.4860
2024-05-23 21:40:01 [INFO]: Epoch 148 - training loss: 22744.8476, validation loss: 0.4913
2024-05-23 21:40:02 [INFO]: Epoch 149 - training loss: 22744.8628, validation loss: 0.4846
2024-05-23 21:40:02 [INFO]: Epoch 150 - training loss: 22744.8208, validation loss: 0.4916
2024-05-23 21:40:03 [INFO]: Epoch 151 - training loss: 22744.9728, validation loss: 0.4825
2024-05-23 21:40:04 [INFO]: Epoch 152 - training loss: 22745.9574, validation loss: 0.4918
2024-05-23 21:40:04 [INFO]: Epoch 153 - training loss: 22745.7511, validation loss: 0.4854
2024-05-23 21:40:05 [INFO]: Epoch 154 - training loss: 22744.5675, validation loss: 0.4841
2024-05-23 21:40:05 [INFO]: Epoch 155 - training loss: 22744.1337, validation loss: 0.4834
2024-05-23 21:40:06 [INFO]: Epoch 156 - training loss: 22744.0332, validation loss: 0.4823
2024-05-23 21:40:07 [INFO]: Epoch 157 - training loss: 22743.7260, validation loss: 0.4809
2024-05-23 21:40:07 [INFO]: Epoch 158 - training loss: 22743.7657, validation loss: 0.4829
2024-05-23 21:40:08 [INFO]: Epoch 159 - training loss: 22743.5120, validation loss: 0.4818
2024-05-23 21:40:08 [INFO]: Epoch 160 - training loss: 22744.0604, validation loss: 0.4790
2024-05-23 21:40:09 [INFO]: Epoch 161 - training loss: 22743.7055, validation loss: 0.4801
2024-05-23 21:40:10 [INFO]: Epoch 162 - training loss: 22743.2406, validation loss: 0.4801
2024-05-23 21:40:10 [INFO]: Epoch 163 - training loss: 22742.9651, validation loss: 0.4776
2024-05-23 21:40:11 [INFO]: Epoch 164 - training loss: 22742.7478, validation loss: 0.4757
2024-05-23 21:40:12 [INFO]: Epoch 165 - training loss: 22743.3636, validation loss: 0.4806
2024-05-23 21:40:12 [INFO]: Epoch 166 - training loss: 22743.5706, validation loss: 0.4768
2024-05-23 21:40:13 [INFO]: Epoch 167 - training loss: 22743.3235, validation loss: 0.4775
2024-05-23 21:40:13 [INFO]: Epoch 168 - training loss: 22743.3904, validation loss: 0.4767
2024-05-23 21:40:14 [INFO]: Epoch 169 - training loss: 22742.9314, validation loss: 0.4765
2024-05-23 21:40:15 [INFO]: Epoch 170 - training loss: 22744.8783, validation loss: 0.4727
2024-05-23 21:40:15 [INFO]: Epoch 171 - training loss: 22742.7188, validation loss: 0.4738
2024-05-23 21:40:16 [INFO]: Epoch 172 - training loss: 22742.8523, validation loss: 0.4732
2024-05-23 21:40:16 [INFO]: Epoch 173 - training loss: 22742.8153, validation loss: 0.4716
2024-05-23 21:40:17 [INFO]: Epoch 174 - training loss: 22742.9774, validation loss: 0.4778
2024-05-23 21:40:18 [INFO]: Epoch 175 - training loss: 22742.4673, validation loss: 0.4741
2024-05-23 21:40:18 [INFO]: Epoch 176 - training loss: 22744.1667, validation loss: 0.4768
2024-05-23 21:40:19 [INFO]: Epoch 177 - training loss: 22742.4410, validation loss: 0.4728
2024-05-23 21:40:20 [INFO]: Epoch 178 - training loss: 22743.0391, validation loss: 0.4723
2024-05-23 21:40:20 [INFO]: Epoch 179 - training loss: 22742.2788, validation loss: 0.4693
2024-05-23 21:40:21 [INFO]: Epoch 180 - training loss: 22741.6905, validation loss: 0.4688
2024-05-23 21:40:21 [INFO]: Epoch 181 - training loss: 22741.4960, validation loss: 0.4697
2024-05-23 21:40:22 [INFO]: Epoch 182 - training loss: 22741.9961, validation loss: 0.4682
2024-05-23 21:40:23 [INFO]: Epoch 183 - training loss: 22741.6144, validation loss: 0.4661
2024-05-23 21:40:23 [INFO]: Epoch 184 - training loss: 22741.4444, validation loss: 0.4654
2024-05-23 21:40:24 [INFO]: Epoch 185 - training loss: 22741.5457, validation loss: 0.4700
2024-05-23 21:40:24 [INFO]: Epoch 186 - training loss: 22741.5743, validation loss: 0.4640
2024-05-23 21:40:25 [INFO]: Epoch 187 - training loss: 22741.7322, validation loss: 0.4675
2024-05-23 21:40:26 [INFO]: Epoch 188 - training loss: 22741.5005, validation loss: 0.4623
2024-05-23 21:40:26 [INFO]: Epoch 189 - training loss: 22741.9686, validation loss: 0.4671
2024-05-23 21:40:27 [INFO]: Epoch 190 - training loss: 22741.5155, validation loss: 0.4636
2024-05-23 21:40:27 [INFO]: Epoch 191 - training loss: 22741.5444, validation loss: 0.4688
2024-05-23 21:40:28 [INFO]: Epoch 192 - training loss: 22741.5436, validation loss: 0.4657
2024-05-23 21:40:29 [INFO]: Epoch 193 - training loss: 22741.6216, validation loss: 0.4651
2024-05-23 21:40:29 [INFO]: Epoch 194 - training loss: 22741.1136, validation loss: 0.4605
2024-05-23 21:40:30 [INFO]: Epoch 195 - training loss: 22740.8287, validation loss: 0.4671
2024-05-23 21:40:31 [INFO]: Epoch 196 - training loss: 22740.5790, validation loss: 0.4606
2024-05-23 21:40:31 [INFO]: Epoch 197 - training loss: 22740.6410, validation loss: 0.4663
2024-05-23 21:40:32 [INFO]: Epoch 198 - training loss: 22741.0021, validation loss: 0.4613
2024-05-23 21:40:32 [INFO]: Epoch 199 - training loss: 22740.9198, validation loss: 0.4620
2024-05-23 21:40:33 [INFO]: Epoch 200 - training loss: 22740.5114, validation loss: 0.4608
2024-05-23 21:40:34 [INFO]: Epoch 201 - training loss: 22740.6735, validation loss: 0.4633
2024-05-23 21:40:34 [INFO]: Epoch 202 - training loss: 22741.0426, validation loss: 0.4622
2024-05-23 21:40:35 [INFO]: Epoch 203 - training loss: 22740.4870, validation loss: 0.4614
2024-05-23 21:40:35 [INFO]: Epoch 204 - training loss: 22740.2863, validation loss: 0.4589
2024-05-23 21:40:36 [INFO]: Epoch 205 - training loss: 22740.3360, validation loss: 0.4653
2024-05-23 21:40:37 [INFO]: Epoch 206 - training loss: 22740.0474, validation loss: 0.4593
2024-05-23 21:40:37 [INFO]: Epoch 207 - training loss: 22740.1956, validation loss: 0.4598
2024-05-23 21:40:38 [INFO]: Epoch 208 - training loss: 22739.7387, validation loss: 0.4657
2024-05-23 21:40:39 [INFO]: Epoch 209 - training loss: 22740.2427, validation loss: 0.4560
2024-05-23 21:40:39 [INFO]: Epoch 210 - training loss: 22740.2067, validation loss: 0.4629
2024-05-23 21:40:40 [INFO]: Epoch 211 - training loss: 22740.3931, validation loss: 0.4574
2024-05-23 21:40:40 [INFO]: Epoch 212 - training loss: 22739.7683, validation loss: 0.4596
2024-05-23 21:40:41 [INFO]: Epoch 213 - training loss: 22739.7724, validation loss: 0.4588
2024-05-23 21:40:42 [INFO]: Epoch 214 - training loss: 22739.7766, validation loss: 0.4582
2024-05-23 21:40:42 [INFO]: Epoch 215 - training loss: 22739.6930, validation loss: 0.4578
2024-05-23 21:40:43 [INFO]: Epoch 216 - training loss: 22739.3464, validation loss: 0.4633
2024-05-23 21:40:43 [INFO]: Epoch 217 - training loss: 22739.9404, validation loss: 0.4598
2024-05-23 21:40:44 [INFO]: Epoch 218 - training loss: 22739.7323, validation loss: 0.4572
2024-05-23 21:40:45 [INFO]: Epoch 219 - training loss: 22740.0187, validation loss: 0.4598
2024-05-23 21:40:45 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-05-23 21:40:45 [INFO]: Finished training. The best model is from epoch#209.
2024-05-23 21:40:45 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_1/GPVAE_physionet_2012_seta/20240523_T213830/GPVAE.pypots
2024-05-23 21:40:45 [INFO]: GP-VAE on PhysioNet-2012-seta: MAE=0.3982, MSE=0.4047
2024-05-23 21:40:45 [INFO]: Successfully saved to augmentation_premask_post_norm_saved_results/round_1/GPVAE_physionet_2012_seta/imputation.pkl
2024-05-23 21:40:45 [INFO]: Using the given device: cuda:0
2024-05-23 21:40:45 [INFO]: Model files will be saved to augmentation_premask_post_norm_saved_results/round_1/USGAN_physionet_2012_seta/20240523_T214045
2024-05-23 21:40:45 [INFO]: Tensorboard file will be saved to augmentation_premask_post_norm_saved_results/round_1/USGAN_physionet_2012_seta/20240523_T214045/tensorboard
2024-05-23 21:40:45 [INFO]: USGAN initialized with the given hyperparameters, the number of trainable parameters: 16,010,261
2024-05-23 21:41:07 [INFO]: Epoch 001 - generator training loss: 0.6001, discriminator training loss: 0.3813, validation loss: 0.6382
2024-05-23 21:41:26 [INFO]: Epoch 002 - generator training loss: 0.4778, discriminator training loss: 0.2717, validation loss: 0.5455
2024-05-23 21:41:45 [INFO]: Epoch 003 - generator training loss: 0.4406, discriminator training loss: 0.2350, validation loss: 0.5378
2024-05-23 21:42:04 [INFO]: Epoch 004 - generator training loss: 0.4477, discriminator training loss: 0.1883, validation loss: 0.5183
2024-05-23 21:42:23 [INFO]: Epoch 005 - generator training loss: 0.4480, discriminator training loss: 0.1579, validation loss: 0.5191
2024-05-23 21:42:42 [INFO]: Epoch 006 - generator training loss: 0.4407, discriminator training loss: 0.1384, validation loss: 0.5006
2024-05-23 21:43:02 [INFO]: Epoch 007 - generator training loss: 0.4307, discriminator training loss: 0.1243, validation loss: 0.4808
2024-05-23 21:43:21 [INFO]: Epoch 008 - generator training loss: 0.4159, discriminator training loss: 0.1138, validation loss: 0.4680
2024-05-23 21:43:39 [INFO]: Epoch 009 - generator training loss: 0.4021, discriminator training loss: 0.1055, validation loss: 0.4608
2024-05-23 21:43:58 [INFO]: Epoch 010 - generator training loss: 0.3926, discriminator training loss: 0.0984, validation loss: 0.4544
2024-05-23 21:44:17 [INFO]: Epoch 011 - generator training loss: 0.3884, discriminator training loss: 0.0925, validation loss: 0.4431
2024-05-23 21:44:36 [INFO]: Epoch 012 - generator training loss: 0.3814, discriminator training loss: 0.0874, validation loss: 0.4400
2024-05-23 21:44:56 [INFO]: Epoch 013 - generator training loss: 0.3779, discriminator training loss: 0.0828, validation loss: 0.4337
2024-05-23 21:45:15 [INFO]: Epoch 014 - generator training loss: 0.3724, discriminator training loss: 0.0789, validation loss: 0.4284
2024-05-23 21:45:34 [INFO]: Epoch 015 - generator training loss: 0.3679, discriminator training loss: 0.0753, validation loss: 0.4232
2024-05-23 21:45:52 [INFO]: Epoch 016 - generator training loss: 0.3631, discriminator training loss: 0.0722, validation loss: 0.4177
2024-05-23 21:46:11 [INFO]: Epoch 017 - generator training loss: 0.3600, discriminator training loss: 0.0694, validation loss: 0.4143
2024-05-23 21:46:31 [INFO]: Epoch 018 - generator training loss: 0.3545, discriminator training loss: 0.0670, validation loss: 0.4092
2024-05-23 21:46:50 [INFO]: Epoch 019 - generator training loss: 0.3520, discriminator training loss: 0.0647, validation loss: 0.4064
2024-05-23 21:47:09 [INFO]: Epoch 020 - generator training loss: 0.3474, discriminator training loss: 0.0625, validation loss: 0.4037
2024-05-23 21:47:28 [INFO]: Epoch 021 - generator training loss: 0.3445, discriminator training loss: 0.0607, validation loss: 0.3990
2024-05-23 21:47:47 [INFO]: Epoch 022 - generator training loss: 0.3378, discriminator training loss: 0.0588, validation loss: 0.3932
2024-05-23 21:48:06 [INFO]: Epoch 023 - generator training loss: 0.3336, discriminator training loss: 0.0576, validation loss: 0.3953
2024-05-23 21:48:25 [INFO]: Epoch 024 - generator training loss: 0.3312, discriminator training loss: 0.0560, validation loss: 0.3879
2024-05-23 21:48:44 [INFO]: Epoch 025 - generator training loss: 0.3277, discriminator training loss: 0.0547, validation loss: 0.3839
2024-05-23 21:49:03 [INFO]: Epoch 026 - generator training loss: 0.3241, discriminator training loss: 0.0536, validation loss: 0.3829
2024-05-23 21:49:22 [INFO]: Epoch 027 - generator training loss: 0.3178, discriminator training loss: 0.0528, validation loss: 0.3798
2024-05-23 21:49:41 [INFO]: Epoch 028 - generator training loss: 0.3152, discriminator training loss: 0.0516, validation loss: 0.3755
2024-05-23 21:49:59 [INFO]: Epoch 029 - generator training loss: 0.3137, discriminator training loss: 0.0512, validation loss: 0.3743
2024-05-23 21:50:18 [INFO]: Epoch 030 - generator training loss: 0.3080, discriminator training loss: 0.0503, validation loss: 0.3691
2024-05-23 21:50:37 [INFO]: Epoch 031 - generator training loss: 0.3049, discriminator training loss: 0.0496, validation loss: 0.3698
2024-05-23 21:50:56 [INFO]: Epoch 032 - generator training loss: 0.3062, discriminator training loss: 0.0494, validation loss: 0.3645
2024-05-23 21:51:15 [INFO]: Epoch 033 - generator training loss: 0.2982, discriminator training loss: 0.0485, validation loss: 0.3643
2024-05-23 21:51:34 [INFO]: Epoch 034 - generator training loss: 0.2959, discriminator training loss: 0.0482, validation loss: 0.3629
2024-05-23 21:51:53 [INFO]: Epoch 035 - generator training loss: 0.2933, discriminator training loss: 0.0477, validation loss: 0.3677
2024-05-23 21:52:12 [INFO]: Epoch 036 - generator training loss: 0.2912, discriminator training loss: 0.0473, validation loss: 0.3670
2024-05-23 21:52:31 [INFO]: Epoch 037 - generator training loss: 0.2844, discriminator training loss: 0.0468, validation loss: 0.3544
2024-05-23 21:52:49 [INFO]: Epoch 038 - generator training loss: 0.2814, discriminator training loss: 0.0465, validation loss: 0.3534
2024-05-23 21:53:08 [INFO]: Epoch 039 - generator training loss: 0.2764, discriminator training loss: 0.0461, validation loss: 0.3569
2024-05-23 21:53:27 [INFO]: Epoch 040 - generator training loss: 0.2729, discriminator training loss: 0.0457, validation loss: 0.3549
2024-05-23 21:53:46 [INFO]: Epoch 041 - generator training loss: 0.2739, discriminator training loss: 0.0455, validation loss: 0.3607
2024-05-23 21:54:05 [INFO]: Epoch 042 - generator training loss: 0.2796, discriminator training loss: 0.0452, validation loss: 0.3491
2024-05-23 21:54:23 [INFO]: Epoch 043 - generator training loss: 0.2707, discriminator training loss: 0.0449, validation loss: 0.3589
2024-05-23 21:54:42 [INFO]: Epoch 044 - generator training loss: 0.2653, discriminator training loss: 0.0447, validation loss: 0.3439
2024-05-23 21:55:01 [INFO]: Epoch 045 - generator training loss: 0.2629, discriminator training loss: 0.0443, validation loss: 0.3468
2024-05-23 21:55:19 [INFO]: Epoch 046 - generator training loss: 0.2610, discriminator training loss: 0.0442, validation loss: 0.3490
2024-05-23 21:55:38 [INFO]: Epoch 047 - generator training loss: 0.2586, discriminator training loss: 0.0439, validation loss: 0.3467
2024-05-23 21:55:57 [INFO]: Epoch 048 - generator training loss: 0.2598, discriminator training loss: 0.0437, validation loss: 0.3516
2024-05-23 21:56:16 [INFO]: Epoch 049 - generator training loss: 0.2516, discriminator training loss: 0.0433, validation loss: 0.3397
2024-05-23 21:56:34 [INFO]: Epoch 050 - generator training loss: 0.2493, discriminator training loss: 0.0430, validation loss: 0.3435
2024-05-23 21:56:53 [INFO]: Epoch 051 - generator training loss: 0.2500, discriminator training loss: 0.0429, validation loss: 0.3420
2024-05-23 21:57:12 [INFO]: Epoch 052 - generator training loss: 0.2518, discriminator training loss: 0.0429, validation loss: 0.3469
2024-05-23 21:57:31 [INFO]: Epoch 053 - generator training loss: 0.2430, discriminator training loss: 0.0425, validation loss: 0.3424
2024-05-23 21:57:50 [INFO]: Epoch 054 - generator training loss: 0.2388, discriminator training loss: 0.0421, validation loss: 0.3396
2024-05-23 21:58:08 [INFO]: Epoch 055 - generator training loss: 0.2362, discriminator training loss: 0.0422, validation loss: 0.3417
2024-05-23 21:58:27 [INFO]: Epoch 056 - generator training loss: 0.2377, discriminator training loss: 0.0422, validation loss: 0.3392
2024-05-23 21:58:46 [INFO]: Epoch 057 - generator training loss: 0.2337, discriminator training loss: 0.0420, validation loss: 0.3405
2024-05-23 21:59:05 [INFO]: Epoch 058 - generator training loss: 0.2305, discriminator training loss: 0.0420, validation loss: 0.3402
2024-05-23 21:59:23 [INFO]: Epoch 059 - generator training loss: 0.2276, discriminator training loss: 0.0418, validation loss: 0.3395
2024-05-23 21:59:42 [INFO]: Epoch 060 - generator training loss: 0.2237, discriminator training loss: 0.0416, validation loss: 0.3399
2024-05-23 22:00:01 [INFO]: Epoch 061 - generator training loss: 0.2211, discriminator training loss: 0.0416, validation loss: 0.3390
2024-05-23 22:00:20 [INFO]: Epoch 062 - generator training loss: 0.2230, discriminator training loss: 0.0415, validation loss: 0.3443
2024-05-23 22:00:38 [INFO]: Epoch 063 - generator training loss: 0.2217, discriminator training loss: 0.0414, validation loss: 0.3396
2024-05-23 22:00:57 [INFO]: Epoch 064 - generator training loss: 0.2200, discriminator training loss: 0.0413, validation loss: 0.3424
2024-05-23 22:01:16 [INFO]: Epoch 065 - generator training loss: 0.2224, discriminator training loss: 0.0415, validation loss: 0.3384
2024-05-23 22:01:35 [INFO]: Epoch 066 - generator training loss: 0.2134, discriminator training loss: 0.0411, validation loss: 0.3405
2024-05-23 22:01:53 [INFO]: Epoch 067 - generator training loss: 0.2095, discriminator training loss: 0.0409, validation loss: 0.3392
2024-05-23 22:02:12 [INFO]: Epoch 068 - generator training loss: 0.2105, discriminator training loss: 0.0409, validation loss: 0.3429
2024-05-23 22:02:31 [INFO]: Epoch 069 - generator training loss: 0.2113, discriminator training loss: 0.0408, validation loss: 0.3420
2024-05-23 22:02:50 [INFO]: Epoch 070 - generator training loss: 0.2092, discriminator training loss: 0.0407, validation loss: 0.3440
2024-05-23 22:03:08 [INFO]: Epoch 071 - generator training loss: 0.2051, discriminator training loss: 0.0405, validation loss: 0.3409
2024-05-23 22:03:27 [INFO]: Epoch 072 - generator training loss: 0.2028, discriminator training loss: 0.0405, validation loss: 0.3436
2024-05-23 22:03:46 [INFO]: Epoch 073 - generator training loss: 0.2025, discriminator training loss: 0.0404, validation loss: 0.3422
2024-05-23 22:04:05 [INFO]: Epoch 074 - generator training loss: 0.2094, discriminator training loss: 0.0404, validation loss: 0.3494
2024-05-23 22:04:24 [INFO]: Epoch 075 - generator training loss: 0.2101, discriminator training loss: 0.0403, validation loss: 0.3428
2024-05-23 22:04:24 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-05-23 22:04:24 [INFO]: Finished training. The best model is from epoch#65.
2024-05-23 22:04:24 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_1/USGAN_physionet_2012_seta/20240523_T214045/USGAN.pypots
2024-05-23 22:04:26 [INFO]: US-GAN on PhysioNet-2012-seta: MAE=0.2948, MSE=0.2645
2024-05-23 22:04:36 [INFO]: Successfully saved to augmentation_premask_post_norm_saved_results/round_1/USGAN_physionet_2012_seta/imputation.pkl
2024-05-23 22:04:36 [INFO]: Using the given device: cuda:0
2024-05-23 22:04:36 [INFO]: Model files will be saved to augmentation_premask_post_norm_saved_results/round_1/BRITS_physionet_2012_seta/20240523_T220436
2024-05-23 22:04:36 [INFO]: Tensorboard file will be saved to augmentation_premask_post_norm_saved_results/round_1/BRITS_physionet_2012_seta/20240523_T220436/tensorboard
2024-05-23 22:04:36 [INFO]: BRITS initialized with the given hyperparameters, the number of trainable parameters: 9,176,048
2024-05-23 22:04:51 [INFO]: Epoch 001 - training loss: 1.1456, validation loss: 0.5580
2024-05-23 22:05:04 [INFO]: Epoch 002 - training loss: 0.9351, validation loss: 0.5014
2024-05-23 22:05:16 [INFO]: Epoch 003 - training loss: 0.8723, validation loss: 0.4707
2024-05-23 22:05:28 [INFO]: Epoch 004 - training loss: 0.8345, validation loss: 0.4468
2024-05-23 22:05:41 [INFO]: Epoch 005 - training loss: 0.8073, validation loss: 0.4314
2024-05-23 22:05:53 [INFO]: Epoch 006 - training loss: 0.7855, validation loss: 0.4173
2024-05-23 22:06:05 [INFO]: Epoch 007 - training loss: 0.7664, validation loss: 0.4081
2024-05-23 22:06:18 [INFO]: Epoch 008 - training loss: 0.7498, validation loss: 0.3998
2024-05-23 22:06:30 [INFO]: Epoch 009 - training loss: 0.7365, validation loss: 0.3943
2024-05-23 22:06:43 [INFO]: Epoch 010 - training loss: 0.7244, validation loss: 0.3878
2024-05-23 22:06:55 [INFO]: Epoch 011 - training loss: 0.7137, validation loss: 0.3859
2024-05-23 22:07:07 [INFO]: Epoch 012 - training loss: 0.7051, validation loss: 0.3844
2024-05-23 22:07:19 [INFO]: Epoch 013 - training loss: 0.6973, validation loss: 0.3820
2024-05-23 22:07:32 [INFO]: Epoch 014 - training loss: 0.6899, validation loss: 0.3816
2024-05-23 22:07:44 [INFO]: Epoch 015 - training loss: 0.6840, validation loss: 0.3772
2024-05-23 22:07:56 [INFO]: Epoch 016 - training loss: 0.6788, validation loss: 0.3793
2024-05-23 22:08:08 [INFO]: Epoch 017 - training loss: 0.6734, validation loss: 0.3782
2024-05-23 22:08:21 [INFO]: Epoch 018 - training loss: 0.6696, validation loss: 0.3782
2024-05-23 22:08:33 [INFO]: Epoch 019 - training loss: 0.6642, validation loss: 0.3767
2024-05-23 22:08:45 [INFO]: Epoch 020 - training loss: 0.6663, validation loss: 0.3769
2024-05-23 22:08:57 [INFO]: Epoch 021 - training loss: 0.6614, validation loss: 0.3766
2024-05-23 22:09:10 [INFO]: Epoch 022 - training loss: 0.6557, validation loss: 0.3748
2024-05-23 22:09:22 [INFO]: Epoch 023 - training loss: 0.6511, validation loss: 0.3750
2024-05-23 22:09:34 [INFO]: Epoch 024 - training loss: 0.6490, validation loss: 0.3752
2024-05-23 22:09:46 [INFO]: Epoch 025 - training loss: 0.6461, validation loss: 0.3730
2024-05-23 22:09:59 [INFO]: Epoch 026 - training loss: 0.6424, validation loss: 0.3729
2024-05-23 22:10:11 [INFO]: Epoch 027 - training loss: 0.6426, validation loss: 0.3725
2024-05-23 22:10:23 [INFO]: Epoch 028 - training loss: 0.6370, validation loss: 0.3701
2024-05-23 22:10:36 [INFO]: Epoch 029 - training loss: 0.6342, validation loss: 0.3720
2024-05-23 22:10:48 [INFO]: Epoch 030 - training loss: 0.6319, validation loss: 0.3716
2024-05-23 22:11:00 [INFO]: Epoch 031 - training loss: 0.6309, validation loss: 0.3721
2024-05-23 22:11:12 [INFO]: Epoch 032 - training loss: 0.6265, validation loss: 0.3702
2024-05-23 22:11:25 [INFO]: Epoch 033 - training loss: 0.6235, validation loss: 0.3715
2024-05-23 22:11:37 [INFO]: Epoch 034 - training loss: 0.6220, validation loss: 0.3719
2024-05-23 22:11:49 [INFO]: Epoch 035 - training loss: 0.6218, validation loss: 0.3702
2024-05-23 22:12:01 [INFO]: Epoch 036 - training loss: 0.6177, validation loss: 0.3723
2024-05-23 22:12:14 [INFO]: Epoch 037 - training loss: 0.6177, validation loss: 0.3732
2024-05-23 22:12:26 [INFO]: Epoch 038 - training loss: 0.6147, validation loss: 0.3727
2024-05-23 22:12:26 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-05-23 22:12:26 [INFO]: Finished training. The best model is from epoch#28.
2024-05-23 22:12:26 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_1/BRITS_physionet_2012_seta/20240523_T220436/BRITS.pypots
2024-05-23 22:12:28 [INFO]: BRITS on PhysioNet-2012-seta: MAE=0.2594, MSE=0.2573
2024-05-23 22:12:38 [INFO]: Successfully saved to augmentation_premask_post_norm_saved_results/round_1/BRITS_physionet_2012_seta/imputation.pkl
2024-05-23 22:12:38 [INFO]: Using the given device: cuda:0
2024-05-23 22:12:38 [INFO]: Model files will be saved to augmentation_premask_post_norm_saved_results/round_1/MRNN_physionet_2012_seta/20240523_T221238
2024-05-23 22:12:38 [INFO]: Tensorboard file will be saved to augmentation_premask_post_norm_saved_results/round_1/MRNN_physionet_2012_seta/20240523_T221238/tensorboard
2024-05-23 22:12:38 [INFO]: MRNN initialized with the given hyperparameters, the number of trainable parameters: 7,599
2024-05-23 22:12:44 [INFO]: Epoch 001 - training loss: 1.1724, validation loss: 0.9974
2024-05-23 22:12:44 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_1/MRNN_physionet_2012_seta/20240523_T221238/MRNN_epoch1_loss0.9974340498447418.pypots
2024-05-23 22:12:47 [INFO]: Epoch 002 - training loss: 0.7151, validation loss: 0.9730
2024-05-23 22:12:47 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_1/MRNN_physionet_2012_seta/20240523_T221238/MRNN_epoch2_loss0.9730372220277786.pypots
2024-05-23 22:12:50 [INFO]: Epoch 003 - training loss: 0.5949, validation loss: 0.9489
2024-05-23 22:12:50 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_1/MRNN_physionet_2012_seta/20240523_T221238/MRNN_epoch3_loss0.9488742411136627.pypots
2024-05-23 22:12:52 [INFO]: Epoch 004 - training loss: 0.5557, validation loss: 0.9380
2024-05-23 22:12:52 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_1/MRNN_physionet_2012_seta/20240523_T221238/MRNN_epoch4_loss0.9379984647035599.pypots
2024-05-23 22:12:55 [INFO]: Epoch 005 - training loss: 0.5344, validation loss: 0.9305
2024-05-23 22:12:55 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_1/MRNN_physionet_2012_seta/20240523_T221238/MRNN_epoch5_loss0.9305228590965271.pypots
2024-05-23 22:12:58 [INFO]: Epoch 006 - training loss: 0.5125, validation loss: 0.9276
2024-05-23 22:12:58 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_1/MRNN_physionet_2012_seta/20240523_T221238/MRNN_epoch6_loss0.9275795727968216.pypots
2024-05-23 22:13:01 [INFO]: Epoch 007 - training loss: 0.4983, validation loss: 0.9249
2024-05-23 22:13:01 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_1/MRNN_physionet_2012_seta/20240523_T221238/MRNN_epoch7_loss0.9249159842729568.pypots
2024-05-23 22:13:04 [INFO]: Epoch 008 - training loss: 0.4900, validation loss: 0.9226
2024-05-23 22:13:04 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_1/MRNN_physionet_2012_seta/20240523_T221238/MRNN_epoch8_loss0.9225645691156388.pypots
2024-05-23 22:13:06 [INFO]: Epoch 009 - training loss: 0.4842, validation loss: 0.9227
2024-05-23 22:13:06 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_1/MRNN_physionet_2012_seta/20240523_T221238/MRNN_epoch9_loss0.9226585060358048.pypots
2024-05-23 22:13:09 [INFO]: Epoch 010 - training loss: 0.4770, validation loss: 0.9222
2024-05-23 22:13:09 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_1/MRNN_physionet_2012_seta/20240523_T221238/MRNN_epoch10_loss0.9221542328596115.pypots
2024-05-23 22:13:12 [INFO]: Epoch 011 - training loss: 0.4733, validation loss: 0.9225
2024-05-23 22:13:12 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_1/MRNN_physionet_2012_seta/20240523_T221238/MRNN_epoch11_loss0.9225457191467286.pypots
2024-05-23 22:13:15 [INFO]: Epoch 012 - training loss: 0.4750, validation loss: 0.9227
2024-05-23 22:13:15 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_1/MRNN_physionet_2012_seta/20240523_T221238/MRNN_epoch12_loss0.9226988255977631.pypots
2024-05-23 22:13:18 [INFO]: Epoch 013 - training loss: 0.4538, validation loss: 0.9236
2024-05-23 22:13:18 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_1/MRNN_physionet_2012_seta/20240523_T221238/MRNN_epoch13_loss0.9235566616058349.pypots
2024-05-23 22:13:20 [INFO]: Epoch 014 - training loss: 0.4520, validation loss: 0.9258
2024-05-23 22:13:20 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_1/MRNN_physionet_2012_seta/20240523_T221238/MRNN_epoch14_loss0.9257976055145264.pypots
2024-05-23 22:13:23 [INFO]: Epoch 015 - training loss: 0.4553, validation loss: 0.9293
2024-05-23 22:13:23 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_1/MRNN_physionet_2012_seta/20240523_T221238/MRNN_epoch15_loss0.9292713701725006.pypots
2024-05-23 22:13:26 [INFO]: Epoch 016 - training loss: 0.4416, validation loss: 0.9296
2024-05-23 22:13:26 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_1/MRNN_physionet_2012_seta/20240523_T221238/MRNN_epoch16_loss0.9295557647943496.pypots
2024-05-23 22:13:29 [INFO]: Epoch 017 - training loss: 0.4452, validation loss: 0.9325
2024-05-23 22:13:29 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_1/MRNN_physionet_2012_seta/20240523_T221238/MRNN_epoch17_loss0.9324929028749466.pypots
2024-05-23 22:13:32 [INFO]: Epoch 018 - training loss: 0.4406, validation loss: 0.9346
2024-05-23 22:13:32 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_1/MRNN_physionet_2012_seta/20240523_T221238/MRNN_epoch18_loss0.9346256643533707.pypots
2024-05-23 22:13:34 [INFO]: Epoch 019 - training loss: 0.4405, validation loss: 0.9353
2024-05-23 22:13:34 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_1/MRNN_physionet_2012_seta/20240523_T221238/MRNN_epoch19_loss0.9352604955434799.pypots
2024-05-23 22:13:37 [INFO]: Epoch 020 - training loss: 0.4373, validation loss: 0.9384
2024-05-23 22:13:37 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_1/MRNN_physionet_2012_seta/20240523_T221238/MRNN_epoch20_loss0.9384088575839996.pypots
2024-05-23 22:13:37 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-05-23 22:13:37 [INFO]: Finished training. The best model is from epoch#10.
2024-05-23 22:13:37 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_1/MRNN_physionet_2012_seta/20240523_T221238/MRNN.pypots
2024-05-23 22:13:38 [INFO]: MRNN on PhysioNet-2012-seta: MAE=0.6912, MSE=0.9033
2024-05-23 22:13:42 [INFO]: Successfully saved to augmentation_premask_post_norm_saved_results/round_1/MRNN_physionet_2012_seta/imputation.pkl
2024-05-23 22:13:42 [INFO]: Using the given device: cpu
2024-05-23 22:13:42 [INFO]: LOCF on PhysioNet-2012-seta: MAE=0.4038, MSE=0.5072
2024-05-23 22:13:43 [INFO]: Successfully created the given path "saved_results/round_1/LOCF_physionet_2012_seta".
2024-05-23 22:13:43 [INFO]: Successfully saved to augmentation_premask_post_norm_saved_results/round_1/LOCF_physionet_2012_seta/imputation.pkl
2024-05-23 22:13:43 [INFO]: Median on PhysioNet-2012-seta: MAE=0.6908, MSE=1.0216
2024-05-23 22:13:43 [INFO]: Successfully created the given path "saved_results/round_1/Median_physionet_2012_seta".
2024-05-23 22:13:43 [INFO]: Successfully saved to augmentation_premask_post_norm_saved_results/round_1/Median_physionet_2012_seta/imputation.pkl
2024-05-23 22:13:43 [INFO]: Mean on PhysioNet-2012-seta: MAE=0.7063, MSE=0.9786
2024-05-23 22:13:43 [INFO]: Successfully created the given path "saved_results/round_1/Mean_physionet_2012_seta".
2024-05-23 22:13:43 [INFO]: Successfully saved to augmentation_premask_post_norm_saved_results/round_1/Mean_physionet_2012_seta/imputation.pkl
2024-05-23 22:13:43 [INFO]: Have set the random seed as 2025 for numpy and pytorch.
2024-05-23 22:13:43 [INFO]: Using the given device: cuda:0
2024-05-23 22:13:43 [INFO]: Model files will be saved to augmentation_premask_post_norm_saved_results/round_2/SAITS_physionet_2012_seta/20240523_T221343
2024-05-23 22:13:43 [INFO]: Tensorboard file will be saved to augmentation_premask_post_norm_saved_results/round_2/SAITS_physionet_2012_seta/20240523_T221343/tensorboard
2024-05-23 22:13:43 [INFO]: SAITS initialized with the given hyperparameters, the number of trainable parameters: 5,322,806
2024-05-23 22:13:44 [INFO]: Epoch 001 - training loss: 1.0747, validation loss: 0.5152
2024-05-23 22:13:45 [INFO]: Epoch 002 - training loss: 0.6842, validation loss: 0.4516
2024-05-23 22:13:46 [INFO]: Epoch 003 - training loss: 0.5683, validation loss: 0.4267
2024-05-23 22:13:48 [INFO]: Epoch 004 - training loss: 0.5126, validation loss: 0.4028
2024-05-23 22:13:49 [INFO]: Epoch 005 - training loss: 0.4732, validation loss: 0.3899
2024-05-23 22:13:50 [INFO]: Epoch 006 - training loss: 0.4375, validation loss: 0.3822
2024-05-23 22:13:51 [INFO]: Epoch 007 - training loss: 0.4168, validation loss: 0.3792
2024-05-23 22:13:52 [INFO]: Epoch 008 - training loss: 0.3975, validation loss: 0.3604
2024-05-23 22:13:53 [INFO]: Epoch 009 - training loss: 0.3697, validation loss: 0.3634
2024-05-23 22:13:55 [INFO]: Epoch 010 - training loss: 0.3511, validation loss: 0.3577
2024-05-23 22:13:56 [INFO]: Epoch 011 - training loss: 0.3433, validation loss: 0.3514
2024-05-23 22:13:57 [INFO]: Epoch 012 - training loss: 0.3204, validation loss: 0.3466
2024-05-23 22:13:58 [INFO]: Epoch 013 - training loss: 0.3079, validation loss: 0.3395
2024-05-23 22:13:59 [INFO]: Epoch 014 - training loss: 0.2949, validation loss: 0.3419
2024-05-23 22:14:00 [INFO]: Epoch 015 - training loss: 0.2828, validation loss: 0.3380
2024-05-23 22:14:02 [INFO]: Epoch 016 - training loss: 0.2757, validation loss: 0.3431
2024-05-23 22:14:03 [INFO]: Epoch 017 - training loss: 0.2696, validation loss: 0.3417
2024-05-23 22:14:04 [INFO]: Epoch 018 - training loss: 0.2586, validation loss: 0.3401
2024-05-23 22:14:05 [INFO]: Epoch 019 - training loss: 0.2528, validation loss: 0.3378
2024-05-23 22:14:06 [INFO]: Epoch 020 - training loss: 0.2423, validation loss: 0.3373
2024-05-23 22:14:08 [INFO]: Epoch 021 - training loss: 0.2344, validation loss: 0.3338
2024-05-23 22:14:09 [INFO]: Epoch 022 - training loss: 0.2342, validation loss: 0.3353
2024-05-23 22:14:10 [INFO]: Epoch 023 - training loss: 0.2270, validation loss: 0.3356
2024-05-23 22:14:11 [INFO]: Epoch 024 - training loss: 0.2217, validation loss: 0.3353
2024-05-23 22:14:12 [INFO]: Epoch 025 - training loss: 0.2139, validation loss: 0.3395
2024-05-23 22:14:13 [INFO]: Epoch 026 - training loss: 0.2095, validation loss: 0.3325
2024-05-23 22:14:15 [INFO]: Epoch 027 - training loss: 0.2022, validation loss: 0.3296
2024-05-23 22:14:16 [INFO]: Epoch 028 - training loss: 0.2010, validation loss: 0.3353
2024-05-23 22:14:17 [INFO]: Epoch 029 - training loss: 0.1982, validation loss: 0.3316
2024-05-23 22:14:18 [INFO]: Epoch 030 - training loss: 0.1948, validation loss: 0.3339
2024-05-23 22:14:19 [INFO]: Epoch 031 - training loss: 0.1890, validation loss: 0.3292
2024-05-23 22:14:20 [INFO]: Epoch 032 - training loss: 0.1850, validation loss: 0.3317
2024-05-23 22:14:22 [INFO]: Epoch 033 - training loss: 0.1845, validation loss: 0.3318
2024-05-23 22:14:23 [INFO]: Epoch 034 - training loss: 0.1785, validation loss: 0.3318
2024-05-23 22:14:24 [INFO]: Epoch 035 - training loss: 0.1775, validation loss: 0.3289
2024-05-23 22:14:25 [INFO]: Epoch 036 - training loss: 0.1745, validation loss: 0.3344
2024-05-23 22:14:26 [INFO]: Epoch 037 - training loss: 0.1711, validation loss: 0.3276
2024-05-23 22:14:27 [INFO]: Epoch 038 - training loss: 0.1676, validation loss: 0.3283
2024-05-23 22:14:29 [INFO]: Epoch 039 - training loss: 0.1641, validation loss: 0.3325
2024-05-23 22:14:30 [INFO]: Epoch 040 - training loss: 0.1628, validation loss: 0.3290
2024-05-23 22:14:31 [INFO]: Epoch 041 - training loss: 0.1608, validation loss: 0.3298
2024-05-23 22:14:32 [INFO]: Epoch 042 - training loss: 0.1593, validation loss: 0.3293
2024-05-23 22:14:34 [INFO]: Epoch 043 - training loss: 0.1586, validation loss: 0.3308
2024-05-23 22:14:35 [INFO]: Epoch 044 - training loss: 0.1539, validation loss: 0.3259
2024-05-23 22:14:36 [INFO]: Epoch 045 - training loss: 0.1529, validation loss: 0.3268
2024-05-23 22:14:37 [INFO]: Epoch 046 - training loss: 0.1490, validation loss: 0.3334
2024-05-23 22:14:38 [INFO]: Epoch 047 - training loss: 0.1502, validation loss: 0.3312
2024-05-23 22:14:40 [INFO]: Epoch 048 - training loss: 0.1492, validation loss: 0.3215
2024-05-23 22:14:41 [INFO]: Epoch 049 - training loss: 0.1452, validation loss: 0.3291
2024-05-23 22:14:42 [INFO]: Epoch 050 - training loss: 0.1431, validation loss: 0.3291
2024-05-23 22:14:43 [INFO]: Epoch 051 - training loss: 0.1438, validation loss: 0.3318
2024-05-23 22:14:44 [INFO]: Epoch 052 - training loss: 0.1427, validation loss: 0.3253
2024-05-23 22:14:45 [INFO]: Epoch 053 - training loss: 0.1412, validation loss: 0.3357
2024-05-23 22:14:47 [INFO]: Epoch 054 - training loss: 0.1392, validation loss: 0.3285
2024-05-23 22:14:48 [INFO]: Epoch 055 - training loss: 0.1366, validation loss: 0.3265
2024-05-23 22:14:49 [INFO]: Epoch 056 - training loss: 0.1349, validation loss: 0.3274
2024-05-23 22:14:50 [INFO]: Epoch 057 - training loss: 0.1354, validation loss: 0.3313
2024-05-23 22:14:51 [INFO]: Epoch 058 - training loss: 0.1325, validation loss: 0.3220
2024-05-23 22:14:51 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-05-23 22:14:51 [INFO]: Finished training. The best model is from epoch#48.
2024-05-23 22:14:51 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_2/SAITS_physionet_2012_seta/20240523_T221343/SAITS.pypots
2024-05-23 22:14:51 [INFO]: SAITS on PhysioNet-2012-seta: MAE=0.2605, MSE=0.2897
2024-05-23 22:14:52 [INFO]: Successfully saved to augmentation_premask_post_norm_saved_results/round_2/SAITS_physionet_2012_seta/imputation.pkl
2024-05-23 22:14:52 [INFO]: Using the given device: cuda:0
2024-05-23 22:14:52 [INFO]: Model files will be saved to augmentation_premask_post_norm_saved_results/round_2/Transformer_physionet_2012_seta/20240523_T221452
2024-05-23 22:14:52 [INFO]: Tensorboard file will be saved to augmentation_premask_post_norm_saved_results/round_2/Transformer_physionet_2012_seta/20240523_T221452/tensorboard
2024-05-23 22:14:52 [INFO]: Transformer initialized with the given hyperparameters, the number of trainable parameters: 2,152,613
2024-05-23 22:14:52 [INFO]: Epoch 001 - training loss: 1.2019, validation loss: 0.6005
2024-05-23 22:14:53 [INFO]: Epoch 002 - training loss: 0.7617, validation loss: 0.5213
2024-05-23 22:14:54 [INFO]: Epoch 003 - training loss: 0.6480, validation loss: 0.4803
2024-05-23 22:14:54 [INFO]: Epoch 004 - training loss: 0.5858, validation loss: 0.4573
2024-05-23 22:14:55 [INFO]: Epoch 005 - training loss: 0.5500, validation loss: 0.4455
2024-05-23 22:14:55 [INFO]: Epoch 006 - training loss: 0.5080, validation loss: 0.4254
2024-05-23 22:14:56 [INFO]: Epoch 007 - training loss: 0.4770, validation loss: 0.4129
2024-05-23 22:14:57 [INFO]: Epoch 008 - training loss: 0.4570, validation loss: 0.4074
2024-05-23 22:14:57 [INFO]: Epoch 009 - training loss: 0.4366, validation loss: 0.4016
2024-05-23 22:14:58 [INFO]: Epoch 010 - training loss: 0.4231, validation loss: 0.3975
2024-05-23 22:14:58 [INFO]: Epoch 011 - training loss: 0.4110, validation loss: 0.3948
2024-05-23 22:14:59 [INFO]: Epoch 012 - training loss: 0.3939, validation loss: 0.3798
2024-05-23 22:14:59 [INFO]: Epoch 013 - training loss: 0.3915, validation loss: 0.3791
2024-05-23 22:15:00 [INFO]: Epoch 014 - training loss: 0.3734, validation loss: 0.3800
2024-05-23 22:15:01 [INFO]: Epoch 015 - training loss: 0.3644, validation loss: 0.3761
2024-05-23 22:15:01 [INFO]: Epoch 016 - training loss: 0.3537, validation loss: 0.3688
2024-05-23 22:15:02 [INFO]: Epoch 017 - training loss: 0.3380, validation loss: 0.3683
2024-05-23 22:15:02 [INFO]: Epoch 018 - training loss: 0.3344, validation loss: 0.3693
2024-05-23 22:15:03 [INFO]: Epoch 019 - training loss: 0.3275, validation loss: 0.3615
2024-05-23 22:15:04 [INFO]: Epoch 020 - training loss: 0.3243, validation loss: 0.3681
2024-05-23 22:15:04 [INFO]: Epoch 021 - training loss: 0.3183, validation loss: 0.3649
2024-05-23 22:15:05 [INFO]: Epoch 022 - training loss: 0.3061, validation loss: 0.3623
2024-05-23 22:15:05 [INFO]: Epoch 023 - training loss: 0.2992, validation loss: 0.3659
2024-05-23 22:15:06 [INFO]: Epoch 024 - training loss: 0.2959, validation loss: 0.3592
2024-05-23 22:15:07 [INFO]: Epoch 025 - training loss: 0.2937, validation loss: 0.3581
2024-05-23 22:15:07 [INFO]: Epoch 026 - training loss: 0.2887, validation loss: 0.3629
2024-05-23 22:15:08 [INFO]: Epoch 027 - training loss: 0.2813, validation loss: 0.3569
2024-05-23 22:15:08 [INFO]: Epoch 028 - training loss: 0.2771, validation loss: 0.3657
2024-05-23 22:15:09 [INFO]: Epoch 029 - training loss: 0.2709, validation loss: 0.3553
2024-05-23 22:15:10 [INFO]: Epoch 030 - training loss: 0.2651, validation loss: 0.3564
2024-05-23 22:15:10 [INFO]: Epoch 031 - training loss: 0.2619, validation loss: 0.3510
2024-05-23 22:15:11 [INFO]: Epoch 032 - training loss: 0.2568, validation loss: 0.3578
2024-05-23 22:15:12 [INFO]: Epoch 033 - training loss: 0.2559, validation loss: 0.3593
2024-05-23 22:15:12 [INFO]: Epoch 034 - training loss: 0.2513, validation loss: 0.3582
2024-05-23 22:15:13 [INFO]: Epoch 035 - training loss: 0.2473, validation loss: 0.3590
2024-05-23 22:15:13 [INFO]: Epoch 036 - training loss: 0.2423, validation loss: 0.3552
2024-05-23 22:15:14 [INFO]: Epoch 037 - training loss: 0.2410, validation loss: 0.3574
2024-05-23 22:15:15 [INFO]: Epoch 038 - training loss: 0.2362, validation loss: 0.3591
2024-05-23 22:15:15 [INFO]: Epoch 039 - training loss: 0.2354, validation loss: 0.3563
2024-05-23 22:15:16 [INFO]: Epoch 040 - training loss: 0.2313, validation loss: 0.3613
2024-05-23 22:15:16 [INFO]: Epoch 041 - training loss: 0.2292, validation loss: 0.3578
2024-05-23 22:15:16 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-05-23 22:15:16 [INFO]: Finished training. The best model is from epoch#31.
2024-05-23 22:15:16 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_2/Transformer_physionet_2012_seta/20240523_T221452/Transformer.pypots
2024-05-23 22:15:16 [INFO]: Transformer on PhysioNet-2012-seta: MAE=0.2819, MSE=0.3041
2024-05-23 22:15:17 [INFO]: Successfully saved to augmentation_premask_post_norm_saved_results/round_2/Transformer_physionet_2012_seta/imputation.pkl
2024-05-23 22:15:17 [INFO]: Using the given device: cuda:0
2024-05-23 22:15:17 [INFO]: Model files will be saved to augmentation_premask_post_norm_saved_results/round_2/TimesNet_physionet_2012_seta/20240523_T221517
2024-05-23 22:15:17 [INFO]: Tensorboard file will be saved to augmentation_premask_post_norm_saved_results/round_2/TimesNet_physionet_2012_seta/20240523_T221517/tensorboard
2024-05-23 22:15:17 [INFO]: TimesNet initialized with the given hyperparameters, the number of trainable parameters: 21,649,573
2024-05-23 22:15:17 [INFO]: Epoch 001 - training loss: 0.4644, validation loss: 1.6332
2024-05-23 22:15:18 [INFO]: Epoch 002 - training loss: 0.6479, validation loss: 0.5387
2024-05-23 22:15:19 [INFO]: Epoch 003 - training loss: 0.3787, validation loss: 0.3607
2024-05-23 22:15:20 [INFO]: Epoch 004 - training loss: 0.4526, validation loss: 0.6586
2024-05-23 22:15:20 [INFO]: Epoch 005 - training loss: 0.4147, validation loss: 0.4359
2024-05-23 22:15:21 [INFO]: Epoch 006 - training loss: 0.3583, validation loss: 0.3391
2024-05-23 22:15:22 [INFO]: Epoch 007 - training loss: 0.3107, validation loss: 0.3330
2024-05-23 22:15:22 [INFO]: Epoch 008 - training loss: 0.3010, validation loss: 0.3278
2024-05-23 22:15:23 [INFO]: Epoch 009 - training loss: 0.2998, validation loss: 0.3549
2024-05-23 22:15:24 [INFO]: Epoch 010 - training loss: 0.3007, validation loss: 0.3306
2024-05-23 22:15:25 [INFO]: Epoch 011 - training loss: 0.2896, validation loss: 0.3278
2024-05-23 22:15:25 [INFO]: Epoch 012 - training loss: 0.2813, validation loss: 0.3196
2024-05-23 22:15:26 [INFO]: Epoch 013 - training loss: 0.2769, validation loss: 0.3266
2024-05-23 22:15:27 [INFO]: Epoch 014 - training loss: 0.2853, validation loss: 0.3195
2024-05-23 22:15:27 [INFO]: Epoch 015 - training loss: 0.2863, validation loss: 0.3302
2024-05-23 22:15:28 [INFO]: Epoch 016 - training loss: 0.2660, validation loss: 0.3371
2024-05-23 22:15:29 [INFO]: Epoch 017 - training loss: 0.2663, validation loss: 0.3342
2024-05-23 22:15:29 [INFO]: Epoch 018 - training loss: 0.2604, validation loss: 0.3313
2024-05-23 22:15:30 [INFO]: Epoch 019 - training loss: 0.2534, validation loss: 0.3298
2024-05-23 22:15:31 [INFO]: Epoch 020 - training loss: 0.2481, validation loss: 0.3410
2024-05-23 22:15:32 [INFO]: Epoch 021 - training loss: 0.2435, validation loss: 0.3348
2024-05-23 22:15:32 [INFO]: Epoch 022 - training loss: 0.2457, validation loss: 0.3395
2024-05-23 22:15:33 [INFO]: Epoch 023 - training loss: 0.2390, validation loss: 0.3476
2024-05-23 22:15:34 [INFO]: Epoch 024 - training loss: 0.2450, validation loss: 0.3205
2024-05-23 22:15:34 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-05-23 22:15:34 [INFO]: Finished training. The best model is from epoch#14.
2024-05-23 22:15:34 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_2/TimesNet_physionet_2012_seta/20240523_T221517/TimesNet.pypots
2024-05-23 22:15:34 [INFO]: TimesNet on PhysioNet-2012-seta: MAE=0.2914, MSE=0.2834
2024-05-23 22:15:34 [INFO]: Successfully saved to augmentation_premask_post_norm_saved_results/round_2/TimesNet_physionet_2012_seta/imputation.pkl
2024-05-23 22:15:34 [INFO]: Using the given device: cuda:0
2024-05-23 22:15:34 [INFO]: Model files will be saved to augmentation_premask_post_norm_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T221534
2024-05-23 22:15:34 [INFO]: Tensorboard file will be saved to augmentation_premask_post_norm_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T221534/tensorboard
2024-05-23 22:15:34 [INFO]: CSDI initialized with the given hyperparameters, the number of trainable parameters: 1,694,753
2024-05-23 22:16:17 [INFO]: Epoch 001 - training loss: 0.4097, validation loss: 0.3379
2024-05-23 22:16:17 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T221534/CSDI_epoch1_loss0.337942311167717.pypots
2024-05-23 22:17:01 [INFO]: Epoch 002 - training loss: 0.3216, validation loss: 0.3030
2024-05-23 22:17:01 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T221534/CSDI_epoch2_loss0.30296498239040376.pypots
2024-05-23 22:17:44 [INFO]: Epoch 003 - training loss: 0.2988, validation loss: 0.2757
2024-05-23 22:17:44 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T221534/CSDI_epoch3_loss0.275731073319912.pypots
2024-05-23 22:18:28 [INFO]: Epoch 004 - training loss: 0.2722, validation loss: 0.2551
2024-05-23 22:18:28 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T221534/CSDI_epoch4_loss0.2551033072173595.pypots
2024-05-23 22:19:11 [INFO]: Epoch 005 - training loss: 0.2686, validation loss: 0.2587
2024-05-23 22:19:11 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T221534/CSDI_epoch5_loss0.2586582377552986.pypots
2024-05-23 22:19:55 [INFO]: Epoch 006 - training loss: 0.2435, validation loss: 0.2441
2024-05-23 22:19:55 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T221534/CSDI_epoch6_loss0.24407302141189574.pypots
2024-05-23 22:20:39 [INFO]: Epoch 007 - training loss: 0.2440, validation loss: 0.2355
2024-05-23 22:20:39 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T221534/CSDI_epoch7_loss0.23546176999807358.pypots
2024-05-23 22:21:23 [INFO]: Epoch 008 - training loss: 0.2336, validation loss: 0.2295
2024-05-23 22:21:23 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T221534/CSDI_epoch8_loss0.22945289611816405.pypots
2024-05-23 22:22:06 [INFO]: Epoch 009 - training loss: 0.2200, validation loss: 0.2209
2024-05-23 22:22:06 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T221534/CSDI_epoch9_loss0.220936868339777.pypots
2024-05-23 22:22:50 [INFO]: Epoch 010 - training loss: 0.2215, validation loss: 0.2209
2024-05-23 22:22:50 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T221534/CSDI_epoch10_loss0.22089580669999123.pypots
2024-05-23 22:23:34 [INFO]: Epoch 011 - training loss: 0.2183, validation loss: 0.2195
2024-05-23 22:23:34 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T221534/CSDI_epoch11_loss0.21952405646443368.pypots
2024-05-23 22:24:18 [INFO]: Epoch 012 - training loss: 0.2131, validation loss: 0.2101
2024-05-23 22:24:18 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T221534/CSDI_epoch12_loss0.21007950007915496.pypots
2024-05-23 22:25:02 [INFO]: Epoch 013 - training loss: 0.2208, validation loss: 0.2202
2024-05-23 22:25:02 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T221534/CSDI_epoch13_loss0.22019992843270303.pypots
2024-05-23 22:25:46 [INFO]: Epoch 014 - training loss: 0.2212, validation loss: 0.2104
2024-05-23 22:25:46 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T221534/CSDI_epoch14_loss0.21039528995752335.pypots
2024-05-23 22:26:29 [INFO]: Epoch 015 - training loss: 0.2159, validation loss: 0.2142
2024-05-23 22:26:29 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T221534/CSDI_epoch15_loss0.21424990519881248.pypots
2024-05-23 22:27:13 [INFO]: Epoch 016 - training loss: 0.2119, validation loss: 0.2060
2024-05-23 22:27:13 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T221534/CSDI_epoch16_loss0.20604767128825188.pypots
2024-05-23 22:27:57 [INFO]: Epoch 017 - training loss: 0.2166, validation loss: 0.2103
2024-05-23 22:27:57 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T221534/CSDI_epoch17_loss0.21028155460953712.pypots
2024-05-23 22:28:41 [INFO]: Epoch 018 - training loss: 0.2122, validation loss: 0.2047
2024-05-23 22:28:41 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T221534/CSDI_epoch18_loss0.2046932600438595.pypots
2024-05-23 22:29:25 [INFO]: Epoch 019 - training loss: 0.2279, validation loss: 0.2062
2024-05-23 22:29:25 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T221534/CSDI_epoch19_loss0.20619141161441804.pypots
2024-05-23 22:30:08 [INFO]: Epoch 020 - training loss: 0.2191, validation loss: 0.2040
2024-05-23 22:30:08 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T221534/CSDI_epoch20_loss0.20400423109531401.pypots
2024-05-23 22:30:52 [INFO]: Epoch 021 - training loss: 0.2123, validation loss: 0.2032
2024-05-23 22:30:52 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T221534/CSDI_epoch21_loss0.20321100056171418.pypots
2024-05-23 22:31:36 [INFO]: Epoch 022 - training loss: 0.2068, validation loss: 0.1978
2024-05-23 22:31:36 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T221534/CSDI_epoch22_loss0.19775286689400673.pypots
2024-05-23 22:32:20 [INFO]: Epoch 023 - training loss: 0.2091, validation loss: 0.2014
2024-05-23 22:32:20 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T221534/CSDI_epoch23_loss0.2013527937233448.pypots
2024-05-23 22:33:03 [INFO]: Epoch 024 - training loss: 0.2055, validation loss: 0.1997
2024-05-23 22:33:03 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T221534/CSDI_epoch24_loss0.1997416913509369.pypots
2024-05-23 22:33:47 [INFO]: Epoch 025 - training loss: 0.2044, validation loss: 0.1990
2024-05-23 22:33:47 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T221534/CSDI_epoch25_loss0.19897871613502502.pypots
2024-05-23 22:34:31 [INFO]: Epoch 026 - training loss: 0.2040, validation loss: 0.2015
2024-05-23 22:34:31 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T221534/CSDI_epoch26_loss0.20147260501980782.pypots
2024-05-23 22:35:15 [INFO]: Epoch 027 - training loss: 0.2066, validation loss: 0.2008
2024-05-23 22:35:15 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T221534/CSDI_epoch27_loss0.2008018136024475.pypots
2024-05-23 22:35:58 [INFO]: Epoch 028 - training loss: 0.2021, validation loss: 0.1969
2024-05-23 22:35:59 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T221534/CSDI_epoch28_loss0.19689815863966942.pypots
2024-05-23 22:36:42 [INFO]: Epoch 029 - training loss: 0.2036, validation loss: 0.1957
2024-05-23 22:36:42 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T221534/CSDI_epoch29_loss0.1957422636449337.pypots
2024-05-23 22:37:26 [INFO]: Epoch 030 - training loss: 0.2097, validation loss: 0.2002
2024-05-23 22:37:26 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T221534/CSDI_epoch30_loss0.20016493573784827.pypots
2024-05-23 22:38:10 [INFO]: Epoch 031 - training loss: 0.2022, validation loss: 0.2010
2024-05-23 22:38:10 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T221534/CSDI_epoch31_loss0.20098231956362725.pypots
2024-05-23 22:38:53 [INFO]: Epoch 032 - training loss: 0.2026, validation loss: 0.2009
2024-05-23 22:38:53 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T221534/CSDI_epoch32_loss0.20088995322585107.pypots
2024-05-23 22:39:37 [INFO]: Epoch 033 - training loss: 0.2093, validation loss: 0.1972
2024-05-23 22:39:37 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T221534/CSDI_epoch33_loss0.19719572365283966.pypots
2024-05-23 22:40:21 [INFO]: Epoch 034 - training loss: 0.1931, validation loss: 0.1933
2024-05-23 22:40:21 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T221534/CSDI_epoch34_loss0.1933431640267372.pypots
2024-05-23 22:41:05 [INFO]: Epoch 035 - training loss: 0.1929, validation loss: 0.1949
2024-05-23 22:41:05 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T221534/CSDI_epoch35_loss0.19489551410079003.pypots
2024-05-23 22:41:48 [INFO]: Epoch 036 - training loss: 0.2031, validation loss: 0.1994
2024-05-23 22:41:48 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T221534/CSDI_epoch36_loss0.1993846870958805.pypots
2024-05-23 22:42:32 [INFO]: Epoch 037 - training loss: 0.1978, validation loss: 0.1915
2024-05-23 22:42:32 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T221534/CSDI_epoch37_loss0.19151201024651526.pypots
2024-05-23 22:43:16 [INFO]: Epoch 038 - training loss: 0.2018, validation loss: 0.1944
2024-05-23 22:43:16 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T221534/CSDI_epoch38_loss0.19437075555324554.pypots
2024-05-23 22:43:59 [INFO]: Epoch 039 - training loss: 0.1927, validation loss: 0.1938
2024-05-23 22:43:59 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T221534/CSDI_epoch39_loss0.1937556967139244.pypots
2024-05-23 22:44:43 [INFO]: Epoch 040 - training loss: 0.2042, validation loss: 0.1955
2024-05-23 22:44:43 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T221534/CSDI_epoch40_loss0.19546656906604767.pypots
2024-05-23 22:45:27 [INFO]: Epoch 041 - training loss: 0.2074, validation loss: 0.1934
2024-05-23 22:45:27 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T221534/CSDI_epoch41_loss0.1934015206992626.pypots
2024-05-23 22:46:11 [INFO]: Epoch 042 - training loss: 0.2046, validation loss: 0.1915
2024-05-23 22:46:11 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T221534/CSDI_epoch42_loss0.1915026754140854.pypots
2024-05-23 22:46:54 [INFO]: Epoch 043 - training loss: 0.1961, validation loss: 0.1924
2024-05-23 22:46:54 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T221534/CSDI_epoch43_loss0.19244020581245422.pypots
2024-05-23 22:47:38 [INFO]: Epoch 044 - training loss: 0.2049, validation loss: 0.1945
2024-05-23 22:47:38 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T221534/CSDI_epoch44_loss0.19452596306800843.pypots
2024-05-23 22:48:22 [INFO]: Epoch 045 - training loss: 0.2011, validation loss: 0.1898
2024-05-23 22:48:22 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T221534/CSDI_epoch45_loss0.1898113712668419.pypots
2024-05-23 22:49:06 [INFO]: Epoch 046 - training loss: 0.2025, validation loss: 0.1930
2024-05-23 22:49:06 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T221534/CSDI_epoch46_loss0.19299705922603608.pypots
2024-05-23 22:49:49 [INFO]: Epoch 047 - training loss: 0.1837, validation loss: 0.1876
2024-05-23 22:49:49 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T221534/CSDI_epoch47_loss0.18759967088699342.pypots
2024-05-23 22:50:33 [INFO]: Epoch 048 - training loss: 0.1929, validation loss: 0.1922
2024-05-23 22:50:33 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T221534/CSDI_epoch48_loss0.19215061143040657.pypots
2024-05-23 22:51:17 [INFO]: Epoch 049 - training loss: 0.1957, validation loss: 0.1908
2024-05-23 22:51:17 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T221534/CSDI_epoch49_loss0.19080727621912957.pypots
2024-05-23 22:52:01 [INFO]: Epoch 050 - training loss: 0.1903, validation loss: 0.1908
2024-05-23 22:52:01 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T221534/CSDI_epoch50_loss0.19076606929302214.pypots
2024-05-23 22:52:45 [INFO]: Epoch 051 - training loss: 0.2061, validation loss: 0.2003
2024-05-23 22:52:45 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T221534/CSDI_epoch51_loss0.20028808563947678.pypots
2024-05-23 22:53:28 [INFO]: Epoch 052 - training loss: 0.2017, validation loss: 0.1919
2024-05-23 22:53:28 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T221534/CSDI_epoch52_loss0.19192175790667534.pypots
2024-05-23 22:54:12 [INFO]: Epoch 053 - training loss: 0.2032, validation loss: 0.1884
2024-05-23 22:54:12 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T221534/CSDI_epoch53_loss0.18837502002716064.pypots
2024-05-23 22:54:56 [INFO]: Epoch 054 - training loss: 0.1908, validation loss: 0.1892
2024-05-23 22:54:56 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T221534/CSDI_epoch54_loss0.1891848362982273.pypots
2024-05-23 22:55:40 [INFO]: Epoch 055 - training loss: 0.1854, validation loss: 0.1888
2024-05-23 22:55:40 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T221534/CSDI_epoch55_loss0.1887798488140106.pypots
2024-05-23 22:56:24 [INFO]: Epoch 056 - training loss: 0.1857, validation loss: 0.1911
2024-05-23 22:56:24 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T221534/CSDI_epoch56_loss0.19108103439211846.pypots
2024-05-23 22:57:07 [INFO]: Epoch 057 - training loss: 0.1987, validation loss: 0.1890
2024-05-23 22:57:07 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T221534/CSDI_epoch57_loss0.1890377774834633.pypots
2024-05-23 22:57:07 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-05-23 22:57:07 [INFO]: Finished training. The best model is from epoch#47.
2024-05-23 22:57:07 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_2/CSDI_physionet_2012_seta/20240523_T221534/CSDI.pypots
2024-05-23 23:04:30 [INFO]: CSDI on PhysioNet-2012-seta: MAE=0.2620, MSE=0.5445
2024-05-23 23:33:55 [INFO]: Successfully saved to augmentation_premask_post_norm_saved_results/round_2/CSDI_physionet_2012_seta/imputation.pkl
2024-05-23 23:33:55 [INFO]: Using the given device: cuda:0
2024-05-23 23:33:55 [INFO]: Model files will be saved to augmentation_premask_post_norm_saved_results/round_2/GPVAE_physionet_2012_seta/20240523_T233355
2024-05-23 23:33:55 [INFO]: Tensorboard file will be saved to augmentation_premask_post_norm_saved_results/round_2/GPVAE_physionet_2012_seta/20240523_T233355/tensorboard
2024-05-23 23:33:55 [INFO]: GPVAE initialized with the given hyperparameters, the number of trainable parameters: 1,028,244
2024-05-23 23:33:56 [INFO]: Epoch 001 - training loss: 42959.3167, validation loss: 0.9265
2024-05-23 23:33:56 [INFO]: Epoch 002 - training loss: 24412.7149, validation loss: 0.7556
2024-05-23 23:33:57 [INFO]: Epoch 003 - training loss: 23501.5127, validation loss: 0.7356
2024-05-23 23:33:57 [INFO]: Epoch 004 - training loss: 23200.7170, validation loss: 0.7024
2024-05-23 23:33:58 [INFO]: Epoch 005 - training loss: 23049.3859, validation loss: 0.6873
2024-05-23 23:33:59 [INFO]: Epoch 006 - training loss: 22963.2508, validation loss: 0.6742
2024-05-23 23:33:59 [INFO]: Epoch 007 - training loss: 22911.1585, validation loss: 0.6730
2024-05-23 23:34:00 [INFO]: Epoch 008 - training loss: 22878.2221, validation loss: 0.6819
2024-05-23 23:34:00 [INFO]: Epoch 009 - training loss: 22855.4495, validation loss: 0.6687
2024-05-23 23:34:01 [INFO]: Epoch 010 - training loss: 22840.1003, validation loss: 0.6708
2024-05-23 23:34:01 [INFO]: Epoch 011 - training loss: 22829.3382, validation loss: 0.6662
2024-05-23 23:34:02 [INFO]: Epoch 012 - training loss: 22820.6519, validation loss: 0.6641
2024-05-23 23:34:03 [INFO]: Epoch 013 - training loss: 22814.4054, validation loss: 0.6629
2024-05-23 23:34:03 [INFO]: Epoch 014 - training loss: 22809.4474, validation loss: 0.6635
2024-05-23 23:34:04 [INFO]: Epoch 015 - training loss: 22805.4622, validation loss: 0.6646
2024-05-23 23:34:04 [INFO]: Epoch 016 - training loss: 22802.2445, validation loss: 0.6588
2024-05-23 23:34:05 [INFO]: Epoch 017 - training loss: 22798.9147, validation loss: 0.6520
2024-05-23 23:34:05 [INFO]: Epoch 018 - training loss: 22796.7944, validation loss: 0.6538
2024-05-23 23:34:06 [INFO]: Epoch 019 - training loss: 22794.5198, validation loss: 0.6475
2024-05-23 23:34:07 [INFO]: Epoch 020 - training loss: 22792.9717, validation loss: 0.6446
2024-05-23 23:34:07 [INFO]: Epoch 021 - training loss: 22790.7969, validation loss: 0.6409
2024-05-23 23:34:08 [INFO]: Epoch 022 - training loss: 22789.1295, validation loss: 0.6383
2024-05-23 23:34:08 [INFO]: Epoch 023 - training loss: 22787.9784, validation loss: 0.6357
2024-05-23 23:34:09 [INFO]: Epoch 024 - training loss: 22787.2455, validation loss: 0.6383
2024-05-23 23:34:09 [INFO]: Epoch 025 - training loss: 22786.6732, validation loss: 0.6537
2024-05-23 23:34:10 [INFO]: Epoch 026 - training loss: 22785.5493, validation loss: 0.6369
2024-05-23 23:34:11 [INFO]: Epoch 027 - training loss: 22785.3375, validation loss: 0.6337
2024-05-23 23:34:11 [INFO]: Epoch 028 - training loss: 22783.8515, validation loss: 0.6341
2024-05-23 23:34:12 [INFO]: Epoch 029 - training loss: 22784.5607, validation loss: 0.6292
2024-05-23 23:34:12 [INFO]: Epoch 030 - training loss: 22782.6884, validation loss: 0.6277
2024-05-23 23:34:13 [INFO]: Epoch 031 - training loss: 22782.2090, validation loss: 0.6361
2024-05-23 23:34:13 [INFO]: Epoch 032 - training loss: 22781.8388, validation loss: 0.6246
2024-05-23 23:34:14 [INFO]: Epoch 033 - training loss: 22781.2252, validation loss: 0.6236
2024-05-23 23:34:15 [INFO]: Epoch 034 - training loss: 22779.3185, validation loss: 0.6187
2024-05-23 23:34:15 [INFO]: Epoch 035 - training loss: 22777.9320, validation loss: 0.6178
2024-05-23 23:34:16 [INFO]: Epoch 036 - training loss: 22777.0411, validation loss: 0.6135
2024-05-23 23:34:16 [INFO]: Epoch 037 - training loss: 22776.3055, validation loss: 0.6113
2024-05-23 23:34:17 [INFO]: Epoch 038 - training loss: 22775.3008, validation loss: 0.6073
2024-05-23 23:34:17 [INFO]: Epoch 039 - training loss: 22774.1768, validation loss: 0.6025
2024-05-23 23:34:18 [INFO]: Epoch 040 - training loss: 22774.0177, validation loss: 0.6015
2024-05-23 23:34:19 [INFO]: Epoch 041 - training loss: 22772.9142, validation loss: 0.6005
2024-05-23 23:34:19 [INFO]: Epoch 042 - training loss: 22772.4018, validation loss: 0.6012
2024-05-23 23:34:20 [INFO]: Epoch 043 - training loss: 22771.5175, validation loss: 0.5950
2024-05-23 23:34:20 [INFO]: Epoch 044 - training loss: 22770.7604, validation loss: 0.5927
2024-05-23 23:34:21 [INFO]: Epoch 045 - training loss: 22770.2197, validation loss: 0.5933
2024-05-23 23:34:21 [INFO]: Epoch 046 - training loss: 22769.9648, validation loss: 0.5967
2024-05-23 23:34:22 [INFO]: Epoch 047 - training loss: 22769.3374, validation loss: 0.5887
2024-05-23 23:34:23 [INFO]: Epoch 048 - training loss: 22768.6053, validation loss: 0.5878
2024-05-23 23:34:23 [INFO]: Epoch 049 - training loss: 22768.1101, validation loss: 0.5905
2024-05-23 23:34:24 [INFO]: Epoch 050 - training loss: 22767.7621, validation loss: 0.5924
2024-05-23 23:34:24 [INFO]: Epoch 051 - training loss: 22766.5181, validation loss: 0.5810
2024-05-23 23:34:25 [INFO]: Epoch 052 - training loss: 22765.7983, validation loss: 0.5770
2024-05-23 23:34:26 [INFO]: Epoch 053 - training loss: 22764.4627, validation loss: 0.5727
2024-05-23 23:34:26 [INFO]: Epoch 054 - training loss: 22764.3364, validation loss: 0.5793
2024-05-23 23:34:27 [INFO]: Epoch 055 - training loss: 22763.6814, validation loss: 0.5738
2024-05-23 23:34:27 [INFO]: Epoch 056 - training loss: 22763.5811, validation loss: 0.5681
2024-05-23 23:34:28 [INFO]: Epoch 057 - training loss: 22762.6037, validation loss: 0.5700
2024-05-23 23:34:29 [INFO]: Epoch 058 - training loss: 22762.4042, validation loss: 0.5670
2024-05-23 23:34:29 [INFO]: Epoch 059 - training loss: 22761.8742, validation loss: 0.5820
2024-05-23 23:34:30 [INFO]: Epoch 060 - training loss: 22761.0951, validation loss: 0.5714
2024-05-23 23:34:31 [INFO]: Epoch 061 - training loss: 22761.3268, validation loss: 0.5653
2024-05-23 23:34:31 [INFO]: Epoch 062 - training loss: 22760.3420, validation loss: 0.5624
2024-05-23 23:34:32 [INFO]: Epoch 063 - training loss: 22759.9549, validation loss: 0.5605
2024-05-23 23:34:33 [INFO]: Epoch 064 - training loss: 22760.3288, validation loss: 0.5603
2024-05-23 23:34:33 [INFO]: Epoch 065 - training loss: 22759.5150, validation loss: 0.5580
2024-05-23 23:34:34 [INFO]: Epoch 066 - training loss: 22759.4332, validation loss: 0.5571
2024-05-23 23:34:35 [INFO]: Epoch 067 - training loss: 22758.7495, validation loss: 0.5579
2024-05-23 23:34:36 [INFO]: Epoch 068 - training loss: 22758.8984, validation loss: 0.5566
2024-05-23 23:34:36 [INFO]: Epoch 069 - training loss: 22758.4014, validation loss: 0.5522
2024-05-23 23:34:37 [INFO]: Epoch 070 - training loss: 22758.8956, validation loss: 0.5837
2024-05-23 23:34:38 [INFO]: Epoch 071 - training loss: 22761.3064, validation loss: 0.5588
2024-05-23 23:34:38 [INFO]: Epoch 072 - training loss: 22757.7634, validation loss: 0.5480
2024-05-23 23:34:39 [INFO]: Epoch 073 - training loss: 22758.3707, validation loss: 0.5571
2024-05-23 23:34:40 [INFO]: Epoch 074 - training loss: 22758.2428, validation loss: 0.5444
2024-05-23 23:34:40 [INFO]: Epoch 075 - training loss: 22756.7735, validation loss: 0.5480
2024-05-23 23:34:41 [INFO]: Epoch 076 - training loss: 22755.5373, validation loss: 0.5395
2024-05-23 23:34:42 [INFO]: Epoch 077 - training loss: 22755.1530, validation loss: 0.5378
2024-05-23 23:34:42 [INFO]: Epoch 078 - training loss: 22754.3428, validation loss: 0.5356
2024-05-23 23:34:43 [INFO]: Epoch 079 - training loss: 22754.1860, validation loss: 0.5574
2024-05-23 23:34:44 [INFO]: Epoch 080 - training loss: 22754.6368, validation loss: 0.5361
2024-05-23 23:34:45 [INFO]: Epoch 081 - training loss: 22753.6941, validation loss: 0.5474
2024-05-23 23:34:45 [INFO]: Epoch 082 - training loss: 22754.2564, validation loss: 0.5317
2024-05-23 23:34:46 [INFO]: Epoch 083 - training loss: 22753.5006, validation loss: 0.5456
2024-05-23 23:34:47 [INFO]: Epoch 084 - training loss: 22754.5293, validation loss: 0.5340
2024-05-23 23:34:47 [INFO]: Epoch 085 - training loss: 22753.5604, validation loss: 0.5448
2024-05-23 23:34:48 [INFO]: Epoch 086 - training loss: 22757.3550, validation loss: 0.5338
2024-05-23 23:34:49 [INFO]: Epoch 087 - training loss: 22752.9433, validation loss: 0.5458
2024-05-23 23:34:49 [INFO]: Epoch 088 - training loss: 22753.6497, validation loss: 0.5321
2024-05-23 23:34:50 [INFO]: Epoch 089 - training loss: 22752.4592, validation loss: 0.5330
2024-05-23 23:34:51 [INFO]: Epoch 090 - training loss: 22752.6748, validation loss: 0.5307
2024-05-23 23:34:52 [INFO]: Epoch 091 - training loss: 22751.9752, validation loss: 0.5337
2024-05-23 23:34:52 [INFO]: Epoch 092 - training loss: 22752.0192, validation loss: 0.5267
2024-05-23 23:34:53 [INFO]: Epoch 093 - training loss: 22751.4014, validation loss: 0.5282
2024-05-23 23:34:54 [INFO]: Epoch 094 - training loss: 22751.5733, validation loss: 0.5296
2024-05-23 23:34:54 [INFO]: Epoch 095 - training loss: 22751.2280, validation loss: 0.5275
2024-05-23 23:34:55 [INFO]: Epoch 096 - training loss: 22750.9312, validation loss: 0.5234
2024-05-23 23:34:56 [INFO]: Epoch 097 - training loss: 22751.0065, validation loss: 0.5258
2024-05-23 23:34:56 [INFO]: Epoch 098 - training loss: 22750.7581, validation loss: 0.5268
2024-05-23 23:34:57 [INFO]: Epoch 099 - training loss: 22750.9676, validation loss: 0.5238
2024-05-23 23:34:58 [INFO]: Epoch 100 - training loss: 22750.3706, validation loss: 0.5213
2024-05-23 23:34:58 [INFO]: Epoch 101 - training loss: 22750.4440, validation loss: 0.5217
2024-05-23 23:34:59 [INFO]: Epoch 102 - training loss: 22750.2880, validation loss: 0.5202
2024-05-23 23:35:00 [INFO]: Epoch 103 - training loss: 22750.9603, validation loss: 0.5206
2024-05-23 23:35:01 [INFO]: Epoch 104 - training loss: 22749.8488, validation loss: 0.5212
2024-05-23 23:35:01 [INFO]: Epoch 105 - training loss: 22750.2956, validation loss: 0.5199
2024-05-23 23:35:02 [INFO]: Epoch 106 - training loss: 22749.7409, validation loss: 0.5225
2024-05-23 23:35:03 [INFO]: Epoch 107 - training loss: 22750.2780, validation loss: 0.5164
2024-05-23 23:35:03 [INFO]: Epoch 108 - training loss: 22749.7001, validation loss: 0.5192
2024-05-23 23:35:04 [INFO]: Epoch 109 - training loss: 22750.0520, validation loss: 0.5176
2024-05-23 23:35:05 [INFO]: Epoch 110 - training loss: 22749.1322, validation loss: 0.5167
2024-05-23 23:35:05 [INFO]: Epoch 111 - training loss: 22749.4315, validation loss: 0.5212
2024-05-23 23:35:06 [INFO]: Epoch 112 - training loss: 22749.9781, validation loss: 0.5144
2024-05-23 23:35:07 [INFO]: Epoch 113 - training loss: 22748.8713, validation loss: 0.5183
2024-05-23 23:35:07 [INFO]: Epoch 114 - training loss: 22748.4028, validation loss: 0.5133
2024-05-23 23:35:08 [INFO]: Epoch 115 - training loss: 22748.7263, validation loss: 0.5174
2024-05-23 23:35:09 [INFO]: Epoch 116 - training loss: 22748.7731, validation loss: 0.5088
2024-05-23 23:35:09 [INFO]: Epoch 117 - training loss: 22749.4419, validation loss: 0.5103
2024-05-23 23:35:10 [INFO]: Epoch 118 - training loss: 22748.7280, validation loss: 0.5104
2024-05-23 23:35:11 [INFO]: Epoch 119 - training loss: 22750.0360, validation loss: 0.5070
2024-05-23 23:35:12 [INFO]: Epoch 120 - training loss: 22748.4702, validation loss: 0.5086
2024-05-23 23:35:12 [INFO]: Epoch 121 - training loss: 22748.1640, validation loss: 0.5086
2024-05-23 23:35:13 [INFO]: Epoch 122 - training loss: 22747.7346, validation loss: 0.5049
2024-05-23 23:35:14 [INFO]: Epoch 123 - training loss: 22747.4137, validation loss: 0.5054
2024-05-23 23:35:14 [INFO]: Epoch 124 - training loss: 22747.2046, validation loss: 0.5026
2024-05-23 23:35:15 [INFO]: Epoch 125 - training loss: 22746.9766, validation loss: 0.5062
2024-05-23 23:35:16 [INFO]: Epoch 126 - training loss: 22747.0178, validation loss: 0.5047
2024-05-23 23:35:16 [INFO]: Epoch 127 - training loss: 22746.5703, validation loss: 0.5050
2024-05-23 23:35:17 [INFO]: Epoch 128 - training loss: 22746.6799, validation loss: 0.5057
2024-05-23 23:35:18 [INFO]: Epoch 129 - training loss: 22746.3957, validation loss: 0.5033
2024-05-23 23:35:18 [INFO]: Epoch 130 - training loss: 22746.5676, validation loss: 0.5025
2024-05-23 23:35:19 [INFO]: Epoch 131 - training loss: 22746.9261, validation loss: 0.4995
2024-05-23 23:35:20 [INFO]: Epoch 132 - training loss: 22746.5992, validation loss: 0.5025
2024-05-23 23:35:21 [INFO]: Epoch 133 - training loss: 22746.0482, validation loss: 0.5098
2024-05-23 23:35:21 [INFO]: Epoch 134 - training loss: 22746.1990, validation loss: 0.5008
2024-05-23 23:35:22 [INFO]: Epoch 135 - training loss: 22746.3355, validation loss: 0.5013
2024-05-23 23:35:23 [INFO]: Epoch 136 - training loss: 22745.8708, validation loss: 0.5048
2024-05-23 23:35:23 [INFO]: Epoch 137 - training loss: 22746.2040, validation loss: 0.4996
2024-05-23 23:35:24 [INFO]: Epoch 138 - training loss: 22745.9546, validation loss: 0.5044
2024-05-23 23:35:25 [INFO]: Epoch 139 - training loss: 22745.8272, validation loss: 0.4937
2024-05-23 23:35:25 [INFO]: Epoch 140 - training loss: 22745.1780, validation loss: 0.5004
2024-05-23 23:35:26 [INFO]: Epoch 141 - training loss: 22745.8885, validation loss: 0.4961
2024-05-23 23:35:27 [INFO]: Epoch 142 - training loss: 22745.0054, validation loss: 0.4945
2024-05-23 23:35:27 [INFO]: Epoch 143 - training loss: 22744.7999, validation loss: 0.4946
2024-05-23 23:35:28 [INFO]: Epoch 144 - training loss: 22744.7330, validation loss: 0.4930
2024-05-23 23:35:29 [INFO]: Epoch 145 - training loss: 22745.2864, validation loss: 0.4923
2024-05-23 23:35:29 [INFO]: Epoch 146 - training loss: 22745.2669, validation loss: 0.4905
2024-05-23 23:35:30 [INFO]: Epoch 147 - training loss: 22744.7736, validation loss: 0.4961
2024-05-23 23:35:31 [INFO]: Epoch 148 - training loss: 22744.7707, validation loss: 0.4942
2024-05-23 23:35:31 [INFO]: Epoch 149 - training loss: 22745.1479, validation loss: 0.4990
2024-05-23 23:35:32 [INFO]: Epoch 150 - training loss: 22745.6119, validation loss: 0.4896
2024-05-23 23:35:33 [INFO]: Epoch 151 - training loss: 22744.7970, validation loss: 0.4948
2024-05-23 23:35:33 [INFO]: Epoch 152 - training loss: 22745.0241, validation loss: 0.4930
2024-05-23 23:35:34 [INFO]: Epoch 153 - training loss: 22745.1103, validation loss: 0.4953
2024-05-23 23:35:35 [INFO]: Epoch 154 - training loss: 22744.9042, validation loss: 0.4946
2024-05-23 23:35:35 [INFO]: Epoch 155 - training loss: 22745.1778, validation loss: 0.4920
2024-05-23 23:35:36 [INFO]: Epoch 156 - training loss: 22744.4032, validation loss: 0.4868
2024-05-23 23:35:37 [INFO]: Epoch 157 - training loss: 22743.8406, validation loss: 0.4875
2024-05-23 23:35:37 [INFO]: Epoch 158 - training loss: 22743.5992, validation loss: 0.4854
2024-05-23 23:35:38 [INFO]: Epoch 159 - training loss: 22743.8033, validation loss: 0.4869
2024-05-23 23:35:39 [INFO]: Epoch 160 - training loss: 22743.3431, validation loss: 0.4855
2024-05-23 23:35:39 [INFO]: Epoch 161 - training loss: 22743.8082, validation loss: 0.4866
2024-05-23 23:35:40 [INFO]: Epoch 162 - training loss: 22744.6296, validation loss: 0.4924
2024-05-23 23:35:41 [INFO]: Epoch 163 - training loss: 22743.7132, validation loss: 0.4870
2024-05-23 23:35:42 [INFO]: Epoch 164 - training loss: 22743.3629, validation loss: 0.4856
2024-05-23 23:35:42 [INFO]: Epoch 165 - training loss: 22743.1680, validation loss: 0.4875
2024-05-23 23:35:43 [INFO]: Epoch 166 - training loss: 22742.9961, validation loss: 0.4850
2024-05-23 23:35:44 [INFO]: Epoch 167 - training loss: 22743.7821, validation loss: 0.4864
2024-05-23 23:35:44 [INFO]: Epoch 168 - training loss: 22743.3227, validation loss: 0.4946
2024-05-23 23:35:45 [INFO]: Epoch 169 - training loss: 22744.5270, validation loss: 0.4847
2024-05-23 23:35:46 [INFO]: Epoch 170 - training loss: 22744.0114, validation loss: 0.4841
2024-05-23 23:35:46 [INFO]: Epoch 171 - training loss: 22743.8507, validation loss: 0.4832
2024-05-23 23:35:47 [INFO]: Epoch 172 - training loss: 22743.2281, validation loss: 0.4816
2024-05-23 23:35:48 [INFO]: Epoch 173 - training loss: 22743.4689, validation loss: 0.4840
2024-05-23 23:35:48 [INFO]: Epoch 174 - training loss: 22742.6422, validation loss: 0.4868
2024-05-23 23:35:49 [INFO]: Epoch 175 - training loss: 22742.9162, validation loss: 0.4842
2024-05-23 23:35:50 [INFO]: Epoch 176 - training loss: 22742.6543, validation loss: 0.4851
2024-05-23 23:35:50 [INFO]: Epoch 177 - training loss: 22742.5408, validation loss: 0.4805
2024-05-23 23:35:51 [INFO]: Epoch 178 - training loss: 22742.7717, validation loss: 0.4818
2024-05-23 23:35:52 [INFO]: Epoch 179 - training loss: 22743.0409, validation loss: 0.4795
2024-05-23 23:35:52 [INFO]: Epoch 180 - training loss: 22742.4531, validation loss: 0.4778
2024-05-23 23:35:53 [INFO]: Epoch 181 - training loss: 22742.5178, validation loss: 0.4795
2024-05-23 23:35:53 [INFO]: Epoch 182 - training loss: 22742.4878, validation loss: 0.4859
2024-05-23 23:35:54 [INFO]: Epoch 183 - training loss: 22742.7666, validation loss: 0.4785
2024-05-23 23:35:54 [INFO]: Epoch 184 - training loss: 22742.2618, validation loss: 0.4766
2024-05-23 23:35:55 [INFO]: Epoch 185 - training loss: 22741.9828, validation loss: 0.4769
2024-05-23 23:35:56 [INFO]: Epoch 186 - training loss: 22742.6739, validation loss: 0.4757
2024-05-23 23:35:56 [INFO]: Epoch 187 - training loss: 22741.8732, validation loss: 0.4775
2024-05-23 23:35:57 [INFO]: Epoch 188 - training loss: 22742.5575, validation loss: 0.4730
2024-05-23 23:35:57 [INFO]: Epoch 189 - training loss: 22742.3904, validation loss: 0.4781
2024-05-23 23:35:58 [INFO]: Epoch 190 - training loss: 22742.0381, validation loss: 0.4747
2024-05-23 23:35:59 [INFO]: Epoch 191 - training loss: 22741.4213, validation loss: 0.4740
2024-05-23 23:35:59 [INFO]: Epoch 192 - training loss: 22741.2016, validation loss: 0.4728
2024-05-23 23:36:00 [INFO]: Epoch 193 - training loss: 22741.8193, validation loss: 0.4723
2024-05-23 23:36:00 [INFO]: Epoch 194 - training loss: 22741.2308, validation loss: 0.4724
2024-05-23 23:36:01 [INFO]: Epoch 195 - training loss: 22741.6556, validation loss: 0.4765
2024-05-23 23:36:02 [INFO]: Epoch 196 - training loss: 22741.9791, validation loss: 0.4779
2024-05-23 23:36:02 [INFO]: Epoch 197 - training loss: 22741.7883, validation loss: 0.4760
2024-05-23 23:36:03 [INFO]: Epoch 198 - training loss: 22741.0894, validation loss: 0.4707
2024-05-23 23:36:03 [INFO]: Epoch 199 - training loss: 22740.8877, validation loss: 0.4720
2024-05-23 23:36:04 [INFO]: Epoch 200 - training loss: 22741.0732, validation loss: 0.4703
2024-05-23 23:36:04 [INFO]: Epoch 201 - training loss: 22741.0510, validation loss: 0.4715
2024-05-23 23:36:05 [INFO]: Epoch 202 - training loss: 22741.4862, validation loss: 0.4687
2024-05-23 23:36:06 [INFO]: Epoch 203 - training loss: 22741.1789, validation loss: 0.4700
2024-05-23 23:36:06 [INFO]: Epoch 204 - training loss: 22740.8037, validation loss: 0.4676
2024-05-23 23:36:07 [INFO]: Epoch 205 - training loss: 22740.4596, validation loss: 0.4674
2024-05-23 23:36:07 [INFO]: Epoch 206 - training loss: 22740.0391, validation loss: 0.4699
2024-05-23 23:36:08 [INFO]: Epoch 207 - training loss: 22740.0781, validation loss: 0.4712
2024-05-23 23:36:09 [INFO]: Epoch 208 - training loss: 22740.4175, validation loss: 0.4658
2024-05-23 23:36:09 [INFO]: Epoch 209 - training loss: 22740.3210, validation loss: 0.4683
2024-05-23 23:36:10 [INFO]: Epoch 210 - training loss: 22740.3399, validation loss: 0.4665
2024-05-23 23:36:10 [INFO]: Epoch 211 - training loss: 22739.9291, validation loss: 0.4684
2024-05-23 23:36:11 [INFO]: Epoch 212 - training loss: 22740.1496, validation loss: 0.4695
2024-05-23 23:36:11 [INFO]: Epoch 213 - training loss: 22740.4520, validation loss: 0.4655
2024-05-23 23:36:12 [INFO]: Epoch 214 - training loss: 22740.4101, validation loss: 0.4691
2024-05-23 23:36:13 [INFO]: Epoch 215 - training loss: 22740.2441, validation loss: 0.4677
2024-05-23 23:36:13 [INFO]: Epoch 216 - training loss: 22739.6238, validation loss: 0.4648
2024-05-23 23:36:14 [INFO]: Epoch 217 - training loss: 22739.1626, validation loss: 0.4739
2024-05-23 23:36:14 [INFO]: Epoch 218 - training loss: 22739.2167, validation loss: 0.4674
2024-05-23 23:36:15 [INFO]: Epoch 219 - training loss: 22739.3474, validation loss: 0.4663
2024-05-23 23:36:16 [INFO]: Epoch 220 - training loss: 22739.4588, validation loss: 0.4652
2024-05-23 23:36:16 [INFO]: Epoch 221 - training loss: 22740.0032, validation loss: 0.4661
2024-05-23 23:36:17 [INFO]: Epoch 222 - training loss: 22739.6181, validation loss: 0.4677
2024-05-23 23:36:17 [INFO]: Epoch 223 - training loss: 22738.7546, validation loss: 0.4680
2024-05-23 23:36:18 [INFO]: Epoch 224 - training loss: 22738.8500, validation loss: 0.4672
2024-05-23 23:36:18 [INFO]: Epoch 225 - training loss: 22738.4499, validation loss: 0.4700
2024-05-23 23:36:19 [INFO]: Epoch 226 - training loss: 22739.4091, validation loss: 0.4631
2024-05-23 23:36:20 [INFO]: Epoch 227 - training loss: 22739.1699, validation loss: 0.4642
2024-05-23 23:36:20 [INFO]: Epoch 228 - training loss: 22738.5997, validation loss: 0.4627
2024-05-23 23:36:21 [INFO]: Epoch 229 - training loss: 22738.5780, validation loss: 0.4697
2024-05-23 23:36:21 [INFO]: Epoch 230 - training loss: 22738.2025, validation loss: 0.4732
2024-05-23 23:36:22 [INFO]: Epoch 231 - training loss: 22738.0744, validation loss: 0.4740
2024-05-23 23:36:23 [INFO]: Epoch 232 - training loss: 22737.8840, validation loss: 0.4739
2024-05-23 23:36:23 [INFO]: Epoch 233 - training loss: 22738.6857, validation loss: 0.4730
2024-05-23 23:36:24 [INFO]: Epoch 234 - training loss: 22739.8031, validation loss: 0.4657
2024-05-23 23:36:24 [INFO]: Epoch 235 - training loss: 22738.2750, validation loss: 0.4666
2024-05-23 23:36:25 [INFO]: Epoch 236 - training loss: 22737.3022, validation loss: 0.4748
2024-05-23 23:36:25 [INFO]: Epoch 237 - training loss: 22737.0631, validation loss: 0.4754
2024-05-23 23:36:26 [INFO]: Epoch 238 - training loss: 22736.5987, validation loss: 0.4854
2024-05-23 23:36:26 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-05-23 23:36:26 [INFO]: Finished training. The best model is from epoch#228.
2024-05-23 23:36:26 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_2/GPVAE_physionet_2012_seta/20240523_T233355/GPVAE.pypots
2024-05-23 23:36:26 [INFO]: GP-VAE on PhysioNet-2012-seta: MAE=0.3956, MSE=0.3979
2024-05-23 23:36:27 [INFO]: Successfully saved to augmentation_premask_post_norm_saved_results/round_2/GPVAE_physionet_2012_seta/imputation.pkl
2024-05-23 23:36:27 [INFO]: Using the given device: cuda:0
2024-05-23 23:36:27 [INFO]: Model files will be saved to augmentation_premask_post_norm_saved_results/round_2/USGAN_physionet_2012_seta/20240523_T233627
2024-05-23 23:36:27 [INFO]: Tensorboard file will be saved to augmentation_premask_post_norm_saved_results/round_2/USGAN_physionet_2012_seta/20240523_T233627/tensorboard
2024-05-23 23:36:27 [INFO]: USGAN initialized with the given hyperparameters, the number of trainable parameters: 16,010,261
2024-05-23 23:36:48 [INFO]: Epoch 001 - generator training loss: 0.5834, discriminator training loss: 0.3809, validation loss: 0.6336
2024-05-23 23:37:07 [INFO]: Epoch 002 - generator training loss: 0.4827, discriminator training loss: 0.2728, validation loss: 0.5540
2024-05-23 23:37:26 [INFO]: Epoch 003 - generator training loss: 0.4392, discriminator training loss: 0.2374, validation loss: 0.5302
2024-05-23 23:37:45 [INFO]: Epoch 004 - generator training loss: 0.4534, discriminator training loss: 0.1895, validation loss: 0.5267
2024-05-23 23:38:03 [INFO]: Epoch 005 - generator training loss: 0.4531, discriminator training loss: 0.1580, validation loss: 0.5053
2024-05-23 23:38:22 [INFO]: Epoch 006 - generator training loss: 0.4378, discriminator training loss: 0.1391, validation loss: 0.4858
2024-05-23 23:38:41 [INFO]: Epoch 007 - generator training loss: 0.4202, discriminator training loss: 0.1252, validation loss: 0.4728
2024-05-23 23:39:00 [INFO]: Epoch 008 - generator training loss: 0.4063, discriminator training loss: 0.1148, validation loss: 0.4671
2024-05-23 23:39:18 [INFO]: Epoch 009 - generator training loss: 0.3971, discriminator training loss: 0.1064, validation loss: 0.4557
2024-05-23 23:39:37 [INFO]: Epoch 010 - generator training loss: 0.3906, discriminator training loss: 0.0993, validation loss: 0.4486
2024-05-23 23:39:56 [INFO]: Epoch 011 - generator training loss: 0.3845, discriminator training loss: 0.0931, validation loss: 0.4392
2024-05-23 23:40:14 [INFO]: Epoch 012 - generator training loss: 0.3780, discriminator training loss: 0.0880, validation loss: 0.4347
2024-05-23 23:40:33 [INFO]: Epoch 013 - generator training loss: 0.3722, discriminator training loss: 0.0832, validation loss: 0.4273
2024-05-23 23:40:52 [INFO]: Epoch 014 - generator training loss: 0.3681, discriminator training loss: 0.0792, validation loss: 0.4271
2024-05-23 23:41:11 [INFO]: Epoch 015 - generator training loss: 0.3627, discriminator training loss: 0.0756, validation loss: 0.4187
2024-05-23 23:41:29 [INFO]: Epoch 016 - generator training loss: 0.3580, discriminator training loss: 0.0724, validation loss: 0.4150
2024-05-23 23:41:48 [INFO]: Epoch 017 - generator training loss: 0.3550, discriminator training loss: 0.0696, validation loss: 0.4121
2024-05-23 23:42:07 [INFO]: Epoch 018 - generator training loss: 0.3484, discriminator training loss: 0.0668, validation loss: 0.4074
2024-05-23 23:42:25 [INFO]: Epoch 019 - generator training loss: 0.3461, discriminator training loss: 0.0645, validation loss: 0.4031
2024-05-23 23:42:44 [INFO]: Epoch 020 - generator training loss: 0.3395, discriminator training loss: 0.0624, validation loss: 0.3992
2024-05-23 23:43:03 [INFO]: Epoch 021 - generator training loss: 0.3352, discriminator training loss: 0.0608, validation loss: 0.3942
2024-05-23 23:43:22 [INFO]: Epoch 022 - generator training loss: 0.3316, discriminator training loss: 0.0589, validation loss: 0.3944
2024-05-23 23:43:40 [INFO]: Epoch 023 - generator training loss: 0.3275, discriminator training loss: 0.0574, validation loss: 0.3880
2024-05-23 23:43:59 [INFO]: Epoch 024 - generator training loss: 0.3243, discriminator training loss: 0.0563, validation loss: 0.3853
2024-05-23 23:44:18 [INFO]: Epoch 025 - generator training loss: 0.3203, discriminator training loss: 0.0548, validation loss: 0.3809
2024-05-23 23:44:37 [INFO]: Epoch 026 - generator training loss: 0.3169, discriminator training loss: 0.0541, validation loss: 0.3833
2024-05-23 23:44:55 [INFO]: Epoch 027 - generator training loss: 0.3153, discriminator training loss: 0.0529, validation loss: 0.3718
2024-05-23 23:45:14 [INFO]: Epoch 028 - generator training loss: 0.3080, discriminator training loss: 0.0520, validation loss: 0.3698
2024-05-23 23:45:34 [INFO]: Epoch 029 - generator training loss: 0.3040, discriminator training loss: 0.0514, validation loss: 0.3648
2024-05-23 23:45:54 [INFO]: Epoch 030 - generator training loss: 0.2985, discriminator training loss: 0.0506, validation loss: 0.3644
2024-05-23 23:46:13 [INFO]: Epoch 031 - generator training loss: 0.2952, discriminator training loss: 0.0498, validation loss: 0.3655
2024-05-23 23:46:33 [INFO]: Epoch 032 - generator training loss: 0.2916, discriminator training loss: 0.0494, validation loss: 0.3631
2024-05-23 23:46:52 [INFO]: Epoch 033 - generator training loss: 0.2885, discriminator training loss: 0.0487, validation loss: 0.3549
2024-05-23 23:47:11 [INFO]: Epoch 034 - generator training loss: 0.2854, discriminator training loss: 0.0482, validation loss: 0.3526
2024-05-23 23:47:30 [INFO]: Epoch 035 - generator training loss: 0.2818, discriminator training loss: 0.0478, validation loss: 0.3651
2024-05-23 23:47:48 [INFO]: Epoch 036 - generator training loss: 0.2818, discriminator training loss: 0.0475, validation loss: 0.3577
2024-05-23 23:48:07 [INFO]: Epoch 037 - generator training loss: 0.2793, discriminator training loss: 0.0471, validation loss: 0.3533
2024-05-23 23:48:26 [INFO]: Epoch 038 - generator training loss: 0.2733, discriminator training loss: 0.0464, validation loss: 0.3524
2024-05-23 23:48:45 [INFO]: Epoch 039 - generator training loss: 0.2716, discriminator training loss: 0.0463, validation loss: 0.3540
2024-05-23 23:49:04 [INFO]: Epoch 040 - generator training loss: 0.2714, discriminator training loss: 0.0461, validation loss: 0.3485
2024-05-23 23:49:22 [INFO]: Epoch 041 - generator training loss: 0.2689, discriminator training loss: 0.0457, validation loss: 0.3455
2024-05-23 23:49:41 [INFO]: Epoch 042 - generator training loss: 0.2608, discriminator training loss: 0.0451, validation loss: 0.3468
2024-05-23 23:50:00 [INFO]: Epoch 043 - generator training loss: 0.2603, discriminator training loss: 0.0451, validation loss: 0.3449
2024-05-23 23:50:19 [INFO]: Epoch 044 - generator training loss: 0.2688, discriminator training loss: 0.0451, validation loss: 0.3510
2024-05-23 23:50:37 [INFO]: Epoch 045 - generator training loss: 0.2622, discriminator training loss: 0.0446, validation loss: 0.3448
2024-05-23 23:50:56 [INFO]: Epoch 046 - generator training loss: 0.2551, discriminator training loss: 0.0442, validation loss: 0.3440
2024-05-23 23:51:15 [INFO]: Epoch 047 - generator training loss: 0.2500, discriminator training loss: 0.0438, validation loss: 0.3416
2024-05-23 23:51:34 [INFO]: Epoch 048 - generator training loss: 0.2523, discriminator training loss: 0.0438, validation loss: 0.3425
2024-05-23 23:51:53 [INFO]: Epoch 049 - generator training loss: 0.2483, discriminator training loss: 0.0435, validation loss: 0.3410
2024-05-23 23:52:11 [INFO]: Epoch 050 - generator training loss: 0.2443, discriminator training loss: 0.0436, validation loss: 0.3389
2024-05-23 23:52:30 [INFO]: Epoch 051 - generator training loss: 0.2418, discriminator training loss: 0.0431, validation loss: 0.3420
2024-05-23 23:52:49 [INFO]: Epoch 052 - generator training loss: 0.2427, discriminator training loss: 0.0431, validation loss: 0.3404
2024-05-23 23:53:08 [INFO]: Epoch 053 - generator training loss: 0.2387, discriminator training loss: 0.0431, validation loss: 0.3387
2024-05-23 23:53:27 [INFO]: Epoch 054 - generator training loss: 0.2370, discriminator training loss: 0.0427, validation loss: 0.3528
2024-05-23 23:53:46 [INFO]: Epoch 055 - generator training loss: 0.2348, discriminator training loss: 0.0427, validation loss: 0.3405
2024-05-23 23:54:04 [INFO]: Epoch 056 - generator training loss: 0.2331, discriminator training loss: 0.0424, validation loss: 0.3409
2024-05-23 23:54:23 [INFO]: Epoch 057 - generator training loss: 0.2270, discriminator training loss: 0.0423, validation loss: 0.3376
2024-05-23 23:54:42 [INFO]: Epoch 058 - generator training loss: 0.2243, discriminator training loss: 0.0422, validation loss: 0.3404
2024-05-23 23:55:01 [INFO]: Epoch 059 - generator training loss: 0.2255, discriminator training loss: 0.0421, validation loss: 0.3384
2024-05-23 23:55:19 [INFO]: Epoch 060 - generator training loss: 0.2269, discriminator training loss: 0.0418, validation loss: 0.3438
2024-05-23 23:55:38 [INFO]: Epoch 061 - generator training loss: 0.2338, discriminator training loss: 0.0419, validation loss: 0.3442
2024-05-23 23:55:57 [INFO]: Epoch 062 - generator training loss: 0.2263, discriminator training loss: 0.0419, validation loss: 0.3406
2024-05-23 23:56:16 [INFO]: Epoch 063 - generator training loss: 0.2409, discriminator training loss: 0.0421, validation loss: 0.3433
2024-05-23 23:56:35 [INFO]: Epoch 064 - generator training loss: 0.2282, discriminator training loss: 0.0420, validation loss: 0.3424
2024-05-23 23:56:53 [INFO]: Epoch 065 - generator training loss: 0.2214, discriminator training loss: 0.0417, validation loss: 0.3393
2024-05-23 23:57:12 [INFO]: Epoch 066 - generator training loss: 0.2151, discriminator training loss: 0.0416, validation loss: 0.3400
2024-05-23 23:57:31 [INFO]: Epoch 067 - generator training loss: 0.2119, discriminator training loss: 0.0415, validation loss: 0.3353
2024-05-23 23:57:50 [INFO]: Epoch 068 - generator training loss: 0.2093, discriminator training loss: 0.0414, validation loss: 0.3361
2024-05-23 23:58:09 [INFO]: Epoch 069 - generator training loss: 0.2144, discriminator training loss: 0.0415, validation loss: 0.3390
2024-05-23 23:58:27 [INFO]: Epoch 070 - generator training loss: 0.2068, discriminator training loss: 0.0412, validation loss: 0.3378
2024-05-23 23:58:46 [INFO]: Epoch 071 - generator training loss: 0.2060, discriminator training loss: 0.0413, validation loss: 0.3376
2024-05-23 23:59:05 [INFO]: Epoch 072 - generator training loss: 0.2080, discriminator training loss: 0.0412, validation loss: 0.3380
2024-05-23 23:59:24 [INFO]: Epoch 073 - generator training loss: 0.2054, discriminator training loss: 0.0409, validation loss: 0.3371
2024-05-23 23:59:42 [INFO]: Epoch 074 - generator training loss: 0.2034, discriminator training loss: 0.0410, validation loss: 0.3381
2024-05-24 00:00:01 [INFO]: Epoch 075 - generator training loss: 0.2049, discriminator training loss: 0.0408, validation loss: 0.3424
2024-05-24 00:00:20 [INFO]: Epoch 076 - generator training loss: 0.2014, discriminator training loss: 0.0406, validation loss: 0.3393
2024-05-24 00:00:39 [INFO]: Epoch 077 - generator training loss: 0.1978, discriminator training loss: 0.0404, validation loss: 0.3361
2024-05-24 00:00:39 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-05-24 00:00:39 [INFO]: Finished training. The best model is from epoch#67.
2024-05-24 00:00:39 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_2/USGAN_physionet_2012_seta/20240523_T233627/USGAN.pypots
2024-05-24 00:00:41 [INFO]: US-GAN on PhysioNet-2012-seta: MAE=0.2890, MSE=0.2557
2024-05-24 00:00:51 [INFO]: Successfully saved to augmentation_premask_post_norm_saved_results/round_2/USGAN_physionet_2012_seta/imputation.pkl
2024-05-24 00:00:51 [INFO]: Using the given device: cuda:0
2024-05-24 00:00:51 [INFO]: Model files will be saved to augmentation_premask_post_norm_saved_results/round_2/BRITS_physionet_2012_seta/20240524_T000051
2024-05-24 00:00:51 [INFO]: Tensorboard file will be saved to augmentation_premask_post_norm_saved_results/round_2/BRITS_physionet_2012_seta/20240524_T000051/tensorboard
2024-05-24 00:00:51 [INFO]: BRITS initialized with the given hyperparameters, the number of trainable parameters: 9,176,048
2024-05-24 00:01:06 [INFO]: Epoch 001 - training loss: 1.1318, validation loss: 0.5538
2024-05-24 00:01:19 [INFO]: Epoch 002 - training loss: 0.9231, validation loss: 0.5014
2024-05-24 00:01:31 [INFO]: Epoch 003 - training loss: 0.8602, validation loss: 0.4687
2024-05-24 00:01:43 [INFO]: Epoch 004 - training loss: 0.8233, validation loss: 0.4470
2024-05-24 00:01:56 [INFO]: Epoch 005 - training loss: 0.7947, validation loss: 0.4281
2024-05-24 00:02:08 [INFO]: Epoch 006 - training loss: 0.7713, validation loss: 0.4143
2024-05-24 00:02:20 [INFO]: Epoch 007 - training loss: 0.7535, validation loss: 0.4085
2024-05-24 00:02:33 [INFO]: Epoch 008 - training loss: 0.7386, validation loss: 0.3973
2024-05-24 00:02:45 [INFO]: Epoch 009 - training loss: 0.7265, validation loss: 0.3914
2024-05-24 00:02:57 [INFO]: Epoch 010 - training loss: 0.7147, validation loss: 0.3886
2024-05-24 00:03:09 [INFO]: Epoch 011 - training loss: 0.7055, validation loss: 0.3861
2024-05-24 00:03:22 [INFO]: Epoch 012 - training loss: 0.6969, validation loss: 0.3844
2024-05-24 00:03:34 [INFO]: Epoch 013 - training loss: 0.6897, validation loss: 0.3807
2024-05-24 00:03:46 [INFO]: Epoch 014 - training loss: 0.6833, validation loss: 0.3804
2024-05-24 00:03:59 [INFO]: Epoch 015 - training loss: 0.6776, validation loss: 0.3794
2024-05-24 00:04:11 [INFO]: Epoch 016 - training loss: 0.6719, validation loss: 0.3783
2024-05-24 00:04:23 [INFO]: Epoch 017 - training loss: 0.6672, validation loss: 0.3772
2024-05-24 00:04:36 [INFO]: Epoch 018 - training loss: 0.6629, validation loss: 0.3768
2024-05-24 00:04:48 [INFO]: Epoch 019 - training loss: 0.6581, validation loss: 0.3771
2024-05-24 00:05:00 [INFO]: Epoch 020 - training loss: 0.6558, validation loss: 0.3775
2024-05-24 00:05:12 [INFO]: Epoch 021 - training loss: 0.6526, validation loss: 0.3742
2024-05-24 00:05:25 [INFO]: Epoch 022 - training loss: 0.6487, validation loss: 0.3740
2024-05-24 00:05:37 [INFO]: Epoch 023 - training loss: 0.6457, validation loss: 0.3736
2024-05-24 00:05:49 [INFO]: Epoch 024 - training loss: 0.6437, validation loss: 0.3737
2024-05-24 00:06:02 [INFO]: Epoch 025 - training loss: 0.6414, validation loss: 0.3748
2024-05-24 00:06:14 [INFO]: Epoch 026 - training loss: 0.6392, validation loss: 0.3745
2024-05-24 00:06:26 [INFO]: Epoch 027 - training loss: 0.6385, validation loss: 0.3764
2024-05-24 00:06:38 [INFO]: Epoch 028 - training loss: 0.6345, validation loss: 0.3717
2024-05-24 00:06:51 [INFO]: Epoch 029 - training loss: 0.6293, validation loss: 0.3742
2024-05-24 00:07:03 [INFO]: Epoch 030 - training loss: 0.6283, validation loss: 0.3724
2024-05-24 00:07:16 [INFO]: Epoch 031 - training loss: 0.6251, validation loss: 0.3724
2024-05-24 00:07:28 [INFO]: Epoch 032 - training loss: 0.6212, validation loss: 0.3721
2024-05-24 00:07:40 [INFO]: Epoch 033 - training loss: 0.6182, validation loss: 0.3718
2024-05-24 00:07:53 [INFO]: Epoch 034 - training loss: 0.6160, validation loss: 0.3722
2024-05-24 00:08:05 [INFO]: Epoch 035 - training loss: 0.6149, validation loss: 0.3717
2024-05-24 00:08:17 [INFO]: Epoch 036 - training loss: 0.6109, validation loss: 0.3709
2024-05-24 00:08:29 [INFO]: Epoch 037 - training loss: 0.6092, validation loss: 0.3741
2024-05-24 00:08:42 [INFO]: Epoch 038 - training loss: 0.6065, validation loss: 0.3751
2024-05-24 00:08:54 [INFO]: Epoch 039 - training loss: 0.6047, validation loss: 0.3724
2024-05-24 00:09:06 [INFO]: Epoch 040 - training loss: 0.6010, validation loss: 0.3727
2024-05-24 00:09:18 [INFO]: Epoch 041 - training loss: 0.5983, validation loss: 0.3736
2024-05-24 00:09:31 [INFO]: Epoch 042 - training loss: 0.5958, validation loss: 0.3741
2024-05-24 00:09:43 [INFO]: Epoch 043 - training loss: 0.5934, validation loss: 0.3737
2024-05-24 00:09:55 [INFO]: Epoch 044 - training loss: 0.5907, validation loss: 0.3737
2024-05-24 00:10:08 [INFO]: Epoch 045 - training loss: 0.5878, validation loss: 0.3762
2024-05-24 00:10:20 [INFO]: Epoch 046 - training loss: 0.5926, validation loss: 0.3748
2024-05-24 00:10:20 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-05-24 00:10:20 [INFO]: Finished training. The best model is from epoch#36.
2024-05-24 00:10:20 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_2/BRITS_physionet_2012_seta/20240524_T000051/BRITS.pypots
2024-05-24 00:10:22 [INFO]: BRITS on PhysioNet-2012-seta: MAE=0.2561, MSE=0.2566
2024-05-24 00:10:32 [INFO]: Successfully saved to augmentation_premask_post_norm_saved_results/round_2/BRITS_physionet_2012_seta/imputation.pkl
2024-05-24 00:10:32 [INFO]: Using the given device: cuda:0
2024-05-24 00:10:32 [INFO]: Model files will be saved to augmentation_premask_post_norm_saved_results/round_2/MRNN_physionet_2012_seta/20240524_T001032
2024-05-24 00:10:32 [INFO]: Tensorboard file will be saved to augmentation_premask_post_norm_saved_results/round_2/MRNN_physionet_2012_seta/20240524_T001032/tensorboard
2024-05-24 00:10:32 [INFO]: MRNN initialized with the given hyperparameters, the number of trainable parameters: 7,599
2024-05-24 00:10:38 [INFO]: Epoch 001 - training loss: 1.1697, validation loss: 0.9984
2024-05-24 00:10:38 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_2/MRNN_physionet_2012_seta/20240524_T001032/MRNN_epoch1_loss0.9983904868364334.pypots
2024-05-24 00:10:41 [INFO]: Epoch 002 - training loss: 0.7095, validation loss: 0.9728
2024-05-24 00:10:41 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_2/MRNN_physionet_2012_seta/20240524_T001032/MRNN_epoch2_loss0.9728396654129028.pypots
2024-05-24 00:10:43 [INFO]: Epoch 003 - training loss: 0.5942, validation loss: 0.9444
2024-05-24 00:10:43 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_2/MRNN_physionet_2012_seta/20240524_T001032/MRNN_epoch3_loss0.9444229632616044.pypots
2024-05-24 00:10:46 [INFO]: Epoch 004 - training loss: 0.5491, validation loss: 0.9340
2024-05-24 00:10:46 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_2/MRNN_physionet_2012_seta/20240524_T001032/MRNN_epoch4_loss0.9340411454439164.pypots
2024-05-24 00:10:49 [INFO]: Epoch 005 - training loss: 0.5325, validation loss: 0.9288
2024-05-24 00:10:49 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_2/MRNN_physionet_2012_seta/20240524_T001032/MRNN_epoch5_loss0.9288233548402787.pypots
2024-05-24 00:10:52 [INFO]: Epoch 006 - training loss: 0.5094, validation loss: 0.9246
2024-05-24 00:10:52 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_2/MRNN_physionet_2012_seta/20240524_T001032/MRNN_epoch6_loss0.924555441737175.pypots
2024-05-24 00:10:55 [INFO]: Epoch 007 - training loss: 0.4961, validation loss: 0.9215
2024-05-24 00:10:55 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_2/MRNN_physionet_2012_seta/20240524_T001032/MRNN_epoch7_loss0.9214642167091369.pypots
2024-05-24 00:10:57 [INFO]: Epoch 008 - training loss: 0.4839, validation loss: 0.9200
2024-05-24 00:10:57 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_2/MRNN_physionet_2012_seta/20240524_T001032/MRNN_epoch8_loss0.9199943900108337.pypots
2024-05-24 00:11:00 [INFO]: Epoch 009 - training loss: 0.4826, validation loss: 0.9202
2024-05-24 00:11:00 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_2/MRNN_physionet_2012_seta/20240524_T001032/MRNN_epoch9_loss0.9202194392681122.pypots
2024-05-24 00:11:03 [INFO]: Epoch 010 - training loss: 0.4669, validation loss: 0.9192
2024-05-24 00:11:03 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_2/MRNN_physionet_2012_seta/20240524_T001032/MRNN_epoch10_loss0.9192363917827606.pypots
2024-05-24 00:11:06 [INFO]: Epoch 011 - training loss: 0.4700, validation loss: 0.9197
2024-05-24 00:11:06 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_2/MRNN_physionet_2012_seta/20240524_T001032/MRNN_epoch11_loss0.9196608930826187.pypots
2024-05-24 00:11:09 [INFO]: Epoch 012 - training loss: 0.4508, validation loss: 0.9208
2024-05-24 00:11:09 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_2/MRNN_physionet_2012_seta/20240524_T001032/MRNN_epoch12_loss0.9208170980215072.pypots
2024-05-24 00:11:11 [INFO]: Epoch 013 - training loss: 0.4527, validation loss: 0.9228
2024-05-24 00:11:11 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_2/MRNN_physionet_2012_seta/20240524_T001032/MRNN_epoch13_loss0.922821369767189.pypots
2024-05-24 00:11:14 [INFO]: Epoch 014 - training loss: 0.4537, validation loss: 0.9249
2024-05-24 00:11:14 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_2/MRNN_physionet_2012_seta/20240524_T001032/MRNN_epoch14_loss0.9249134629964828.pypots
2024-05-24 00:11:17 [INFO]: Epoch 015 - training loss: 0.4430, validation loss: 0.9267
2024-05-24 00:11:17 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_2/MRNN_physionet_2012_seta/20240524_T001032/MRNN_epoch15_loss0.9267226010560989.pypots
2024-05-24 00:11:20 [INFO]: Epoch 016 - training loss: 0.4473, validation loss: 0.9287
2024-05-24 00:11:20 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_2/MRNN_physionet_2012_seta/20240524_T001032/MRNN_epoch16_loss0.9287496000528336.pypots
2024-05-24 00:11:23 [INFO]: Epoch 017 - training loss: 0.4381, validation loss: 0.9305
2024-05-24 00:11:23 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_2/MRNN_physionet_2012_seta/20240524_T001032/MRNN_epoch17_loss0.9304859489202499.pypots
2024-05-24 00:11:25 [INFO]: Epoch 018 - training loss: 0.4359, validation loss: 0.9316
2024-05-24 00:11:25 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_2/MRNN_physionet_2012_seta/20240524_T001032/MRNN_epoch18_loss0.9315962970256806.pypots
2024-05-24 00:11:28 [INFO]: Epoch 019 - training loss: 0.4363, validation loss: 0.9336
2024-05-24 00:11:28 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_2/MRNN_physionet_2012_seta/20240524_T001032/MRNN_epoch19_loss0.9336102306842804.pypots
2024-05-24 00:11:31 [INFO]: Epoch 020 - training loss: 0.4342, validation loss: 0.9348
2024-05-24 00:11:31 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_2/MRNN_physionet_2012_seta/20240524_T001032/MRNN_epoch20_loss0.934795543551445.pypots
2024-05-24 00:11:31 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-05-24 00:11:31 [INFO]: Finished training. The best model is from epoch#10.
2024-05-24 00:11:31 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_2/MRNN_physionet_2012_seta/20240524_T001032/MRNN.pypots
2024-05-24 00:11:32 [INFO]: MRNN on PhysioNet-2012-seta: MAE=0.6877, MSE=0.9000
2024-05-24 00:11:36 [INFO]: Successfully saved to augmentation_premask_post_norm_saved_results/round_2/MRNN_physionet_2012_seta/imputation.pkl
2024-05-24 00:11:36 [INFO]: Using the given device: cpu
2024-05-24 00:11:36 [INFO]: LOCF on PhysioNet-2012-seta: MAE=0.4038, MSE=0.5072
2024-05-24 00:11:36 [INFO]: Successfully created the given path "saved_results/round_2/LOCF_physionet_2012_seta".
2024-05-24 00:11:37 [INFO]: Successfully saved to augmentation_premask_post_norm_saved_results/round_2/LOCF_physionet_2012_seta/imputation.pkl
2024-05-24 00:11:37 [INFO]: Median on PhysioNet-2012-seta: MAE=0.6908, MSE=1.0216
2024-05-24 00:11:37 [INFO]: Successfully created the given path "saved_results/round_2/Median_physionet_2012_seta".
2024-05-24 00:11:37 [INFO]: Successfully saved to augmentation_premask_post_norm_saved_results/round_2/Median_physionet_2012_seta/imputation.pkl
2024-05-24 00:11:37 [INFO]: Mean on PhysioNet-2012-seta: MAE=0.7063, MSE=0.9786
2024-05-24 00:11:37 [INFO]: Successfully created the given path "saved_results/round_2/Mean_physionet_2012_seta".
2024-05-24 00:11:37 [INFO]: Successfully saved to augmentation_premask_post_norm_saved_results/round_2/Mean_physionet_2012_seta/imputation.pkl
2024-05-24 00:11:37 [INFO]: Have set the random seed as 2026 for numpy and pytorch.
2024-05-24 00:11:37 [INFO]: Using the given device: cuda:0
2024-05-24 00:11:37 [INFO]: Model files will be saved to augmentation_premask_post_norm_saved_results/round_3/SAITS_physionet_2012_seta/20240524_T001137
2024-05-24 00:11:37 [INFO]: Tensorboard file will be saved to augmentation_premask_post_norm_saved_results/round_3/SAITS_physionet_2012_seta/20240524_T001137/tensorboard
2024-05-24 00:11:37 [INFO]: SAITS initialized with the given hyperparameters, the number of trainable parameters: 5,322,806
2024-05-24 00:11:38 [INFO]: Epoch 001 - training loss: 1.0748, validation loss: 0.5207
2024-05-24 00:11:39 [INFO]: Epoch 002 - training loss: 0.6935, validation loss: 0.4553
2024-05-24 00:11:40 [INFO]: Epoch 003 - training loss: 0.5757, validation loss: 0.4236
2024-05-24 00:11:42 [INFO]: Epoch 004 - training loss: 0.5091, validation loss: 0.4062
2024-05-24 00:11:43 [INFO]: Epoch 005 - training loss: 0.4764, validation loss: 0.3927
2024-05-24 00:11:44 [INFO]: Epoch 006 - training loss: 0.4442, validation loss: 0.3833
2024-05-24 00:11:45 [INFO]: Epoch 007 - training loss: 0.4145, validation loss: 0.3743
2024-05-24 00:11:46 [INFO]: Epoch 008 - training loss: 0.3897, validation loss: 0.3724
2024-05-24 00:11:47 [INFO]: Epoch 009 - training loss: 0.3677, validation loss: 0.3511
2024-05-24 00:11:49 [INFO]: Epoch 010 - training loss: 0.3563, validation loss: 0.3554
2024-05-24 00:11:50 [INFO]: Epoch 011 - training loss: 0.3360, validation loss: 0.3427
2024-05-24 00:11:51 [INFO]: Epoch 012 - training loss: 0.3197, validation loss: 0.3446
2024-05-24 00:11:52 [INFO]: Epoch 013 - training loss: 0.3041, validation loss: 0.3451
2024-05-24 00:11:53 [INFO]: Epoch 014 - training loss: 0.2968, validation loss: 0.3374
2024-05-24 00:11:54 [INFO]: Epoch 015 - training loss: 0.2863, validation loss: 0.3387
2024-05-24 00:11:56 [INFO]: Epoch 016 - training loss: 0.2694, validation loss: 0.3384
2024-05-24 00:11:57 [INFO]: Epoch 017 - training loss: 0.2631, validation loss: 0.3348
2024-05-24 00:11:58 [INFO]: Epoch 018 - training loss: 0.2623, validation loss: 0.3398
2024-05-24 00:11:59 [INFO]: Epoch 019 - training loss: 0.2521, validation loss: 0.3297
2024-05-24 00:12:00 [INFO]: Epoch 020 - training loss: 0.2398, validation loss: 0.3333
2024-05-24 00:12:01 [INFO]: Epoch 021 - training loss: 0.2335, validation loss: 0.3307
2024-05-24 00:12:03 [INFO]: Epoch 022 - training loss: 0.2313, validation loss: 0.3399
2024-05-24 00:12:04 [INFO]: Epoch 023 - training loss: 0.2244, validation loss: 0.3333
2024-05-24 00:12:05 [INFO]: Epoch 024 - training loss: 0.2171, validation loss: 0.3323
2024-05-24 00:12:06 [INFO]: Epoch 025 - training loss: 0.2205, validation loss: 0.3357
2024-05-24 00:12:07 [INFO]: Epoch 026 - training loss: 0.2108, validation loss: 0.3354
2024-05-24 00:12:08 [INFO]: Epoch 027 - training loss: 0.2041, validation loss: 0.3373
2024-05-24 00:12:10 [INFO]: Epoch 028 - training loss: 0.2006, validation loss: 0.3353
2024-05-24 00:12:11 [INFO]: Epoch 029 - training loss: 0.1989, validation loss: 0.3354
2024-05-24 00:12:11 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-05-24 00:12:11 [INFO]: Finished training. The best model is from epoch#19.
2024-05-24 00:12:11 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_3/SAITS_physionet_2012_seta/20240524_T001137/SAITS.pypots
2024-05-24 00:12:11 [INFO]: SAITS on PhysioNet-2012-seta: MAE=0.2753, MSE=0.2926
2024-05-24 00:12:11 [INFO]: Successfully saved to augmentation_premask_post_norm_saved_results/round_3/SAITS_physionet_2012_seta/imputation.pkl
2024-05-24 00:12:11 [INFO]: Using the given device: cuda:0
2024-05-24 00:12:11 [INFO]: Model files will be saved to augmentation_premask_post_norm_saved_results/round_3/Transformer_physionet_2012_seta/20240524_T001211
2024-05-24 00:12:11 [INFO]: Tensorboard file will be saved to augmentation_premask_post_norm_saved_results/round_3/Transformer_physionet_2012_seta/20240524_T001211/tensorboard
2024-05-24 00:12:11 [INFO]: Transformer initialized with the given hyperparameters, the number of trainable parameters: 2,152,613
2024-05-24 00:12:12 [INFO]: Epoch 001 - training loss: 1.2370, validation loss: 0.5861
2024-05-24 00:12:13 [INFO]: Epoch 002 - training loss: 0.7505, validation loss: 0.5025
2024-05-24 00:12:13 [INFO]: Epoch 003 - training loss: 0.6388, validation loss: 0.4708
2024-05-24 00:12:14 [INFO]: Epoch 004 - training loss: 0.5792, validation loss: 0.4556
2024-05-24 00:12:14 [INFO]: Epoch 005 - training loss: 0.5398, validation loss: 0.4524
2024-05-24 00:12:15 [INFO]: Epoch 006 - training loss: 0.5088, validation loss: 0.4239
2024-05-24 00:12:15 [INFO]: Epoch 007 - training loss: 0.4812, validation loss: 0.4121
2024-05-24 00:12:16 [INFO]: Epoch 008 - training loss: 0.4657, validation loss: 0.4085
2024-05-24 00:12:17 [INFO]: Epoch 009 - training loss: 0.4348, validation loss: 0.4021
2024-05-24 00:12:17 [INFO]: Epoch 010 - training loss: 0.4233, validation loss: 0.3986
2024-05-24 00:12:18 [INFO]: Epoch 011 - training loss: 0.4118, validation loss: 0.3880
2024-05-24 00:12:18 [INFO]: Epoch 012 - training loss: 0.3960, validation loss: 0.3888
2024-05-24 00:12:19 [INFO]: Epoch 013 - training loss: 0.3886, validation loss: 0.3829
2024-05-24 00:12:20 [INFO]: Epoch 014 - training loss: 0.3789, validation loss: 0.3785
2024-05-24 00:12:20 [INFO]: Epoch 015 - training loss: 0.3619, validation loss: 0.3824
2024-05-24 00:12:21 [INFO]: Epoch 016 - training loss: 0.3555, validation loss: 0.3737
2024-05-24 00:12:21 [INFO]: Epoch 017 - training loss: 0.3431, validation loss: 0.3693
2024-05-24 00:12:22 [INFO]: Epoch 018 - training loss: 0.3359, validation loss: 0.3700
2024-05-24 00:12:23 [INFO]: Epoch 019 - training loss: 0.3342, validation loss: 0.3742
2024-05-24 00:12:23 [INFO]: Epoch 020 - training loss: 0.3257, validation loss: 0.3629
2024-05-24 00:12:24 [INFO]: Epoch 021 - training loss: 0.3136, validation loss: 0.3634
2024-05-24 00:12:24 [INFO]: Epoch 022 - training loss: 0.3109, validation loss: 0.3635
2024-05-24 00:12:25 [INFO]: Epoch 023 - training loss: 0.3055, validation loss: 0.3585
2024-05-24 00:12:26 [INFO]: Epoch 024 - training loss: 0.2970, validation loss: 0.3564
2024-05-24 00:12:26 [INFO]: Epoch 025 - training loss: 0.2899, validation loss: 0.3524
2024-05-24 00:12:27 [INFO]: Epoch 026 - training loss: 0.2922, validation loss: 0.3512
2024-05-24 00:12:27 [INFO]: Epoch 027 - training loss: 0.2834, validation loss: 0.3526
2024-05-24 00:12:28 [INFO]: Epoch 028 - training loss: 0.2773, validation loss: 0.3604
2024-05-24 00:12:28 [INFO]: Epoch 029 - training loss: 0.2724, validation loss: 0.3554
2024-05-24 00:12:29 [INFO]: Epoch 030 - training loss: 0.2683, validation loss: 0.3524
2024-05-24 00:12:30 [INFO]: Epoch 031 - training loss: 0.2646, validation loss: 0.3544
2024-05-24 00:12:30 [INFO]: Epoch 032 - training loss: 0.2656, validation loss: 0.3526
2024-05-24 00:12:31 [INFO]: Epoch 033 - training loss: 0.2614, validation loss: 0.3550
2024-05-24 00:12:31 [INFO]: Epoch 034 - training loss: 0.2518, validation loss: 0.3529
2024-05-24 00:12:32 [INFO]: Epoch 035 - training loss: 0.2513, validation loss: 0.3537
2024-05-24 00:12:33 [INFO]: Epoch 036 - training loss: 0.2483, validation loss: 0.3565
2024-05-24 00:12:33 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-05-24 00:12:33 [INFO]: Finished training. The best model is from epoch#26.
2024-05-24 00:12:33 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_3/Transformer_physionet_2012_seta/20240524_T001211/Transformer.pypots
2024-05-24 00:12:33 [INFO]: Transformer on PhysioNet-2012-seta: MAE=0.2807, MSE=0.3007
2024-05-24 00:12:33 [INFO]: Successfully saved to augmentation_premask_post_norm_saved_results/round_3/Transformer_physionet_2012_seta/imputation.pkl
2024-05-24 00:12:33 [INFO]: Using the given device: cuda:0
2024-05-24 00:12:33 [INFO]: Model files will be saved to augmentation_premask_post_norm_saved_results/round_3/TimesNet_physionet_2012_seta/20240524_T001233
2024-05-24 00:12:33 [INFO]: Tensorboard file will be saved to augmentation_premask_post_norm_saved_results/round_3/TimesNet_physionet_2012_seta/20240524_T001233/tensorboard
2024-05-24 00:12:33 [INFO]: TimesNet initialized with the given hyperparameters, the number of trainable parameters: 21,649,573
2024-05-24 00:12:34 [INFO]: Epoch 001 - training loss: 0.5227, validation loss: 1.1532
2024-05-24 00:12:35 [INFO]: Epoch 002 - training loss: 0.4573, validation loss: 0.9510
2024-05-24 00:12:35 [INFO]: Epoch 003 - training loss: 0.5031, validation loss: 0.6889
2024-05-24 00:12:36 [INFO]: Epoch 004 - training loss: 0.4031, validation loss: 0.3840
2024-05-24 00:12:37 [INFO]: Epoch 005 - training loss: 0.3710, validation loss: 0.3413
2024-05-24 00:12:37 [INFO]: Epoch 006 - training loss: 0.3300, validation loss: 0.3489
2024-05-24 00:12:38 [INFO]: Epoch 007 - training loss: 0.3234, validation loss: 0.3309
2024-05-24 00:12:39 [INFO]: Epoch 008 - training loss: 0.3117, validation loss: 0.3371
2024-05-24 00:12:39 [INFO]: Epoch 009 - training loss: 0.3049, validation loss: 0.3261
2024-05-24 00:12:40 [INFO]: Epoch 010 - training loss: 0.3026, validation loss: 0.3207
2024-05-24 00:12:41 [INFO]: Epoch 011 - training loss: 0.2969, validation loss: 0.3345
2024-05-24 00:12:42 [INFO]: Epoch 012 - training loss: 0.2904, validation loss: 0.3251
2024-05-24 00:12:42 [INFO]: Epoch 013 - training loss: 0.2869, validation loss: 0.3308
2024-05-24 00:12:43 [INFO]: Epoch 014 - training loss: 0.2797, validation loss: 0.3278
2024-05-24 00:12:44 [INFO]: Epoch 015 - training loss: 0.2737, validation loss: 0.3362
2024-05-24 00:12:44 [INFO]: Epoch 016 - training loss: 0.2830, validation loss: 0.3215
2024-05-24 00:12:45 [INFO]: Epoch 017 - training loss: 0.2667, validation loss: 0.3359
2024-05-24 00:12:46 [INFO]: Epoch 018 - training loss: 0.2603, validation loss: 0.3431
2024-05-24 00:12:47 [INFO]: Epoch 019 - training loss: 0.2647, validation loss: 0.3460
2024-05-24 00:12:47 [INFO]: Epoch 020 - training loss: 0.2602, validation loss: 0.3169
2024-05-24 00:12:48 [INFO]: Epoch 021 - training loss: 0.2537, validation loss: 0.3431
2024-05-24 00:12:49 [INFO]: Epoch 022 - training loss: 0.2467, validation loss: 0.3494
2024-05-24 00:12:49 [INFO]: Epoch 023 - training loss: 0.2430, validation loss: 0.3353
2024-05-24 00:12:50 [INFO]: Epoch 024 - training loss: 0.2386, validation loss: 0.3313
2024-05-24 00:12:51 [INFO]: Epoch 025 - training loss: 0.2339, validation loss: 0.3424
2024-05-24 00:12:52 [INFO]: Epoch 026 - training loss: 0.2332, validation loss: 0.3380
2024-05-24 00:12:52 [INFO]: Epoch 027 - training loss: 0.2319, validation loss: 0.3481
2024-05-24 00:12:53 [INFO]: Epoch 028 - training loss: 0.2287, validation loss: 0.3520
2024-05-24 00:12:54 [INFO]: Epoch 029 - training loss: 0.2265, validation loss: 0.3838
2024-05-24 00:12:54 [INFO]: Epoch 030 - training loss: 0.2182, validation loss: 0.3470
2024-05-24 00:12:54 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-05-24 00:12:54 [INFO]: Finished training. The best model is from epoch#20.
2024-05-24 00:12:54 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_3/TimesNet_physionet_2012_seta/20240524_T001233/TimesNet.pypots
2024-05-24 00:12:54 [INFO]: TimesNet on PhysioNet-2012-seta: MAE=0.2907, MSE=0.2775
2024-05-24 00:12:55 [INFO]: Successfully saved to augmentation_premask_post_norm_saved_results/round_3/TimesNet_physionet_2012_seta/imputation.pkl
2024-05-24 00:12:55 [INFO]: Using the given device: cuda:0
2024-05-24 00:12:55 [INFO]: Model files will be saved to augmentation_premask_post_norm_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T001255
2024-05-24 00:12:55 [INFO]: Tensorboard file will be saved to augmentation_premask_post_norm_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T001255/tensorboard
2024-05-24 00:12:55 [INFO]: CSDI initialized with the given hyperparameters, the number of trainable parameters: 1,694,753
2024-05-24 00:13:38 [INFO]: Epoch 001 - training loss: 0.4126, validation loss: 0.3350
2024-05-24 00:13:38 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T001255/CSDI_epoch1_loss0.3350055456161499.pypots
2024-05-24 00:14:21 [INFO]: Epoch 002 - training loss: 0.3207, validation loss: 0.3054
2024-05-24 00:14:21 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T001255/CSDI_epoch2_loss0.3054093405604362.pypots
2024-05-24 00:15:05 [INFO]: Epoch 003 - training loss: 0.2907, validation loss: 0.2850
2024-05-24 00:15:05 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T001255/CSDI_epoch3_loss0.28497460186481477.pypots
2024-05-24 00:15:48 [INFO]: Epoch 004 - training loss: 0.2948, validation loss: 0.2684
2024-05-24 00:15:48 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T001255/CSDI_epoch4_loss0.2683772072196007.pypots
2024-05-24 00:16:32 [INFO]: Epoch 005 - training loss: 0.2694, validation loss: 0.2558
2024-05-24 00:16:32 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T001255/CSDI_epoch5_loss0.2558260053396225.pypots
2024-05-24 00:17:16 [INFO]: Epoch 006 - training loss: 0.2628, validation loss: 0.2510
2024-05-24 00:17:16 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T001255/CSDI_epoch6_loss0.2509896382689476.pypots
2024-05-24 00:17:59 [INFO]: Epoch 007 - training loss: 0.2513, validation loss: 0.2435
2024-05-24 00:17:59 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T001255/CSDI_epoch7_loss0.243456107378006.pypots
2024-05-24 00:18:43 [INFO]: Epoch 008 - training loss: 0.2485, validation loss: 0.2316
2024-05-24 00:18:43 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T001255/CSDI_epoch8_loss0.23156059086322783.pypots
2024-05-24 00:19:27 [INFO]: Epoch 009 - training loss: 0.2269, validation loss: 0.2225
2024-05-24 00:19:27 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T001255/CSDI_epoch9_loss0.22248795703053476.pypots
2024-05-24 00:20:10 [INFO]: Epoch 010 - training loss: 0.2325, validation loss: 0.2302
2024-05-24 00:20:10 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T001255/CSDI_epoch10_loss0.23024665489792823.pypots
2024-05-24 00:20:54 [INFO]: Epoch 011 - training loss: 0.2329, validation loss: 0.2217
2024-05-24 00:20:54 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T001255/CSDI_epoch11_loss0.22171792685985564.pypots
2024-05-24 00:21:38 [INFO]: Epoch 012 - training loss: 0.2281, validation loss: 0.2191
2024-05-24 00:21:38 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T001255/CSDI_epoch12_loss0.21906583085656167.pypots
2024-05-24 00:22:22 [INFO]: Epoch 013 - training loss: 0.2261, validation loss: 0.2130
2024-05-24 00:22:22 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T001255/CSDI_epoch13_loss0.2129574917256832.pypots
2024-05-24 00:23:05 [INFO]: Epoch 014 - training loss: 0.2103, validation loss: 0.2206
2024-05-24 00:23:05 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T001255/CSDI_epoch14_loss0.2206340365111828.pypots
2024-05-24 00:23:49 [INFO]: Epoch 015 - training loss: 0.2140, validation loss: 0.2146
2024-05-24 00:23:49 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T001255/CSDI_epoch15_loss0.21460660248994828.pypots
2024-05-24 00:24:33 [INFO]: Epoch 016 - training loss: 0.2185, validation loss: 0.2182
2024-05-24 00:24:33 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T001255/CSDI_epoch16_loss0.21820478290319442.pypots
2024-05-24 00:25:17 [INFO]: Epoch 017 - training loss: 0.2202, validation loss: 0.2080
2024-05-24 00:25:17 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T001255/CSDI_epoch17_loss0.20798798203468322.pypots
2024-05-24 00:26:00 [INFO]: Epoch 018 - training loss: 0.2138, validation loss: 0.2079
2024-05-24 00:26:00 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T001255/CSDI_epoch18_loss0.20786806643009187.pypots
2024-05-24 00:26:44 [INFO]: Epoch 019 - training loss: 0.2098, validation loss: 0.2040
2024-05-24 00:26:44 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T001255/CSDI_epoch19_loss0.2040305405855179.pypots
2024-05-24 00:27:28 [INFO]: Epoch 020 - training loss: 0.2095, validation loss: 0.2116
2024-05-24 00:27:28 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T001255/CSDI_epoch20_loss0.21159525737166404.pypots
2024-05-24 00:28:12 [INFO]: Epoch 021 - training loss: 0.2053, validation loss: 0.2041
2024-05-24 00:28:12 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T001255/CSDI_epoch21_loss0.20411082357168198.pypots
2024-05-24 00:28:56 [INFO]: Epoch 022 - training loss: 0.2145, validation loss: 0.2033
2024-05-24 00:28:56 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T001255/CSDI_epoch22_loss0.20330890268087387.pypots
2024-05-24 00:29:39 [INFO]: Epoch 023 - training loss: 0.2030, validation loss: 0.2017
2024-05-24 00:29:39 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T001255/CSDI_epoch23_loss0.20166712030768394.pypots
2024-05-24 00:30:23 [INFO]: Epoch 024 - training loss: 0.2075, validation loss: 0.2044
2024-05-24 00:30:23 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T001255/CSDI_epoch24_loss0.20444290712475777.pypots
2024-05-24 00:31:07 [INFO]: Epoch 025 - training loss: 0.2038, validation loss: 0.2001
2024-05-24 00:31:07 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T001255/CSDI_epoch25_loss0.20012386441230773.pypots
2024-05-24 00:31:51 [INFO]: Epoch 026 - training loss: 0.2064, validation loss: 0.1999
2024-05-24 00:31:51 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T001255/CSDI_epoch26_loss0.1999073565006256.pypots
2024-05-24 00:32:34 [INFO]: Epoch 027 - training loss: 0.2157, validation loss: 0.2010
2024-05-24 00:32:34 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T001255/CSDI_epoch27_loss0.20098019242286683.pypots
2024-05-24 00:33:18 [INFO]: Epoch 028 - training loss: 0.2007, validation loss: 0.1998
2024-05-24 00:33:18 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T001255/CSDI_epoch28_loss0.1998012073338032.pypots
2024-05-24 00:34:02 [INFO]: Epoch 029 - training loss: 0.2075, validation loss: 0.2007
2024-05-24 00:34:02 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T001255/CSDI_epoch29_loss0.20071587935090066.pypots
2024-05-24 00:34:45 [INFO]: Epoch 030 - training loss: 0.2101, validation loss: 0.1972
2024-05-24 00:34:45 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T001255/CSDI_epoch30_loss0.19718886092305182.pypots
2024-05-24 00:35:29 [INFO]: Epoch 031 - training loss: 0.2113, validation loss: 0.1984
2024-05-24 00:35:29 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T001255/CSDI_epoch31_loss0.19837809577584267.pypots
2024-05-24 00:36:13 [INFO]: Epoch 032 - training loss: 0.1999, validation loss: 0.2074
2024-05-24 00:36:13 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T001255/CSDI_epoch32_loss0.20742077752947807.pypots
2024-05-24 00:36:56 [INFO]: Epoch 033 - training loss: 0.2082, validation loss: 0.1964
2024-05-24 00:36:56 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T001255/CSDI_epoch33_loss0.19638111740350722.pypots
2024-05-24 00:37:40 [INFO]: Epoch 034 - training loss: 0.2123, validation loss: 0.1998
2024-05-24 00:37:40 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T001255/CSDI_epoch34_loss0.19982834458351134.pypots
2024-05-24 00:38:24 [INFO]: Epoch 035 - training loss: 0.1931, validation loss: 0.1997
2024-05-24 00:38:24 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T001255/CSDI_epoch35_loss0.19965170249342917.pypots
2024-05-24 00:39:08 [INFO]: Epoch 036 - training loss: 0.2038, validation loss: 0.1933
2024-05-24 00:39:08 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T001255/CSDI_epoch36_loss0.19330952689051628.pypots
2024-05-24 00:39:51 [INFO]: Epoch 037 - training loss: 0.2008, validation loss: 0.1950
2024-05-24 00:39:51 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T001255/CSDI_epoch37_loss0.19503117278218268.pypots
2024-05-24 00:40:35 [INFO]: Epoch 038 - training loss: 0.1982, validation loss: 0.2049
2024-05-24 00:40:35 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T001255/CSDI_epoch38_loss0.20491471961140634.pypots
2024-05-24 00:41:19 [INFO]: Epoch 039 - training loss: 0.1950, validation loss: 0.1920
2024-05-24 00:41:19 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T001255/CSDI_epoch39_loss0.1919625833630562.pypots
2024-05-24 00:42:03 [INFO]: Epoch 040 - training loss: 0.1992, validation loss: 0.1900
2024-05-24 00:42:03 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T001255/CSDI_epoch40_loss0.18996693566441536.pypots
2024-05-24 00:42:46 [INFO]: Epoch 041 - training loss: 0.2005, validation loss: 0.1939
2024-05-24 00:42:46 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T001255/CSDI_epoch41_loss0.19393550455570222.pypots
2024-05-24 00:43:30 [INFO]: Epoch 042 - training loss: 0.2068, validation loss: 0.1891
2024-05-24 00:43:30 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T001255/CSDI_epoch42_loss0.18914998769760133.pypots
2024-05-24 00:44:14 [INFO]: Epoch 043 - training loss: 0.1986, validation loss: 0.1950
2024-05-24 00:44:14 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T001255/CSDI_epoch43_loss0.19501798003911971.pypots
2024-05-24 00:44:58 [INFO]: Epoch 044 - training loss: 0.2006, validation loss: 0.1966
2024-05-24 00:44:58 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T001255/CSDI_epoch44_loss0.196646149456501.pypots
2024-05-24 00:45:42 [INFO]: Epoch 045 - training loss: 0.2051, validation loss: 0.1887
2024-05-24 00:45:42 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T001255/CSDI_epoch45_loss0.1887196995317936.pypots
2024-05-24 00:46:25 [INFO]: Epoch 046 - training loss: 0.1942, validation loss: 0.1911
2024-05-24 00:46:25 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T001255/CSDI_epoch46_loss0.1910873271524906.pypots
2024-05-24 00:47:09 [INFO]: Epoch 047 - training loss: 0.1918, validation loss: 0.1931
2024-05-24 00:47:09 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T001255/CSDI_epoch47_loss0.19309098571538924.pypots
2024-05-24 00:47:53 [INFO]: Epoch 048 - training loss: 0.2045, validation loss: 0.1905
2024-05-24 00:47:53 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T001255/CSDI_epoch48_loss0.19054792448878288.pypots
2024-05-24 00:48:37 [INFO]: Epoch 049 - training loss: 0.2053, validation loss: 0.1904
2024-05-24 00:48:37 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T001255/CSDI_epoch49_loss0.19044826850295066.pypots
2024-05-24 00:49:21 [INFO]: Epoch 050 - training loss: 0.1955, validation loss: 0.1891
2024-05-24 00:49:21 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T001255/CSDI_epoch50_loss0.1890552520751953.pypots
2024-05-24 00:50:04 [INFO]: Epoch 051 - training loss: 0.1947, validation loss: 0.1883
2024-05-24 00:50:04 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T001255/CSDI_epoch51_loss0.18834725692868232.pypots
2024-05-24 00:50:48 [INFO]: Epoch 052 - training loss: 0.1931, validation loss: 0.1898
2024-05-24 00:50:48 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T001255/CSDI_epoch52_loss0.1897849939763546.pypots
2024-05-24 00:51:32 [INFO]: Epoch 053 - training loss: 0.1900, validation loss: 0.1898
2024-05-24 00:51:32 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T001255/CSDI_epoch53_loss0.18983635380864144.pypots
2024-05-24 00:52:16 [INFO]: Epoch 054 - training loss: 0.2007, validation loss: 0.1904
2024-05-24 00:52:16 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T001255/CSDI_epoch54_loss0.19040289223194123.pypots
2024-05-24 00:53:00 [INFO]: Epoch 055 - training loss: 0.1883, validation loss: 0.1869
2024-05-24 00:53:00 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T001255/CSDI_epoch55_loss0.1869012162089348.pypots
2024-05-24 00:53:43 [INFO]: Epoch 056 - training loss: 0.1922, validation loss: 0.1896
2024-05-24 00:53:43 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T001255/CSDI_epoch56_loss0.18959611132740975.pypots
2024-05-24 00:54:27 [INFO]: Epoch 057 - training loss: 0.1864, validation loss: 0.1926
2024-05-24 00:54:27 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T001255/CSDI_epoch57_loss0.19259174838662146.pypots
2024-05-24 00:55:11 [INFO]: Epoch 058 - training loss: 0.2010, validation loss: 0.1901
2024-05-24 00:55:11 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T001255/CSDI_epoch58_loss0.19012930914759635.pypots
2024-05-24 00:55:55 [INFO]: Epoch 059 - training loss: 0.1926, validation loss: 0.1935
2024-05-24 00:55:55 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T001255/CSDI_epoch59_loss0.19349330291152.pypots
2024-05-24 00:56:38 [INFO]: Epoch 060 - training loss: 0.1963, validation loss: 0.1856
2024-05-24 00:56:38 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T001255/CSDI_epoch60_loss0.18557429537177086.pypots
2024-05-24 00:57:22 [INFO]: Epoch 061 - training loss: 0.1984, validation loss: 0.1879
2024-05-24 00:57:22 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T001255/CSDI_epoch61_loss0.18792561814188957.pypots
2024-05-24 00:58:06 [INFO]: Epoch 062 - training loss: 0.1855, validation loss: 0.1853
2024-05-24 00:58:06 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T001255/CSDI_epoch62_loss0.18527969643473624.pypots
2024-05-24 00:58:49 [INFO]: Epoch 063 - training loss: 0.1901, validation loss: 0.1867
2024-05-24 00:58:49 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T001255/CSDI_epoch63_loss0.18672717809677125.pypots
2024-05-24 00:59:33 [INFO]: Epoch 064 - training loss: 0.2008, validation loss: 0.1897
2024-05-24 00:59:33 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T001255/CSDI_epoch64_loss0.18966318145394326.pypots
2024-05-24 01:00:17 [INFO]: Epoch 065 - training loss: 0.1995, validation loss: 0.1841
2024-05-24 07:34:01 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T001255/CSDI_epoch65_loss0.18407405763864518.pypots
2024-05-24 07:34:45 [INFO]: Epoch 066 - training loss: 0.1973, validation loss: 0.1853
2024-05-24 07:34:45 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T001255/CSDI_epoch66_loss0.18533337414264678.pypots
2024-05-24 07:35:29 [INFO]: Epoch 067 - training loss: 0.1878, validation loss: 0.1878
2024-05-24 07:35:29 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T001255/CSDI_epoch67_loss0.18783193305134774.pypots
2024-05-24 07:36:13 [INFO]: Epoch 068 - training loss: 0.1868, validation loss: 0.1893
2024-05-24 07:36:13 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T001255/CSDI_epoch68_loss0.1893407016992569.pypots
2024-05-24 07:36:57 [INFO]: Epoch 069 - training loss: 0.2003, validation loss: 0.1877
2024-05-24 07:36:57 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T001255/CSDI_epoch69_loss0.18771236166357994.pypots
2024-05-24 07:37:41 [INFO]: Epoch 070 - training loss: 0.1931, validation loss: 0.1883
2024-05-24 07:37:41 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T001255/CSDI_epoch70_loss0.18831693530082702.pypots
2024-05-24 07:38:25 [INFO]: Epoch 071 - training loss: 0.1873, validation loss: 0.1842
2024-05-24 07:38:25 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T001255/CSDI_epoch71_loss0.18423014283180236.pypots
2024-05-24 07:39:09 [INFO]: Epoch 072 - training loss: 0.1843, validation loss: 0.1857
2024-05-24 07:39:09 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T001255/CSDI_epoch72_loss0.18570383861660958.pypots
2024-05-24 07:39:53 [INFO]: Epoch 073 - training loss: 0.1906, validation loss: 0.1843
2024-05-24 07:39:53 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T001255/CSDI_epoch73_loss0.18428969457745553.pypots
2024-05-24 07:40:37 [INFO]: Epoch 074 - training loss: 0.1914, validation loss: 0.1854
2024-05-24 07:40:37 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T001255/CSDI_epoch74_loss0.1854202404618263.pypots
2024-05-24 07:41:21 [INFO]: Epoch 075 - training loss: 0.1925, validation loss: 0.1842
2024-05-24 07:41:21 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T001255/CSDI_epoch75_loss0.1842399626970291.pypots
2024-05-24 07:41:21 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-05-24 07:41:21 [INFO]: Finished training. The best model is from epoch#65.
2024-05-24 07:41:21 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_3/CSDI_physionet_2012_seta/20240524_T001255/CSDI.pypots
2024-05-24 07:48:43 [INFO]: CSDI on PhysioNet-2012-seta: MAE=0.2285, MSE=0.2507
2024-05-24 08:18:09 [INFO]: Successfully saved to augmentation_premask_post_norm_saved_results/round_3/CSDI_physionet_2012_seta/imputation.pkl
2024-05-24 08:18:09 [INFO]: Using the given device: cuda:0
2024-05-24 08:18:09 [INFO]: Model files will be saved to augmentation_premask_post_norm_saved_results/round_3/GPVAE_physionet_2012_seta/20240524_T081809
2024-05-24 08:18:09 [INFO]: Tensorboard file will be saved to augmentation_premask_post_norm_saved_results/round_3/GPVAE_physionet_2012_seta/20240524_T081809/tensorboard
2024-05-24 08:18:09 [INFO]: GPVAE initialized with the given hyperparameters, the number of trainable parameters: 1,028,244
2024-05-24 08:18:10 [INFO]: Epoch 001 - training loss: 42572.9314, validation loss: 0.9127
2024-05-24 08:18:11 [INFO]: Epoch 002 - training loss: 24403.3316, validation loss: 0.7559
2024-05-24 08:18:11 [INFO]: Epoch 003 - training loss: 23488.5370, validation loss: 0.7418
2024-05-24 08:18:12 [INFO]: Epoch 004 - training loss: 23191.0913, validation loss: 0.7124
2024-05-24 08:18:12 [INFO]: Epoch 005 - training loss: 23041.4109, validation loss: 0.6830
2024-05-24 08:18:13 [INFO]: Epoch 006 - training loss: 22957.5018, validation loss: 0.6792
2024-05-24 08:18:14 [INFO]: Epoch 007 - training loss: 22907.1046, validation loss: 0.6736
2024-05-24 08:18:14 [INFO]: Epoch 008 - training loss: 22874.7189, validation loss: 0.6724
2024-05-24 08:18:15 [INFO]: Epoch 009 - training loss: 22853.9095, validation loss: 0.6711
2024-05-24 08:18:15 [INFO]: Epoch 010 - training loss: 22838.5285, validation loss: 0.6669
2024-05-24 08:18:16 [INFO]: Epoch 011 - training loss: 22827.4677, validation loss: 0.6648
2024-05-24 08:18:17 [INFO]: Epoch 012 - training loss: 22819.3404, validation loss: 0.6680
2024-05-24 08:18:17 [INFO]: Epoch 013 - training loss: 22813.4049, validation loss: 0.6640
2024-05-24 08:18:18 [INFO]: Epoch 014 - training loss: 22808.4191, validation loss: 0.6592
2024-05-24 08:18:18 [INFO]: Epoch 015 - training loss: 22804.3318, validation loss: 0.6586
2024-05-24 08:18:19 [INFO]: Epoch 016 - training loss: 22800.8038, validation loss: 0.6659
2024-05-24 08:18:20 [INFO]: Epoch 017 - training loss: 22798.9188, validation loss: 0.6548
2024-05-24 08:18:20 [INFO]: Epoch 018 - training loss: 22796.2953, validation loss: 0.6587
2024-05-24 08:18:21 [INFO]: Epoch 019 - training loss: 22794.7805, validation loss: 0.6478
2024-05-24 08:18:22 [INFO]: Epoch 020 - training loss: 22792.3385, validation loss: 0.6543
2024-05-24 08:18:22 [INFO]: Epoch 021 - training loss: 22791.2418, validation loss: 0.6432
2024-05-24 08:18:23 [INFO]: Epoch 022 - training loss: 22789.5215, validation loss: 0.6400
2024-05-24 08:18:23 [INFO]: Epoch 023 - training loss: 22788.5628, validation loss: 0.6428
2024-05-24 08:18:24 [INFO]: Epoch 024 - training loss: 22787.4478, validation loss: 0.6370
2024-05-24 08:18:25 [INFO]: Epoch 025 - training loss: 22786.7927, validation loss: 0.6377
2024-05-24 08:18:25 [INFO]: Epoch 026 - training loss: 22785.3978, validation loss: 0.6359
2024-05-24 08:18:26 [INFO]: Epoch 027 - training loss: 22784.8830, validation loss: 0.6360
2024-05-24 08:18:26 [INFO]: Epoch 028 - training loss: 22783.9021, validation loss: 0.6449
2024-05-24 08:18:27 [INFO]: Epoch 029 - training loss: 22784.2887, validation loss: 0.6373
2024-05-24 08:18:28 [INFO]: Epoch 030 - training loss: 22783.4614, validation loss: 0.6506
2024-05-24 08:18:28 [INFO]: Epoch 031 - training loss: 22783.1828, validation loss: 0.6316
2024-05-24 08:18:29 [INFO]: Epoch 032 - training loss: 22782.0742, validation loss: 0.6316
2024-05-24 08:18:29 [INFO]: Epoch 033 - training loss: 22782.4525, validation loss: 0.6290
2024-05-24 08:18:30 [INFO]: Epoch 034 - training loss: 22782.4112, validation loss: 0.6300
2024-05-24 08:18:31 [INFO]: Epoch 035 - training loss: 22780.5611, validation loss: 0.6293
2024-05-24 08:18:31 [INFO]: Epoch 036 - training loss: 22780.4751, validation loss: 0.6271
2024-05-24 08:18:32 [INFO]: Epoch 037 - training loss: 22780.3215, validation loss: 0.6278
2024-05-24 08:18:32 [INFO]: Epoch 038 - training loss: 22779.7682, validation loss: 0.6261
2024-05-24 08:18:33 [INFO]: Epoch 039 - training loss: 22779.8819, validation loss: 0.6257
2024-05-24 08:18:34 [INFO]: Epoch 040 - training loss: 22778.7750, validation loss: 0.6220
2024-05-24 08:18:34 [INFO]: Epoch 041 - training loss: 22777.8424, validation loss: 0.6199
2024-05-24 08:18:35 [INFO]: Epoch 042 - training loss: 22777.0098, validation loss: 0.6166
2024-05-24 08:18:35 [INFO]: Epoch 043 - training loss: 22776.2107, validation loss: 0.6150
2024-05-24 08:18:36 [INFO]: Epoch 044 - training loss: 22774.8988, validation loss: 0.6101
2024-05-24 08:18:37 [INFO]: Epoch 045 - training loss: 22773.7337, validation loss: 0.6075
2024-05-24 08:18:37 [INFO]: Epoch 046 - training loss: 22772.5684, validation loss: 0.6041
2024-05-24 08:18:38 [INFO]: Epoch 047 - training loss: 22771.2966, validation loss: 0.5977
2024-05-24 08:18:38 [INFO]: Epoch 048 - training loss: 22770.7857, validation loss: 0.5964
2024-05-24 08:18:39 [INFO]: Epoch 049 - training loss: 22769.3216, validation loss: 0.5926
2024-05-24 08:18:40 [INFO]: Epoch 050 - training loss: 22768.7494, validation loss: 0.5874
2024-05-24 08:18:40 [INFO]: Epoch 051 - training loss: 22768.6454, validation loss: 0.5891
2024-05-24 08:18:41 [INFO]: Epoch 052 - training loss: 22768.5131, validation loss: 0.5827
2024-05-24 08:18:41 [INFO]: Epoch 053 - training loss: 22768.9116, validation loss: 0.5789
2024-05-24 08:18:42 [INFO]: Epoch 054 - training loss: 22765.4341, validation loss: 0.5742
2024-05-24 08:18:43 [INFO]: Epoch 055 - training loss: 22765.8249, validation loss: 0.5793
2024-05-24 08:18:43 [INFO]: Epoch 056 - training loss: 22764.1777, validation loss: 0.5707
2024-05-24 08:18:44 [INFO]: Epoch 057 - training loss: 22764.2445, validation loss: 0.5824
2024-05-24 08:18:44 [INFO]: Epoch 058 - training loss: 22765.1567, validation loss: 0.5637
2024-05-24 08:18:45 [INFO]: Epoch 059 - training loss: 22762.9415, validation loss: 0.5757
2024-05-24 08:18:46 [INFO]: Epoch 060 - training loss: 22762.9814, validation loss: 0.5586
2024-05-24 08:18:46 [INFO]: Epoch 061 - training loss: 22761.4842, validation loss: 0.5522
2024-05-24 08:18:47 [INFO]: Epoch 062 - training loss: 22759.9875, validation loss: 0.5525
2024-05-24 08:18:47 [INFO]: Epoch 063 - training loss: 22759.3330, validation loss: 0.5454
2024-05-24 08:18:48 [INFO]: Epoch 064 - training loss: 22758.4320, validation loss: 0.5446
2024-05-24 08:18:49 [INFO]: Epoch 065 - training loss: 22758.0905, validation loss: 0.5469
2024-05-24 08:18:49 [INFO]: Epoch 066 - training loss: 22757.4690, validation loss: 0.5411
2024-05-24 08:18:50 [INFO]: Epoch 067 - training loss: 22757.4648, validation loss: 0.5449
2024-05-24 08:18:50 [INFO]: Epoch 068 - training loss: 22757.3007, validation loss: 0.5378
2024-05-24 08:18:51 [INFO]: Epoch 069 - training loss: 22757.0825, validation loss: 0.5398
2024-05-24 08:18:52 [INFO]: Epoch 070 - training loss: 22756.3874, validation loss: 0.5413
2024-05-24 08:18:52 [INFO]: Epoch 071 - training loss: 22756.3485, validation loss: 0.5355
2024-05-24 08:18:53 [INFO]: Epoch 072 - training loss: 22755.8459, validation loss: 0.5383
2024-05-24 08:18:53 [INFO]: Epoch 073 - training loss: 22755.6357, validation loss: 0.5367
2024-05-24 08:18:54 [INFO]: Epoch 074 - training loss: 22755.3668, validation loss: 0.5344
2024-05-24 08:18:54 [INFO]: Epoch 075 - training loss: 22754.9327, validation loss: 0.5416
2024-05-24 08:18:55 [INFO]: Epoch 076 - training loss: 22755.0704, validation loss: 0.5361
2024-05-24 08:18:56 [INFO]: Epoch 077 - training loss: 22754.7185, validation loss: 0.5346
2024-05-24 08:18:56 [INFO]: Epoch 078 - training loss: 22754.8064, validation loss: 0.5354
2024-05-24 08:18:57 [INFO]: Epoch 079 - training loss: 22754.3882, validation loss: 0.5381
2024-05-24 08:18:57 [INFO]: Epoch 080 - training loss: 22754.3846, validation loss: 0.5385
2024-05-24 08:18:58 [INFO]: Epoch 081 - training loss: 22754.0552, validation loss: 0.5347
2024-05-24 08:18:59 [INFO]: Epoch 082 - training loss: 22754.3921, validation loss: 0.5378
2024-05-24 08:18:59 [INFO]: Epoch 083 - training loss: 22753.7738, validation loss: 0.5293
2024-05-24 08:19:00 [INFO]: Epoch 084 - training loss: 22753.7091, validation loss: 0.5340
2024-05-24 08:19:00 [INFO]: Epoch 085 - training loss: 22753.1427, validation loss: 0.5331
2024-05-24 08:19:01 [INFO]: Epoch 086 - training loss: 22753.6604, validation loss: 0.5303
2024-05-24 08:19:02 [INFO]: Epoch 087 - training loss: 22752.7545, validation loss: 0.5271
2024-05-24 08:19:02 [INFO]: Epoch 088 - training loss: 22751.8691, validation loss: 0.5316
2024-05-24 08:19:03 [INFO]: Epoch 089 - training loss: 22751.9027, validation loss: 0.5274
2024-05-24 08:19:03 [INFO]: Epoch 090 - training loss: 22751.7815, validation loss: 0.5289
2024-05-24 08:19:04 [INFO]: Epoch 091 - training loss: 22752.6999, validation loss: 0.5263
2024-05-24 08:19:05 [INFO]: Epoch 092 - training loss: 22751.7154, validation loss: 0.5221
2024-05-24 08:19:05 [INFO]: Epoch 093 - training loss: 22752.2387, validation loss: 0.5342
2024-05-24 08:19:06 [INFO]: Epoch 094 - training loss: 22751.9533, validation loss: 0.5469
2024-05-24 08:19:06 [INFO]: Epoch 095 - training loss: 22754.1088, validation loss: 0.5345
2024-05-24 08:19:07 [INFO]: Epoch 096 - training loss: 22755.2069, validation loss: 0.5213
2024-05-24 08:19:08 [INFO]: Epoch 097 - training loss: 22753.8417, validation loss: 0.5244
2024-05-24 08:19:08 [INFO]: Epoch 098 - training loss: 22751.4295, validation loss: 0.5194
2024-05-24 08:19:09 [INFO]: Epoch 099 - training loss: 22750.7807, validation loss: 0.5181
2024-05-24 08:19:09 [INFO]: Epoch 100 - training loss: 22749.9600, validation loss: 0.5154
2024-05-24 08:19:10 [INFO]: Epoch 101 - training loss: 22749.8252, validation loss: 0.5153
2024-05-24 08:19:11 [INFO]: Epoch 102 - training loss: 22749.4973, validation loss: 0.5162
2024-05-24 08:19:11 [INFO]: Epoch 103 - training loss: 22749.3270, validation loss: 0.5139
2024-05-24 08:19:12 [INFO]: Epoch 104 - training loss: 22749.0557, validation loss: 0.5125
2024-05-24 08:19:12 [INFO]: Epoch 105 - training loss: 22749.1467, validation loss: 0.5132
2024-05-24 08:19:13 [INFO]: Epoch 106 - training loss: 22749.0540, validation loss: 0.5102
2024-05-24 08:19:14 [INFO]: Epoch 107 - training loss: 22748.8985, validation loss: 0.5129
2024-05-24 08:19:14 [INFO]: Epoch 108 - training loss: 22748.8862, validation loss: 0.5084
2024-05-24 08:19:15 [INFO]: Epoch 109 - training loss: 22748.3480, validation loss: 0.5081
2024-05-24 08:19:15 [INFO]: Epoch 110 - training loss: 22748.2771, validation loss: 0.5084
2024-05-24 08:19:16 [INFO]: Epoch 111 - training loss: 22748.4114, validation loss: 0.5102
2024-05-24 08:19:17 [INFO]: Epoch 112 - training loss: 22749.0561, validation loss: 0.5069
2024-05-24 08:19:17 [INFO]: Epoch 113 - training loss: 22748.5552, validation loss: 0.5089
2024-05-24 08:19:18 [INFO]: Epoch 114 - training loss: 22749.1413, validation loss: 0.5072
2024-05-24 08:19:18 [INFO]: Epoch 115 - training loss: 22748.5987, validation loss: 0.5140
2024-05-24 08:19:19 [INFO]: Epoch 116 - training loss: 22749.2465, validation loss: 0.5112
2024-05-24 08:19:20 [INFO]: Epoch 117 - training loss: 22748.3167, validation loss: 0.5113
2024-05-24 08:19:20 [INFO]: Epoch 118 - training loss: 22747.8096, validation loss: 0.5065
2024-05-24 08:19:21 [INFO]: Epoch 119 - training loss: 22747.2762, validation loss: 0.5090
2024-05-24 08:19:21 [INFO]: Epoch 120 - training loss: 22747.3048, validation loss: 0.5104
2024-05-24 08:19:22 [INFO]: Epoch 121 - training loss: 22747.5885, validation loss: 0.5035
2024-05-24 08:19:23 [INFO]: Epoch 122 - training loss: 22747.0687, validation loss: 0.5006
2024-05-24 08:19:23 [INFO]: Epoch 123 - training loss: 22747.7183, validation loss: 0.5021
2024-05-24 08:19:24 [INFO]: Epoch 124 - training loss: 22746.7662, validation loss: 0.5035
2024-05-24 08:19:24 [INFO]: Epoch 125 - training loss: 22746.7559, validation loss: 0.5019
2024-05-24 08:19:25 [INFO]: Epoch 126 - training loss: 22747.5656, validation loss: 0.5009
2024-05-24 08:19:26 [INFO]: Epoch 127 - training loss: 22746.6227, validation loss: 0.5028
2024-05-24 08:19:26 [INFO]: Epoch 128 - training loss: 22747.1848, validation loss: 0.5038
2024-05-24 08:19:27 [INFO]: Epoch 129 - training loss: 22748.8631, validation loss: 0.5001
2024-05-24 08:19:27 [INFO]: Epoch 130 - training loss: 22748.2466, validation loss: 0.4983
2024-05-24 08:19:28 [INFO]: Epoch 131 - training loss: 22746.9481, validation loss: 0.5017
2024-05-24 08:19:29 [INFO]: Epoch 132 - training loss: 22746.3707, validation loss: 0.4998
2024-05-24 08:19:29 [INFO]: Epoch 133 - training loss: 22745.9908, validation loss: 0.4969
2024-05-24 08:19:30 [INFO]: Epoch 134 - training loss: 22745.4370, validation loss: 0.4984
2024-05-24 08:19:30 [INFO]: Epoch 135 - training loss: 22745.3948, validation loss: 0.4983
2024-05-24 08:19:31 [INFO]: Epoch 136 - training loss: 22745.8671, validation loss: 0.4949
2024-05-24 08:19:32 [INFO]: Epoch 137 - training loss: 22745.8813, validation loss: 0.5007
2024-05-24 08:19:32 [INFO]: Epoch 138 - training loss: 22745.8122, validation loss: 0.4918
2024-05-24 08:19:33 [INFO]: Epoch 139 - training loss: 22746.1089, validation loss: 0.4950
2024-05-24 08:19:33 [INFO]: Epoch 140 - training loss: 22746.5593, validation loss: 0.4932
2024-05-24 08:19:34 [INFO]: Epoch 141 - training loss: 22745.3773, validation loss: 0.4932
2024-05-24 08:19:35 [INFO]: Epoch 142 - training loss: 22744.8069, validation loss: 0.4940
2024-05-24 08:19:35 [INFO]: Epoch 143 - training loss: 22744.8515, validation loss: 0.4913
2024-05-24 08:19:36 [INFO]: Epoch 144 - training loss: 22744.6986, validation loss: 0.4907
2024-05-24 08:19:36 [INFO]: Epoch 145 - training loss: 22744.7131, validation loss: 0.4907
2024-05-24 08:19:37 [INFO]: Epoch 146 - training loss: 22744.7336, validation loss: 0.4893
2024-05-24 08:19:38 [INFO]: Epoch 147 - training loss: 22745.4414, validation loss: 0.4951
2024-05-24 08:19:38 [INFO]: Epoch 148 - training loss: 22746.1706, validation loss: 0.4900
2024-05-24 08:19:39 [INFO]: Epoch 149 - training loss: 22745.4383, validation loss: 0.4932
2024-05-24 08:19:39 [INFO]: Epoch 150 - training loss: 22745.4112, validation loss: 0.4888
2024-05-24 08:19:40 [INFO]: Epoch 151 - training loss: 22744.7913, validation loss: 0.4901
2024-05-24 08:19:41 [INFO]: Epoch 152 - training loss: 22744.6061, validation loss: 0.4903
2024-05-24 08:19:41 [INFO]: Epoch 153 - training loss: 22744.3022, validation loss: 0.4897
2024-05-24 08:19:42 [INFO]: Epoch 154 - training loss: 22743.8851, validation loss: 0.4859
2024-05-24 08:19:42 [INFO]: Epoch 155 - training loss: 22744.0551, validation loss: 0.4886
2024-05-24 08:19:43 [INFO]: Epoch 156 - training loss: 22744.0899, validation loss: 0.4877
2024-05-24 08:19:44 [INFO]: Epoch 157 - training loss: 22743.4465, validation loss: 0.4890
2024-05-24 08:19:44 [INFO]: Epoch 158 - training loss: 22743.9353, validation loss: 0.4869
2024-05-24 08:19:45 [INFO]: Epoch 159 - training loss: 22744.1663, validation loss: 0.4900
2024-05-24 08:19:45 [INFO]: Epoch 160 - training loss: 22744.0039, validation loss: 0.4825
2024-05-24 08:19:46 [INFO]: Epoch 161 - training loss: 22743.7581, validation loss: 0.4886
2024-05-24 08:19:47 [INFO]: Epoch 162 - training loss: 22743.8335, validation loss: 0.4819
2024-05-24 08:19:47 [INFO]: Epoch 163 - training loss: 22743.7850, validation loss: 0.4856
2024-05-24 08:19:48 [INFO]: Epoch 164 - training loss: 22743.5319, validation loss: 0.4858
2024-05-24 08:19:48 [INFO]: Epoch 165 - training loss: 22743.5977, validation loss: 0.4845
2024-05-24 08:19:49 [INFO]: Epoch 166 - training loss: 22743.4484, validation loss: 0.4848
2024-05-24 08:19:50 [INFO]: Epoch 167 - training loss: 22743.5255, validation loss: 0.4824
2024-05-24 08:19:50 [INFO]: Epoch 168 - training loss: 22743.2869, validation loss: 0.4842
2024-05-24 08:19:51 [INFO]: Epoch 169 - training loss: 22743.8441, validation loss: 0.4825
2024-05-24 08:19:51 [INFO]: Epoch 170 - training loss: 22743.8333, validation loss: 0.4846
2024-05-24 08:19:52 [INFO]: Epoch 171 - training loss: 22743.6194, validation loss: 0.4815
2024-05-24 08:19:53 [INFO]: Epoch 172 - training loss: 22743.2222, validation loss: 0.4807
2024-05-24 08:19:53 [INFO]: Epoch 173 - training loss: 22743.0130, validation loss: 0.4832
2024-05-24 08:19:54 [INFO]: Epoch 174 - training loss: 22743.2429, validation loss: 0.4795
2024-05-24 08:19:54 [INFO]: Epoch 175 - training loss: 22743.1680, validation loss: 0.4775
2024-05-24 08:19:55 [INFO]: Epoch 176 - training loss: 22742.8966, validation loss: 0.4807
2024-05-24 08:19:55 [INFO]: Epoch 177 - training loss: 22742.1997, validation loss: 0.4750
2024-05-24 08:19:56 [INFO]: Epoch 178 - training loss: 22742.3571, validation loss: 0.4799
2024-05-24 08:19:57 [INFO]: Epoch 179 - training loss: 22742.3348, validation loss: 0.4761
2024-05-24 08:19:57 [INFO]: Epoch 180 - training loss: 22742.3273, validation loss: 0.4771
2024-05-24 08:19:58 [INFO]: Epoch 181 - training loss: 22742.3863, validation loss: 0.4778
2024-05-24 08:19:58 [INFO]: Epoch 182 - training loss: 22743.4369, validation loss: 0.4759
2024-05-24 08:19:59 [INFO]: Epoch 183 - training loss: 22742.7297, validation loss: 0.4779
2024-05-24 08:20:00 [INFO]: Epoch 184 - training loss: 22742.7176, validation loss: 0.4752
2024-05-24 08:20:00 [INFO]: Epoch 185 - training loss: 22742.5955, validation loss: 0.4752
2024-05-24 08:20:01 [INFO]: Epoch 186 - training loss: 22741.9087, validation loss: 0.4776
2024-05-24 08:20:01 [INFO]: Epoch 187 - training loss: 22741.9004, validation loss: 0.4744
2024-05-24 08:20:02 [INFO]: Epoch 188 - training loss: 22741.5200, validation loss: 0.4743
2024-05-24 08:20:03 [INFO]: Epoch 189 - training loss: 22741.5853, validation loss: 0.4725
2024-05-24 08:20:03 [INFO]: Epoch 190 - training loss: 22741.7290, validation loss: 0.4702
2024-05-24 08:20:04 [INFO]: Epoch 191 - training loss: 22741.4883, validation loss: 0.4722
2024-05-24 08:20:04 [INFO]: Epoch 192 - training loss: 22741.7432, validation loss: 0.4720
2024-05-24 08:20:05 [INFO]: Epoch 193 - training loss: 22741.1670, validation loss: 0.4736
2024-05-24 08:20:06 [INFO]: Epoch 194 - training loss: 22741.5740, validation loss: 0.4719
2024-05-24 08:20:06 [INFO]: Epoch 195 - training loss: 22741.9984, validation loss: 0.4714
2024-05-24 08:20:07 [INFO]: Epoch 196 - training loss: 22741.6286, validation loss: 0.4715
2024-05-24 08:20:07 [INFO]: Epoch 197 - training loss: 22741.7115, validation loss: 0.4676
2024-05-24 08:20:08 [INFO]: Epoch 198 - training loss: 22741.5414, validation loss: 0.4719
2024-05-24 08:20:09 [INFO]: Epoch 199 - training loss: 22741.4485, validation loss: 0.4684
2024-05-24 08:20:09 [INFO]: Epoch 200 - training loss: 22741.3125, validation loss: 0.4656
2024-05-24 08:20:10 [INFO]: Epoch 201 - training loss: 22741.1304, validation loss: 0.4670
2024-05-24 08:20:11 [INFO]: Epoch 202 - training loss: 22741.2651, validation loss: 0.4655
2024-05-24 08:20:11 [INFO]: Epoch 203 - training loss: 22740.7138, validation loss: 0.4652
2024-05-24 08:20:12 [INFO]: Epoch 204 - training loss: 22740.4397, validation loss: 0.4672
2024-05-24 08:20:12 [INFO]: Epoch 205 - training loss: 22739.9549, validation loss: 0.4631
2024-05-24 08:20:13 [INFO]: Epoch 206 - training loss: 22740.4446, validation loss: 0.4620
2024-05-24 08:20:14 [INFO]: Epoch 207 - training loss: 22740.7264, validation loss: 0.4605
2024-05-24 08:20:14 [INFO]: Epoch 208 - training loss: 22740.5403, validation loss: 0.4636
2024-05-24 08:20:15 [INFO]: Epoch 209 - training loss: 22740.4764, validation loss: 0.4650
2024-05-24 08:20:15 [INFO]: Epoch 210 - training loss: 22740.0254, validation loss: 0.4655
2024-05-24 08:20:16 [INFO]: Epoch 211 - training loss: 22739.7531, validation loss: 0.4672
2024-05-24 08:20:17 [INFO]: Epoch 212 - training loss: 22740.2499, validation loss: 0.4639
2024-05-24 08:20:17 [INFO]: Epoch 213 - training loss: 22739.5287, validation loss: 0.4642
2024-05-24 08:20:18 [INFO]: Epoch 214 - training loss: 22740.3231, validation loss: 0.4612
2024-05-24 08:20:18 [INFO]: Epoch 215 - training loss: 22740.4464, validation loss: 0.4612
2024-05-24 08:20:19 [INFO]: Epoch 216 - training loss: 22739.7796, validation loss: 0.4590
2024-05-24 08:20:20 [INFO]: Epoch 217 - training loss: 22739.3784, validation loss: 0.4614
2024-05-24 08:20:20 [INFO]: Epoch 218 - training loss: 22739.6712, validation loss: 0.4575
2024-05-24 08:20:21 [INFO]: Epoch 219 - training loss: 22739.1746, validation loss: 0.4630
2024-05-24 08:20:21 [INFO]: Epoch 220 - training loss: 22738.9981, validation loss: 0.4603
2024-05-24 08:20:22 [INFO]: Epoch 221 - training loss: 22739.0738, validation loss: 0.4633
2024-05-24 08:20:23 [INFO]: Epoch 222 - training loss: 22739.6637, validation loss: 0.4603
2024-05-24 08:20:23 [INFO]: Epoch 223 - training loss: 22739.5830, validation loss: 0.4602
2024-05-24 08:20:24 [INFO]: Epoch 224 - training loss: 22739.3414, validation loss: 0.4566
2024-05-24 08:20:25 [INFO]: Epoch 225 - training loss: 22738.6578, validation loss: 0.4589
2024-05-24 08:20:25 [INFO]: Epoch 226 - training loss: 22738.9319, validation loss: 0.4577
2024-05-24 08:20:26 [INFO]: Epoch 227 - training loss: 22738.3841, validation loss: 0.4630
2024-05-24 08:20:26 [INFO]: Epoch 228 - training loss: 22738.8213, validation loss: 0.4589
2024-05-24 08:20:27 [INFO]: Epoch 229 - training loss: 22738.5312, validation loss: 0.4599
2024-05-24 08:20:28 [INFO]: Epoch 230 - training loss: 22739.2615, validation loss: 0.4613
2024-05-24 08:20:28 [INFO]: Epoch 231 - training loss: 22739.1052, validation loss: 0.4626
2024-05-24 08:20:29 [INFO]: Epoch 232 - training loss: 22738.4630, validation loss: 0.4601
2024-05-24 08:20:29 [INFO]: Epoch 233 - training loss: 22738.1784, validation loss: 0.4626
2024-05-24 08:20:30 [INFO]: Epoch 234 - training loss: 22737.9462, validation loss: 0.4640
2024-05-24 08:20:30 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-05-24 08:20:30 [INFO]: Finished training. The best model is from epoch#224.
2024-05-24 08:20:30 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_3/GPVAE_physionet_2012_seta/20240524_T081809/GPVAE.pypots
2024-05-24 08:20:30 [INFO]: GP-VAE on PhysioNet-2012-seta: MAE=0.3957, MSE=0.3994
2024-05-24 08:20:31 [INFO]: Successfully saved to augmentation_premask_post_norm_saved_results/round_3/GPVAE_physionet_2012_seta/imputation.pkl
2024-05-24 08:20:31 [INFO]: Using the given device: cuda:0
2024-05-24 08:20:31 [INFO]: Model files will be saved to augmentation_premask_post_norm_saved_results/round_3/USGAN_physionet_2012_seta/20240524_T082031
2024-05-24 08:20:31 [INFO]: Tensorboard file will be saved to augmentation_premask_post_norm_saved_results/round_3/USGAN_physionet_2012_seta/20240524_T082031/tensorboard
2024-05-24 08:20:31 [INFO]: USGAN initialized with the given hyperparameters, the number of trainable parameters: 16,010,261
2024-05-24 08:20:52 [INFO]: Epoch 001 - generator training loss: 0.6186, discriminator training loss: 0.3852, validation loss: 0.6470
2024-05-24 08:21:11 [INFO]: Epoch 002 - generator training loss: 0.4897, discriminator training loss: 0.2752, validation loss: 0.5582
2024-05-24 08:21:30 [INFO]: Epoch 003 - generator training loss: 0.4398, discriminator training loss: 0.2386, validation loss: 0.5293
2024-05-24 08:21:49 [INFO]: Epoch 004 - generator training loss: 0.4408, discriminator training loss: 0.1924, validation loss: 0.5243
2024-05-24 08:22:07 [INFO]: Epoch 005 - generator training loss: 0.4452, discriminator training loss: 0.1619, validation loss: 0.5105
2024-05-24 08:22:26 [INFO]: Epoch 006 - generator training loss: 0.4366, discriminator training loss: 0.1425, validation loss: 0.4993
2024-05-24 08:22:45 [INFO]: Epoch 007 - generator training loss: 0.4306, discriminator training loss: 0.1281, validation loss: 0.4891
2024-05-24 08:23:03 [INFO]: Epoch 008 - generator training loss: 0.4182, discriminator training loss: 0.1176, validation loss: 0.4813
2024-05-24 08:23:22 [INFO]: Epoch 009 - generator training loss: 0.4098, discriminator training loss: 0.1090, validation loss: 0.4685
2024-05-24 08:23:41 [INFO]: Epoch 010 - generator training loss: 0.3996, discriminator training loss: 0.1017, validation loss: 0.4648
2024-05-24 08:23:59 [INFO]: Epoch 011 - generator training loss: 0.3965, discriminator training loss: 0.0956, validation loss: 0.4584
2024-05-24 08:24:18 [INFO]: Epoch 012 - generator training loss: 0.3906, discriminator training loss: 0.0904, validation loss: 0.4521
2024-05-24 08:24:37 [INFO]: Epoch 013 - generator training loss: 0.3838, discriminator training loss: 0.0856, validation loss: 0.4482
2024-05-24 08:24:56 [INFO]: Epoch 014 - generator training loss: 0.3814, discriminator training loss: 0.0816, validation loss: 0.4421
2024-05-24 08:25:14 [INFO]: Epoch 015 - generator training loss: 0.3767, discriminator training loss: 0.0778, validation loss: 0.4379
2024-05-24 08:25:33 [INFO]: Epoch 016 - generator training loss: 0.3732, discriminator training loss: 0.0746, validation loss: 0.4366
2024-05-24 08:25:51 [INFO]: Epoch 017 - generator training loss: 0.3696, discriminator training loss: 0.0719, validation loss: 0.4300
2024-05-24 08:26:10 [INFO]: Epoch 018 - generator training loss: 0.3646, discriminator training loss: 0.0694, validation loss: 0.4277
2024-05-24 08:26:29 [INFO]: Epoch 019 - generator training loss: 0.3621, discriminator training loss: 0.0672, validation loss: 0.4258
2024-05-24 08:26:48 [INFO]: Epoch 020 - generator training loss: 0.3587, discriminator training loss: 0.0648, validation loss: 0.4190
2024-05-24 08:27:06 [INFO]: Epoch 021 - generator training loss: 0.3558, discriminator training loss: 0.0628, validation loss: 0.4164
2024-05-24 08:27:25 [INFO]: Epoch 022 - generator training loss: 0.3509, discriminator training loss: 0.0610, validation loss: 0.4153
2024-05-24 08:27:44 [INFO]: Epoch 023 - generator training loss: 0.3480, discriminator training loss: 0.0594, validation loss: 0.4142
2024-05-24 08:28:02 [INFO]: Epoch 024 - generator training loss: 0.3439, discriminator training loss: 0.0580, validation loss: 0.4095
2024-05-24 08:28:21 [INFO]: Epoch 025 - generator training loss: 0.3407, discriminator training loss: 0.0565, validation loss: 0.4054
2024-05-24 08:28:40 [INFO]: Epoch 026 - generator training loss: 0.3372, discriminator training loss: 0.0551, validation loss: 0.4008
2024-05-24 08:28:59 [INFO]: Epoch 027 - generator training loss: 0.3381, discriminator training loss: 0.0541, validation loss: 0.4039
2024-05-24 08:29:17 [INFO]: Epoch 028 - generator training loss: 0.3321, discriminator training loss: 0.0530, validation loss: 0.3984
2024-05-24 08:29:36 [INFO]: Epoch 029 - generator training loss: 0.3336, discriminator training loss: 0.0522, validation loss: 0.3996
2024-05-24 08:29:55 [INFO]: Epoch 030 - generator training loss: 0.3270, discriminator training loss: 0.0513, validation loss: 0.3961
2024-05-24 08:30:13 [INFO]: Epoch 031 - generator training loss: 0.3196, discriminator training loss: 0.0505, validation loss: 0.3857
2024-05-24 08:30:32 [INFO]: Epoch 032 - generator training loss: 0.3193, discriminator training loss: 0.0499, validation loss: 0.3880
2024-05-24 08:30:51 [INFO]: Epoch 033 - generator training loss: 0.3164, discriminator training loss: 0.0492, validation loss: 0.3873
2024-05-24 08:31:09 [INFO]: Epoch 034 - generator training loss: 0.3130, discriminator training loss: 0.0486, validation loss: 0.3853
2024-05-24 08:31:28 [INFO]: Epoch 035 - generator training loss: 0.3101, discriminator training loss: 0.0482, validation loss: 0.3866
2024-05-24 08:31:47 [INFO]: Epoch 036 - generator training loss: 0.3085, discriminator training loss: 0.0476, validation loss: 0.3907
2024-05-24 08:32:05 [INFO]: Epoch 037 - generator training loss: 0.3093, discriminator training loss: 0.0474, validation loss: 0.3811
2024-05-24 08:32:24 [INFO]: Epoch 038 - generator training loss: 0.2990, discriminator training loss: 0.0469, validation loss: 0.3782
2024-05-24 08:32:43 [INFO]: Epoch 039 - generator training loss: 0.2937, discriminator training loss: 0.0464, validation loss: 0.3786
2024-05-24 08:33:02 [INFO]: Epoch 040 - generator training loss: 0.2922, discriminator training loss: 0.0461, validation loss: 0.3724
2024-05-24 08:33:20 [INFO]: Epoch 041 - generator training loss: 0.2886, discriminator training loss: 0.0458, validation loss: 0.3805
2024-05-24 08:33:39 [INFO]: Epoch 042 - generator training loss: 0.2860, discriminator training loss: 0.0454, validation loss: 0.3721
2024-05-24 08:33:58 [INFO]: Epoch 043 - generator training loss: 0.2881, discriminator training loss: 0.0453, validation loss: 0.3713
2024-05-24 08:34:16 [INFO]: Epoch 044 - generator training loss: 0.2892, discriminator training loss: 0.0450, validation loss: 0.3735
2024-05-24 08:34:35 [INFO]: Epoch 045 - generator training loss: 0.2808, discriminator training loss: 0.0449, validation loss: 0.3690
2024-05-24 08:34:54 [INFO]: Epoch 046 - generator training loss: 0.2786, discriminator training loss: 0.0445, validation loss: 0.3636
2024-05-24 08:35:12 [INFO]: Epoch 047 - generator training loss: 0.2719, discriminator training loss: 0.0443, validation loss: 0.3735
2024-05-24 08:35:31 [INFO]: Epoch 048 - generator training loss: 0.2778, discriminator training loss: 0.0440, validation loss: 0.3691
2024-05-24 08:35:50 [INFO]: Epoch 049 - generator training loss: 0.2728, discriminator training loss: 0.0438, validation loss: 0.3634
2024-05-24 08:36:08 [INFO]: Epoch 050 - generator training loss: 0.2731, discriminator training loss: 0.0437, validation loss: 0.3628
2024-05-24 08:36:27 [INFO]: Epoch 051 - generator training loss: 0.2665, discriminator training loss: 0.0433, validation loss: 0.3546
2024-05-24 08:36:46 [INFO]: Epoch 052 - generator training loss: 0.2606, discriminator training loss: 0.0433, validation loss: 0.3545
2024-05-24 08:37:04 [INFO]: Epoch 053 - generator training loss: 0.2563, discriminator training loss: 0.0432, validation loss: 0.3512
2024-05-24 08:37:23 [INFO]: Epoch 054 - generator training loss: 0.2509, discriminator training loss: 0.0428, validation loss: 0.3551
2024-05-24 08:37:42 [INFO]: Epoch 055 - generator training loss: 0.2513, discriminator training loss: 0.0430, validation loss: 0.3517
2024-05-24 08:38:00 [INFO]: Epoch 056 - generator training loss: 0.2449, discriminator training loss: 0.0428, validation loss: 0.3493
2024-05-24 08:38:19 [INFO]: Epoch 057 - generator training loss: 0.2439, discriminator training loss: 0.0427, validation loss: 0.3470
2024-05-24 08:38:38 [INFO]: Epoch 058 - generator training loss: 0.2394, discriminator training loss: 0.0426, validation loss: 0.3455
2024-05-24 08:38:56 [INFO]: Epoch 059 - generator training loss: 0.2393, discriminator training loss: 0.0422, validation loss: 0.3552
2024-05-24 08:39:15 [INFO]: Epoch 060 - generator training loss: 0.2431, discriminator training loss: 0.0424, validation loss: 0.3482
2024-05-24 08:39:34 [INFO]: Epoch 061 - generator training loss: 0.2408, discriminator training loss: 0.0422, validation loss: 0.3428
2024-05-24 08:39:53 [INFO]: Epoch 062 - generator training loss: 0.2326, discriminator training loss: 0.0419, validation loss: 0.3475
2024-05-24 08:40:11 [INFO]: Epoch 063 - generator training loss: 0.2295, discriminator training loss: 0.0418, validation loss: 0.3490
2024-05-24 08:40:30 [INFO]: Epoch 064 - generator training loss: 0.2303, discriminator training loss: 0.0419, validation loss: 0.3502
2024-05-24 08:40:49 [INFO]: Epoch 065 - generator training loss: 0.2318, discriminator training loss: 0.0416, validation loss: 0.3467
2024-05-24 08:41:07 [INFO]: Epoch 066 - generator training loss: 0.2246, discriminator training loss: 0.0418, validation loss: 0.3462
2024-05-24 08:41:26 [INFO]: Epoch 067 - generator training loss: 0.2200, discriminator training loss: 0.0417, validation loss: 0.3444
2024-05-24 08:41:45 [INFO]: Epoch 068 - generator training loss: 0.2174, discriminator training loss: 0.0414, validation loss: 0.3411
2024-05-24 08:42:03 [INFO]: Epoch 069 - generator training loss: 0.2199, discriminator training loss: 0.0415, validation loss: 0.3435
2024-05-24 08:42:22 [INFO]: Epoch 070 - generator training loss: 0.2157, discriminator training loss: 0.0414, validation loss: 0.3438
2024-05-24 08:42:41 [INFO]: Epoch 071 - generator training loss: 0.2129, discriminator training loss: 0.0413, validation loss: 0.3446
2024-05-24 08:43:00 [INFO]: Epoch 072 - generator training loss: 0.2126, discriminator training loss: 0.0412, validation loss: 0.3457
2024-05-24 08:43:18 [INFO]: Epoch 073 - generator training loss: 0.2080, discriminator training loss: 0.0410, validation loss: 0.3437
2024-05-24 08:43:37 [INFO]: Epoch 074 - generator training loss: 0.2050, discriminator training loss: 0.0407, validation loss: 0.3417
2024-05-24 08:43:56 [INFO]: Epoch 075 - generator training loss: 0.2080, discriminator training loss: 0.0407, validation loss: 0.3496
2024-05-24 08:44:14 [INFO]: Epoch 076 - generator training loss: 0.2162, discriminator training loss: 0.0408, validation loss: 0.3478
2024-05-24 08:44:33 [INFO]: Epoch 077 - generator training loss: 0.2150, discriminator training loss: 0.0409, validation loss: 0.3428
2024-05-24 08:44:52 [INFO]: Epoch 078 - generator training loss: 0.2054, discriminator training loss: 0.0406, validation loss: 0.3427
2024-05-24 08:44:52 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-05-24 08:44:52 [INFO]: Finished training. The best model is from epoch#68.
2024-05-24 08:44:52 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_3/USGAN_physionet_2012_seta/20240524_T082031/USGAN.pypots
2024-05-24 08:44:54 [INFO]: US-GAN on PhysioNet-2012-seta: MAE=0.2955, MSE=0.2614
2024-05-24 08:45:04 [INFO]: Successfully saved to augmentation_premask_post_norm_saved_results/round_3/USGAN_physionet_2012_seta/imputation.pkl
2024-05-24 08:45:04 [INFO]: Using the given device: cuda:0
2024-05-24 08:45:04 [INFO]: Model files will be saved to augmentation_premask_post_norm_saved_results/round_3/BRITS_physionet_2012_seta/20240524_T084504
2024-05-24 08:45:04 [INFO]: Tensorboard file will be saved to augmentation_premask_post_norm_saved_results/round_3/BRITS_physionet_2012_seta/20240524_T084504/tensorboard
2024-05-24 08:45:04 [INFO]: BRITS initialized with the given hyperparameters, the number of trainable parameters: 9,176,048
2024-05-24 08:45:20 [INFO]: Epoch 001 - training loss: 1.1340, validation loss: 0.5548
2024-05-24 08:45:32 [INFO]: Epoch 002 - training loss: 0.9261, validation loss: 0.4960
2024-05-24 08:45:44 [INFO]: Epoch 003 - training loss: 0.8609, validation loss: 0.4665
2024-05-24 08:45:56 [INFO]: Epoch 004 - training loss: 0.8222, validation loss: 0.4447
2024-05-24 08:46:08 [INFO]: Epoch 005 - training loss: 0.7929, validation loss: 0.4277
2024-05-24 08:46:21 [INFO]: Epoch 006 - training loss: 0.7707, validation loss: 0.4170
2024-05-24 08:46:33 [INFO]: Epoch 007 - training loss: 0.7537, validation loss: 0.4048
2024-05-24 08:46:45 [INFO]: Epoch 008 - training loss: 0.7391, validation loss: 0.3992
2024-05-24 08:46:58 [INFO]: Epoch 009 - training loss: 0.7271, validation loss: 0.3932
2024-05-24 08:47:10 [INFO]: Epoch 010 - training loss: 0.7162, validation loss: 0.3908
2024-05-24 08:47:22 [INFO]: Epoch 011 - training loss: 0.7074, validation loss: 0.3894
2024-05-24 08:47:34 [INFO]: Epoch 012 - training loss: 0.6989, validation loss: 0.3856
2024-05-24 08:47:47 [INFO]: Epoch 013 - training loss: 0.6915, validation loss: 0.3835
2024-05-24 08:47:59 [INFO]: Epoch 014 - training loss: 0.6857, validation loss: 0.3821
2024-05-24 08:48:11 [INFO]: Epoch 015 - training loss: 0.6795, validation loss: 0.3799
2024-05-24 08:48:23 [INFO]: Epoch 016 - training loss: 0.6752, validation loss: 0.3804
2024-05-24 08:48:36 [INFO]: Epoch 017 - training loss: 0.6703, validation loss: 0.3780
2024-05-24 08:48:48 [INFO]: Epoch 018 - training loss: 0.6661, validation loss: 0.3777
2024-05-24 08:49:00 [INFO]: Epoch 019 - training loss: 0.6631, validation loss: 0.3774
2024-05-24 08:49:13 [INFO]: Epoch 020 - training loss: 0.6591, validation loss: 0.3779
2024-05-24 08:49:25 [INFO]: Epoch 021 - training loss: 0.6546, validation loss: 0.3743
2024-05-24 08:49:37 [INFO]: Epoch 022 - training loss: 0.6520, validation loss: 0.3754
2024-05-24 08:49:49 [INFO]: Epoch 023 - training loss: 0.6488, validation loss: 0.3746
2024-05-24 08:50:01 [INFO]: Epoch 024 - training loss: 0.6461, validation loss: 0.3729
2024-05-24 08:50:14 [INFO]: Epoch 025 - training loss: 0.6432, validation loss: 0.3723
2024-05-24 08:50:26 [INFO]: Epoch 026 - training loss: 0.6417, validation loss: 0.3730
2024-05-24 08:50:38 [INFO]: Epoch 027 - training loss: 0.6362, validation loss: 0.3714
2024-05-24 08:50:51 [INFO]: Epoch 028 - training loss: 0.6355, validation loss: 0.3721
2024-05-24 08:51:03 [INFO]: Epoch 029 - training loss: 0.6336, validation loss: 0.3773
2024-05-24 08:51:15 [INFO]: Epoch 030 - training loss: 0.6376, validation loss: 0.3714
2024-05-24 08:51:27 [INFO]: Epoch 031 - training loss: 0.6307, validation loss: 0.3704
2024-05-24 08:51:39 [INFO]: Epoch 032 - training loss: 0.6273, validation loss: 0.3718
2024-05-24 08:51:52 [INFO]: Epoch 033 - training loss: 0.6229, validation loss: 0.3711
2024-05-24 08:52:04 [INFO]: Epoch 034 - training loss: 0.6211, validation loss: 0.3721
2024-05-24 08:52:16 [INFO]: Epoch 035 - training loss: 0.6174, validation loss: 0.3713
2024-05-24 08:52:28 [INFO]: Epoch 036 - training loss: 0.6195, validation loss: 0.3745
2024-05-24 08:52:41 [INFO]: Epoch 037 - training loss: 0.6162, validation loss: 0.3713
2024-05-24 08:52:53 [INFO]: Epoch 038 - training loss: 0.6110, validation loss: 0.3744
2024-05-24 08:53:05 [INFO]: Epoch 039 - training loss: 0.6092, validation loss: 0.3728
2024-05-24 08:53:18 [INFO]: Epoch 040 - training loss: 0.6097, validation loss: 0.3761
2024-05-24 08:53:30 [INFO]: Epoch 041 - training loss: 0.6064, validation loss: 0.3735
2024-05-24 08:53:30 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-05-24 08:53:30 [INFO]: Finished training. The best model is from epoch#31.
2024-05-24 08:53:30 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_3/BRITS_physionet_2012_seta/20240524_T084504/BRITS.pypots
2024-05-24 08:53:32 [INFO]: BRITS on PhysioNet-2012-seta: MAE=0.2570, MSE=0.2593
2024-05-24 08:53:42 [INFO]: Successfully saved to augmentation_premask_post_norm_saved_results/round_3/BRITS_physionet_2012_seta/imputation.pkl
2024-05-24 08:53:42 [INFO]: Using the given device: cuda:0
2024-05-24 08:53:42 [INFO]: Model files will be saved to augmentation_premask_post_norm_saved_results/round_3/MRNN_physionet_2012_seta/20240524_T085342
2024-05-24 08:53:42 [INFO]: Tensorboard file will be saved to augmentation_premask_post_norm_saved_results/round_3/MRNN_physionet_2012_seta/20240524_T085342/tensorboard
2024-05-24 08:53:42 [INFO]: MRNN initialized with the given hyperparameters, the number of trainable parameters: 7,599
2024-05-24 08:53:48 [INFO]: Epoch 001 - training loss: 1.2581, validation loss: 1.0046
2024-05-24 08:53:48 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_3/MRNN_physionet_2012_seta/20240524_T085342/MRNN_epoch1_loss1.0045989722013473.pypots
2024-05-24 08:53:51 [INFO]: Epoch 002 - training loss: 0.8055, validation loss: 0.9776
2024-05-24 08:53:51 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_3/MRNN_physionet_2012_seta/20240524_T085342/MRNN_epoch2_loss0.9776041030883789.pypots
2024-05-24 08:53:53 [INFO]: Epoch 003 - training loss: 0.6390, validation loss: 0.9548
2024-05-24 08:53:53 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_3/MRNN_physionet_2012_seta/20240524_T085342/MRNN_epoch3_loss0.9548022359609604.pypots
2024-05-24 08:53:56 [INFO]: Epoch 004 - training loss: 0.5952, validation loss: 0.9397
2024-05-24 08:53:56 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_3/MRNN_physionet_2012_seta/20240524_T085342/MRNN_epoch4_loss0.9396747380495072.pypots
2024-05-24 08:53:59 [INFO]: Epoch 005 - training loss: 0.5643, validation loss: 0.9316
2024-05-24 08:53:59 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_3/MRNN_physionet_2012_seta/20240524_T085342/MRNN_epoch5_loss0.9315821170806885.pypots
2024-05-24 08:54:02 [INFO]: Epoch 006 - training loss: 0.5337, validation loss: 0.9279
2024-05-24 08:54:02 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_3/MRNN_physionet_2012_seta/20240524_T085342/MRNN_epoch6_loss0.9278724223375321.pypots
2024-05-24 08:54:05 [INFO]: Epoch 007 - training loss: 0.5213, validation loss: 0.9253
2024-05-24 08:54:05 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_3/MRNN_physionet_2012_seta/20240524_T085342/MRNN_epoch7_loss0.9252930492162704.pypots
2024-05-24 08:54:07 [INFO]: Epoch 008 - training loss: 0.5052, validation loss: 0.9223
2024-05-24 08:54:07 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_3/MRNN_physionet_2012_seta/20240524_T085342/MRNN_epoch8_loss0.9222647726535798.pypots
2024-05-24 08:54:10 [INFO]: Epoch 009 - training loss: 0.5038, validation loss: 0.9227
2024-05-24 08:54:10 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_3/MRNN_physionet_2012_seta/20240524_T085342/MRNN_epoch9_loss0.9227250635623931.pypots
2024-05-24 08:54:13 [INFO]: Epoch 010 - training loss: 0.4781, validation loss: 0.9217
2024-05-24 08:54:13 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_3/MRNN_physionet_2012_seta/20240524_T085342/MRNN_epoch10_loss0.9216996729373932.pypots
2024-05-24 08:54:16 [INFO]: Epoch 011 - training loss: 0.4787, validation loss: 0.9224
2024-05-24 08:54:16 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_3/MRNN_physionet_2012_seta/20240524_T085342/MRNN_epoch11_loss0.9224360913038254.pypots
2024-05-24 08:54:19 [INFO]: Epoch 012 - training loss: 0.4692, validation loss: 0.9236
2024-05-24 08:54:19 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_3/MRNN_physionet_2012_seta/20240524_T085342/MRNN_epoch12_loss0.9235906392335892.pypots
2024-05-24 08:54:21 [INFO]: Epoch 013 - training loss: 0.4771, validation loss: 0.9250
2024-05-24 08:54:21 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_3/MRNN_physionet_2012_seta/20240524_T085342/MRNN_epoch13_loss0.9249982982873917.pypots
2024-05-24 08:54:24 [INFO]: Epoch 014 - training loss: 0.4649, validation loss: 0.9265
2024-05-24 08:54:24 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_3/MRNN_physionet_2012_seta/20240524_T085342/MRNN_epoch14_loss0.9265385508537293.pypots
2024-05-24 08:54:27 [INFO]: Epoch 015 - training loss: 0.4565, validation loss: 0.9280
2024-05-24 08:54:27 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_3/MRNN_physionet_2012_seta/20240524_T085342/MRNN_epoch15_loss0.9279926180839538.pypots
2024-05-24 08:54:30 [INFO]: Epoch 016 - training loss: 0.4499, validation loss: 0.9298
2024-05-24 08:54:30 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_3/MRNN_physionet_2012_seta/20240524_T085342/MRNN_epoch16_loss0.9298025220632553.pypots
2024-05-24 08:54:33 [INFO]: Epoch 017 - training loss: 0.4522, validation loss: 0.9311
2024-05-24 08:54:33 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_3/MRNN_physionet_2012_seta/20240524_T085342/MRNN_epoch17_loss0.9311347961425781.pypots
2024-05-24 08:54:35 [INFO]: Epoch 018 - training loss: 0.4503, validation loss: 0.9335
2024-05-24 08:54:35 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_3/MRNN_physionet_2012_seta/20240524_T085342/MRNN_epoch18_loss0.9335487723350525.pypots
2024-05-24 08:54:38 [INFO]: Epoch 019 - training loss: 0.4462, validation loss: 0.9332
2024-05-24 08:54:38 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_3/MRNN_physionet_2012_seta/20240524_T085342/MRNN_epoch19_loss0.93316590487957.pypots
2024-05-24 08:54:41 [INFO]: Epoch 020 - training loss: 0.4395, validation loss: 0.9349
2024-05-24 08:54:41 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_3/MRNN_physionet_2012_seta/20240524_T085342/MRNN_epoch20_loss0.9349310487508774.pypots
2024-05-24 08:54:41 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-05-24 08:54:41 [INFO]: Finished training. The best model is from epoch#10.
2024-05-24 08:54:41 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_3/MRNN_physionet_2012_seta/20240524_T085342/MRNN.pypots
2024-05-24 08:54:42 [INFO]: MRNN on PhysioNet-2012-seta: MAE=0.6905, MSE=0.9010
2024-05-24 08:54:46 [INFO]: Successfully saved to augmentation_premask_post_norm_saved_results/round_3/MRNN_physionet_2012_seta/imputation.pkl
2024-05-24 08:54:46 [INFO]: Using the given device: cpu
2024-05-24 08:54:46 [INFO]: LOCF on PhysioNet-2012-seta: MAE=0.4038, MSE=0.5072
2024-05-24 08:54:46 [INFO]: Successfully created the given path "saved_results/round_3/LOCF_physionet_2012_seta".
2024-05-24 08:54:46 [INFO]: Successfully saved to augmentation_premask_post_norm_saved_results/round_3/LOCF_physionet_2012_seta/imputation.pkl
2024-05-24 08:54:46 [INFO]: Median on PhysioNet-2012-seta: MAE=0.6908, MSE=1.0216
2024-05-24 08:54:46 [INFO]: Successfully created the given path "saved_results/round_3/Median_physionet_2012_seta".
2024-05-24 08:54:46 [INFO]: Successfully saved to augmentation_premask_post_norm_saved_results/round_3/Median_physionet_2012_seta/imputation.pkl
2024-05-24 08:54:47 [INFO]: Mean on PhysioNet-2012-seta: MAE=0.7063, MSE=0.9786
2024-05-24 08:54:47 [INFO]: Successfully created the given path "saved_results/round_3/Mean_physionet_2012_seta".
2024-05-24 08:54:47 [INFO]: Successfully saved to augmentation_premask_post_norm_saved_results/round_3/Mean_physionet_2012_seta/imputation.pkl
2024-05-24 08:54:47 [INFO]: Have set the random seed as 2027 for numpy and pytorch.
2024-05-24 08:54:47 [INFO]: Using the given device: cuda:0
2024-05-24 08:54:47 [INFO]: Model files will be saved to augmentation_premask_post_norm_saved_results/round_4/SAITS_physionet_2012_seta/20240524_T085447
2024-05-24 08:54:47 [INFO]: Tensorboard file will be saved to augmentation_premask_post_norm_saved_results/round_4/SAITS_physionet_2012_seta/20240524_T085447/tensorboard
2024-05-24 08:54:48 [INFO]: SAITS initialized with the given hyperparameters, the number of trainable parameters: 5,322,806
2024-05-24 08:54:49 [INFO]: Epoch 001 - training loss: 1.1274, validation loss: 0.5345
2024-05-24 08:54:50 [INFO]: Epoch 002 - training loss: 0.7152, validation loss: 0.4669
2024-05-24 08:54:51 [INFO]: Epoch 003 - training loss: 0.5970, validation loss: 0.4393
2024-05-24 08:54:52 [INFO]: Epoch 004 - training loss: 0.5208, validation loss: 0.4050
2024-05-24 08:54:53 [INFO]: Epoch 005 - training loss: 0.4803, validation loss: 0.3945
2024-05-24 08:54:55 [INFO]: Epoch 006 - training loss: 0.4488, validation loss: 0.3840
2024-05-24 08:54:56 [INFO]: Epoch 007 - training loss: 0.4199, validation loss: 0.3701
2024-05-24 08:54:57 [INFO]: Epoch 008 - training loss: 0.3953, validation loss: 0.3627
2024-05-24 08:54:58 [INFO]: Epoch 009 - training loss: 0.3708, validation loss: 0.3543
2024-05-24 08:54:59 [INFO]: Epoch 010 - training loss: 0.3534, validation loss: 0.3420
2024-05-24 08:55:00 [INFO]: Epoch 011 - training loss: 0.3369, validation loss: 0.3462
2024-05-24 08:55:02 [INFO]: Epoch 012 - training loss: 0.3214, validation loss: 0.3414
2024-05-24 08:55:03 [INFO]: Epoch 013 - training loss: 0.3082, validation loss: 0.3387
2024-05-24 08:55:04 [INFO]: Epoch 014 - training loss: 0.3042, validation loss: 0.3469
2024-05-24 08:55:05 [INFO]: Epoch 015 - training loss: 0.2899, validation loss: 0.3330
2024-05-24 08:55:06 [INFO]: Epoch 016 - training loss: 0.2803, validation loss: 0.3339
2024-05-24 08:55:07 [INFO]: Epoch 017 - training loss: 0.2651, validation loss: 0.3373
2024-05-24 08:55:09 [INFO]: Epoch 018 - training loss: 0.2593, validation loss: 0.3390
2024-05-24 08:55:10 [INFO]: Epoch 019 - training loss: 0.2532, validation loss: 0.3365
2024-05-24 08:55:11 [INFO]: Epoch 020 - training loss: 0.2425, validation loss: 0.3346
2024-05-24 08:55:12 [INFO]: Epoch 021 - training loss: 0.2360, validation loss: 0.3365
2024-05-24 08:55:13 [INFO]: Epoch 022 - training loss: 0.2277, validation loss: 0.3374
2024-05-24 08:55:14 [INFO]: Epoch 023 - training loss: 0.2250, validation loss: 0.3344
2024-05-24 08:55:16 [INFO]: Epoch 024 - training loss: 0.2198, validation loss: 0.3324
2024-05-24 08:55:17 [INFO]: Epoch 025 - training loss: 0.2118, validation loss: 0.3339
2024-05-24 08:55:18 [INFO]: Epoch 026 - training loss: 0.2087, validation loss: 0.3318
2024-05-24 08:55:19 [INFO]: Epoch 027 - training loss: 0.2039, validation loss: 0.3373
2024-05-24 08:55:20 [INFO]: Epoch 028 - training loss: 0.2022, validation loss: 0.3323
2024-05-24 08:55:21 [INFO]: Epoch 029 - training loss: 0.1954, validation loss: 0.3379
2024-05-24 08:55:23 [INFO]: Epoch 030 - training loss: 0.1926, validation loss: 0.3326
2024-05-24 08:55:24 [INFO]: Epoch 031 - training loss: 0.1869, validation loss: 0.3332
2024-05-24 08:55:25 [INFO]: Epoch 032 - training loss: 0.1814, validation loss: 0.3318
2024-05-24 08:55:26 [INFO]: Epoch 033 - training loss: 0.1793, validation loss: 0.3297
2024-05-24 08:55:27 [INFO]: Epoch 034 - training loss: 0.1779, validation loss: 0.3316
2024-05-24 08:55:28 [INFO]: Epoch 035 - training loss: 0.1738, validation loss: 0.3345
2024-05-24 08:55:30 [INFO]: Epoch 036 - training loss: 0.1697, validation loss: 0.3279
2024-05-24 08:55:31 [INFO]: Epoch 037 - training loss: 0.1701, validation loss: 0.3308
2024-05-24 08:55:32 [INFO]: Epoch 038 - training loss: 0.1686, validation loss: 0.3353
2024-05-24 08:55:33 [INFO]: Epoch 039 - training loss: 0.1648, validation loss: 0.3301
2024-05-24 08:55:34 [INFO]: Epoch 040 - training loss: 0.1592, validation loss: 0.3371
2024-05-24 08:55:37 [INFO]: Epoch 041 - training loss: 0.1556, validation loss: 0.3222
2024-05-24 08:55:38 [INFO]: Epoch 042 - training loss: 0.1527, validation loss: 0.3289
2024-05-24 08:55:40 [INFO]: Epoch 043 - training loss: 0.1516, validation loss: 0.3331
2024-05-24 08:55:41 [INFO]: Epoch 044 - training loss: 0.1508, validation loss: 0.3291
2024-05-24 08:55:42 [INFO]: Epoch 045 - training loss: 0.1501, validation loss: 0.3339
2024-05-24 08:55:43 [INFO]: Epoch 046 - training loss: 0.1485, validation loss: 0.3256
2024-05-24 08:55:45 [INFO]: Epoch 047 - training loss: 0.1489, validation loss: 0.3350
2024-05-24 08:55:46 [INFO]: Epoch 048 - training loss: 0.1485, validation loss: 0.3296
2024-05-24 08:55:47 [INFO]: Epoch 049 - training loss: 0.1451, validation loss: 0.3348
2024-05-24 08:55:48 [INFO]: Epoch 050 - training loss: 0.1417, validation loss: 0.3288
2024-05-24 08:55:49 [INFO]: Epoch 051 - training loss: 0.1427, validation loss: 0.3278
2024-05-24 08:55:49 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-05-24 08:55:49 [INFO]: Finished training. The best model is from epoch#41.
2024-05-24 08:55:49 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_4/SAITS_physionet_2012_seta/20240524_T085447/SAITS.pypots
2024-05-24 08:55:49 [INFO]: SAITS on PhysioNet-2012-seta: MAE=0.2613, MSE=0.2896
2024-05-24 08:55:50 [INFO]: Successfully saved to augmentation_premask_post_norm_saved_results/round_4/SAITS_physionet_2012_seta/imputation.pkl
2024-05-24 08:55:50 [INFO]: Using the given device: cuda:0
2024-05-24 08:55:50 [INFO]: Model files will be saved to augmentation_premask_post_norm_saved_results/round_4/Transformer_physionet_2012_seta/20240524_T085550
2024-05-24 08:55:50 [INFO]: Tensorboard file will be saved to augmentation_premask_post_norm_saved_results/round_4/Transformer_physionet_2012_seta/20240524_T085550/tensorboard
2024-05-24 08:55:50 [INFO]: Transformer initialized with the given hyperparameters, the number of trainable parameters: 2,152,613
2024-05-24 08:55:51 [INFO]: Epoch 001 - training loss: 1.2135, validation loss: 0.6253
2024-05-24 08:55:51 [INFO]: Epoch 002 - training loss: 0.7791, validation loss: 0.4998
2024-05-24 08:55:52 [INFO]: Epoch 003 - training loss: 0.6495, validation loss: 0.4760
2024-05-24 08:55:52 [INFO]: Epoch 004 - training loss: 0.5883, validation loss: 0.4572
2024-05-24 08:55:53 [INFO]: Epoch 005 - training loss: 0.5503, validation loss: 0.4449
2024-05-24 08:55:53 [INFO]: Epoch 006 - training loss: 0.5121, validation loss: 0.4282
2024-05-24 08:55:54 [INFO]: Epoch 007 - training loss: 0.4867, validation loss: 0.4282
2024-05-24 08:55:55 [INFO]: Epoch 008 - training loss: 0.4679, validation loss: 0.4153
2024-05-24 08:55:55 [INFO]: Epoch 009 - training loss: 0.4465, validation loss: 0.4040
2024-05-24 08:55:56 [INFO]: Epoch 010 - training loss: 0.4253, validation loss: 0.3954
2024-05-24 08:55:57 [INFO]: Epoch 011 - training loss: 0.4106, validation loss: 0.3949
2024-05-24 08:55:57 [INFO]: Epoch 012 - training loss: 0.3970, validation loss: 0.3853
2024-05-24 08:55:58 [INFO]: Epoch 013 - training loss: 0.3828, validation loss: 0.3853
2024-05-24 08:55:58 [INFO]: Epoch 014 - training loss: 0.3756, validation loss: 0.3762
2024-05-24 08:55:59 [INFO]: Epoch 015 - training loss: 0.3632, validation loss: 0.3783
2024-05-24 08:56:00 [INFO]: Epoch 016 - training loss: 0.3542, validation loss: 0.3740
2024-05-24 08:56:00 [INFO]: Epoch 017 - training loss: 0.3474, validation loss: 0.3805
2024-05-24 08:56:01 [INFO]: Epoch 018 - training loss: 0.3419, validation loss: 0.3713
2024-05-24 08:56:01 [INFO]: Epoch 019 - training loss: 0.3296, validation loss: 0.3673
2024-05-24 08:56:02 [INFO]: Epoch 020 - training loss: 0.3226, validation loss: 0.3638
2024-05-24 08:56:03 [INFO]: Epoch 021 - training loss: 0.3176, validation loss: 0.3654
2024-05-24 08:56:03 [INFO]: Epoch 022 - training loss: 0.3093, validation loss: 0.3652
2024-05-24 08:56:04 [INFO]: Epoch 023 - training loss: 0.3103, validation loss: 0.3630
2024-05-24 08:56:04 [INFO]: Epoch 024 - training loss: 0.2951, validation loss: 0.3619
2024-05-24 08:56:05 [INFO]: Epoch 025 - training loss: 0.2888, validation loss: 0.3638
2024-05-24 08:56:06 [INFO]: Epoch 026 - training loss: 0.2849, validation loss: 0.3605
2024-05-24 08:56:07 [INFO]: Epoch 027 - training loss: 0.2859, validation loss: 0.3558
2024-05-24 08:56:07 [INFO]: Epoch 028 - training loss: 0.2749, validation loss: 0.3595
2024-05-24 08:56:08 [INFO]: Epoch 029 - training loss: 0.2736, validation loss: 0.3605
2024-05-24 08:56:08 [INFO]: Epoch 030 - training loss: 0.2673, validation loss: 0.3616
2024-05-24 08:56:09 [INFO]: Epoch 031 - training loss: 0.2619, validation loss: 0.3564
2024-05-24 08:56:10 [INFO]: Epoch 032 - training loss: 0.2580, validation loss: 0.3605
2024-05-24 08:56:10 [INFO]: Epoch 033 - training loss: 0.2560, validation loss: 0.3558
2024-05-24 08:56:11 [INFO]: Epoch 034 - training loss: 0.2513, validation loss: 0.3577
2024-05-24 08:56:11 [INFO]: Epoch 035 - training loss: 0.2482, validation loss: 0.3564
2024-05-24 08:56:12 [INFO]: Epoch 036 - training loss: 0.2437, validation loss: 0.3603
2024-05-24 08:56:13 [INFO]: Epoch 037 - training loss: 0.2389, validation loss: 0.3532
2024-05-24 08:56:13 [INFO]: Epoch 038 - training loss: 0.2349, validation loss: 0.3571
2024-05-24 08:56:14 [INFO]: Epoch 039 - training loss: 0.2312, validation loss: 0.3619
2024-05-24 08:56:14 [INFO]: Epoch 040 - training loss: 0.2306, validation loss: 0.3567
2024-05-24 08:56:15 [INFO]: Epoch 041 - training loss: 0.2267, validation loss: 0.3593
2024-05-24 08:56:16 [INFO]: Epoch 042 - training loss: 0.2246, validation loss: 0.3556
2024-05-24 08:56:16 [INFO]: Epoch 043 - training loss: 0.2221, validation loss: 0.3607
2024-05-24 08:56:17 [INFO]: Epoch 044 - training loss: 0.2195, validation loss: 0.3632
2024-05-24 08:56:18 [INFO]: Epoch 045 - training loss: 0.2195, validation loss: 0.3601
2024-05-24 08:56:18 [INFO]: Epoch 046 - training loss: 0.2131, validation loss: 0.3613
2024-05-24 08:56:19 [INFO]: Epoch 047 - training loss: 0.2152, validation loss: 0.3601
2024-05-24 08:56:19 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-05-24 08:56:19 [INFO]: Finished training. The best model is from epoch#37.
2024-05-24 08:56:19 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_4/Transformer_physionet_2012_seta/20240524_T085550/Transformer.pypots
2024-05-24 08:56:19 [INFO]: Transformer on PhysioNet-2012-seta: MAE=0.2841, MSE=0.3084
2024-05-24 08:56:19 [INFO]: Successfully saved to augmentation_premask_post_norm_saved_results/round_4/Transformer_physionet_2012_seta/imputation.pkl
2024-05-24 08:56:19 [INFO]: Using the given device: cuda:0
2024-05-24 08:56:19 [INFO]: Model files will be saved to augmentation_premask_post_norm_saved_results/round_4/TimesNet_physionet_2012_seta/20240524_T085619
2024-05-24 08:56:19 [INFO]: Tensorboard file will be saved to augmentation_premask_post_norm_saved_results/round_4/TimesNet_physionet_2012_seta/20240524_T085619/tensorboard
2024-05-24 08:56:19 [INFO]: TimesNet initialized with the given hyperparameters, the number of trainable parameters: 21,649,573
2024-05-24 08:56:20 [INFO]: Epoch 001 - training loss: 0.4413, validation loss: 1.3369
2024-05-24 08:56:21 [INFO]: Epoch 002 - training loss: 0.5892, validation loss: 1.1207
2024-05-24 08:56:21 [INFO]: Epoch 003 - training loss: 0.4986, validation loss: 0.4653
2024-05-24 08:56:22 [INFO]: Epoch 004 - training loss: 0.3548, validation loss: 0.3707
2024-05-24 08:56:23 [INFO]: Epoch 005 - training loss: 0.3337, validation loss: 0.3372
2024-05-24 08:56:23 [INFO]: Epoch 006 - training loss: 0.3115, validation loss: 0.3283
2024-05-24 08:56:24 [INFO]: Epoch 007 - training loss: 0.3044, validation loss: 0.3306
2024-05-24 08:56:25 [INFO]: Epoch 008 - training loss: 0.3047, validation loss: 0.3174
2024-05-24 08:56:26 [INFO]: Epoch 009 - training loss: 0.2887, validation loss: 0.3356
2024-05-24 08:56:26 [INFO]: Epoch 010 - training loss: 0.2847, validation loss: 0.3222
2024-05-24 08:56:27 [INFO]: Epoch 011 - training loss: 0.2782, validation loss: 0.3270
2024-05-24 08:56:28 [INFO]: Epoch 012 - training loss: 0.2814, validation loss: 0.3082
2024-05-24 08:56:29 [INFO]: Epoch 013 - training loss: 0.2712, validation loss: 0.3258
2024-05-24 08:56:29 [INFO]: Epoch 014 - training loss: 0.2716, validation loss: 0.3369
2024-05-24 08:56:30 [INFO]: Epoch 015 - training loss: 0.2605, validation loss: 0.3346
2024-05-24 08:56:31 [INFO]: Epoch 016 - training loss: 0.2537, validation loss: 0.3180
2024-05-24 08:56:31 [INFO]: Epoch 017 - training loss: 0.2452, validation loss: 0.3310
2024-05-24 08:56:32 [INFO]: Epoch 018 - training loss: 0.2432, validation loss: 0.3293
2024-05-24 08:56:33 [INFO]: Epoch 019 - training loss: 0.2407, validation loss: 0.3216
2024-05-24 08:56:33 [INFO]: Epoch 020 - training loss: 0.2380, validation loss: 0.3333
2024-05-24 08:56:34 [INFO]: Epoch 021 - training loss: 0.2310, validation loss: 0.3256
2024-05-24 08:56:35 [INFO]: Epoch 022 - training loss: 0.2319, validation loss: 0.3459
2024-05-24 08:56:35 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-05-24 08:56:35 [INFO]: Finished training. The best model is from epoch#12.
2024-05-24 08:56:35 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_4/TimesNet_physionet_2012_seta/20240524_T085619/TimesNet.pypots
2024-05-24 08:56:35 [INFO]: TimesNet on PhysioNet-2012-seta: MAE=0.2916, MSE=0.2862
2024-05-24 08:56:35 [INFO]: Successfully saved to augmentation_premask_post_norm_saved_results/round_4/TimesNet_physionet_2012_seta/imputation.pkl
2024-05-24 08:56:35 [INFO]: Using the given device: cuda:0
2024-05-24 08:56:35 [INFO]: Model files will be saved to augmentation_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T085635
2024-05-24 08:56:35 [INFO]: Tensorboard file will be saved to augmentation_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T085635/tensorboard
2024-05-24 08:56:35 [INFO]: CSDI initialized with the given hyperparameters, the number of trainable parameters: 1,694,753
2024-05-24 08:57:19 [INFO]: Epoch 001 - training loss: 0.4092, validation loss: 0.3273
2024-05-24 08:57:19 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T085635/CSDI_epoch1_loss0.3272926524281502.pypots
2024-05-24 08:58:02 [INFO]: Epoch 002 - training loss: 0.3151, validation loss: 0.2934
2024-05-24 08:58:02 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T085635/CSDI_epoch2_loss0.29341454356908797.pypots
2024-05-24 08:58:46 [INFO]: Epoch 003 - training loss: 0.2756, validation loss: 0.2680
2024-05-24 08:58:46 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T085635/CSDI_epoch3_loss0.2679973214864731.pypots
2024-05-24 08:59:29 [INFO]: Epoch 004 - training loss: 0.2630, validation loss: 0.2551
2024-05-24 08:59:29 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T085635/CSDI_epoch4_loss0.255083616822958.pypots
2024-05-24 09:00:13 [INFO]: Epoch 005 - training loss: 0.2607, validation loss: 0.2441
2024-05-24 09:00:13 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T085635/CSDI_epoch5_loss0.24409114569425583.pypots
2024-05-24 09:00:57 [INFO]: Epoch 006 - training loss: 0.2404, validation loss: 0.2372
2024-05-24 09:00:57 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T085635/CSDI_epoch6_loss0.23724871948361398.pypots
2024-05-24 09:01:40 [INFO]: Epoch 007 - training loss: 0.2407, validation loss: 0.2239
2024-05-24 09:01:40 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T085635/CSDI_epoch7_loss0.2239414356648922.pypots
2024-05-24 09:02:24 [INFO]: Epoch 008 - training loss: 0.2295, validation loss: 0.2228
2024-05-24 09:02:24 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T085635/CSDI_epoch8_loss0.222789653390646.pypots
2024-05-24 09:03:08 [INFO]: Epoch 009 - training loss: 0.2285, validation loss: 0.2184
2024-05-24 09:03:08 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T085635/CSDI_epoch9_loss0.2183716118335724.pypots
2024-05-24 09:03:52 [INFO]: Epoch 010 - training loss: 0.2263, validation loss: 0.2202
2024-05-24 09:03:52 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T085635/CSDI_epoch10_loss0.22019874155521393.pypots
2024-05-24 09:04:35 [INFO]: Epoch 011 - training loss: 0.2208, validation loss: 0.2189
2024-05-24 09:04:35 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T085635/CSDI_epoch11_loss0.21888298243284227.pypots
2024-05-24 09:05:19 [INFO]: Epoch 012 - training loss: 0.2250, validation loss: 0.2135
2024-05-24 09:05:19 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T085635/CSDI_epoch12_loss0.21351032704114914.pypots
2024-05-24 09:06:03 [INFO]: Epoch 013 - training loss: 0.2242, validation loss: 0.2065
2024-05-24 09:06:03 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T085635/CSDI_epoch13_loss0.20654498115181924.pypots
2024-05-24 09:06:47 [INFO]: Epoch 014 - training loss: 0.2108, validation loss: 0.2105
2024-05-24 09:06:47 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T085635/CSDI_epoch14_loss0.2105405144393444.pypots
2024-05-24 09:07:31 [INFO]: Epoch 015 - training loss: 0.2181, validation loss: 0.2106
2024-05-24 09:07:31 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T085635/CSDI_epoch15_loss0.21063896045088767.pypots
2024-05-24 09:08:14 [INFO]: Epoch 016 - training loss: 0.2203, validation loss: 0.2058
2024-05-24 09:08:14 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T085635/CSDI_epoch16_loss0.2057539276778698.pypots
2024-05-24 09:08:58 [INFO]: Epoch 017 - training loss: 0.2077, validation loss: 0.2028
2024-05-24 09:08:58 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T085635/CSDI_epoch17_loss0.20284972935914994.pypots
2024-05-24 09:09:42 [INFO]: Epoch 018 - training loss: 0.2082, validation loss: 0.2050
2024-05-24 09:09:42 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T085635/CSDI_epoch18_loss0.20500903204083443.pypots
2024-05-24 09:10:25 [INFO]: Epoch 019 - training loss: 0.2229, validation loss: 0.2105
2024-05-24 09:10:25 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T085635/CSDI_epoch19_loss0.2105148032307625.pypots
2024-05-24 09:11:09 [INFO]: Epoch 020 - training loss: 0.1969, validation loss: 0.2033
2024-05-24 09:11:09 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T085635/CSDI_epoch20_loss0.20334958359599115.pypots
2024-05-24 09:11:53 [INFO]: Epoch 021 - training loss: 0.1987, validation loss: 0.2047
2024-05-24 09:11:53 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T085635/CSDI_epoch21_loss0.20471856892108917.pypots
2024-05-24 09:12:37 [INFO]: Epoch 022 - training loss: 0.2135, validation loss: 0.1970
2024-05-24 09:12:37 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T085635/CSDI_epoch22_loss0.19701514765620232.pypots
2024-05-24 09:13:20 [INFO]: Epoch 023 - training loss: 0.2155, validation loss: 0.2008
2024-05-24 09:13:20 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T085635/CSDI_epoch23_loss0.20078470781445504.pypots
2024-05-24 09:14:04 [INFO]: Epoch 024 - training loss: 0.2016, validation loss: 0.1984
2024-05-24 09:14:04 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T085635/CSDI_epoch24_loss0.19842798113822938.pypots
2024-05-24 09:14:48 [INFO]: Epoch 025 - training loss: 0.1920, validation loss: 0.2005
2024-05-24 09:14:48 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T085635/CSDI_epoch25_loss0.20049089342355728.pypots
2024-05-24 09:15:31 [INFO]: Epoch 026 - training loss: 0.1993, validation loss: 0.1998
2024-05-24 09:15:31 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T085635/CSDI_epoch26_loss0.1997554399073124.pypots
2024-05-24 09:16:15 [INFO]: Epoch 027 - training loss: 0.2049, validation loss: 0.1959
2024-05-24 09:16:15 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T085635/CSDI_epoch27_loss0.1959042452275753.pypots
2024-05-24 09:16:59 [INFO]: Epoch 028 - training loss: 0.1982, validation loss: 0.1976
2024-05-24 09:16:59 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T085635/CSDI_epoch28_loss0.19764281511306764.pypots
2024-05-24 09:17:43 [INFO]: Epoch 029 - training loss: 0.2003, validation loss: 0.1944
2024-05-24 09:17:43 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T085635/CSDI_epoch29_loss0.19440480470657348.pypots
2024-05-24 09:18:26 [INFO]: Epoch 030 - training loss: 0.1978, validation loss: 0.1964
2024-05-24 09:18:26 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T085635/CSDI_epoch30_loss0.19644039496779442.pypots
2024-05-24 09:19:10 [INFO]: Epoch 031 - training loss: 0.2081, validation loss: 0.1945
2024-05-24 09:19:10 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T085635/CSDI_epoch31_loss0.19454597011208535.pypots
2024-05-24 09:19:54 [INFO]: Epoch 032 - training loss: 0.1973, validation loss: 0.1960
2024-05-24 09:19:54 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T085635/CSDI_epoch32_loss0.19599566534161567.pypots
2024-05-24 09:20:38 [INFO]: Epoch 033 - training loss: 0.1998, validation loss: 0.1939
2024-05-24 09:20:38 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T085635/CSDI_epoch33_loss0.1939394310116768.pypots
2024-05-24 09:21:21 [INFO]: Epoch 034 - training loss: 0.2016, validation loss: 0.1970
2024-05-24 09:21:21 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T085635/CSDI_epoch34_loss0.19703150987625123.pypots
2024-05-24 09:22:05 [INFO]: Epoch 035 - training loss: 0.2024, validation loss: 0.1945
2024-05-24 09:22:05 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T085635/CSDI_epoch35_loss0.1944901540875435.pypots
2024-05-24 09:22:49 [INFO]: Epoch 036 - training loss: 0.2058, validation loss: 0.1945
2024-05-24 09:22:49 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T085635/CSDI_epoch36_loss0.19451359957456588.pypots
2024-05-24 09:23:33 [INFO]: Epoch 037 - training loss: 0.2070, validation loss: 0.1991
2024-05-24 09:23:33 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T085635/CSDI_epoch37_loss0.19906925559043884.pypots
2024-05-24 09:24:17 [INFO]: Epoch 038 - training loss: 0.2004, validation loss: 0.1920
2024-05-24 09:24:17 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T085635/CSDI_epoch38_loss0.19197447821497918.pypots
2024-05-24 09:25:00 [INFO]: Epoch 039 - training loss: 0.1917, validation loss: 0.1925
2024-05-24 09:25:00 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T085635/CSDI_epoch39_loss0.19252463728189467.pypots
2024-05-24 09:25:44 [INFO]: Epoch 040 - training loss: 0.2037, validation loss: 0.1967
2024-05-24 09:25:44 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T085635/CSDI_epoch40_loss0.1966864697635174.pypots
2024-05-24 09:26:28 [INFO]: Epoch 041 - training loss: 0.1999, validation loss: 0.1913
2024-05-24 09:26:28 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T085635/CSDI_epoch41_loss0.19134990647435188.pypots
2024-05-24 09:27:11 [INFO]: Epoch 042 - training loss: 0.2064, validation loss: 0.1924
2024-05-24 09:27:12 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T085635/CSDI_epoch42_loss0.1924295172095299.pypots
2024-05-24 09:27:55 [INFO]: Epoch 043 - training loss: 0.1914, validation loss: 0.1971
2024-05-24 09:27:55 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T085635/CSDI_epoch43_loss0.19710415825247765.pypots
2024-05-24 09:28:39 [INFO]: Epoch 044 - training loss: 0.1937, validation loss: 0.1972
2024-05-24 09:28:39 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T085635/CSDI_epoch44_loss0.19724688827991485.pypots
2024-05-24 09:29:23 [INFO]: Epoch 045 - training loss: 0.2036, validation loss: 0.1907
2024-05-24 09:29:23 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T085635/CSDI_epoch45_loss0.19067743346095084.pypots
2024-05-24 09:30:07 [INFO]: Epoch 046 - training loss: 0.2094, validation loss: 0.1899
2024-05-24 09:30:07 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T085635/CSDI_epoch46_loss0.18987416997551917.pypots
2024-05-24 09:30:51 [INFO]: Epoch 047 - training loss: 0.1918, validation loss: 0.1902
2024-05-24 09:30:51 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T085635/CSDI_epoch47_loss0.19018134698271752.pypots
2024-05-24 09:31:35 [INFO]: Epoch 048 - training loss: 0.1912, validation loss: 0.1903
2024-05-24 09:31:35 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T085635/CSDI_epoch48_loss0.1902913086116314.pypots
2024-05-24 09:32:19 [INFO]: Epoch 049 - training loss: 0.1936, validation loss: 0.1946
2024-05-24 09:32:19 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T085635/CSDI_epoch49_loss0.19455197378993033.pypots
2024-05-24 09:33:03 [INFO]: Epoch 050 - training loss: 0.1947, validation loss: 0.1925
2024-05-24 09:33:03 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T085635/CSDI_epoch50_loss0.1925172194838524.pypots
2024-05-24 09:33:47 [INFO]: Epoch 051 - training loss: 0.2156, validation loss: 0.1977
2024-05-24 09:33:47 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T085635/CSDI_epoch51_loss0.1977286756038666.pypots
2024-05-24 09:34:30 [INFO]: Epoch 052 - training loss: 0.1846, validation loss: 0.1925
2024-05-24 09:34:31 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T085635/CSDI_epoch52_loss0.192499353736639.pypots
2024-05-24 09:35:14 [INFO]: Epoch 053 - training loss: 0.1913, validation loss: 0.1869
2024-05-24 09:35:14 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T085635/CSDI_epoch53_loss0.18687156960368156.pypots
2024-05-24 09:35:58 [INFO]: Epoch 054 - training loss: 0.1903, validation loss: 0.1878
2024-05-24 09:35:58 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T085635/CSDI_epoch54_loss0.1877501890063286.pypots
2024-05-24 09:36:42 [INFO]: Epoch 055 - training loss: 0.1957, validation loss: 0.1958
2024-05-24 09:36:42 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T085635/CSDI_epoch55_loss0.1957538552582264.pypots
2024-05-24 09:37:26 [INFO]: Epoch 056 - training loss: 0.1996, validation loss: 0.1900
2024-05-24 09:37:26 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T085635/CSDI_epoch56_loss0.19001310542225838.pypots
2024-05-24 09:38:10 [INFO]: Epoch 057 - training loss: 0.2052, validation loss: 0.1880
2024-05-24 09:38:10 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T085635/CSDI_epoch57_loss0.18802264109253883.pypots
2024-05-24 09:38:54 [INFO]: Epoch 058 - training loss: 0.1970, validation loss: 0.1865
2024-05-24 09:38:54 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T085635/CSDI_epoch58_loss0.18647924959659576.pypots
2024-05-24 09:39:38 [INFO]: Epoch 059 - training loss: 0.1961, validation loss: 0.1873
2024-05-24 09:39:38 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T085635/CSDI_epoch59_loss0.1873145006597042.pypots
2024-05-24 09:40:22 [INFO]: Epoch 060 - training loss: 0.1939, validation loss: 0.1858
2024-05-24 09:40:22 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T085635/CSDI_epoch60_loss0.18579167798161506.pypots
2024-05-24 09:41:06 [INFO]: Epoch 061 - training loss: 0.1985, validation loss: 0.1880
2024-05-24 09:41:06 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T085635/CSDI_epoch61_loss0.18797128796577453.pypots
2024-05-24 09:41:49 [INFO]: Epoch 062 - training loss: 0.1940, validation loss: 0.1881
2024-05-24 09:41:50 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T085635/CSDI_epoch62_loss0.1880960777401924.pypots
2024-05-24 09:42:33 [INFO]: Epoch 063 - training loss: 0.1950, validation loss: 0.1889
2024-05-24 09:42:33 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T085635/CSDI_epoch63_loss0.18893865123391151.pypots
2024-05-24 09:43:17 [INFO]: Epoch 064 - training loss: 0.1945, validation loss: 0.1910
2024-05-24 09:43:17 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T085635/CSDI_epoch64_loss0.19104257300496102.pypots
2024-05-24 09:44:01 [INFO]: Epoch 065 - training loss: 0.2048, validation loss: 0.1873
2024-05-24 09:44:01 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T085635/CSDI_epoch65_loss0.18725219815969468.pypots
2024-05-24 09:44:45 [INFO]: Epoch 066 - training loss: 0.1866, validation loss: 0.1861
2024-05-24 09:44:45 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T085635/CSDI_epoch66_loss0.18608633056282997.pypots
2024-05-24 09:45:28 [INFO]: Epoch 067 - training loss: 0.1943, validation loss: 0.1884
2024-05-24 09:45:28 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T085635/CSDI_epoch67_loss0.18842279389500619.pypots
2024-05-24 09:46:12 [INFO]: Epoch 068 - training loss: 0.1934, validation loss: 0.1854
2024-05-24 09:46:12 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T085635/CSDI_epoch68_loss0.1853771038353443.pypots
2024-05-24 09:46:56 [INFO]: Epoch 069 - training loss: 0.1853, validation loss: 0.1890
2024-05-24 09:46:56 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T085635/CSDI_epoch69_loss0.18899772316217422.pypots
2024-05-24 09:47:40 [INFO]: Epoch 070 - training loss: 0.1827, validation loss: 0.1875
2024-05-24 09:47:40 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T085635/CSDI_epoch70_loss0.1874862030148506.pypots
2024-05-24 09:48:24 [INFO]: Epoch 071 - training loss: 0.1999, validation loss: 0.1880
2024-05-24 09:48:24 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T085635/CSDI_epoch71_loss0.1880340211093426.pypots
2024-05-24 09:49:08 [INFO]: Epoch 072 - training loss: 0.1769, validation loss: 0.1843
2024-05-24 09:49:08 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T085635/CSDI_epoch72_loss0.1842801071703434.pypots
2024-05-24 09:49:52 [INFO]: Epoch 073 - training loss: 0.1945, validation loss: 0.1824
2024-05-24 09:49:52 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T085635/CSDI_epoch73_loss0.18241913244128227.pypots
2024-05-24 09:50:36 [INFO]: Epoch 074 - training loss: 0.1859, validation loss: 0.1840
2024-05-24 09:50:36 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T085635/CSDI_epoch74_loss0.18400766029953958.pypots
2024-05-24 09:51:20 [INFO]: Epoch 075 - training loss: 0.1979, validation loss: 0.1892
2024-05-24 09:51:20 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T085635/CSDI_epoch75_loss0.18923934176564217.pypots
2024-05-24 09:52:04 [INFO]: Epoch 076 - training loss: 0.1919, validation loss: 0.1881
2024-05-24 09:52:04 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T085635/CSDI_epoch76_loss0.18805048391222953.pypots
2024-05-24 09:52:48 [INFO]: Epoch 077 - training loss: 0.1832, validation loss: 0.1872
2024-05-24 09:52:48 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T085635/CSDI_epoch77_loss0.18722732812166215.pypots
2024-05-24 09:53:31 [INFO]: Epoch 078 - training loss: 0.1857, validation loss: 0.1846
2024-05-24 09:53:31 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T085635/CSDI_epoch78_loss0.1846321403980255.pypots
2024-05-24 09:54:15 [INFO]: Epoch 079 - training loss: 0.1829, validation loss: 0.1866
2024-05-24 09:54:15 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T085635/CSDI_epoch79_loss0.18660803586244584.pypots
2024-05-24 09:54:59 [INFO]: Epoch 080 - training loss: 0.1827, validation loss: 0.1852
2024-05-24 09:54:59 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T085635/CSDI_epoch80_loss0.18518586605787277.pypots
2024-05-24 09:55:43 [INFO]: Epoch 081 - training loss: 0.1910, validation loss: 0.1900
2024-05-24 09:55:43 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T085635/CSDI_epoch81_loss0.18998383358120918.pypots
2024-05-24 09:56:26 [INFO]: Epoch 082 - training loss: 0.1922, validation loss: 0.1845
2024-05-24 09:56:26 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T085635/CSDI_epoch82_loss0.18452695682644843.pypots
2024-05-24 09:57:10 [INFO]: Epoch 083 - training loss: 0.1885, validation loss: 0.1833
2024-05-24 09:57:10 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T085635/CSDI_epoch83_loss0.18330873772501946.pypots
2024-05-24 09:57:10 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-05-24 09:57:10 [INFO]: Finished training. The best model is from epoch#73.
2024-05-24 09:57:10 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/20240524_T085635/CSDI.pypots
2024-05-24 10:04:31 [INFO]: CSDI on PhysioNet-2012-seta: MAE=0.2326, MSE=0.6116
2024-05-24 10:34:01 [INFO]: Successfully saved to augmentation_premask_post_norm_saved_results/round_4/CSDI_physionet_2012_seta/imputation.pkl
2024-05-24 10:34:01 [INFO]: Using the given device: cuda:0
2024-05-24 10:34:01 [INFO]: Model files will be saved to augmentation_premask_post_norm_saved_results/round_4/GPVAE_physionet_2012_seta/20240524_T103401
2024-05-24 10:34:01 [INFO]: Tensorboard file will be saved to augmentation_premask_post_norm_saved_results/round_4/GPVAE_physionet_2012_seta/20240524_T103401/tensorboard
2024-05-24 10:34:01 [INFO]: GPVAE initialized with the given hyperparameters, the number of trainable parameters: 1,028,244
2024-05-24 10:34:02 [INFO]: Epoch 001 - training loss: 42281.1903, validation loss: 0.9186
2024-05-24 10:34:02 [INFO]: Epoch 002 - training loss: 24387.7105, validation loss: 0.7542
2024-05-24 10:34:03 [INFO]: Epoch 003 - training loss: 23482.2972, validation loss: 0.7201
2024-05-24 10:34:03 [INFO]: Epoch 004 - training loss: 23184.3221, validation loss: 0.6925
2024-05-24 10:34:04 [INFO]: Epoch 005 - training loss: 23038.0083, validation loss: 0.6910
2024-05-24 10:34:05 [INFO]: Epoch 006 - training loss: 22955.1810, validation loss: 0.6730
2024-05-24 10:34:05 [INFO]: Epoch 007 - training loss: 22905.2361, validation loss: 0.6694
2024-05-24 10:34:06 [INFO]: Epoch 008 - training loss: 22872.9381, validation loss: 0.6702
2024-05-24 10:34:06 [INFO]: Epoch 009 - training loss: 22851.5796, validation loss: 0.6689
2024-05-24 10:34:07 [INFO]: Epoch 010 - training loss: 22837.0462, validation loss: 0.6688
2024-05-24 10:34:08 [INFO]: Epoch 011 - training loss: 22826.1831, validation loss: 0.6646
2024-05-24 10:34:08 [INFO]: Epoch 012 - training loss: 22818.4103, validation loss: 0.6648
2024-05-24 10:34:09 [INFO]: Epoch 013 - training loss: 22812.9750, validation loss: 0.6655
2024-05-24 10:34:09 [INFO]: Epoch 014 - training loss: 22808.0830, validation loss: 0.6617
2024-05-24 10:34:10 [INFO]: Epoch 015 - training loss: 22804.1789, validation loss: 0.6600
2024-05-24 10:34:11 [INFO]: Epoch 016 - training loss: 22800.4339, validation loss: 0.6552
2024-05-24 10:34:11 [INFO]: Epoch 017 - training loss: 22797.9609, validation loss: 0.6552
2024-05-24 10:34:12 [INFO]: Epoch 018 - training loss: 22795.7376, validation loss: 0.6512
2024-05-24 10:34:12 [INFO]: Epoch 019 - training loss: 22794.0546, validation loss: 0.6465
2024-05-24 10:34:13 [INFO]: Epoch 020 - training loss: 22792.5814, validation loss: 0.6500
2024-05-24 10:34:14 [INFO]: Epoch 021 - training loss: 22791.0302, validation loss: 0.6414
2024-05-24 10:34:14 [INFO]: Epoch 022 - training loss: 22789.5650, validation loss: 0.6413
2024-05-24 10:34:15 [INFO]: Epoch 023 - training loss: 22788.0605, validation loss: 0.6433
2024-05-24 10:34:16 [INFO]: Epoch 024 - training loss: 22786.8192, validation loss: 0.6370
2024-05-24 10:34:16 [INFO]: Epoch 025 - training loss: 22786.5193, validation loss: 0.6353
2024-05-24 10:34:17 [INFO]: Epoch 026 - training loss: 22785.1501, validation loss: 0.6371
2024-05-24 10:34:17 [INFO]: Epoch 027 - training loss: 22785.0670, validation loss: 0.6376
2024-05-24 10:34:18 [INFO]: Epoch 028 - training loss: 22784.6821, validation loss: 0.6353
2024-05-24 10:34:19 [INFO]: Epoch 029 - training loss: 22784.1665, validation loss: 0.6335
2024-05-24 10:34:19 [INFO]: Epoch 030 - training loss: 22782.8959, validation loss: 0.6319
2024-05-24 10:34:20 [INFO]: Epoch 031 - training loss: 22782.6369, validation loss: 0.6378
2024-05-24 10:34:20 [INFO]: Epoch 032 - training loss: 22782.3714, validation loss: 0.6274
2024-05-24 10:34:21 [INFO]: Epoch 033 - training loss: 22780.9032, validation loss: 0.6261
2024-05-24 10:34:22 [INFO]: Epoch 034 - training loss: 22780.2873, validation loss: 0.6228
2024-05-24 10:34:22 [INFO]: Epoch 035 - training loss: 22779.5843, validation loss: 0.6225
2024-05-24 10:34:23 [INFO]: Epoch 036 - training loss: 22778.5059, validation loss: 0.6188
2024-05-24 10:34:23 [INFO]: Epoch 037 - training loss: 22777.4346, validation loss: 0.6177
2024-05-24 10:34:24 [INFO]: Epoch 038 - training loss: 22776.9485, validation loss: 0.6122
2024-05-24 10:34:25 [INFO]: Epoch 039 - training loss: 22775.3582, validation loss: 0.6048
2024-05-24 10:34:25 [INFO]: Epoch 040 - training loss: 22775.0224, validation loss: 0.6044
2024-05-24 10:34:26 [INFO]: Epoch 041 - training loss: 22773.8657, validation loss: 0.6046
2024-05-24 10:34:26 [INFO]: Epoch 042 - training loss: 22772.5430, validation loss: 0.6011
2024-05-24 10:34:27 [INFO]: Epoch 043 - training loss: 22771.9111, validation loss: 0.6010
2024-05-24 10:34:28 [INFO]: Epoch 044 - training loss: 22771.6668, validation loss: 0.5948
2024-05-24 10:34:28 [INFO]: Epoch 045 - training loss: 22770.3727, validation loss: 0.5969
2024-05-24 10:34:29 [INFO]: Epoch 046 - training loss: 22770.3097, validation loss: 0.5949
2024-05-24 10:34:29 [INFO]: Epoch 047 - training loss: 22769.0205, validation loss: 0.5886
2024-05-24 10:34:30 [INFO]: Epoch 048 - training loss: 22768.2365, validation loss: 0.5859
2024-05-24 10:34:31 [INFO]: Epoch 049 - training loss: 22767.3158, validation loss: 0.6008
2024-05-24 10:34:31 [INFO]: Epoch 050 - training loss: 22766.2360, validation loss: 0.5855
2024-05-24 10:34:32 [INFO]: Epoch 051 - training loss: 22766.4323, validation loss: 0.5783
2024-05-24 10:34:32 [INFO]: Epoch 052 - training loss: 22765.0198, validation loss: 0.5748
2024-05-24 10:34:33 [INFO]: Epoch 053 - training loss: 22765.1416, validation loss: 0.5735
2024-05-24 10:34:34 [INFO]: Epoch 054 - training loss: 22763.5702, validation loss: 0.5737
2024-05-24 10:34:34 [INFO]: Epoch 055 - training loss: 22763.1634, validation loss: 0.5771
2024-05-24 10:34:35 [INFO]: Epoch 056 - training loss: 22762.9170, validation loss: 0.5717
2024-05-24 10:34:35 [INFO]: Epoch 057 - training loss: 22762.3010, validation loss: 0.5693
2024-05-24 10:34:36 [INFO]: Epoch 058 - training loss: 22762.3305, validation loss: 0.5679
2024-05-24 10:34:37 [INFO]: Epoch 059 - training loss: 22761.8784, validation loss: 0.5708
2024-05-24 10:34:37 [INFO]: Epoch 060 - training loss: 22761.5745, validation loss: 0.5691
2024-05-24 10:34:38 [INFO]: Epoch 061 - training loss: 22761.7308, validation loss: 0.5784
2024-05-24 10:34:38 [INFO]: Epoch 062 - training loss: 22761.1941, validation loss: 0.5683
2024-05-24 10:34:39 [INFO]: Epoch 063 - training loss: 22760.6654, validation loss: 0.5684
2024-05-24 10:34:40 [INFO]: Epoch 064 - training loss: 22760.2542, validation loss: 0.5703
2024-05-24 10:34:40 [INFO]: Epoch 065 - training loss: 22760.2705, validation loss: 0.5625
2024-05-24 10:34:41 [INFO]: Epoch 066 - training loss: 22760.7684, validation loss: 0.5653
2024-05-24 10:34:41 [INFO]: Epoch 067 - training loss: 22759.6117, validation loss: 0.5646
2024-05-24 10:34:42 [INFO]: Epoch 068 - training loss: 22759.4010, validation loss: 0.5630
2024-05-24 10:34:43 [INFO]: Epoch 069 - training loss: 22759.0635, validation loss: 0.5629
2024-05-24 10:34:43 [INFO]: Epoch 070 - training loss: 22759.9967, validation loss: 0.5566
2024-05-24 10:34:44 [INFO]: Epoch 071 - training loss: 22759.0582, validation loss: 0.5554
2024-05-24 10:34:44 [INFO]: Epoch 072 - training loss: 22758.0621, validation loss: 0.5553
2024-05-24 10:34:45 [INFO]: Epoch 073 - training loss: 22757.2531, validation loss: 0.5511
2024-05-24 10:34:46 [INFO]: Epoch 074 - training loss: 22756.8770, validation loss: 0.5728
2024-05-24 10:34:46 [INFO]: Epoch 075 - training loss: 22759.0821, validation loss: 0.5496
2024-05-24 10:34:47 [INFO]: Epoch 076 - training loss: 22759.2822, validation loss: 0.5567
2024-05-24 10:34:48 [INFO]: Epoch 077 - training loss: 22756.8531, validation loss: 0.5423
2024-05-24 10:34:48 [INFO]: Epoch 078 - training loss: 22756.2987, validation loss: 0.5450
2024-05-24 10:34:49 [INFO]: Epoch 079 - training loss: 22755.5570, validation loss: 0.5379
2024-05-24 10:34:50 [INFO]: Epoch 080 - training loss: 22755.0638, validation loss: 0.5445
2024-05-24 10:34:50 [INFO]: Epoch 081 - training loss: 22755.2536, validation loss: 0.5340
2024-05-24 10:34:51 [INFO]: Epoch 082 - training loss: 22754.8823, validation loss: 0.5419
2024-05-24 10:34:51 [INFO]: Epoch 083 - training loss: 22754.9692, validation loss: 0.5307
2024-05-24 10:34:52 [INFO]: Epoch 084 - training loss: 22753.7388, validation loss: 0.5451
2024-05-24 10:34:53 [INFO]: Epoch 085 - training loss: 22753.8766, validation loss: 0.5304
2024-05-24 10:34:53 [INFO]: Epoch 086 - training loss: 22752.8709, validation loss: 0.5439
2024-05-24 10:34:54 [INFO]: Epoch 087 - training loss: 22753.6472, validation loss: 0.5334
2024-05-24 10:34:54 [INFO]: Epoch 088 - training loss: 22753.1729, validation loss: 0.5296
2024-05-24 10:34:55 [INFO]: Epoch 089 - training loss: 22751.9966, validation loss: 0.5301
2024-05-24 10:34:56 [INFO]: Epoch 090 - training loss: 22752.0593, validation loss: 0.5280
2024-05-24 10:34:56 [INFO]: Epoch 091 - training loss: 22751.8429, validation loss: 0.5266
2024-05-24 10:34:57 [INFO]: Epoch 092 - training loss: 22752.0115, validation loss: 0.5290
2024-05-24 10:34:57 [INFO]: Epoch 093 - training loss: 22751.5554, validation loss: 0.5263
2024-05-24 10:34:58 [INFO]: Epoch 094 - training loss: 22751.6959, validation loss: 0.5306
2024-05-24 10:34:59 [INFO]: Epoch 095 - training loss: 22751.7329, validation loss: 0.5243
2024-05-24 10:34:59 [INFO]: Epoch 096 - training loss: 22751.5175, validation loss: 0.5267
2024-05-24 10:35:00 [INFO]: Epoch 097 - training loss: 22751.3100, validation loss: 0.5207
2024-05-24 10:35:00 [INFO]: Epoch 098 - training loss: 22750.8991, validation loss: 0.5253
2024-05-24 10:35:01 [INFO]: Epoch 099 - training loss: 22750.7779, validation loss: 0.5228
2024-05-24 10:35:02 [INFO]: Epoch 100 - training loss: 22750.7468, validation loss: 0.5200
2024-05-24 10:35:02 [INFO]: Epoch 101 - training loss: 22751.1268, validation loss: 0.5195
2024-05-24 10:35:03 [INFO]: Epoch 102 - training loss: 22750.7432, validation loss: 0.5214
2024-05-24 10:35:03 [INFO]: Epoch 103 - training loss: 22750.6066, validation loss: 0.5174
2024-05-24 10:35:04 [INFO]: Epoch 104 - training loss: 22750.0297, validation loss: 0.5184
2024-05-24 10:35:05 [INFO]: Epoch 105 - training loss: 22750.2721, validation loss: 0.5151
2024-05-24 10:35:05 [INFO]: Epoch 106 - training loss: 22749.5406, validation loss: 0.5162
2024-05-24 10:35:06 [INFO]: Epoch 107 - training loss: 22749.1742, validation loss: 0.5161
2024-05-24 10:35:06 [INFO]: Epoch 108 - training loss: 22749.1041, validation loss: 0.5113
2024-05-24 10:35:07 [INFO]: Epoch 109 - training loss: 22748.8805, validation loss: 0.5121
2024-05-24 10:35:08 [INFO]: Epoch 110 - training loss: 22748.7397, validation loss: 0.5087
2024-05-24 10:35:08 [INFO]: Epoch 111 - training loss: 22748.6758, validation loss: 0.5089
2024-05-24 10:35:09 [INFO]: Epoch 112 - training loss: 22748.2091, validation loss: 0.5074
2024-05-24 10:35:10 [INFO]: Epoch 113 - training loss: 22747.9002, validation loss: 0.5061
2024-05-24 10:35:10 [INFO]: Epoch 114 - training loss: 22747.7538, validation loss: 0.5068
2024-05-24 10:35:11 [INFO]: Epoch 115 - training loss: 22748.1988, validation loss: 0.5044
2024-05-24 10:35:11 [INFO]: Epoch 116 - training loss: 22748.8758, validation loss: 0.5059
2024-05-24 10:35:12 [INFO]: Epoch 117 - training loss: 22748.1369, validation loss: 0.5103
2024-05-24 10:35:13 [INFO]: Epoch 118 - training loss: 22749.7198, validation loss: 0.5069
2024-05-24 10:35:13 [INFO]: Epoch 119 - training loss: 22749.9916, validation loss: 0.5066
2024-05-24 10:35:14 [INFO]: Epoch 120 - training loss: 22749.4686, validation loss: 0.5077
2024-05-24 10:35:14 [INFO]: Epoch 121 - training loss: 22747.8370, validation loss: 0.5116
2024-05-24 10:35:15 [INFO]: Epoch 122 - training loss: 22747.8442, validation loss: 0.4994
2024-05-24 10:35:16 [INFO]: Epoch 123 - training loss: 22747.1103, validation loss: 0.5007
2024-05-24 10:35:16 [INFO]: Epoch 124 - training loss: 22746.7043, validation loss: 0.4960
2024-05-24 10:35:17 [INFO]: Epoch 125 - training loss: 22746.5319, validation loss: 0.4978
2024-05-24 10:35:17 [INFO]: Epoch 126 - training loss: 22747.1194, validation loss: 0.4970
2024-05-24 10:35:18 [INFO]: Epoch 127 - training loss: 22745.9991, validation loss: 0.4952
2024-05-24 10:35:19 [INFO]: Epoch 128 - training loss: 22747.1367, validation loss: 0.4930
2024-05-24 10:35:19 [INFO]: Epoch 129 - training loss: 22745.8875, validation loss: 0.4935
2024-05-24 10:35:20 [INFO]: Epoch 130 - training loss: 22746.3716, validation loss: 0.4957
2024-05-24 10:35:20 [INFO]: Epoch 131 - training loss: 22746.0779, validation loss: 0.4943
2024-05-24 10:35:21 [INFO]: Epoch 132 - training loss: 22745.3439, validation loss: 0.4964
2024-05-24 10:35:22 [INFO]: Epoch 133 - training loss: 22745.4445, validation loss: 0.4902
2024-05-24 10:35:22 [INFO]: Epoch 134 - training loss: 22745.3238, validation loss: 0.4941
2024-05-24 10:35:23 [INFO]: Epoch 135 - training loss: 22745.4592, validation loss: 0.4928
2024-05-24 10:35:23 [INFO]: Epoch 136 - training loss: 22746.0092, validation loss: 0.4959
2024-05-24 10:35:24 [INFO]: Epoch 137 - training loss: 22746.8677, validation loss: 0.4884
2024-05-24 10:35:25 [INFO]: Epoch 138 - training loss: 22746.8370, validation loss: 0.4966
2024-05-24 10:35:25 [INFO]: Epoch 139 - training loss: 22746.3254, validation loss: 0.4866
2024-05-24 10:35:26 [INFO]: Epoch 140 - training loss: 22745.6503, validation loss: 0.4985
2024-05-24 10:35:26 [INFO]: Epoch 141 - training loss: 22745.1605, validation loss: 0.4867
2024-05-24 10:35:27 [INFO]: Epoch 142 - training loss: 22745.1246, validation loss: 0.4864
2024-05-24 10:35:28 [INFO]: Epoch 143 - training loss: 22744.3657, validation loss: 0.4843
2024-05-24 10:35:28 [INFO]: Epoch 144 - training loss: 22744.3032, validation loss: 0.4896
2024-05-24 10:35:29 [INFO]: Epoch 145 - training loss: 22744.4385, validation loss: 0.4883
2024-05-24 10:35:29 [INFO]: Epoch 146 - training loss: 22744.3651, validation loss: 0.4846
2024-05-24 10:35:30 [INFO]: Epoch 147 - training loss: 22744.1717, validation loss: 0.4847
2024-05-24 10:35:31 [INFO]: Epoch 148 - training loss: 22744.3998, validation loss: 0.4839
2024-05-24 10:35:31 [INFO]: Epoch 149 - training loss: 22744.4713, validation loss: 0.4853
2024-05-24 10:35:32 [INFO]: Epoch 150 - training loss: 22744.1159, validation loss: 0.4867
2024-05-24 10:35:32 [INFO]: Epoch 151 - training loss: 22744.9533, validation loss: 0.4846
2024-05-24 10:35:33 [INFO]: Epoch 152 - training loss: 22744.5546, validation loss: 0.4888
2024-05-24 10:35:34 [INFO]: Epoch 153 - training loss: 22744.5704, validation loss: 0.4846
2024-05-24 10:35:34 [INFO]: Epoch 154 - training loss: 22744.6713, validation loss: 0.4834
2024-05-24 10:35:35 [INFO]: Epoch 155 - training loss: 22743.9843, validation loss: 0.4807
2024-05-24 10:35:35 [INFO]: Epoch 156 - training loss: 22743.7368, validation loss: 0.4811
2024-05-24 10:35:36 [INFO]: Epoch 157 - training loss: 22743.9305, validation loss: 0.4832
2024-05-24 10:35:37 [INFO]: Epoch 158 - training loss: 22743.9180, validation loss: 0.4814
2024-05-24 10:35:37 [INFO]: Epoch 159 - training loss: 22743.6052, validation loss: 0.4887
2024-05-24 10:35:38 [INFO]: Epoch 160 - training loss: 22745.0066, validation loss: 0.4797
2024-05-24 10:35:38 [INFO]: Epoch 161 - training loss: 22744.1646, validation loss: 0.4800
2024-05-24 10:35:39 [INFO]: Epoch 162 - training loss: 22743.7577, validation loss: 0.4796
2024-05-24 10:35:40 [INFO]: Epoch 163 - training loss: 22743.1702, validation loss: 0.4795
2024-05-24 10:35:40 [INFO]: Epoch 164 - training loss: 22743.0926, validation loss: 0.4834
2024-05-24 10:35:41 [INFO]: Epoch 165 - training loss: 22743.2136, validation loss: 0.4785
2024-05-24 10:35:41 [INFO]: Epoch 166 - training loss: 22742.9528, validation loss: 0.4801
2024-05-24 10:35:42 [INFO]: Epoch 167 - training loss: 22743.9928, validation loss: 0.4801
2024-05-24 10:35:43 [INFO]: Epoch 168 - training loss: 22745.0084, validation loss: 0.4778
2024-05-24 10:35:43 [INFO]: Epoch 169 - training loss: 22743.1715, validation loss: 0.4780
2024-05-24 10:35:44 [INFO]: Epoch 170 - training loss: 22742.5536, validation loss: 0.4764
2024-05-24 10:35:45 [INFO]: Epoch 171 - training loss: 22742.7328, validation loss: 0.4747
2024-05-24 10:35:45 [INFO]: Epoch 172 - training loss: 22742.4748, validation loss: 0.4768
2024-05-24 10:35:46 [INFO]: Epoch 173 - training loss: 22742.8164, validation loss: 0.4725
2024-05-24 10:35:46 [INFO]: Epoch 174 - training loss: 22742.7258, validation loss: 0.4823
2024-05-24 10:35:47 [INFO]: Epoch 175 - training loss: 22742.7165, validation loss: 0.4779
2024-05-24 10:35:48 [INFO]: Epoch 176 - training loss: 22743.5023, validation loss: 0.4732
2024-05-24 10:35:48 [INFO]: Epoch 177 - training loss: 22742.7906, validation loss: 0.4737
2024-05-24 10:35:49 [INFO]: Epoch 178 - training loss: 22742.3879, validation loss: 0.4767
2024-05-24 10:35:49 [INFO]: Epoch 179 - training loss: 22743.7295, validation loss: 0.4737
2024-05-24 10:35:50 [INFO]: Epoch 180 - training loss: 22742.9818, validation loss: 0.4790
2024-05-24 10:35:51 [INFO]: Epoch 181 - training loss: 22744.0282, validation loss: 0.4737
2024-05-24 10:35:51 [INFO]: Epoch 182 - training loss: 22742.6647, validation loss: 0.4711
2024-05-24 10:35:52 [INFO]: Epoch 183 - training loss: 22742.3704, validation loss: 0.4722
2024-05-24 10:35:52 [INFO]: Epoch 184 - training loss: 22742.0330, validation loss: 0.4692
2024-05-24 10:35:53 [INFO]: Epoch 185 - training loss: 22741.8286, validation loss: 0.4702
2024-05-24 10:35:54 [INFO]: Epoch 186 - training loss: 22741.6562, validation loss: 0.4701
2024-05-24 10:35:54 [INFO]: Epoch 187 - training loss: 22741.9282, validation loss: 0.4716
2024-05-24 10:35:55 [INFO]: Epoch 188 - training loss: 22741.9544, validation loss: 0.4707
2024-05-24 10:35:55 [INFO]: Epoch 189 - training loss: 22741.8700, validation loss: 0.4713
2024-05-24 10:35:56 [INFO]: Epoch 190 - training loss: 22742.3236, validation loss: 0.4689
2024-05-24 10:35:57 [INFO]: Epoch 191 - training loss: 22741.9597, validation loss: 0.4672
2024-05-24 10:35:57 [INFO]: Epoch 192 - training loss: 22741.3978, validation loss: 0.4683
2024-05-24 10:35:58 [INFO]: Epoch 193 - training loss: 22741.6457, validation loss: 0.4696
2024-05-24 10:35:58 [INFO]: Epoch 194 - training loss: 22741.5191, validation loss: 0.4698
2024-05-24 10:35:59 [INFO]: Epoch 195 - training loss: 22741.6622, validation loss: 0.4723
2024-05-24 10:36:00 [INFO]: Epoch 196 - training loss: 22742.1665, validation loss: 0.4690
2024-05-24 10:36:00 [INFO]: Epoch 197 - training loss: 22742.9070, validation loss: 0.4658
2024-05-24 10:36:01 [INFO]: Epoch 198 - training loss: 22742.6555, validation loss: 0.4716
2024-05-24 10:36:01 [INFO]: Epoch 199 - training loss: 22741.0601, validation loss: 0.4682
2024-05-24 10:36:02 [INFO]: Epoch 200 - training loss: 22741.2943, validation loss: 0.4665
2024-05-24 10:36:03 [INFO]: Epoch 201 - training loss: 22740.9028, validation loss: 0.4651
2024-05-24 10:36:03 [INFO]: Epoch 202 - training loss: 22741.0385, validation loss: 0.4648
2024-05-24 10:36:04 [INFO]: Epoch 203 - training loss: 22740.8900, validation loss: 0.4629
2024-05-24 10:36:04 [INFO]: Epoch 204 - training loss: 22742.1484, validation loss: 0.4641
2024-05-24 10:36:05 [INFO]: Epoch 205 - training loss: 22741.8677, validation loss: 0.4728
2024-05-24 10:36:06 [INFO]: Epoch 206 - training loss: 22741.9189, validation loss: 0.4725
2024-05-24 10:36:06 [INFO]: Epoch 207 - training loss: 22741.3896, validation loss: 0.4622
2024-05-24 10:36:07 [INFO]: Epoch 208 - training loss: 22741.0575, validation loss: 0.4651
2024-05-24 10:36:07 [INFO]: Epoch 209 - training loss: 22740.6925, validation loss: 0.4650
2024-05-24 10:36:08 [INFO]: Epoch 210 - training loss: 22740.5485, validation loss: 0.4687
2024-05-24 10:36:09 [INFO]: Epoch 211 - training loss: 22741.9284, validation loss: 0.4691
2024-05-24 10:36:09 [INFO]: Epoch 212 - training loss: 22740.9576, validation loss: 0.4650
2024-05-24 10:36:10 [INFO]: Epoch 213 - training loss: 22740.3941, validation loss: 0.4587
2024-05-24 10:36:11 [INFO]: Epoch 214 - training loss: 22740.1901, validation loss: 0.4623
2024-05-24 10:36:11 [INFO]: Epoch 215 - training loss: 22740.3246, validation loss: 0.4607
2024-05-24 10:36:12 [INFO]: Epoch 216 - training loss: 22739.8024, validation loss: 0.4586
2024-05-24 10:36:12 [INFO]: Epoch 217 - training loss: 22739.6848, validation loss: 0.4614
2024-05-24 10:36:13 [INFO]: Epoch 218 - training loss: 22740.0978, validation loss: 0.4580
2024-05-24 10:36:14 [INFO]: Epoch 219 - training loss: 22739.6063, validation loss: 0.4582
2024-05-24 10:36:14 [INFO]: Epoch 220 - training loss: 22739.8345, validation loss: 0.4617
2024-05-24 10:36:15 [INFO]: Epoch 221 - training loss: 22740.3061, validation loss: 0.4574
2024-05-24 10:36:15 [INFO]: Epoch 222 - training loss: 22740.3028, validation loss: 0.4580
2024-05-24 10:36:16 [INFO]: Epoch 223 - training loss: 22739.7404, validation loss: 0.4605
2024-05-24 10:36:17 [INFO]: Epoch 224 - training loss: 22739.4768, validation loss: 0.4664
2024-05-24 10:36:17 [INFO]: Epoch 225 - training loss: 22739.6251, validation loss: 0.4590
2024-05-24 10:36:18 [INFO]: Epoch 226 - training loss: 22738.7748, validation loss: 0.4609
2024-05-24 10:36:18 [INFO]: Epoch 227 - training loss: 22739.0294, validation loss: 0.4608
2024-05-24 10:36:19 [INFO]: Epoch 228 - training loss: 22739.1050, validation loss: 0.4596
2024-05-24 10:36:20 [INFO]: Epoch 229 - training loss: 22739.9105, validation loss: 0.4568
2024-05-24 10:36:20 [INFO]: Epoch 230 - training loss: 22739.4550, validation loss: 0.4652
2024-05-24 10:36:21 [INFO]: Epoch 231 - training loss: 22739.1934, validation loss: 0.4561
2024-05-24 10:36:21 [INFO]: Epoch 232 - training loss: 22738.8741, validation loss: 0.4618
2024-05-24 10:36:22 [INFO]: Epoch 233 - training loss: 22737.9548, validation loss: 0.4596
2024-05-24 10:36:23 [INFO]: Epoch 234 - training loss: 22738.2375, validation loss: 0.4608
2024-05-24 10:36:23 [INFO]: Epoch 235 - training loss: 22738.5351, validation loss: 0.4593
2024-05-24 10:36:24 [INFO]: Epoch 236 - training loss: 22739.7511, validation loss: 0.4557
2024-05-24 10:36:24 [INFO]: Epoch 237 - training loss: 22738.9927, validation loss: 0.4557
2024-05-24 10:36:25 [INFO]: Epoch 238 - training loss: 22738.6684, validation loss: 0.4569
2024-05-24 10:36:26 [INFO]: Epoch 239 - training loss: 22738.3913, validation loss: 0.4577
2024-05-24 10:36:26 [INFO]: Epoch 240 - training loss: 22738.1907, validation loss: 0.4573
2024-05-24 10:36:27 [INFO]: Epoch 241 - training loss: 22737.7439, validation loss: 0.4618
2024-05-24 10:36:27 [INFO]: Epoch 242 - training loss: 22737.7584, validation loss: 0.4599
2024-05-24 10:36:28 [INFO]: Epoch 243 - training loss: 22738.4866, validation loss: 0.4582
2024-05-24 10:36:29 [INFO]: Epoch 244 - training loss: 22738.2567, validation loss: 0.4596
2024-05-24 10:36:29 [INFO]: Epoch 245 - training loss: 22737.9695, validation loss: 0.4605
2024-05-24 10:36:30 [INFO]: Epoch 246 - training loss: 22737.9703, validation loss: 0.4675
2024-05-24 10:36:30 [INFO]: Epoch 247 - training loss: 22737.6781, validation loss: 0.4654
2024-05-24 10:36:30 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-05-24 10:36:30 [INFO]: Finished training. The best model is from epoch#237.
2024-05-24 10:36:30 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_4/GPVAE_physionet_2012_seta/20240524_T103401/GPVAE.pypots
2024-05-24 10:36:31 [INFO]: GP-VAE on PhysioNet-2012-seta: MAE=0.3951, MSE=0.3950
2024-05-24 10:36:31 [INFO]: Successfully saved to augmentation_premask_post_norm_saved_results/round_4/GPVAE_physionet_2012_seta/imputation.pkl
2024-05-24 10:36:31 [INFO]: Using the given device: cuda:0
2024-05-24 10:36:31 [INFO]: Model files will be saved to augmentation_premask_post_norm_saved_results/round_4/USGAN_physionet_2012_seta/20240524_T103631
2024-05-24 10:36:31 [INFO]: Tensorboard file will be saved to augmentation_premask_post_norm_saved_results/round_4/USGAN_physionet_2012_seta/20240524_T103631/tensorboard
2024-05-24 10:36:31 [INFO]: USGAN initialized with the given hyperparameters, the number of trainable parameters: 16,010,261
2024-05-24 10:36:53 [INFO]: Epoch 001 - generator training loss: 0.5965, discriminator training loss: 0.3839, validation loss: 0.6385
2024-05-24 10:37:11 [INFO]: Epoch 002 - generator training loss: 0.4830, discriminator training loss: 0.2720, validation loss: 0.5513
2024-05-24 10:37:30 [INFO]: Epoch 003 - generator training loss: 0.4425, discriminator training loss: 0.2353, validation loss: 0.5310
2024-05-24 10:37:49 [INFO]: Epoch 004 - generator training loss: 0.4518, discriminator training loss: 0.1882, validation loss: 0.5187
2024-05-24 10:38:08 [INFO]: Epoch 005 - generator training loss: 0.4480, discriminator training loss: 0.1584, validation loss: 0.5121
2024-05-24 10:38:26 [INFO]: Epoch 006 - generator training loss: 0.4396, discriminator training loss: 0.1390, validation loss: 0.5003
2024-05-24 10:38:45 [INFO]: Epoch 007 - generator training loss: 0.4260, discriminator training loss: 0.1255, validation loss: 0.4875
2024-05-24 10:39:04 [INFO]: Epoch 008 - generator training loss: 0.4146, discriminator training loss: 0.1149, validation loss: 0.4755
2024-05-24 10:39:23 [INFO]: Epoch 009 - generator training loss: 0.4032, discriminator training loss: 0.1064, validation loss: 0.4622
2024-05-24 10:39:41 [INFO]: Epoch 010 - generator training loss: 0.3943, discriminator training loss: 0.0995, validation loss: 0.4548
2024-05-24 10:40:00 [INFO]: Epoch 011 - generator training loss: 0.3868, discriminator training loss: 0.0935, validation loss: 0.4466
2024-05-24 10:40:19 [INFO]: Epoch 012 - generator training loss: 0.3830, discriminator training loss: 0.0883, validation loss: 0.4424
2024-05-24 10:40:38 [INFO]: Epoch 013 - generator training loss: 0.3763, discriminator training loss: 0.0838, validation loss: 0.4390
2024-05-24 10:40:56 [INFO]: Epoch 014 - generator training loss: 0.3729, discriminator training loss: 0.0798, validation loss: 0.4308
2024-05-24 10:41:15 [INFO]: Epoch 015 - generator training loss: 0.3682, discriminator training loss: 0.0765, validation loss: 0.4264
2024-05-24 10:41:34 [INFO]: Epoch 016 - generator training loss: 0.3639, discriminator training loss: 0.0733, validation loss: 0.4223
2024-05-24 10:41:52 [INFO]: Epoch 017 - generator training loss: 0.3602, discriminator training loss: 0.0704, validation loss: 0.4225
2024-05-24 10:42:11 [INFO]: Epoch 018 - generator training loss: 0.3565, discriminator training loss: 0.0679, validation loss: 0.4130
2024-05-24 10:42:30 [INFO]: Epoch 019 - generator training loss: 0.3495, discriminator training loss: 0.0656, validation loss: 0.4067
2024-05-24 10:42:49 [INFO]: Epoch 020 - generator training loss: 0.3444, discriminator training loss: 0.0636, validation loss: 0.4073
2024-05-24 10:43:08 [INFO]: Epoch 021 - generator training loss: 0.3411, discriminator training loss: 0.0615, validation loss: 0.4018
2024-05-24 10:43:26 [INFO]: Epoch 022 - generator training loss: 0.3356, discriminator training loss: 0.0597, validation loss: 0.3968
2024-05-24 10:43:45 [INFO]: Epoch 023 - generator training loss: 0.3323, discriminator training loss: 0.0583, validation loss: 0.3918
2024-05-24 10:44:04 [INFO]: Epoch 024 - generator training loss: 0.3286, discriminator training loss: 0.0568, validation loss: 0.3941
2024-05-24 10:44:22 [INFO]: Epoch 025 - generator training loss: 0.3248, discriminator training loss: 0.0555, validation loss: 0.3863
2024-05-24 10:44:41 [INFO]: Epoch 026 - generator training loss: 0.3223, discriminator training loss: 0.0544, validation loss: 0.3865
2024-05-24 10:45:00 [INFO]: Epoch 027 - generator training loss: 0.3175, discriminator training loss: 0.0533, validation loss: 0.3831
2024-05-24 10:45:19 [INFO]: Epoch 028 - generator training loss: 0.3124, discriminator training loss: 0.0525, validation loss: 0.3771
2024-05-24 10:45:37 [INFO]: Epoch 029 - generator training loss: 0.3103, discriminator training loss: 0.0517, validation loss: 0.3742
2024-05-24 10:45:56 [INFO]: Epoch 030 - generator training loss: 0.3039, discriminator training loss: 0.0508, validation loss: 0.3725
2024-05-24 10:46:14 [INFO]: Epoch 031 - generator training loss: 0.2981, discriminator training loss: 0.0502, validation loss: 0.3681
2024-05-24 10:46:33 [INFO]: Epoch 032 - generator training loss: 0.2937, discriminator training loss: 0.0495, validation loss: 0.3629
2024-05-24 10:46:52 [INFO]: Epoch 033 - generator training loss: 0.2902, discriminator training loss: 0.0491, validation loss: 0.3602
2024-05-24 10:47:11 [INFO]: Epoch 034 - generator training loss: 0.2898, discriminator training loss: 0.0486, validation loss: 0.3586
2024-05-24 10:47:29 [INFO]: Epoch 035 - generator training loss: 0.2868, discriminator training loss: 0.0479, validation loss: 0.3558
2024-05-24 10:47:48 [INFO]: Epoch 036 - generator training loss: 0.2806, discriminator training loss: 0.0475, validation loss: 0.3628
2024-05-24 10:48:07 [INFO]: Epoch 037 - generator training loss: 0.2741, discriminator training loss: 0.0471, validation loss: 0.3523
2024-05-24 10:48:25 [INFO]: Epoch 038 - generator training loss: 0.2712, discriminator training loss: 0.0467, validation loss: 0.3529
2024-05-24 10:48:44 [INFO]: Epoch 039 - generator training loss: 0.2739, discriminator training loss: 0.0464, validation loss: 0.3540
2024-05-24 10:49:03 [INFO]: Epoch 040 - generator training loss: 0.2707, discriminator training loss: 0.0460, validation loss: 0.3552
2024-05-24 10:49:22 [INFO]: Epoch 041 - generator training loss: 0.2936, discriminator training loss: 0.0460, validation loss: 0.3543
2024-05-24 10:49:40 [INFO]: Epoch 042 - generator training loss: 0.2783, discriminator training loss: 0.0456, validation loss: 0.3469
2024-05-24 10:49:59 [INFO]: Epoch 043 - generator training loss: 0.2692, discriminator training loss: 0.0451, validation loss: 0.3547
2024-05-24 10:50:17 [INFO]: Epoch 044 - generator training loss: 0.2639, discriminator training loss: 0.0449, validation loss: 0.3462
2024-05-24 10:50:36 [INFO]: Epoch 045 - generator training loss: 0.2575, discriminator training loss: 0.0447, validation loss: 0.3427
2024-05-24 10:50:55 [INFO]: Epoch 046 - generator training loss: 0.2602, discriminator training loss: 0.0444, validation loss: 0.3457
2024-05-24 10:51:14 [INFO]: Epoch 047 - generator training loss: 0.2582, discriminator training loss: 0.0441, validation loss: 0.3443
2024-05-24 10:51:32 [INFO]: Epoch 048 - generator training loss: 0.2537, discriminator training loss: 0.0438, validation loss: 0.3438
2024-05-24 10:51:51 [INFO]: Epoch 049 - generator training loss: 0.2520, discriminator training loss: 0.0436, validation loss: 0.3410
2024-05-24 10:52:10 [INFO]: Epoch 050 - generator training loss: 0.2476, discriminator training loss: 0.0435, validation loss: 0.3414
2024-05-24 10:52:28 [INFO]: Epoch 051 - generator training loss: 0.2439, discriminator training loss: 0.0434, validation loss: 0.3382
2024-05-24 10:52:47 [INFO]: Epoch 052 - generator training loss: 0.2403, discriminator training loss: 0.0433, validation loss: 0.3394
2024-05-24 10:53:06 [INFO]: Epoch 053 - generator training loss: 0.2386, discriminator training loss: 0.0431, validation loss: 0.3366
2024-05-24 10:53:25 [INFO]: Epoch 054 - generator training loss: 0.2370, discriminator training loss: 0.0426, validation loss: 0.3382
2024-05-24 10:53:43 [INFO]: Epoch 055 - generator training loss: 0.2390, discriminator training loss: 0.0427, validation loss: 0.3414
2024-05-24 10:54:02 [INFO]: Epoch 056 - generator training loss: 0.2351, discriminator training loss: 0.0426, validation loss: 0.3432
2024-05-24 10:54:21 [INFO]: Epoch 057 - generator training loss: 0.2362, discriminator training loss: 0.0423, validation loss: 0.3405
2024-05-24 10:54:39 [INFO]: Epoch 058 - generator training loss: 0.2377, discriminator training loss: 0.0422, validation loss: 0.3382
2024-05-24 10:54:58 [INFO]: Epoch 059 - generator training loss: 0.2319, discriminator training loss: 0.0421, validation loss: 0.3398
2024-05-24 10:55:17 [INFO]: Epoch 060 - generator training loss: 0.2310, discriminator training loss: 0.0418, validation loss: 0.3364
2024-05-24 10:55:36 [INFO]: Epoch 061 - generator training loss: 0.2271, discriminator training loss: 0.0418, validation loss: 0.3377
2024-05-24 10:55:54 [INFO]: Epoch 062 - generator training loss: 0.2280, discriminator training loss: 0.0416, validation loss: 0.3377
2024-05-24 10:56:13 [INFO]: Epoch 063 - generator training loss: 0.2251, discriminator training loss: 0.0416, validation loss: 0.3368
2024-05-24 10:56:32 [INFO]: Epoch 064 - generator training loss: 0.2220, discriminator training loss: 0.0414, validation loss: 0.3373
2024-05-24 10:56:51 [INFO]: Epoch 065 - generator training loss: 0.2239, discriminator training loss: 0.0413, validation loss: 0.3403
2024-05-24 10:57:09 [INFO]: Epoch 066 - generator training loss: 0.2284, discriminator training loss: 0.0412, validation loss: 0.3391
2024-05-24 10:57:28 [INFO]: Epoch 067 - generator training loss: 0.2245, discriminator training loss: 0.0412, validation loss: 0.3369
2024-05-24 10:57:47 [INFO]: Epoch 068 - generator training loss: 0.2177, discriminator training loss: 0.0411, validation loss: 0.3347
2024-05-24 10:58:06 [INFO]: Epoch 069 - generator training loss: 0.2170, discriminator training loss: 0.0411, validation loss: 0.3369
2024-05-24 10:58:24 [INFO]: Epoch 070 - generator training loss: 0.2139, discriminator training loss: 0.0411, validation loss: 0.3379
2024-05-24 10:58:43 [INFO]: Epoch 071 - generator training loss: 0.2114, discriminator training loss: 0.0407, validation loss: 0.3362
2024-05-24 10:59:02 [INFO]: Epoch 072 - generator training loss: 0.2076, discriminator training loss: 0.0409, validation loss: 0.3355
2024-05-24 10:59:20 [INFO]: Epoch 073 - generator training loss: 0.2079, discriminator training loss: 0.0409, validation loss: 0.3386
2024-05-24 10:59:39 [INFO]: Epoch 074 - generator training loss: 0.2076, discriminator training loss: 0.0408, validation loss: 0.3392
2024-05-24 10:59:58 [INFO]: Epoch 075 - generator training loss: 0.2093, discriminator training loss: 0.0405, validation loss: 0.3351
2024-05-24 11:00:17 [INFO]: Epoch 076 - generator training loss: 0.2054, discriminator training loss: 0.0406, validation loss: 0.3363
2024-05-24 11:00:35 [INFO]: Epoch 077 - generator training loss: 0.2002, discriminator training loss: 0.0406, validation loss: 0.3404
2024-05-24 11:00:54 [INFO]: Epoch 078 - generator training loss: 0.1987, discriminator training loss: 0.0406, validation loss: 0.3346
2024-05-24 11:01:13 [INFO]: Epoch 079 - generator training loss: 0.1954, discriminator training loss: 0.0407, validation loss: 0.3375
2024-05-24 11:01:32 [INFO]: Epoch 080 - generator training loss: 0.1944, discriminator training loss: 0.0404, validation loss: 0.3376
2024-05-24 11:01:50 [INFO]: Epoch 081 - generator training loss: 0.1973, discriminator training loss: 0.0402, validation loss: 0.3437
2024-05-24 11:02:09 [INFO]: Epoch 082 - generator training loss: 0.2037, discriminator training loss: 0.0402, validation loss: 0.3398
2024-05-24 11:02:28 [INFO]: Epoch 083 - generator training loss: 0.2143, discriminator training loss: 0.0402, validation loss: 0.3363
2024-05-24 11:02:46 [INFO]: Epoch 084 - generator training loss: 0.1982, discriminator training loss: 0.0400, validation loss: 0.3378
2024-05-24 11:03:05 [INFO]: Epoch 085 - generator training loss: 0.1982, discriminator training loss: 0.0398, validation loss: 0.3396
2024-05-24 11:03:24 [INFO]: Epoch 086 - generator training loss: 0.1895, discriminator training loss: 0.0399, validation loss: 0.3360
2024-05-24 11:03:43 [INFO]: Epoch 087 - generator training loss: 0.1866, discriminator training loss: 0.0396, validation loss: 0.3364
2024-05-24 11:04:01 [INFO]: Epoch 088 - generator training loss: 0.1837, discriminator training loss: 0.0397, validation loss: 0.3387
2024-05-24 11:04:01 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-05-24 11:04:01 [INFO]: Finished training. The best model is from epoch#78.
2024-05-24 11:04:01 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_4/USGAN_physionet_2012_seta/20240524_T103631/USGAN.pypots
2024-05-24 11:04:04 [INFO]: US-GAN on PhysioNet-2012-seta: MAE=0.2908, MSE=0.2610
2024-05-24 11:04:14 [INFO]: Successfully saved to augmentation_premask_post_norm_saved_results/round_4/USGAN_physionet_2012_seta/imputation.pkl
2024-05-24 11:04:14 [INFO]: Using the given device: cuda:0
2024-05-24 11:04:14 [INFO]: Model files will be saved to augmentation_premask_post_norm_saved_results/round_4/BRITS_physionet_2012_seta/20240524_T110414
2024-05-24 11:04:14 [INFO]: Tensorboard file will be saved to augmentation_premask_post_norm_saved_results/round_4/BRITS_physionet_2012_seta/20240524_T110414/tensorboard
2024-05-24 11:04:14 [INFO]: BRITS initialized with the given hyperparameters, the number of trainable parameters: 9,176,048
2024-05-24 11:04:29 [INFO]: Epoch 001 - training loss: 1.1342, validation loss: 0.5482
2024-05-24 11:04:41 [INFO]: Epoch 002 - training loss: 0.9239, validation loss: 0.4971
2024-05-24 11:04:53 [INFO]: Epoch 003 - training loss: 0.8657, validation loss: 0.4704
2024-05-24 11:05:06 [INFO]: Epoch 004 - training loss: 0.8310, validation loss: 0.4479
2024-05-24 11:05:18 [INFO]: Epoch 005 - training loss: 0.8022, validation loss: 0.4303
2024-05-24 11:05:30 [INFO]: Epoch 006 - training loss: 0.7811, validation loss: 0.4171
2024-05-24 11:05:42 [INFO]: Epoch 007 - training loss: 0.7605, validation loss: 0.4065
2024-05-24 11:05:55 [INFO]: Epoch 008 - training loss: 0.7448, validation loss: 0.4013
2024-05-24 11:06:07 [INFO]: Epoch 009 - training loss: 0.7318, validation loss: 0.3926
2024-05-24 11:06:19 [INFO]: Epoch 010 - training loss: 0.7210, validation loss: 0.3925
2024-05-24 11:06:32 [INFO]: Epoch 011 - training loss: 0.7097, validation loss: 0.3888
2024-05-24 11:06:44 [INFO]: Epoch 012 - training loss: 0.7025, validation loss: 0.3841
2024-05-24 11:06:56 [INFO]: Epoch 013 - training loss: 0.6958, validation loss: 0.3824
2024-05-24 11:07:08 [INFO]: Epoch 014 - training loss: 0.6884, validation loss: 0.3835
2024-05-24 11:07:21 [INFO]: Epoch 015 - training loss: 0.6818, validation loss: 0.3797
2024-05-24 11:07:33 [INFO]: Epoch 016 - training loss: 0.6767, validation loss: 0.3791
2024-05-24 11:07:45 [INFO]: Epoch 017 - training loss: 0.6729, validation loss: 0.3769
2024-05-24 11:07:57 [INFO]: Epoch 018 - training loss: 0.6678, validation loss: 0.3761
2024-05-24 11:08:10 [INFO]: Epoch 019 - training loss: 0.6642, validation loss: 0.3761
2024-05-24 11:08:22 [INFO]: Epoch 020 - training loss: 0.6599, validation loss: 0.3753
2024-05-24 11:08:34 [INFO]: Epoch 021 - training loss: 0.6567, validation loss: 0.3734
2024-05-24 11:08:46 [INFO]: Epoch 022 - training loss: 0.6538, validation loss: 0.3741
2024-05-24 11:08:59 [INFO]: Epoch 023 - training loss: 0.6494, validation loss: 0.3718
2024-05-24 11:09:11 [INFO]: Epoch 024 - training loss: 0.6474, validation loss: 0.3719
2024-05-24 11:09:23 [INFO]: Epoch 025 - training loss: 0.6442, validation loss: 0.3728
2024-05-24 11:09:35 [INFO]: Epoch 026 - training loss: 0.6410, validation loss: 0.3717
2024-05-24 11:09:48 [INFO]: Epoch 027 - training loss: 0.6379, validation loss: 0.3722
2024-05-24 11:10:00 [INFO]: Epoch 028 - training loss: 0.6353, validation loss: 0.3711
2024-05-24 11:10:12 [INFO]: Epoch 029 - training loss: 0.6355, validation loss: 0.3716
2024-05-24 11:10:24 [INFO]: Epoch 030 - training loss: 0.6314, validation loss: 0.3701
2024-05-24 11:10:37 [INFO]: Epoch 031 - training loss: 0.6279, validation loss: 0.3704
2024-05-24 11:10:49 [INFO]: Epoch 032 - training loss: 0.6288, validation loss: 0.3712
2024-05-24 11:11:01 [INFO]: Epoch 033 - training loss: 0.6256, validation loss: 0.3715
2024-05-24 11:11:13 [INFO]: Epoch 034 - training loss: 0.6205, validation loss: 0.3722
2024-05-24 11:11:26 [INFO]: Epoch 035 - training loss: 0.6209, validation loss: 0.3709
2024-05-24 11:11:38 [INFO]: Epoch 036 - training loss: 0.6157, validation loss: 0.3708
2024-05-24 11:11:50 [INFO]: Epoch 037 - training loss: 0.6156, validation loss: 0.3695
2024-05-24 11:12:02 [INFO]: Epoch 038 - training loss: 0.6095, validation loss: 0.3716
2024-05-24 11:12:15 [INFO]: Epoch 039 - training loss: 0.6065, validation loss: 0.3704
2024-05-24 11:12:27 [INFO]: Epoch 040 - training loss: 0.6034, validation loss: 0.3698
2024-05-24 11:12:39 [INFO]: Epoch 041 - training loss: 0.6009, validation loss: 0.3711
2024-05-24 11:12:51 [INFO]: Epoch 042 - training loss: 0.5983, validation loss: 0.3728
2024-05-24 11:13:04 [INFO]: Epoch 043 - training loss: 0.5977, validation loss: 0.3740
2024-05-24 11:13:16 [INFO]: Epoch 044 - training loss: 0.5950, validation loss: 0.3731
2024-05-24 11:13:28 [INFO]: Epoch 045 - training loss: 0.5991, validation loss: 0.3745
2024-05-24 11:13:41 [INFO]: Epoch 046 - training loss: 0.5932, validation loss: 0.3726
2024-05-24 11:13:53 [INFO]: Epoch 047 - training loss: 0.5869, validation loss: 0.3742
2024-05-24 11:13:53 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-05-24 11:13:53 [INFO]: Finished training. The best model is from epoch#37.
2024-05-24 11:13:53 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_4/BRITS_physionet_2012_seta/20240524_T110414/BRITS.pypots
2024-05-24 11:13:55 [INFO]: BRITS on PhysioNet-2012-seta: MAE=0.2561, MSE=0.2585
2024-05-24 11:14:05 [INFO]: Successfully saved to augmentation_premask_post_norm_saved_results/round_4/BRITS_physionet_2012_seta/imputation.pkl
2024-05-24 11:14:05 [INFO]: Using the given device: cuda:0
2024-05-24 11:14:05 [INFO]: Model files will be saved to augmentation_premask_post_norm_saved_results/round_4/MRNN_physionet_2012_seta/20240524_T111405
2024-05-24 11:14:05 [INFO]: Tensorboard file will be saved to augmentation_premask_post_norm_saved_results/round_4/MRNN_physionet_2012_seta/20240524_T111405/tensorboard
2024-05-24 11:14:05 [INFO]: MRNN initialized with the given hyperparameters, the number of trainable parameters: 7,599
2024-05-24 11:14:11 [INFO]: Epoch 001 - training loss: 1.1739, validation loss: 1.0039
2024-05-24 11:14:11 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_4/MRNN_physionet_2012_seta/20240524_T111405/MRNN_epoch1_loss1.00386683344841.pypots
2024-05-24 11:14:14 [INFO]: Epoch 002 - training loss: 0.6952, validation loss: 0.9812
2024-05-24 11:14:14 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_4/MRNN_physionet_2012_seta/20240524_T111405/MRNN_epoch2_loss0.981220680475235.pypots
2024-05-24 11:14:17 [INFO]: Epoch 003 - training loss: 0.5936, validation loss: 0.9534
2024-05-24 11:14:17 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_4/MRNN_physionet_2012_seta/20240524_T111405/MRNN_epoch3_loss0.953416058421135.pypots
2024-05-24 11:14:19 [INFO]: Epoch 004 - training loss: 0.5562, validation loss: 0.9393
2024-05-24 11:14:19 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_4/MRNN_physionet_2012_seta/20240524_T111405/MRNN_epoch4_loss0.9392638266086578.pypots
2024-05-24 11:14:22 [INFO]: Epoch 005 - training loss: 0.5331, validation loss: 0.9315
2024-05-24 11:14:22 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_4/MRNN_physionet_2012_seta/20240524_T111405/MRNN_epoch5_loss0.9314788073301316.pypots
2024-05-24 11:14:25 [INFO]: Epoch 006 - training loss: 0.5112, validation loss: 0.9272
2024-05-24 11:14:25 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_4/MRNN_physionet_2012_seta/20240524_T111405/MRNN_epoch6_loss0.9272175699472427.pypots
2024-05-24 11:14:28 [INFO]: Epoch 007 - training loss: 0.4992, validation loss: 0.9248
2024-05-24 11:14:28 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_4/MRNN_physionet_2012_seta/20240524_T111405/MRNN_epoch7_loss0.9247946172952652.pypots
2024-05-24 11:14:31 [INFO]: Epoch 008 - training loss: 0.4882, validation loss: 0.9229
2024-05-24 11:14:31 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_4/MRNN_physionet_2012_seta/20240524_T111405/MRNN_epoch8_loss0.9229019492864609.pypots
2024-05-24 11:14:34 [INFO]: Epoch 009 - training loss: 0.4861, validation loss: 0.9239
2024-05-24 11:14:34 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_4/MRNN_physionet_2012_seta/20240524_T111405/MRNN_epoch9_loss0.9238787055015564.pypots
2024-05-24 11:14:36 [INFO]: Epoch 010 - training loss: 0.4763, validation loss: 0.9233
2024-05-24 11:14:36 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_4/MRNN_physionet_2012_seta/20240524_T111405/MRNN_epoch10_loss0.9233087092638016.pypots
2024-05-24 11:14:39 [INFO]: Epoch 011 - training loss: 0.4709, validation loss: 0.9242
2024-05-24 11:14:39 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_4/MRNN_physionet_2012_seta/20240524_T111405/MRNN_epoch11_loss0.9242238909006119.pypots
2024-05-24 11:14:42 [INFO]: Epoch 012 - training loss: 0.4656, validation loss: 0.9244
2024-05-24 11:14:42 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_4/MRNN_physionet_2012_seta/20240524_T111405/MRNN_epoch12_loss0.9244045048952103.pypots
2024-05-24 11:14:45 [INFO]: Epoch 013 - training loss: 0.4639, validation loss: 0.9265
2024-05-24 11:14:45 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_4/MRNN_physionet_2012_seta/20240524_T111405/MRNN_epoch13_loss0.9265127718448639.pypots
2024-05-24 11:14:48 [INFO]: Epoch 014 - training loss: 0.4555, validation loss: 0.9273
2024-05-24 11:14:48 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_4/MRNN_physionet_2012_seta/20240524_T111405/MRNN_epoch14_loss0.9272747367620469.pypots
2024-05-24 11:14:50 [INFO]: Epoch 015 - training loss: 0.4486, validation loss: 0.9295
2024-05-24 11:14:50 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_4/MRNN_physionet_2012_seta/20240524_T111405/MRNN_epoch15_loss0.9294710189104081.pypots
2024-05-24 11:14:53 [INFO]: Epoch 016 - training loss: 0.4382, validation loss: 0.9311
2024-05-24 11:14:53 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_4/MRNN_physionet_2012_seta/20240524_T111405/MRNN_epoch16_loss0.9311180144548417.pypots
2024-05-24 11:14:56 [INFO]: Epoch 017 - training loss: 0.4476, validation loss: 0.9324
2024-05-24 11:14:56 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_4/MRNN_physionet_2012_seta/20240524_T111405/MRNN_epoch17_loss0.9323848247528076.pypots
2024-05-24 11:14:59 [INFO]: Epoch 018 - training loss: 0.4588, validation loss: 0.9361
2024-05-24 11:14:59 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_4/MRNN_physionet_2012_seta/20240524_T111405/MRNN_epoch18_loss0.9361374020576477.pypots
2024-05-24 11:14:59 [INFO]: Exceeded the training patience. Terminating the training procedure...
2024-05-24 11:14:59 [INFO]: Finished training. The best model is from epoch#8.
2024-05-24 11:14:59 [INFO]: Saved the model to augmentation_premask_post_norm_saved_results/round_4/MRNN_physionet_2012_seta/20240524_T111405/MRNN.pypots
2024-05-24 11:15:00 [INFO]: MRNN on PhysioNet-2012-seta: MAE=0.6910, MSE=0.9009
2024-05-24 11:15:04 [INFO]: Successfully saved to augmentation_premask_post_norm_saved_results/round_4/MRNN_physionet_2012_seta/imputation.pkl
2024-05-24 11:15:04 [INFO]: Using the given device: cpu
2024-05-24 11:15:04 [INFO]: LOCF on PhysioNet-2012-seta: MAE=0.4038, MSE=0.5072
2024-05-24 11:15:04 [INFO]: Successfully created the given path "saved_results/round_4/LOCF_physionet_2012_seta".
2024-05-24 11:15:04 [INFO]: Successfully saved to augmentation_premask_post_norm_saved_results/round_4/LOCF_physionet_2012_seta/imputation.pkl
2024-05-24 11:15:04 [INFO]: Median on PhysioNet-2012-seta: MAE=0.6908, MSE=1.0216
2024-05-24 11:15:04 [INFO]: Successfully created the given path "saved_results/round_4/Median_physionet_2012_seta".
2024-05-24 11:15:04 [INFO]: Successfully saved to augmentation_premask_post_norm_saved_results/round_4/Median_physionet_2012_seta/imputation.pkl
2024-05-24 11:15:04 [INFO]: Mean on PhysioNet-2012-seta: MAE=0.7063, MSE=0.9786
2024-05-24 11:15:04 [INFO]: Successfully created the given path "saved_results/round_4/Mean_physionet_2012_seta".
2024-05-24 11:15:05 [INFO]: Successfully saved to augmentation_premask_post_norm_saved_results/round_4/Mean_physionet_2012_seta/imputation.pkl
2024-05-24 11:15:05 [INFO]: 
SAITS on data/physionet_2012_seta: MAE=0.267±0.00677239168663133, MSE=0.290±0.0014125298392904013
Transformer on data/physionet_2012_seta: MAE=0.283±0.0017582902715019679, MSE=0.303±0.0029765399755313725
TimesNet on data/physionet_2012_seta: MAE=0.290±0.002241745783531926, MSE=0.279±0.004936482047324267
CSDI on data/physionet_2012_seta: MAE=0.241±0.01659623201178895, MSE=0.430±0.1512086026166457
GPVAE on data/physionet_2012_seta: MAE=0.396±0.0012441167032757534, MSE=0.401±0.004058801798087908
USGAN on data/physionet_2012_seta: MAE=0.293±0.0024838246239185027, MSE=0.261±0.0028398684673864925
BRITS on data/physionet_2012_seta: MAE=0.257±0.0012355022750020067, MSE=0.258±0.0014658889508933984
MRNN on data/physionet_2012_seta: MAE=0.690±0.0012571664959933188, MSE=0.901±0.0011166943103799365
LOCF on data/physionet_2012_seta: MAE=0.404±0.0, MSE=0.507±0.0
Median on data/physionet_2012_seta: MAE=0.691±0.0, MSE=1.022±0.0
Mean on data/physionet_2012_seta: MAE=0.706±0.0, MSE=0.979±1.1102230246251565e-16